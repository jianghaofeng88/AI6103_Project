{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7be1314e5b6049eb80959c5580e1f8f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_761d78cfb0984b17911246f7ac5b14fb",
              "IPY_MODEL_92cd9afac92747998d0819e71168e7d3",
              "IPY_MODEL_e1d70bdabd674e57b244c0b346e39750"
            ],
            "layout": "IPY_MODEL_72c092bd879c48078e859b419795bdc0"
          }
        },
        "761d78cfb0984b17911246f7ac5b14fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f487c683a6ec48deb7bfa2a45d4457aa",
            "placeholder": "​",
            "style": "IPY_MODEL_26507aba632b44cc99332441fbcf97c4",
            "value": "100%"
          }
        },
        "92cd9afac92747998d0819e71168e7d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22dd70f6678944a8b416f12a2aacedf8",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_169220c8404844df847c90107e9fab25",
            "value": 182040794
          }
        },
        "e1d70bdabd674e57b244c0b346e39750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0777a3447244f199a514a699f3ea616",
            "placeholder": "​",
            "style": "IPY_MODEL_a6c174326fb44ad4b1bd49806400c9af",
            "value": " 182040794/182040794 [00:06&lt;00:00, 38778432.78it/s]"
          }
        },
        "72c092bd879c48078e859b419795bdc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f487c683a6ec48deb7bfa2a45d4457aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26507aba632b44cc99332441fbcf97c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22dd70f6678944a8b416f12a2aacedf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "169220c8404844df847c90107e9fab25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0777a3447244f199a514a699f3ea616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6c174326fb44ad4b1bd49806400c9af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a457414c0401460ca6fdbcf4d1dd29af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44489617705547408cba6ef9f5891782",
              "IPY_MODEL_ea70d7acec634550b638c19e5b7a740d",
              "IPY_MODEL_f1187874789b49198ffd3804a585a234"
            ],
            "layout": "IPY_MODEL_33ebdccc46174e05b86eae092d6bfe17"
          }
        },
        "44489617705547408cba6ef9f5891782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf37a00758134d99b337e7e3792d134a",
            "placeholder": "​",
            "style": "IPY_MODEL_88276cb3159a4294a1fe3754bb52ecbd",
            "value": "100%"
          }
        },
        "ea70d7acec634550b638c19e5b7a740d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8f4b3df58b8467fa7084a04201284ab",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccd68a1e288a46d392c5a55b19c74bfd",
            "value": 64275384
          }
        },
        "f1187874789b49198ffd3804a585a234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_269e1f40a20d49d385d9b6b797839666",
            "placeholder": "​",
            "style": "IPY_MODEL_af8fe1ff5e324bab8a1bc7cf7b08c19b",
            "value": " 64275384/64275384 [00:03&lt;00:00, 38487555.21it/s]"
          }
        },
        "33ebdccc46174e05b86eae092d6bfe17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf37a00758134d99b337e7e3792d134a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88276cb3159a4294a1fe3754bb52ecbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8f4b3df58b8467fa7084a04201284ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccd68a1e288a46d392c5a55b19c74bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "269e1f40a20d49d385d9b6b797839666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af8fe1ff5e324bab8a1bc7cf7b08c19b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Source code for unpickle function: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "# def unpickle(file):\n",
        "#     import pickle\n",
        "#     with open(file, 'rb') as fo:\n",
        "#         dict = pickle.load(fo, encoding='bytes')\n",
        "#     return dict\n",
        "\n",
        "# import numpy as np\n",
        "# def get_mean_color():\n",
        "#     d=unpickle('./data/cifar-10-batches-py/data_batch_1')\n",
        "#     channels = d[b'data']\n",
        "#     for i in range(2,6):\n",
        "#         d=unpickle('./data/cifar-10-batches-py/data_batch_'+str(i))\n",
        "#         channels=np.concatenate((channels, d[b'data']), axis=0)\n",
        "#     r=np.mean(channels[:,:1024])/255  \n",
        "#     g=np.mean(channels[:,1024:2048])/255\n",
        "#     b=np.mean(channels[:,2048:])/255\n",
        "#     return(r,g,b)\n",
        "# get_mean_color()"
      ],
      "metadata": {
        "id": "8ntKy6oKQGJv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "7be1314e5b6049eb80959c5580e1f8f9",
            "761d78cfb0984b17911246f7ac5b14fb",
            "92cd9afac92747998d0819e71168e7d3",
            "e1d70bdabd674e57b244c0b346e39750",
            "72c092bd879c48078e859b419795bdc0",
            "f487c683a6ec48deb7bfa2a45d4457aa",
            "26507aba632b44cc99332441fbcf97c4",
            "22dd70f6678944a8b416f12a2aacedf8",
            "169220c8404844df847c90107e9fab25",
            "b0777a3447244f199a514a699f3ea616",
            "a6c174326fb44ad4b1bd49806400c9af",
            "a457414c0401460ca6fdbcf4d1dd29af",
            "44489617705547408cba6ef9f5891782",
            "ea70d7acec634550b638c19e5b7a740d",
            "f1187874789b49198ffd3804a585a234",
            "33ebdccc46174e05b86eae092d6bfe17",
            "bf37a00758134d99b337e7e3792d134a",
            "88276cb3159a4294a1fe3754bb52ecbd",
            "a8f4b3df58b8467fa7084a04201284ab",
            "ccd68a1e288a46d392c5a55b19c74bfd",
            "269e1f40a20d49d385d9b6b797839666",
            "af8fe1ff5e324bab8a1bc7cf7b08c19b"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "06276f65-69dd-4da7-fb0a-6798fb68c0a6"
      },
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    #transforms.RandomErasing(value=get_mean_color()),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7be1314e5b6049eb80959c5580e1f8f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a457414c0401460ca6fdbcf4d1dd29af"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "0a11f769-27c5-4664-e2e2-e5d411849f57"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te71lQ17B1L_"
      },
      "source": [
        "#Data Augmentation\n",
        "Data augmentation performs random modifications of the image as a preprocessing step. It serves the following purposes:\n",
        "1. It increases the amount of data for training.\n",
        "2. By deleting features, it prevents the network from relying on a narrow set of features, which may not generalize.\n",
        "3. By changing features while maintaining the same output, it helps the network become tolerant of changes that do not change the image lab. \n",
        "\n",
        "In short, data augmentation desensitivizes the network, so it extracts features that are invariant to changes that should not affect the prediction. \n",
        "\n",
        "We showcase a few random data augmentation provided by PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyG26xoJC0Pa"
      },
      "source": [
        "# import torch.nn as nn\n",
        "# transforms = torch.nn.Sequential(\n",
        "#     T.Resize(256), # resize the short edge to 256.\n",
        "#     T.RandomCrop(224), #randomly crop a 224x224 region from the image\n",
        "#     T.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
        "#     #T.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0.1)\n",
        "#     #T.AutoAugment()\n",
        "# )\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# dog1 = dog1.to(device)\n",
        "# # dog2 = dog2.to(device)\n",
        "\n",
        "# # transformed_dog1 = transforms(dog1)\n",
        "# transformed_dog1 = transforms(dog1)\n",
        "# show([transformed_dog1])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # This is the \"stem\"\n",
        "        # For CIFAR (32x32 images), it does not perform downsampling\n",
        "        # It should downsample for ImageNet\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "1949e3ed-848a-4c3a-fc0b-7ad1cfb82e89"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "ad69b288-30d2-4ffc-b831-e2af1f5e072c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArgupDVRwB8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca9ea02-91b5-414d-c4aa-b9a405de7dd7"
      },
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3439, accuracy : 16.98\n",
            "iteration : 100, loss : 2.2795, accuracy : 19.23\n",
            "iteration : 150, loss : 2.1878, accuracy : 22.60\n",
            "iteration : 200, loss : 2.0178, accuracy : 28.35\n",
            "iteration : 250, loss : 1.8353, accuracy : 35.16\n",
            "iteration : 300, loss : 1.6640, accuracy : 41.48\n",
            "iteration : 350, loss : 1.5272, accuracy : 46.56\n",
            "Epoch :   1, training loss : 1.4802, training accuracy : 48.28, test loss : 0.8332, test accuracy : 73.41\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.5901, accuracy : 81.06\n",
            "iteration : 100, loss : 0.5778, accuracy : 81.60\n",
            "iteration : 150, loss : 0.5600, accuracy : 82.21\n",
            "iteration : 200, loss : 0.5468, accuracy : 82.66\n",
            "iteration : 250, loss : 0.5323, accuracy : 83.04\n",
            "iteration : 300, loss : 0.5221, accuracy : 83.37\n",
            "iteration : 350, loss : 0.5173, accuracy : 83.54\n",
            "Epoch :   2, training loss : 0.5146, training accuracy : 83.64, test loss : 0.6243, test accuracy : 79.92\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.4377, accuracy : 86.19\n",
            "iteration : 100, loss : 0.4487, accuracy : 85.96\n",
            "iteration : 150, loss : 0.4413, accuracy : 86.22\n",
            "iteration : 200, loss : 0.4343, accuracy : 86.52\n",
            "iteration : 250, loss : 0.4343, accuracy : 86.54\n",
            "iteration : 300, loss : 0.4309, accuracy : 86.67\n",
            "iteration : 350, loss : 0.4277, accuracy : 86.78\n",
            "Epoch :   3, training loss : 0.4271, training accuracy : 86.80, test loss : 0.4590, test accuracy : 86.05\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3978, accuracy : 88.00\n",
            "iteration : 100, loss : 0.3979, accuracy : 88.01\n",
            "iteration : 150, loss : 0.3946, accuracy : 87.94\n",
            "iteration : 200, loss : 0.4003, accuracy : 87.84\n",
            "iteration : 250, loss : 0.3960, accuracy : 87.95\n",
            "iteration : 300, loss : 0.3947, accuracy : 87.98\n",
            "iteration : 350, loss : 0.3916, accuracy : 88.11\n",
            "Epoch :   4, training loss : 0.3912, training accuracy : 88.12, test loss : 0.4804, test accuracy : 85.25\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.3649, accuracy : 88.64\n",
            "iteration : 100, loss : 0.3662, accuracy : 88.77\n",
            "iteration : 150, loss : 0.3699, accuracy : 88.70\n",
            "iteration : 200, loss : 0.3744, accuracy : 88.54\n",
            "iteration : 250, loss : 0.3724, accuracy : 88.69\n",
            "iteration : 300, loss : 0.3678, accuracy : 88.80\n",
            "iteration : 350, loss : 0.3673, accuracy : 88.85\n",
            "Epoch :   5, training loss : 0.3667, training accuracy : 88.89, test loss : 0.4275, test accuracy : 87.03\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.3403, accuracy : 89.92\n",
            "iteration : 100, loss : 0.3489, accuracy : 89.38\n",
            "iteration : 150, loss : 0.3532, accuracy : 89.30\n",
            "iteration : 200, loss : 0.3491, accuracy : 89.34\n",
            "iteration : 250, loss : 0.3516, accuracy : 89.31\n",
            "iteration : 300, loss : 0.3491, accuracy : 89.38\n",
            "iteration : 350, loss : 0.3470, accuracy : 89.47\n",
            "Epoch :   6, training loss : 0.3449, training accuracy : 89.55, test loss : 0.4091, test accuracy : 87.61\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.3186, accuracy : 90.02\n",
            "iteration : 100, loss : 0.3281, accuracy : 89.88\n",
            "iteration : 150, loss : 0.3321, accuracy : 89.72\n",
            "iteration : 200, loss : 0.3303, accuracy : 89.81\n",
            "iteration : 250, loss : 0.3278, accuracy : 89.89\n",
            "iteration : 300, loss : 0.3283, accuracy : 89.86\n",
            "iteration : 350, loss : 0.3276, accuracy : 89.95\n",
            "Epoch :   7, training loss : 0.3272, training accuracy : 89.99, test loss : 0.3809, test accuracy : 88.40\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.3123, accuracy : 90.56\n",
            "iteration : 100, loss : 0.3117, accuracy : 90.70\n",
            "iteration : 150, loss : 0.3172, accuracy : 90.61\n",
            "iteration : 200, loss : 0.3135, accuracy : 90.81\n",
            "iteration : 250, loss : 0.3153, accuracy : 90.79\n",
            "iteration : 300, loss : 0.3193, accuracy : 90.65\n",
            "iteration : 350, loss : 0.3172, accuracy : 90.68\n",
            "Epoch :   8, training loss : 0.3161, training accuracy : 90.70, test loss : 0.3557, test accuracy : 89.32\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.3133, accuracy : 91.05\n",
            "iteration : 100, loss : 0.3086, accuracy : 91.01\n",
            "iteration : 150, loss : 0.3074, accuracy : 90.72\n",
            "iteration : 200, loss : 0.3096, accuracy : 90.69\n",
            "iteration : 250, loss : 0.3081, accuracy : 90.71\n",
            "iteration : 300, loss : 0.3065, accuracy : 90.84\n",
            "iteration : 350, loss : 0.3048, accuracy : 90.91\n",
            "Epoch :   9, training loss : 0.3041, training accuracy : 90.96, test loss : 0.3353, test accuracy : 89.87\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2869, accuracy : 91.39\n",
            "iteration : 100, loss : 0.2918, accuracy : 91.27\n",
            "iteration : 150, loss : 0.2909, accuracy : 91.43\n",
            "iteration : 200, loss : 0.2904, accuracy : 91.44\n",
            "iteration : 250, loss : 0.2893, accuracy : 91.45\n",
            "iteration : 300, loss : 0.2912, accuracy : 91.31\n",
            "iteration : 350, loss : 0.2937, accuracy : 91.35\n",
            "Epoch :  10, training loss : 0.2937, training accuracy : 91.36, test loss : 0.3060, test accuracy : 90.96\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.2738, accuracy : 91.66\n",
            "iteration : 100, loss : 0.2820, accuracy : 91.62\n",
            "iteration : 150, loss : 0.2757, accuracy : 91.77\n",
            "iteration : 200, loss : 0.2783, accuracy : 91.69\n",
            "iteration : 250, loss : 0.2827, accuracy : 91.51\n",
            "iteration : 300, loss : 0.2801, accuracy : 91.54\n",
            "iteration : 350, loss : 0.2813, accuracy : 91.57\n",
            "Epoch :  11, training loss : 0.2816, training accuracy : 91.56, test loss : 0.3284, test accuracy : 90.20\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.2618, accuracy : 91.92\n",
            "iteration : 100, loss : 0.2702, accuracy : 91.91\n",
            "iteration : 150, loss : 0.2847, accuracy : 91.50\n",
            "iteration : 200, loss : 0.2817, accuracy : 91.67\n",
            "iteration : 250, loss : 0.2790, accuracy : 91.74\n",
            "iteration : 300, loss : 0.2774, accuracy : 91.72\n",
            "iteration : 350, loss : 0.2752, accuracy : 91.72\n",
            "Epoch :  12, training loss : 0.2770, training accuracy : 91.69, test loss : 0.2955, test accuracy : 91.43\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.2689, accuracy : 92.06\n",
            "iteration : 100, loss : 0.2749, accuracy : 91.88\n",
            "iteration : 150, loss : 0.2778, accuracy : 91.89\n",
            "iteration : 200, loss : 0.2757, accuracy : 91.94\n",
            "iteration : 250, loss : 0.2719, accuracy : 92.01\n",
            "iteration : 300, loss : 0.2709, accuracy : 92.05\n",
            "iteration : 350, loss : 0.2706, accuracy : 92.03\n",
            "Epoch :  13, training loss : 0.2688, training accuracy : 92.08, test loss : 0.2818, test accuracy : 91.59\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.2514, accuracy : 92.67\n",
            "iteration : 100, loss : 0.2629, accuracy : 92.16\n",
            "iteration : 150, loss : 0.2617, accuracy : 92.25\n",
            "iteration : 200, loss : 0.2620, accuracy : 92.23\n",
            "iteration : 250, loss : 0.2625, accuracy : 92.19\n",
            "iteration : 300, loss : 0.2638, accuracy : 92.12\n",
            "iteration : 350, loss : 0.2641, accuracy : 92.14\n",
            "Epoch :  14, training loss : 0.2642, training accuracy : 92.15, test loss : 0.3089, test accuracy : 91.10\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.2259, accuracy : 93.33\n",
            "iteration : 100, loss : 0.2353, accuracy : 93.21\n",
            "iteration : 150, loss : 0.2351, accuracy : 93.20\n",
            "iteration : 200, loss : 0.2463, accuracy : 92.84\n",
            "iteration : 250, loss : 0.2470, accuracy : 92.82\n",
            "iteration : 300, loss : 0.2500, accuracy : 92.72\n",
            "iteration : 350, loss : 0.2510, accuracy : 92.73\n",
            "Epoch :  15, training loss : 0.2538, training accuracy : 92.66, test loss : 0.3016, test accuracy : 91.30\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.2467, accuracy : 92.97\n",
            "iteration : 100, loss : 0.2483, accuracy : 92.77\n",
            "iteration : 150, loss : 0.2474, accuracy : 92.64\n",
            "iteration : 200, loss : 0.2485, accuracy : 92.63\n",
            "iteration : 250, loss : 0.2495, accuracy : 92.62\n",
            "iteration : 300, loss : 0.2496, accuracy : 92.65\n",
            "iteration : 350, loss : 0.2505, accuracy : 92.63\n",
            "Epoch :  16, training loss : 0.2503, training accuracy : 92.62, test loss : 0.2874, test accuracy : 91.72\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.2546, accuracy : 92.89\n",
            "iteration : 100, loss : 0.2470, accuracy : 93.08\n",
            "iteration : 150, loss : 0.2447, accuracy : 92.94\n",
            "iteration : 200, loss : 0.2429, accuracy : 92.95\n",
            "iteration : 250, loss : 0.2441, accuracy : 92.83\n",
            "iteration : 300, loss : 0.2457, accuracy : 92.78\n",
            "iteration : 350, loss : 0.2463, accuracy : 92.81\n",
            "Epoch :  17, training loss : 0.2471, training accuracy : 92.80, test loss : 0.3041, test accuracy : 91.10\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.2355, accuracy : 93.34\n",
            "iteration : 100, loss : 0.2449, accuracy : 93.07\n",
            "iteration : 150, loss : 0.2388, accuracy : 93.18\n",
            "iteration : 200, loss : 0.2379, accuracy : 93.19\n",
            "iteration : 250, loss : 0.2375, accuracy : 93.19\n",
            "iteration : 300, loss : 0.2380, accuracy : 93.14\n",
            "iteration : 350, loss : 0.2382, accuracy : 93.14\n",
            "Epoch :  18, training loss : 0.2385, training accuracy : 93.10, test loss : 0.2781, test accuracy : 91.83\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.2378, accuracy : 93.31\n",
            "iteration : 100, loss : 0.2319, accuracy : 93.34\n",
            "iteration : 150, loss : 0.2358, accuracy : 93.16\n",
            "iteration : 200, loss : 0.2356, accuracy : 93.18\n",
            "iteration : 250, loss : 0.2347, accuracy : 93.18\n",
            "iteration : 300, loss : 0.2362, accuracy : 93.16\n",
            "iteration : 350, loss : 0.2388, accuracy : 93.10\n",
            "Epoch :  19, training loss : 0.2399, training accuracy : 93.09, test loss : 0.2855, test accuracy : 91.69\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.2177, accuracy : 93.64\n",
            "iteration : 100, loss : 0.2272, accuracy : 93.32\n",
            "iteration : 150, loss : 0.2283, accuracy : 93.15\n",
            "iteration : 200, loss : 0.2254, accuracy : 93.23\n",
            "iteration : 250, loss : 0.2272, accuracy : 93.20\n",
            "iteration : 300, loss : 0.2289, accuracy : 93.19\n",
            "iteration : 350, loss : 0.2313, accuracy : 93.13\n",
            "Epoch :  20, training loss : 0.2323, training accuracy : 93.12, test loss : 0.2775, test accuracy : 91.82\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.2273, accuracy : 93.27\n",
            "iteration : 100, loss : 0.2287, accuracy : 93.32\n",
            "iteration : 150, loss : 0.2259, accuracy : 93.44\n",
            "iteration : 200, loss : 0.2262, accuracy : 93.31\n",
            "iteration : 250, loss : 0.2247, accuracy : 93.38\n",
            "iteration : 300, loss : 0.2270, accuracy : 93.30\n",
            "iteration : 350, loss : 0.2270, accuracy : 93.28\n",
            "Epoch :  21, training loss : 0.2279, training accuracy : 93.28, test loss : 0.2592, test accuracy : 92.55\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.2140, accuracy : 93.73\n",
            "iteration : 100, loss : 0.2195, accuracy : 93.52\n",
            "iteration : 150, loss : 0.2236, accuracy : 93.42\n",
            "iteration : 200, loss : 0.2273, accuracy : 93.34\n",
            "iteration : 250, loss : 0.2234, accuracy : 93.44\n",
            "iteration : 300, loss : 0.2244, accuracy : 93.41\n",
            "iteration : 350, loss : 0.2252, accuracy : 93.40\n",
            "Epoch :  22, training loss : 0.2257, training accuracy : 93.38, test loss : 0.2489, test accuracy : 92.83\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.2187, accuracy : 93.91\n",
            "iteration : 100, loss : 0.2228, accuracy : 93.63\n",
            "iteration : 150, loss : 0.2237, accuracy : 93.57\n",
            "iteration : 200, loss : 0.2236, accuracy : 93.61\n",
            "iteration : 250, loss : 0.2211, accuracy : 93.63\n",
            "iteration : 300, loss : 0.2240, accuracy : 93.53\n",
            "iteration : 350, loss : 0.2247, accuracy : 93.49\n",
            "Epoch :  23, training loss : 0.2250, training accuracy : 93.47, test loss : 0.2645, test accuracy : 92.39\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.2055, accuracy : 94.34\n",
            "iteration : 100, loss : 0.2172, accuracy : 93.82\n",
            "iteration : 150, loss : 0.2147, accuracy : 93.88\n",
            "iteration : 200, loss : 0.2153, accuracy : 93.78\n",
            "iteration : 250, loss : 0.2149, accuracy : 93.85\n",
            "iteration : 300, loss : 0.2145, accuracy : 93.82\n",
            "iteration : 350, loss : 0.2166, accuracy : 93.73\n",
            "Epoch :  24, training loss : 0.2194, training accuracy : 93.68, test loss : 0.2969, test accuracy : 91.19\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.1989, accuracy : 93.86\n",
            "iteration : 100, loss : 0.2044, accuracy : 93.86\n",
            "iteration : 150, loss : 0.2090, accuracy : 93.72\n",
            "iteration : 200, loss : 0.2122, accuracy : 93.71\n",
            "iteration : 250, loss : 0.2140, accuracy : 93.65\n",
            "iteration : 300, loss : 0.2174, accuracy : 93.60\n",
            "iteration : 350, loss : 0.2171, accuracy : 93.63\n",
            "Epoch :  25, training loss : 0.2184, training accuracy : 93.60, test loss : 0.2676, test accuracy : 92.22\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.2169, accuracy : 93.94\n",
            "iteration : 100, loss : 0.2100, accuracy : 93.84\n",
            "iteration : 150, loss : 0.2107, accuracy : 93.76\n",
            "iteration : 200, loss : 0.2103, accuracy : 93.82\n",
            "iteration : 250, loss : 0.2097, accuracy : 93.88\n",
            "iteration : 300, loss : 0.2117, accuracy : 93.78\n",
            "iteration : 350, loss : 0.2150, accuracy : 93.69\n",
            "Epoch :  26, training loss : 0.2156, training accuracy : 93.65, test loss : 0.2462, test accuracy : 92.82\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.1829, accuracy : 94.72\n",
            "iteration : 100, loss : 0.2084, accuracy : 94.09\n",
            "iteration : 150, loss : 0.2123, accuracy : 94.04\n",
            "iteration : 200, loss : 0.2156, accuracy : 93.96\n",
            "iteration : 250, loss : 0.2153, accuracy : 93.89\n",
            "iteration : 300, loss : 0.2135, accuracy : 93.97\n",
            "iteration : 350, loss : 0.2141, accuracy : 93.91\n",
            "Epoch :  27, training loss : 0.2141, training accuracy : 93.93, test loss : 0.2657, test accuracy : 92.24\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.2074, accuracy : 93.86\n",
            "iteration : 100, loss : 0.2057, accuracy : 94.02\n",
            "iteration : 150, loss : 0.2074, accuracy : 94.03\n",
            "iteration : 200, loss : 0.2111, accuracy : 93.87\n",
            "iteration : 250, loss : 0.2117, accuracy : 93.90\n",
            "iteration : 300, loss : 0.2126, accuracy : 93.85\n",
            "iteration : 350, loss : 0.2110, accuracy : 93.88\n",
            "Epoch :  28, training loss : 0.2112, training accuracy : 93.88, test loss : 0.2502, test accuracy : 92.82\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.1859, accuracy : 94.36\n",
            "iteration : 100, loss : 0.2048, accuracy : 93.96\n",
            "iteration : 150, loss : 0.2121, accuracy : 93.87\n",
            "iteration : 200, loss : 0.2167, accuracy : 93.76\n",
            "iteration : 250, loss : 0.2145, accuracy : 93.75\n",
            "iteration : 300, loss : 0.2144, accuracy : 93.76\n",
            "iteration : 350, loss : 0.2135, accuracy : 93.80\n",
            "Epoch :  29, training loss : 0.2124, training accuracy : 93.83, test loss : 0.2519, test accuracy : 92.71\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.1913, accuracy : 94.56\n",
            "iteration : 100, loss : 0.1957, accuracy : 94.55\n",
            "iteration : 150, loss : 0.1939, accuracy : 94.52\n",
            "iteration : 200, loss : 0.1986, accuracy : 94.34\n",
            "iteration : 250, loss : 0.1991, accuracy : 94.32\n",
            "iteration : 300, loss : 0.2036, accuracy : 94.18\n",
            "iteration : 350, loss : 0.2079, accuracy : 94.04\n",
            "Epoch :  30, training loss : 0.2060, training accuracy : 94.10, test loss : 0.2460, test accuracy : 92.94\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.2008, accuracy : 94.41\n",
            "iteration : 100, loss : 0.1947, accuracy : 94.42\n",
            "iteration : 150, loss : 0.1944, accuracy : 94.48\n",
            "iteration : 200, loss : 0.1949, accuracy : 94.46\n",
            "iteration : 250, loss : 0.2008, accuracy : 94.28\n",
            "iteration : 300, loss : 0.2035, accuracy : 94.18\n",
            "iteration : 350, loss : 0.2058, accuracy : 94.14\n",
            "Epoch :  31, training loss : 0.2047, training accuracy : 94.15, test loss : 0.2657, test accuracy : 92.41\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.1985, accuracy : 94.38\n",
            "iteration : 100, loss : 0.2023, accuracy : 94.30\n",
            "iteration : 150, loss : 0.2001, accuracy : 94.25\n",
            "iteration : 200, loss : 0.1971, accuracy : 94.26\n",
            "iteration : 250, loss : 0.2038, accuracy : 94.07\n",
            "iteration : 300, loss : 0.2017, accuracy : 94.11\n",
            "iteration : 350, loss : 0.2034, accuracy : 94.04\n",
            "Epoch :  32, training loss : 0.2037, training accuracy : 94.03, test loss : 0.2461, test accuracy : 92.89\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.1897, accuracy : 94.59\n",
            "iteration : 100, loss : 0.1922, accuracy : 94.41\n",
            "iteration : 150, loss : 0.1938, accuracy : 94.43\n",
            "iteration : 200, loss : 0.1968, accuracy : 94.37\n",
            "iteration : 250, loss : 0.1998, accuracy : 94.32\n",
            "iteration : 300, loss : 0.2024, accuracy : 94.21\n",
            "iteration : 350, loss : 0.2021, accuracy : 94.16\n",
            "Epoch :  33, training loss : 0.2035, training accuracy : 94.13, test loss : 0.2656, test accuracy : 92.42\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.1932, accuracy : 94.34\n",
            "iteration : 100, loss : 0.1966, accuracy : 94.36\n",
            "iteration : 150, loss : 0.2005, accuracy : 94.37\n",
            "iteration : 200, loss : 0.2010, accuracy : 94.34\n",
            "iteration : 250, loss : 0.1983, accuracy : 94.35\n",
            "iteration : 300, loss : 0.2013, accuracy : 94.23\n",
            "iteration : 350, loss : 0.2021, accuracy : 94.18\n",
            "Epoch :  34, training loss : 0.2025, training accuracy : 94.16, test loss : 0.2666, test accuracy : 92.20\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.2012, accuracy : 94.34\n",
            "iteration : 100, loss : 0.1938, accuracy : 94.34\n",
            "iteration : 150, loss : 0.1950, accuracy : 94.37\n",
            "iteration : 200, loss : 0.1945, accuracy : 94.36\n",
            "iteration : 250, loss : 0.1964, accuracy : 94.30\n",
            "iteration : 300, loss : 0.1946, accuracy : 94.36\n",
            "iteration : 350, loss : 0.1971, accuracy : 94.31\n",
            "Epoch :  35, training loss : 0.1971, training accuracy : 94.31, test loss : 0.2784, test accuracy : 92.03\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.1974, accuracy : 94.30\n",
            "iteration : 100, loss : 0.1902, accuracy : 94.39\n",
            "iteration : 150, loss : 0.1948, accuracy : 94.33\n",
            "iteration : 200, loss : 0.1967, accuracy : 94.29\n",
            "iteration : 250, loss : 0.1989, accuracy : 94.20\n",
            "iteration : 300, loss : 0.2005, accuracy : 94.20\n",
            "iteration : 350, loss : 0.2006, accuracy : 94.18\n",
            "Epoch :  36, training loss : 0.1986, training accuracy : 94.21, test loss : 0.2499, test accuracy : 92.72\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.2095, accuracy : 94.08\n",
            "iteration : 100, loss : 0.2079, accuracy : 94.11\n",
            "iteration : 150, loss : 0.2023, accuracy : 94.18\n",
            "iteration : 200, loss : 0.1986, accuracy : 94.24\n",
            "iteration : 250, loss : 0.1990, accuracy : 94.23\n",
            "iteration : 300, loss : 0.1960, accuracy : 94.28\n",
            "iteration : 350, loss : 0.1974, accuracy : 94.19\n",
            "Epoch :  37, training loss : 0.1974, training accuracy : 94.20, test loss : 0.2481, test accuracy : 92.87\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.1979, accuracy : 94.44\n",
            "iteration : 100, loss : 0.1955, accuracy : 94.33\n",
            "iteration : 150, loss : 0.1928, accuracy : 94.46\n",
            "iteration : 200, loss : 0.1962, accuracy : 94.40\n",
            "iteration : 250, loss : 0.1970, accuracy : 94.31\n",
            "iteration : 300, loss : 0.1971, accuracy : 94.28\n",
            "iteration : 350, loss : 0.1988, accuracy : 94.25\n",
            "Epoch :  38, training loss : 0.1982, training accuracy : 94.26, test loss : 0.2449, test accuracy : 92.96\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.1837, accuracy : 94.95\n",
            "iteration : 100, loss : 0.1851, accuracy : 94.69\n",
            "iteration : 150, loss : 0.1872, accuracy : 94.61\n",
            "iteration : 200, loss : 0.1887, accuracy : 94.61\n",
            "iteration : 250, loss : 0.1901, accuracy : 94.55\n",
            "iteration : 300, loss : 0.1911, accuracy : 94.51\n",
            "iteration : 350, loss : 0.1912, accuracy : 94.49\n",
            "Epoch :  39, training loss : 0.1905, training accuracy : 94.53, test loss : 0.2487, test accuracy : 92.84\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.1739, accuracy : 95.09\n",
            "iteration : 100, loss : 0.1794, accuracy : 94.86\n",
            "iteration : 150, loss : 0.1825, accuracy : 94.74\n",
            "iteration : 200, loss : 0.1885, accuracy : 94.59\n",
            "iteration : 250, loss : 0.1918, accuracy : 94.54\n",
            "iteration : 300, loss : 0.1894, accuracy : 94.61\n",
            "iteration : 350, loss : 0.1957, accuracy : 94.42\n",
            "Epoch :  40, training loss : 0.1952, training accuracy : 94.46, test loss : 0.2409, test accuracy : 93.14\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.1725, accuracy : 94.94\n",
            "iteration : 100, loss : 0.1777, accuracy : 94.95\n",
            "iteration : 150, loss : 0.1768, accuracy : 94.89\n",
            "iteration : 200, loss : 0.1844, accuracy : 94.70\n",
            "iteration : 250, loss : 0.1877, accuracy : 94.61\n",
            "iteration : 300, loss : 0.1906, accuracy : 94.51\n",
            "iteration : 350, loss : 0.1915, accuracy : 94.50\n",
            "Epoch :  41, training loss : 0.1918, training accuracy : 94.50, test loss : 0.2354, test accuracy : 93.33\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.1809, accuracy : 94.95\n",
            "iteration : 100, loss : 0.1825, accuracy : 94.80\n",
            "iteration : 150, loss : 0.1784, accuracy : 94.87\n",
            "iteration : 200, loss : 0.1836, accuracy : 94.68\n",
            "iteration : 250, loss : 0.1847, accuracy : 94.68\n",
            "iteration : 300, loss : 0.1877, accuracy : 94.63\n",
            "iteration : 350, loss : 0.1883, accuracy : 94.59\n",
            "Epoch :  42, training loss : 0.1883, training accuracy : 94.57, test loss : 0.2476, test accuracy : 92.99\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.1771, accuracy : 95.05\n",
            "iteration : 100, loss : 0.1871, accuracy : 94.56\n",
            "iteration : 150, loss : 0.1866, accuracy : 94.47\n",
            "iteration : 200, loss : 0.1882, accuracy : 94.44\n",
            "iteration : 250, loss : 0.1870, accuracy : 94.45\n",
            "iteration : 300, loss : 0.1886, accuracy : 94.42\n",
            "iteration : 350, loss : 0.1900, accuracy : 94.42\n",
            "Epoch :  43, training loss : 0.1907, training accuracy : 94.41, test loss : 0.2477, test accuracy : 93.01\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.1724, accuracy : 94.73\n",
            "iteration : 100, loss : 0.1741, accuracy : 94.89\n",
            "iteration : 150, loss : 0.1732, accuracy : 94.95\n",
            "iteration : 200, loss : 0.1813, accuracy : 94.79\n",
            "iteration : 250, loss : 0.1866, accuracy : 94.67\n",
            "iteration : 300, loss : 0.1875, accuracy : 94.64\n",
            "iteration : 350, loss : 0.1884, accuracy : 94.58\n",
            "Epoch :  44, training loss : 0.1881, training accuracy : 94.58, test loss : 0.2472, test accuracy : 93.03\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.1746, accuracy : 94.84\n",
            "iteration : 100, loss : 0.1873, accuracy : 94.45\n",
            "iteration : 150, loss : 0.1854, accuracy : 94.52\n",
            "iteration : 200, loss : 0.1853, accuracy : 94.57\n",
            "iteration : 250, loss : 0.1854, accuracy : 94.58\n",
            "iteration : 300, loss : 0.1859, accuracy : 94.58\n",
            "iteration : 350, loss : 0.1868, accuracy : 94.58\n",
            "Epoch :  45, training loss : 0.1875, training accuracy : 94.53, test loss : 0.2396, test accuracy : 93.22\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.1827, accuracy : 94.86\n",
            "iteration : 100, loss : 0.1774, accuracy : 94.91\n",
            "iteration : 150, loss : 0.1784, accuracy : 94.88\n",
            "iteration : 200, loss : 0.1817, accuracy : 94.81\n",
            "iteration : 250, loss : 0.1864, accuracy : 94.68\n",
            "iteration : 300, loss : 0.1884, accuracy : 94.63\n",
            "iteration : 350, loss : 0.1876, accuracy : 94.66\n",
            "Epoch :  46, training loss : 0.1876, training accuracy : 94.62, test loss : 0.2466, test accuracy : 92.97\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.1774, accuracy : 95.00\n",
            "iteration : 100, loss : 0.1757, accuracy : 94.91\n",
            "iteration : 150, loss : 0.1775, accuracy : 94.94\n",
            "iteration : 200, loss : 0.1794, accuracy : 94.94\n",
            "iteration : 250, loss : 0.1829, accuracy : 94.83\n",
            "iteration : 300, loss : 0.1834, accuracy : 94.79\n",
            "iteration : 350, loss : 0.1854, accuracy : 94.76\n",
            "Epoch :  47, training loss : 0.1849, training accuracy : 94.77, test loss : 0.2384, test accuracy : 93.29\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.1774, accuracy : 95.00\n",
            "iteration : 100, loss : 0.1690, accuracy : 95.27\n",
            "iteration : 150, loss : 0.1791, accuracy : 94.95\n",
            "iteration : 200, loss : 0.1831, accuracy : 94.78\n",
            "iteration : 250, loss : 0.1859, accuracy : 94.67\n",
            "iteration : 300, loss : 0.1874, accuracy : 94.62\n",
            "iteration : 350, loss : 0.1872, accuracy : 94.65\n",
            "Epoch :  48, training loss : 0.1866, training accuracy : 94.67, test loss : 0.2449, test accuracy : 93.04\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.1741, accuracy : 94.92\n",
            "iteration : 100, loss : 0.1778, accuracy : 94.71\n",
            "iteration : 150, loss : 0.1777, accuracy : 94.77\n",
            "iteration : 200, loss : 0.1779, accuracy : 94.83\n",
            "iteration : 250, loss : 0.1814, accuracy : 94.77\n",
            "iteration : 300, loss : 0.1870, accuracy : 94.65\n",
            "iteration : 350, loss : 0.1874, accuracy : 94.64\n",
            "Epoch :  49, training loss : 0.1872, training accuracy : 94.65, test loss : 0.2343, test accuracy : 93.26\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.1818, accuracy : 94.83\n",
            "iteration : 100, loss : 0.1791, accuracy : 94.86\n",
            "iteration : 150, loss : 0.1790, accuracy : 94.83\n",
            "iteration : 200, loss : 0.1814, accuracy : 94.71\n",
            "iteration : 250, loss : 0.1839, accuracy : 94.68\n",
            "iteration : 300, loss : 0.1850, accuracy : 94.67\n",
            "iteration : 350, loss : 0.1841, accuracy : 94.67\n",
            "Epoch :  50, training loss : 0.1836, training accuracy : 94.68, test loss : 0.2599, test accuracy : 92.66\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.1668, accuracy : 95.41\n",
            "iteration : 100, loss : 0.1727, accuracy : 95.21\n",
            "iteration : 150, loss : 0.1763, accuracy : 95.04\n",
            "iteration : 200, loss : 0.1804, accuracy : 94.94\n",
            "iteration : 250, loss : 0.1830, accuracy : 94.82\n",
            "iteration : 300, loss : 0.1820, accuracy : 94.79\n",
            "iteration : 350, loss : 0.1814, accuracy : 94.77\n",
            "Epoch :  51, training loss : 0.1822, training accuracy : 94.74, test loss : 0.2622, test accuracy : 92.59\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.1768, accuracy : 94.84\n",
            "iteration : 100, loss : 0.1800, accuracy : 94.89\n",
            "iteration : 150, loss : 0.1800, accuracy : 94.92\n",
            "iteration : 200, loss : 0.1827, accuracy : 94.73\n",
            "iteration : 250, loss : 0.1819, accuracy : 94.75\n",
            "iteration : 300, loss : 0.1817, accuracy : 94.75\n",
            "iteration : 350, loss : 0.1828, accuracy : 94.70\n",
            "Epoch :  52, training loss : 0.1832, training accuracy : 94.71, test loss : 0.2333, test accuracy : 93.41\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.1649, accuracy : 95.36\n",
            "iteration : 100, loss : 0.1685, accuracy : 95.27\n",
            "iteration : 150, loss : 0.1696, accuracy : 95.24\n",
            "iteration : 200, loss : 0.1744, accuracy : 95.14\n",
            "iteration : 250, loss : 0.1793, accuracy : 94.93\n",
            "iteration : 300, loss : 0.1763, accuracy : 94.99\n",
            "iteration : 350, loss : 0.1789, accuracy : 94.93\n",
            "Epoch :  53, training loss : 0.1783, training accuracy : 94.96, test loss : 0.2362, test accuracy : 93.12\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.1693, accuracy : 95.27\n",
            "iteration : 100, loss : 0.1812, accuracy : 94.95\n",
            "iteration : 150, loss : 0.1848, accuracy : 94.77\n",
            "iteration : 200, loss : 0.1837, accuracy : 94.75\n",
            "iteration : 250, loss : 0.1848, accuracy : 94.72\n",
            "iteration : 300, loss : 0.1845, accuracy : 94.72\n",
            "iteration : 350, loss : 0.1842, accuracy : 94.69\n",
            "Epoch :  54, training loss : 0.1837, training accuracy : 94.68, test loss : 0.2403, test accuracy : 93.28\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.1654, accuracy : 95.27\n",
            "iteration : 100, loss : 0.1746, accuracy : 95.11\n",
            "iteration : 150, loss : 0.1708, accuracy : 95.09\n",
            "iteration : 200, loss : 0.1737, accuracy : 95.00\n",
            "iteration : 250, loss : 0.1779, accuracy : 94.88\n",
            "iteration : 300, loss : 0.1780, accuracy : 94.88\n",
            "iteration : 350, loss : 0.1789, accuracy : 94.88\n",
            "Epoch :  55, training loss : 0.1781, training accuracy : 94.88, test loss : 0.2526, test accuracy : 92.85\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.1678, accuracy : 95.09\n",
            "iteration : 100, loss : 0.1688, accuracy : 95.08\n",
            "iteration : 150, loss : 0.1696, accuracy : 95.09\n",
            "iteration : 200, loss : 0.1734, accuracy : 94.94\n",
            "iteration : 250, loss : 0.1776, accuracy : 94.86\n",
            "iteration : 300, loss : 0.1801, accuracy : 94.79\n",
            "iteration : 350, loss : 0.1830, accuracy : 94.71\n",
            "Epoch :  56, training loss : 0.1832, training accuracy : 94.71, test loss : 0.2374, test accuracy : 93.31\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.1630, accuracy : 95.20\n",
            "iteration : 100, loss : 0.1679, accuracy : 95.06\n",
            "iteration : 150, loss : 0.1706, accuracy : 95.03\n",
            "iteration : 200, loss : 0.1735, accuracy : 94.89\n",
            "iteration : 250, loss : 0.1785, accuracy : 94.76\n",
            "iteration : 300, loss : 0.1796, accuracy : 94.77\n",
            "iteration : 350, loss : 0.1777, accuracy : 94.83\n",
            "Epoch :  57, training loss : 0.1778, training accuracy : 94.83, test loss : 0.2335, test accuracy : 93.42\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.1764, accuracy : 94.77\n",
            "iteration : 100, loss : 0.1679, accuracy : 95.03\n",
            "iteration : 150, loss : 0.1723, accuracy : 94.98\n",
            "iteration : 200, loss : 0.1739, accuracy : 94.91\n",
            "iteration : 250, loss : 0.1759, accuracy : 94.91\n",
            "iteration : 300, loss : 0.1742, accuracy : 95.00\n",
            "iteration : 350, loss : 0.1750, accuracy : 94.95\n",
            "Epoch :  58, training loss : 0.1764, training accuracy : 94.91, test loss : 0.2509, test accuracy : 93.00\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.1687, accuracy : 94.84\n",
            "iteration : 100, loss : 0.1611, accuracy : 95.20\n",
            "iteration : 150, loss : 0.1595, accuracy : 95.39\n",
            "iteration : 200, loss : 0.1679, accuracy : 95.15\n",
            "iteration : 250, loss : 0.1708, accuracy : 95.06\n",
            "iteration : 300, loss : 0.1714, accuracy : 95.02\n",
            "iteration : 350, loss : 0.1719, accuracy : 95.01\n",
            "Epoch :  59, training loss : 0.1740, training accuracy : 94.92, test loss : 0.2439, test accuracy : 93.09\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.1787, accuracy : 94.84\n",
            "iteration : 100, loss : 0.1821, accuracy : 94.74\n",
            "iteration : 150, loss : 0.1734, accuracy : 94.97\n",
            "iteration : 200, loss : 0.1771, accuracy : 94.87\n",
            "iteration : 250, loss : 0.1798, accuracy : 94.80\n",
            "iteration : 300, loss : 0.1789, accuracy : 94.88\n",
            "iteration : 350, loss : 0.1769, accuracy : 94.96\n",
            "Epoch :  60, training loss : 0.1764, training accuracy : 94.99, test loss : 0.2328, test accuracy : 93.52\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.1672, accuracy : 95.41\n",
            "iteration : 100, loss : 0.1648, accuracy : 95.35\n",
            "iteration : 150, loss : 0.1712, accuracy : 95.06\n",
            "iteration : 200, loss : 0.1728, accuracy : 95.03\n",
            "iteration : 250, loss : 0.1749, accuracy : 94.95\n",
            "iteration : 300, loss : 0.1747, accuracy : 94.94\n",
            "iteration : 350, loss : 0.1773, accuracy : 94.90\n",
            "Epoch :  61, training loss : 0.1755, training accuracy : 94.95, test loss : 0.2481, test accuracy : 93.01\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.1622, accuracy : 95.11\n",
            "iteration : 100, loss : 0.1595, accuracy : 95.26\n",
            "iteration : 150, loss : 0.1667, accuracy : 95.09\n",
            "iteration : 200, loss : 0.1751, accuracy : 94.89\n",
            "iteration : 250, loss : 0.1764, accuracy : 94.84\n",
            "iteration : 300, loss : 0.1780, accuracy : 94.84\n",
            "iteration : 350, loss : 0.1778, accuracy : 94.86\n",
            "Epoch :  62, training loss : 0.1768, training accuracy : 94.89, test loss : 0.2351, test accuracy : 93.37\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.1669, accuracy : 95.39\n",
            "iteration : 100, loss : 0.1674, accuracy : 95.20\n",
            "iteration : 150, loss : 0.1699, accuracy : 95.24\n",
            "iteration : 200, loss : 0.1718, accuracy : 95.22\n",
            "iteration : 250, loss : 0.1715, accuracy : 95.22\n",
            "iteration : 300, loss : 0.1728, accuracy : 95.18\n",
            "iteration : 350, loss : 0.1734, accuracy : 95.13\n",
            "Epoch :  63, training loss : 0.1729, training accuracy : 95.14, test loss : 0.2409, test accuracy : 93.21\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.1621, accuracy : 95.25\n",
            "iteration : 100, loss : 0.1638, accuracy : 95.10\n",
            "iteration : 150, loss : 0.1636, accuracy : 95.18\n",
            "iteration : 200, loss : 0.1650, accuracy : 95.17\n",
            "iteration : 250, loss : 0.1684, accuracy : 95.16\n",
            "iteration : 300, loss : 0.1676, accuracy : 95.16\n",
            "iteration : 350, loss : 0.1683, accuracy : 95.11\n",
            "Epoch :  64, training loss : 0.1693, training accuracy : 95.11, test loss : 0.2442, test accuracy : 92.99\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.1508, accuracy : 95.53\n",
            "iteration : 100, loss : 0.1574, accuracy : 95.45\n",
            "iteration : 150, loss : 0.1655, accuracy : 95.21\n",
            "iteration : 200, loss : 0.1683, accuracy : 95.17\n",
            "iteration : 250, loss : 0.1699, accuracy : 95.12\n",
            "iteration : 300, loss : 0.1703, accuracy : 95.14\n",
            "iteration : 350, loss : 0.1685, accuracy : 95.18\n",
            "Epoch :  65, training loss : 0.1699, training accuracy : 95.14, test loss : 0.2373, test accuracy : 93.39\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.1667, accuracy : 95.22\n",
            "iteration : 100, loss : 0.1751, accuracy : 94.96\n",
            "iteration : 150, loss : 0.1724, accuracy : 95.03\n",
            "iteration : 200, loss : 0.1743, accuracy : 94.98\n",
            "iteration : 250, loss : 0.1741, accuracy : 94.96\n",
            "iteration : 300, loss : 0.1741, accuracy : 94.97\n",
            "iteration : 350, loss : 0.1737, accuracy : 94.98\n",
            "Epoch :  66, training loss : 0.1739, training accuracy : 94.98, test loss : 0.2592, test accuracy : 92.60\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.1604, accuracy : 95.39\n",
            "iteration : 100, loss : 0.1621, accuracy : 95.38\n",
            "iteration : 150, loss : 0.1641, accuracy : 95.29\n",
            "iteration : 200, loss : 0.1680, accuracy : 95.23\n",
            "iteration : 250, loss : 0.1681, accuracy : 95.20\n",
            "iteration : 300, loss : 0.1684, accuracy : 95.18\n",
            "iteration : 350, loss : 0.1702, accuracy : 95.12\n",
            "Epoch :  67, training loss : 0.1708, training accuracy : 95.09, test loss : 0.2407, test accuracy : 93.18\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.1571, accuracy : 95.55\n",
            "iteration : 100, loss : 0.1601, accuracy : 95.55\n",
            "iteration : 150, loss : 0.1649, accuracy : 95.33\n",
            "iteration : 200, loss : 0.1655, accuracy : 95.33\n",
            "iteration : 250, loss : 0.1642, accuracy : 95.38\n",
            "iteration : 300, loss : 0.1651, accuracy : 95.36\n",
            "iteration : 350, loss : 0.1673, accuracy : 95.28\n",
            "Epoch :  68, training loss : 0.1682, training accuracy : 95.23, test loss : 0.2356, test accuracy : 93.35\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.1557, accuracy : 95.58\n",
            "iteration : 100, loss : 0.1633, accuracy : 95.20\n",
            "iteration : 150, loss : 0.1676, accuracy : 95.20\n",
            "iteration : 200, loss : 0.1685, accuracy : 95.18\n",
            "iteration : 250, loss : 0.1682, accuracy : 95.20\n",
            "iteration : 300, loss : 0.1681, accuracy : 95.19\n",
            "iteration : 350, loss : 0.1687, accuracy : 95.20\n",
            "Epoch :  69, training loss : 0.1690, training accuracy : 95.18, test loss : 0.2247, test accuracy : 93.68\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.1603, accuracy : 95.34\n",
            "iteration : 100, loss : 0.1637, accuracy : 95.19\n",
            "iteration : 150, loss : 0.1721, accuracy : 94.95\n",
            "iteration : 200, loss : 0.1691, accuracy : 94.99\n",
            "iteration : 250, loss : 0.1686, accuracy : 95.05\n",
            "iteration : 300, loss : 0.1699, accuracy : 95.03\n",
            "iteration : 350, loss : 0.1701, accuracy : 95.02\n",
            "Epoch :  70, training loss : 0.1705, training accuracy : 95.00, test loss : 0.2398, test accuracy : 93.28\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.1462, accuracy : 95.83\n",
            "iteration : 100, loss : 0.1534, accuracy : 95.68\n",
            "iteration : 150, loss : 0.1587, accuracy : 95.52\n",
            "iteration : 200, loss : 0.1601, accuracy : 95.44\n",
            "iteration : 250, loss : 0.1608, accuracy : 95.38\n",
            "iteration : 300, loss : 0.1651, accuracy : 95.22\n",
            "iteration : 350, loss : 0.1688, accuracy : 95.14\n",
            "Epoch :  71, training loss : 0.1687, training accuracy : 95.13, test loss : 0.2293, test accuracy : 93.52\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.1606, accuracy : 95.33\n",
            "iteration : 100, loss : 0.1618, accuracy : 95.35\n",
            "iteration : 150, loss : 0.1580, accuracy : 95.33\n",
            "iteration : 200, loss : 0.1624, accuracy : 95.19\n",
            "iteration : 250, loss : 0.1637, accuracy : 95.20\n",
            "iteration : 300, loss : 0.1658, accuracy : 95.13\n",
            "iteration : 350, loss : 0.1676, accuracy : 95.07\n",
            "Epoch :  72, training loss : 0.1672, training accuracy : 95.07, test loss : 0.2265, test accuracy : 93.66\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.1615, accuracy : 95.00\n",
            "iteration : 100, loss : 0.1617, accuracy : 95.12\n",
            "iteration : 150, loss : 0.1662, accuracy : 95.15\n",
            "iteration : 200, loss : 0.1618, accuracy : 95.24\n",
            "iteration : 250, loss : 0.1635, accuracy : 95.24\n",
            "iteration : 300, loss : 0.1644, accuracy : 95.26\n",
            "iteration : 350, loss : 0.1662, accuracy : 95.19\n",
            "Epoch :  73, training loss : 0.1659, training accuracy : 95.18, test loss : 0.2350, test accuracy : 93.52\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.1683, accuracy : 95.22\n",
            "iteration : 100, loss : 0.1612, accuracy : 95.34\n",
            "iteration : 150, loss : 0.1588, accuracy : 95.31\n",
            "iteration : 200, loss : 0.1556, accuracy : 95.47\n",
            "iteration : 250, loss : 0.1594, accuracy : 95.39\n",
            "iteration : 300, loss : 0.1609, accuracy : 95.37\n",
            "iteration : 350, loss : 0.1625, accuracy : 95.32\n",
            "Epoch :  74, training loss : 0.1628, training accuracy : 95.33, test loss : 0.2445, test accuracy : 93.12\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.1544, accuracy : 95.56\n",
            "iteration : 100, loss : 0.1587, accuracy : 95.45\n",
            "iteration : 150, loss : 0.1593, accuracy : 95.46\n",
            "iteration : 200, loss : 0.1598, accuracy : 95.47\n",
            "iteration : 250, loss : 0.1601, accuracy : 95.44\n",
            "iteration : 300, loss : 0.1635, accuracy : 95.30\n",
            "iteration : 350, loss : 0.1667, accuracy : 95.25\n",
            "Epoch :  75, training loss : 0.1659, training accuracy : 95.27, test loss : 0.2456, test accuracy : 92.99\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.1544, accuracy : 95.58\n",
            "iteration : 100, loss : 0.1570, accuracy : 95.47\n",
            "iteration : 150, loss : 0.1616, accuracy : 95.28\n",
            "iteration : 200, loss : 0.1611, accuracy : 95.26\n",
            "iteration : 250, loss : 0.1606, accuracy : 95.25\n",
            "iteration : 300, loss : 0.1609, accuracy : 95.26\n",
            "iteration : 350, loss : 0.1639, accuracy : 95.21\n",
            "Epoch :  76, training loss : 0.1636, training accuracy : 95.22, test loss : 0.2386, test accuracy : 93.27\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.1458, accuracy : 95.81\n",
            "iteration : 100, loss : 0.1525, accuracy : 95.54\n",
            "iteration : 150, loss : 0.1561, accuracy : 95.47\n",
            "iteration : 200, loss : 0.1541, accuracy : 95.53\n",
            "iteration : 250, loss : 0.1577, accuracy : 95.47\n",
            "iteration : 300, loss : 0.1600, accuracy : 95.41\n",
            "iteration : 350, loss : 0.1603, accuracy : 95.40\n",
            "Epoch :  77, training loss : 0.1614, training accuracy : 95.36, test loss : 0.2473, test accuracy : 93.15\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.1575, accuracy : 95.73\n",
            "iteration : 100, loss : 0.1610, accuracy : 95.56\n",
            "iteration : 150, loss : 0.1607, accuracy : 95.54\n",
            "iteration : 200, loss : 0.1635, accuracy : 95.41\n",
            "iteration : 250, loss : 0.1631, accuracy : 95.34\n",
            "iteration : 300, loss : 0.1622, accuracy : 95.33\n",
            "iteration : 350, loss : 0.1625, accuracy : 95.31\n",
            "Epoch :  78, training loss : 0.1624, training accuracy : 95.29, test loss : 0.2432, test accuracy : 93.39\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.1449, accuracy : 95.81\n",
            "iteration : 100, loss : 0.1591, accuracy : 95.55\n",
            "iteration : 150, loss : 0.1537, accuracy : 95.72\n",
            "iteration : 200, loss : 0.1575, accuracy : 95.55\n",
            "iteration : 250, loss : 0.1596, accuracy : 95.51\n",
            "iteration : 300, loss : 0.1598, accuracy : 95.50\n",
            "iteration : 350, loss : 0.1629, accuracy : 95.41\n",
            "Epoch :  79, training loss : 0.1630, training accuracy : 95.40, test loss : 0.2358, test accuracy : 93.45\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.1495, accuracy : 95.72\n",
            "iteration : 100, loss : 0.1545, accuracy : 95.68\n",
            "iteration : 150, loss : 0.1568, accuracy : 95.52\n",
            "iteration : 200, loss : 0.1574, accuracy : 95.47\n",
            "iteration : 250, loss : 0.1575, accuracy : 95.46\n",
            "iteration : 300, loss : 0.1593, accuracy : 95.39\n",
            "iteration : 350, loss : 0.1617, accuracy : 95.34\n",
            "Epoch :  80, training loss : 0.1627, training accuracy : 95.32, test loss : 0.2369, test accuracy : 93.32\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.1644, accuracy : 95.36\n",
            "iteration : 100, loss : 0.1553, accuracy : 95.64\n",
            "iteration : 150, loss : 0.1557, accuracy : 95.59\n",
            "iteration : 200, loss : 0.1569, accuracy : 95.50\n",
            "iteration : 250, loss : 0.1598, accuracy : 95.41\n",
            "iteration : 300, loss : 0.1590, accuracy : 95.45\n",
            "iteration : 350, loss : 0.1594, accuracy : 95.46\n",
            "Epoch :  81, training loss : 0.1592, training accuracy : 95.48, test loss : 0.2317, test accuracy : 93.44\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.1550, accuracy : 95.44\n",
            "iteration : 100, loss : 0.1572, accuracy : 95.54\n",
            "iteration : 150, loss : 0.1585, accuracy : 95.46\n",
            "iteration : 200, loss : 0.1557, accuracy : 95.53\n",
            "iteration : 250, loss : 0.1553, accuracy : 95.48\n",
            "iteration : 300, loss : 0.1555, accuracy : 95.49\n",
            "iteration : 350, loss : 0.1570, accuracy : 95.40\n",
            "Epoch :  82, training loss : 0.1587, training accuracy : 95.36, test loss : 0.2389, test accuracy : 93.38\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.1534, accuracy : 95.48\n",
            "iteration : 100, loss : 0.1534, accuracy : 95.66\n",
            "iteration : 150, loss : 0.1538, accuracy : 95.62\n",
            "iteration : 200, loss : 0.1541, accuracy : 95.61\n",
            "iteration : 250, loss : 0.1563, accuracy : 95.53\n",
            "iteration : 300, loss : 0.1585, accuracy : 95.47\n",
            "iteration : 350, loss : 0.1603, accuracy : 95.39\n",
            "Epoch :  83, training loss : 0.1624, training accuracy : 95.35, test loss : 0.2416, test accuracy : 93.46\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.1490, accuracy : 95.48\n",
            "iteration : 100, loss : 0.1471, accuracy : 95.55\n",
            "iteration : 150, loss : 0.1481, accuracy : 95.57\n",
            "iteration : 200, loss : 0.1542, accuracy : 95.49\n",
            "iteration : 250, loss : 0.1577, accuracy : 95.42\n",
            "iteration : 300, loss : 0.1570, accuracy : 95.47\n",
            "iteration : 350, loss : 0.1569, accuracy : 95.45\n",
            "Epoch :  84, training loss : 0.1576, training accuracy : 95.43, test loss : 0.2268, test accuracy : 93.60\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.1593, accuracy : 95.48\n",
            "iteration : 100, loss : 0.1543, accuracy : 95.60\n",
            "iteration : 150, loss : 0.1542, accuracy : 95.58\n",
            "iteration : 200, loss : 0.1585, accuracy : 95.41\n",
            "iteration : 250, loss : 0.1582, accuracy : 95.46\n",
            "iteration : 300, loss : 0.1584, accuracy : 95.46\n",
            "iteration : 350, loss : 0.1582, accuracy : 95.45\n",
            "Epoch :  85, training loss : 0.1586, training accuracy : 95.47, test loss : 0.2431, test accuracy : 93.42\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.1508, accuracy : 95.70\n",
            "iteration : 100, loss : 0.1374, accuracy : 96.09\n",
            "iteration : 150, loss : 0.1398, accuracy : 96.01\n",
            "iteration : 200, loss : 0.1468, accuracy : 95.81\n",
            "iteration : 250, loss : 0.1487, accuracy : 95.70\n",
            "iteration : 300, loss : 0.1543, accuracy : 95.53\n",
            "iteration : 350, loss : 0.1568, accuracy : 95.45\n",
            "Epoch :  86, training loss : 0.1579, training accuracy : 95.43, test loss : 0.2399, test accuracy : 93.35\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.1496, accuracy : 95.84\n",
            "iteration : 100, loss : 0.1428, accuracy : 95.94\n",
            "iteration : 150, loss : 0.1444, accuracy : 95.94\n",
            "iteration : 200, loss : 0.1458, accuracy : 95.89\n",
            "iteration : 250, loss : 0.1522, accuracy : 95.72\n",
            "iteration : 300, loss : 0.1537, accuracy : 95.63\n",
            "iteration : 350, loss : 0.1534, accuracy : 95.59\n",
            "Epoch :  87, training loss : 0.1542, training accuracy : 95.57, test loss : 0.2406, test accuracy : 93.32\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.1480, accuracy : 95.70\n",
            "iteration : 100, loss : 0.1516, accuracy : 95.61\n",
            "iteration : 150, loss : 0.1546, accuracy : 95.51\n",
            "iteration : 200, loss : 0.1520, accuracy : 95.55\n",
            "iteration : 250, loss : 0.1521, accuracy : 95.54\n",
            "iteration : 300, loss : 0.1535, accuracy : 95.47\n",
            "iteration : 350, loss : 0.1555, accuracy : 95.41\n",
            "Epoch :  88, training loss : 0.1552, training accuracy : 95.42, test loss : 0.2310, test accuracy : 93.71\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.1359, accuracy : 96.03\n",
            "iteration : 100, loss : 0.1408, accuracy : 95.91\n",
            "iteration : 150, loss : 0.1459, accuracy : 95.72\n",
            "iteration : 200, loss : 0.1497, accuracy : 95.66\n",
            "iteration : 250, loss : 0.1512, accuracy : 95.61\n",
            "iteration : 300, loss : 0.1519, accuracy : 95.55\n",
            "iteration : 350, loss : 0.1554, accuracy : 95.47\n",
            "Epoch :  89, training loss : 0.1563, training accuracy : 95.46, test loss : 0.2320, test accuracy : 93.42\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.1399, accuracy : 96.20\n",
            "iteration : 100, loss : 0.1494, accuracy : 95.78\n",
            "iteration : 150, loss : 0.1533, accuracy : 95.71\n",
            "iteration : 200, loss : 0.1539, accuracy : 95.72\n",
            "iteration : 250, loss : 0.1559, accuracy : 95.64\n",
            "iteration : 300, loss : 0.1549, accuracy : 95.66\n",
            "iteration : 350, loss : 0.1555, accuracy : 95.63\n",
            "Epoch :  90, training loss : 0.1546, training accuracy : 95.63, test loss : 0.2402, test accuracy : 93.35\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.1465, accuracy : 95.80\n",
            "iteration : 100, loss : 0.1494, accuracy : 95.49\n",
            "iteration : 150, loss : 0.1537, accuracy : 95.54\n",
            "iteration : 200, loss : 0.1535, accuracy : 95.49\n",
            "iteration : 250, loss : 0.1538, accuracy : 95.50\n",
            "iteration : 300, loss : 0.1541, accuracy : 95.55\n",
            "iteration : 350, loss : 0.1538, accuracy : 95.57\n",
            "Epoch :  91, training loss : 0.1544, training accuracy : 95.56, test loss : 0.2377, test accuracy : 93.50\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.1475, accuracy : 95.61\n",
            "iteration : 100, loss : 0.1526, accuracy : 95.71\n",
            "iteration : 150, loss : 0.1548, accuracy : 95.62\n",
            "iteration : 200, loss : 0.1536, accuracy : 95.68\n",
            "iteration : 250, loss : 0.1551, accuracy : 95.63\n",
            "iteration : 300, loss : 0.1566, accuracy : 95.57\n",
            "iteration : 350, loss : 0.1567, accuracy : 95.53\n",
            "Epoch :  92, training loss : 0.1572, training accuracy : 95.52, test loss : 0.2340, test accuracy : 93.48\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.1572, accuracy : 95.81\n",
            "iteration : 100, loss : 0.1520, accuracy : 95.85\n",
            "iteration : 150, loss : 0.1472, accuracy : 95.88\n",
            "iteration : 200, loss : 0.1481, accuracy : 95.86\n",
            "iteration : 250, loss : 0.1500, accuracy : 95.77\n",
            "iteration : 300, loss : 0.1499, accuracy : 95.74\n",
            "iteration : 350, loss : 0.1501, accuracy : 95.75\n",
            "Epoch :  93, training loss : 0.1505, training accuracy : 95.74, test loss : 0.2280, test accuracy : 93.66\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.1332, accuracy : 96.03\n",
            "iteration : 100, loss : 0.1371, accuracy : 96.07\n",
            "iteration : 150, loss : 0.1408, accuracy : 95.90\n",
            "iteration : 200, loss : 0.1453, accuracy : 95.82\n",
            "iteration : 250, loss : 0.1456, accuracy : 95.85\n",
            "iteration : 300, loss : 0.1471, accuracy : 95.82\n",
            "iteration : 350, loss : 0.1488, accuracy : 95.76\n",
            "Epoch :  94, training loss : 0.1495, training accuracy : 95.72, test loss : 0.2437, test accuracy : 93.37\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.1380, accuracy : 96.02\n",
            "iteration : 100, loss : 0.1388, accuracy : 96.11\n",
            "iteration : 150, loss : 0.1450, accuracy : 95.90\n",
            "iteration : 200, loss : 0.1474, accuracy : 95.85\n",
            "iteration : 250, loss : 0.1484, accuracy : 95.82\n",
            "iteration : 300, loss : 0.1493, accuracy : 95.72\n",
            "iteration : 350, loss : 0.1510, accuracy : 95.65\n",
            "Epoch :  95, training loss : 0.1505, training accuracy : 95.67, test loss : 0.2520, test accuracy : 93.12\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.1482, accuracy : 95.64\n",
            "iteration : 100, loss : 0.1400, accuracy : 95.83\n",
            "iteration : 150, loss : 0.1409, accuracy : 95.89\n",
            "iteration : 200, loss : 0.1468, accuracy : 95.69\n",
            "iteration : 250, loss : 0.1466, accuracy : 95.69\n",
            "iteration : 300, loss : 0.1507, accuracy : 95.61\n",
            "iteration : 350, loss : 0.1519, accuracy : 95.60\n",
            "Epoch :  96, training loss : 0.1531, training accuracy : 95.58, test loss : 0.2446, test accuracy : 93.17\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.1305, accuracy : 96.03\n",
            "iteration : 100, loss : 0.1369, accuracy : 95.95\n",
            "iteration : 150, loss : 0.1377, accuracy : 95.94\n",
            "iteration : 200, loss : 0.1422, accuracy : 95.91\n",
            "iteration : 250, loss : 0.1446, accuracy : 95.85\n",
            "iteration : 300, loss : 0.1479, accuracy : 95.75\n",
            "iteration : 350, loss : 0.1496, accuracy : 95.67\n",
            "Epoch :  97, training loss : 0.1489, training accuracy : 95.67, test loss : 0.2420, test accuracy : 93.25\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.1330, accuracy : 96.17\n",
            "iteration : 100, loss : 0.1388, accuracy : 96.08\n",
            "iteration : 150, loss : 0.1382, accuracy : 96.13\n",
            "iteration : 200, loss : 0.1404, accuracy : 96.07\n",
            "iteration : 250, loss : 0.1409, accuracy : 96.03\n",
            "iteration : 300, loss : 0.1429, accuracy : 95.96\n",
            "iteration : 350, loss : 0.1470, accuracy : 95.80\n",
            "Epoch :  98, training loss : 0.1479, training accuracy : 95.76, test loss : 0.2364, test accuracy : 93.45\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.1510, accuracy : 95.70\n",
            "iteration : 100, loss : 0.1495, accuracy : 95.70\n",
            "iteration : 150, loss : 0.1520, accuracy : 95.73\n",
            "iteration : 200, loss : 0.1508, accuracy : 95.71\n",
            "iteration : 250, loss : 0.1525, accuracy : 95.65\n",
            "iteration : 300, loss : 0.1515, accuracy : 95.67\n",
            "iteration : 350, loss : 0.1516, accuracy : 95.65\n",
            "Epoch :  99, training loss : 0.1511, training accuracy : 95.66, test loss : 0.2285, test accuracy : 93.72\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.1373, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1420, accuracy : 95.97\n",
            "iteration : 150, loss : 0.1416, accuracy : 95.97\n",
            "iteration : 200, loss : 0.1458, accuracy : 95.88\n",
            "iteration : 250, loss : 0.1451, accuracy : 95.88\n",
            "iteration : 300, loss : 0.1460, accuracy : 95.83\n",
            "iteration : 350, loss : 0.1450, accuracy : 95.83\n",
            "Epoch : 100, training loss : 0.1461, training accuracy : 95.81, test loss : 0.2370, test accuracy : 93.40\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.1309, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1381, accuracy : 96.03\n",
            "iteration : 150, loss : 0.1439, accuracy : 95.86\n",
            "iteration : 200, loss : 0.1449, accuracy : 95.83\n",
            "iteration : 250, loss : 0.1455, accuracy : 95.83\n",
            "iteration : 300, loss : 0.1483, accuracy : 95.73\n",
            "iteration : 350, loss : 0.1504, accuracy : 95.68\n",
            "Epoch : 101, training loss : 0.1509, training accuracy : 95.69, test loss : 0.2356, test accuracy : 93.50\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.1392, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1388, accuracy : 96.05\n",
            "iteration : 150, loss : 0.1446, accuracy : 95.80\n",
            "iteration : 200, loss : 0.1461, accuracy : 95.80\n",
            "iteration : 250, loss : 0.1464, accuracy : 95.77\n",
            "iteration : 300, loss : 0.1472, accuracy : 95.74\n",
            "iteration : 350, loss : 0.1481, accuracy : 95.70\n",
            "Epoch : 102, training loss : 0.1485, training accuracy : 95.72, test loss : 0.2294, test accuracy : 93.58\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.1458, accuracy : 95.84\n",
            "iteration : 100, loss : 0.1449, accuracy : 95.85\n",
            "iteration : 150, loss : 0.1409, accuracy : 95.92\n",
            "iteration : 200, loss : 0.1415, accuracy : 95.95\n",
            "iteration : 250, loss : 0.1426, accuracy : 95.95\n",
            "iteration : 300, loss : 0.1443, accuracy : 95.88\n",
            "iteration : 350, loss : 0.1446, accuracy : 95.89\n",
            "Epoch : 103, training loss : 0.1450, training accuracy : 95.85, test loss : 0.2248, test accuracy : 93.99\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.1415, accuracy : 95.78\n",
            "iteration : 100, loss : 0.1384, accuracy : 95.86\n",
            "iteration : 150, loss : 0.1404, accuracy : 95.93\n",
            "iteration : 200, loss : 0.1429, accuracy : 95.90\n",
            "iteration : 250, loss : 0.1451, accuracy : 95.87\n",
            "iteration : 300, loss : 0.1451, accuracy : 95.85\n",
            "iteration : 350, loss : 0.1451, accuracy : 95.88\n",
            "Epoch : 104, training loss : 0.1458, training accuracy : 95.86, test loss : 0.2420, test accuracy : 93.27\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.1184, accuracy : 96.62\n",
            "iteration : 100, loss : 0.1256, accuracy : 96.33\n",
            "iteration : 150, loss : 0.1328, accuracy : 96.23\n",
            "iteration : 200, loss : 0.1364, accuracy : 96.07\n",
            "iteration : 250, loss : 0.1399, accuracy : 95.99\n",
            "iteration : 300, loss : 0.1404, accuracy : 95.97\n",
            "iteration : 350, loss : 0.1406, accuracy : 95.97\n",
            "Epoch : 105, training loss : 0.1408, training accuracy : 95.96, test loss : 0.2488, test accuracy : 93.02\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.1375, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1392, accuracy : 95.98\n",
            "iteration : 150, loss : 0.1361, accuracy : 96.06\n",
            "iteration : 200, loss : 0.1375, accuracy : 96.03\n",
            "iteration : 250, loss : 0.1390, accuracy : 96.00\n",
            "iteration : 300, loss : 0.1415, accuracy : 95.95\n",
            "iteration : 350, loss : 0.1423, accuracy : 95.95\n",
            "Epoch : 106, training loss : 0.1430, training accuracy : 95.92, test loss : 0.2350, test accuracy : 93.62\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.1371, accuracy : 95.98\n",
            "iteration : 100, loss : 0.1391, accuracy : 95.96\n",
            "iteration : 150, loss : 0.1383, accuracy : 96.09\n",
            "iteration : 200, loss : 0.1392, accuracy : 96.04\n",
            "iteration : 250, loss : 0.1419, accuracy : 95.93\n",
            "iteration : 300, loss : 0.1428, accuracy : 95.89\n",
            "iteration : 350, loss : 0.1442, accuracy : 95.87\n",
            "Epoch : 107, training loss : 0.1451, training accuracy : 95.85, test loss : 0.2345, test accuracy : 93.52\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.1377, accuracy : 96.02\n",
            "iteration : 100, loss : 0.1393, accuracy : 96.01\n",
            "iteration : 150, loss : 0.1414, accuracy : 95.91\n",
            "iteration : 200, loss : 0.1420, accuracy : 95.83\n",
            "iteration : 250, loss : 0.1448, accuracy : 95.78\n",
            "iteration : 300, loss : 0.1450, accuracy : 95.79\n",
            "iteration : 350, loss : 0.1442, accuracy : 95.80\n",
            "Epoch : 108, training loss : 0.1448, training accuracy : 95.79, test loss : 0.2393, test accuracy : 93.39\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.1371, accuracy : 96.23\n",
            "iteration : 100, loss : 0.1382, accuracy : 96.10\n",
            "iteration : 150, loss : 0.1346, accuracy : 96.23\n",
            "iteration : 200, loss : 0.1313, accuracy : 96.28\n",
            "iteration : 250, loss : 0.1316, accuracy : 96.20\n",
            "iteration : 300, loss : 0.1359, accuracy : 96.12\n",
            "iteration : 350, loss : 0.1392, accuracy : 96.04\n",
            "Epoch : 109, training loss : 0.1404, training accuracy : 96.02, test loss : 0.2230, test accuracy : 93.92\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.1348, accuracy : 96.27\n",
            "iteration : 100, loss : 0.1388, accuracy : 96.07\n",
            "iteration : 150, loss : 0.1375, accuracy : 96.01\n",
            "iteration : 200, loss : 0.1371, accuracy : 96.00\n",
            "iteration : 250, loss : 0.1390, accuracy : 96.02\n",
            "iteration : 300, loss : 0.1381, accuracy : 96.03\n",
            "iteration : 350, loss : 0.1382, accuracy : 96.03\n",
            "Epoch : 110, training loss : 0.1393, training accuracy : 95.99, test loss : 0.2327, test accuracy : 93.72\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.1463, accuracy : 95.80\n",
            "iteration : 100, loss : 0.1432, accuracy : 95.88\n",
            "iteration : 150, loss : 0.1417, accuracy : 95.83\n",
            "iteration : 200, loss : 0.1409, accuracy : 95.91\n",
            "iteration : 250, loss : 0.1384, accuracy : 95.98\n",
            "iteration : 300, loss : 0.1379, accuracy : 95.99\n",
            "iteration : 350, loss : 0.1387, accuracy : 95.98\n",
            "Epoch : 111, training loss : 0.1372, training accuracy : 96.02, test loss : 0.2301, test accuracy : 93.86\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.1402, accuracy : 96.11\n",
            "iteration : 100, loss : 0.1377, accuracy : 96.12\n",
            "iteration : 150, loss : 0.1429, accuracy : 95.94\n",
            "iteration : 200, loss : 0.1402, accuracy : 96.00\n",
            "iteration : 250, loss : 0.1409, accuracy : 96.00\n",
            "iteration : 300, loss : 0.1410, accuracy : 96.02\n",
            "iteration : 350, loss : 0.1417, accuracy : 95.99\n",
            "Epoch : 112, training loss : 0.1421, training accuracy : 95.98, test loss : 0.2274, test accuracy : 93.78\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.1331, accuracy : 96.05\n",
            "iteration : 100, loss : 0.1351, accuracy : 96.08\n",
            "iteration : 150, loss : 0.1361, accuracy : 96.15\n",
            "iteration : 200, loss : 0.1375, accuracy : 96.06\n",
            "iteration : 250, loss : 0.1379, accuracy : 96.03\n",
            "iteration : 300, loss : 0.1396, accuracy : 95.96\n",
            "iteration : 350, loss : 0.1410, accuracy : 95.94\n",
            "Epoch : 113, training loss : 0.1410, training accuracy : 95.94, test loss : 0.2403, test accuracy : 93.37\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.1356, accuracy : 96.33\n",
            "iteration : 100, loss : 0.1369, accuracy : 96.14\n",
            "iteration : 150, loss : 0.1364, accuracy : 96.20\n",
            "iteration : 200, loss : 0.1355, accuracy : 96.25\n",
            "iteration : 250, loss : 0.1375, accuracy : 96.15\n",
            "iteration : 300, loss : 0.1388, accuracy : 96.13\n",
            "iteration : 350, loss : 0.1385, accuracy : 96.12\n",
            "Epoch : 114, training loss : 0.1387, training accuracy : 96.09, test loss : 0.2303, test accuracy : 93.69\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.1221, accuracy : 96.50\n",
            "iteration : 100, loss : 0.1318, accuracy : 96.15\n",
            "iteration : 150, loss : 0.1348, accuracy : 96.09\n",
            "iteration : 200, loss : 0.1379, accuracy : 96.02\n",
            "iteration : 250, loss : 0.1363, accuracy : 96.05\n",
            "iteration : 300, loss : 0.1364, accuracy : 96.01\n",
            "iteration : 350, loss : 0.1379, accuracy : 96.01\n",
            "Epoch : 115, training loss : 0.1381, training accuracy : 96.00, test loss : 0.2323, test accuracy : 93.66\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.1311, accuracy : 96.36\n",
            "iteration : 100, loss : 0.1282, accuracy : 96.36\n",
            "iteration : 150, loss : 0.1272, accuracy : 96.44\n",
            "iteration : 200, loss : 0.1311, accuracy : 96.36\n",
            "iteration : 250, loss : 0.1336, accuracy : 96.22\n",
            "iteration : 300, loss : 0.1346, accuracy : 96.15\n",
            "iteration : 350, loss : 0.1348, accuracy : 96.12\n",
            "Epoch : 116, training loss : 0.1344, training accuracy : 96.12, test loss : 0.2310, test accuracy : 93.81\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.1263, accuracy : 96.48\n",
            "iteration : 100, loss : 0.1314, accuracy : 96.32\n",
            "iteration : 150, loss : 0.1290, accuracy : 96.36\n",
            "iteration : 200, loss : 0.1311, accuracy : 96.30\n",
            "iteration : 250, loss : 0.1343, accuracy : 96.22\n",
            "iteration : 300, loss : 0.1338, accuracy : 96.21\n",
            "iteration : 350, loss : 0.1335, accuracy : 96.16\n",
            "Epoch : 117, training loss : 0.1341, training accuracy : 96.15, test loss : 0.2350, test accuracy : 93.49\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.1302, accuracy : 95.98\n",
            "iteration : 100, loss : 0.1321, accuracy : 96.12\n",
            "iteration : 150, loss : 0.1257, accuracy : 96.40\n",
            "iteration : 200, loss : 0.1311, accuracy : 96.29\n",
            "iteration : 250, loss : 0.1301, accuracy : 96.29\n",
            "iteration : 300, loss : 0.1335, accuracy : 96.17\n",
            "iteration : 350, loss : 0.1347, accuracy : 96.14\n",
            "Epoch : 118, training loss : 0.1343, training accuracy : 96.16, test loss : 0.2322, test accuracy : 93.67\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.1204, accuracy : 96.47\n",
            "iteration : 100, loss : 0.1264, accuracy : 96.49\n",
            "iteration : 150, loss : 0.1303, accuracy : 96.28\n",
            "iteration : 200, loss : 0.1314, accuracy : 96.24\n",
            "iteration : 250, loss : 0.1314, accuracy : 96.23\n",
            "iteration : 300, loss : 0.1314, accuracy : 96.23\n",
            "iteration : 350, loss : 0.1313, accuracy : 96.25\n",
            "Epoch : 119, training loss : 0.1322, training accuracy : 96.22, test loss : 0.2435, test accuracy : 93.40\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.1122, accuracy : 96.70\n",
            "iteration : 100, loss : 0.1149, accuracy : 96.70\n",
            "iteration : 150, loss : 0.1226, accuracy : 96.49\n",
            "iteration : 200, loss : 0.1273, accuracy : 96.33\n",
            "iteration : 250, loss : 0.1290, accuracy : 96.29\n",
            "iteration : 300, loss : 0.1310, accuracy : 96.21\n",
            "iteration : 350, loss : 0.1315, accuracy : 96.20\n",
            "Epoch : 120, training loss : 0.1316, training accuracy : 96.19, test loss : 0.2342, test accuracy : 93.66\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.1249, accuracy : 96.67\n",
            "iteration : 100, loss : 0.1226, accuracy : 96.49\n",
            "iteration : 150, loss : 0.1267, accuracy : 96.41\n",
            "iteration : 200, loss : 0.1295, accuracy : 96.35\n",
            "iteration : 250, loss : 0.1297, accuracy : 96.30\n",
            "iteration : 300, loss : 0.1321, accuracy : 96.21\n",
            "iteration : 350, loss : 0.1321, accuracy : 96.24\n",
            "Epoch : 121, training loss : 0.1327, training accuracy : 96.21, test loss : 0.2342, test accuracy : 93.68\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.1013, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1151, accuracy : 96.52\n",
            "iteration : 150, loss : 0.1208, accuracy : 96.42\n",
            "iteration : 200, loss : 0.1265, accuracy : 96.27\n",
            "iteration : 250, loss : 0.1262, accuracy : 96.29\n",
            "iteration : 300, loss : 0.1269, accuracy : 96.28\n",
            "iteration : 350, loss : 0.1307, accuracy : 96.19\n",
            "Epoch : 122, training loss : 0.1318, training accuracy : 96.17, test loss : 0.2290, test accuracy : 93.67\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.1182, accuracy : 96.48\n",
            "iteration : 100, loss : 0.1196, accuracy : 96.58\n",
            "iteration : 150, loss : 0.1227, accuracy : 96.53\n",
            "iteration : 200, loss : 0.1238, accuracy : 96.48\n",
            "iteration : 250, loss : 0.1275, accuracy : 96.41\n",
            "iteration : 300, loss : 0.1273, accuracy : 96.38\n",
            "iteration : 350, loss : 0.1268, accuracy : 96.37\n",
            "Epoch : 123, training loss : 0.1274, training accuracy : 96.36, test loss : 0.2354, test accuracy : 93.62\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.1279, accuracy : 96.42\n",
            "iteration : 100, loss : 0.1250, accuracy : 96.48\n",
            "iteration : 150, loss : 0.1239, accuracy : 96.45\n",
            "iteration : 200, loss : 0.1225, accuracy : 96.50\n",
            "iteration : 250, loss : 0.1244, accuracy : 96.44\n",
            "iteration : 300, loss : 0.1266, accuracy : 96.36\n",
            "iteration : 350, loss : 0.1276, accuracy : 96.36\n",
            "Epoch : 124, training loss : 0.1268, training accuracy : 96.39, test loss : 0.2273, test accuracy : 94.08\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.1181, accuracy : 96.73\n",
            "iteration : 100, loss : 0.1228, accuracy : 96.65\n",
            "iteration : 150, loss : 0.1228, accuracy : 96.47\n",
            "iteration : 200, loss : 0.1248, accuracy : 96.39\n",
            "iteration : 250, loss : 0.1266, accuracy : 96.37\n",
            "iteration : 300, loss : 0.1285, accuracy : 96.30\n",
            "iteration : 350, loss : 0.1277, accuracy : 96.31\n",
            "Epoch : 125, training loss : 0.1294, training accuracy : 96.26, test loss : 0.2473, test accuracy : 93.60\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.1141, accuracy : 96.94\n",
            "iteration : 100, loss : 0.1188, accuracy : 96.66\n",
            "iteration : 150, loss : 0.1232, accuracy : 96.52\n",
            "iteration : 200, loss : 0.1244, accuracy : 96.42\n",
            "iteration : 250, loss : 0.1281, accuracy : 96.30\n",
            "iteration : 300, loss : 0.1292, accuracy : 96.28\n",
            "iteration : 350, loss : 0.1319, accuracy : 96.21\n",
            "Epoch : 126, training loss : 0.1312, training accuracy : 96.24, test loss : 0.2319, test accuracy : 93.65\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.1172, accuracy : 96.47\n",
            "iteration : 100, loss : 0.1187, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1203, accuracy : 96.52\n",
            "iteration : 200, loss : 0.1229, accuracy : 96.46\n",
            "iteration : 250, loss : 0.1242, accuracy : 96.43\n",
            "iteration : 300, loss : 0.1252, accuracy : 96.42\n",
            "iteration : 350, loss : 0.1271, accuracy : 96.36\n",
            "Epoch : 127, training loss : 0.1272, training accuracy : 96.36, test loss : 0.2373, test accuracy : 93.65\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.1131, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1205, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1166, accuracy : 96.68\n",
            "iteration : 200, loss : 0.1179, accuracy : 96.66\n",
            "iteration : 250, loss : 0.1189, accuracy : 96.62\n",
            "iteration : 300, loss : 0.1199, accuracy : 96.59\n",
            "iteration : 350, loss : 0.1233, accuracy : 96.54\n",
            "Epoch : 128, training loss : 0.1227, training accuracy : 96.57, test loss : 0.2368, test accuracy : 93.62\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.1086, accuracy : 96.69\n",
            "iteration : 100, loss : 0.1184, accuracy : 96.56\n",
            "iteration : 150, loss : 0.1210, accuracy : 96.45\n",
            "iteration : 200, loss : 0.1237, accuracy : 96.38\n",
            "iteration : 250, loss : 0.1262, accuracy : 96.32\n",
            "iteration : 300, loss : 0.1262, accuracy : 96.35\n",
            "iteration : 350, loss : 0.1264, accuracy : 96.35\n",
            "Epoch : 129, training loss : 0.1257, training accuracy : 96.38, test loss : 0.2382, test accuracy : 93.61\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.1183, accuracy : 96.52\n",
            "iteration : 100, loss : 0.1245, accuracy : 96.48\n",
            "iteration : 150, loss : 0.1224, accuracy : 96.52\n",
            "iteration : 200, loss : 0.1202, accuracy : 96.57\n",
            "iteration : 250, loss : 0.1235, accuracy : 96.52\n",
            "iteration : 300, loss : 0.1229, accuracy : 96.56\n",
            "iteration : 350, loss : 0.1233, accuracy : 96.54\n",
            "Epoch : 130, training loss : 0.1229, training accuracy : 96.55, test loss : 0.2423, test accuracy : 93.49\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.1056, accuracy : 96.88\n",
            "iteration : 100, loss : 0.1167, accuracy : 96.58\n",
            "iteration : 150, loss : 0.1200, accuracy : 96.47\n",
            "iteration : 200, loss : 0.1188, accuracy : 96.53\n",
            "iteration : 250, loss : 0.1214, accuracy : 96.46\n",
            "iteration : 300, loss : 0.1227, accuracy : 96.41\n",
            "iteration : 350, loss : 0.1235, accuracy : 96.40\n",
            "Epoch : 131, training loss : 0.1243, training accuracy : 96.39, test loss : 0.2405, test accuracy : 93.58\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.1206, accuracy : 96.22\n",
            "iteration : 100, loss : 0.1296, accuracy : 96.22\n",
            "iteration : 150, loss : 0.1286, accuracy : 96.27\n",
            "iteration : 200, loss : 0.1292, accuracy : 96.29\n",
            "iteration : 250, loss : 0.1279, accuracy : 96.32\n",
            "iteration : 300, loss : 0.1269, accuracy : 96.34\n",
            "iteration : 350, loss : 0.1279, accuracy : 96.30\n",
            "Epoch : 132, training loss : 0.1271, training accuracy : 96.34, test loss : 0.2307, test accuracy : 93.74\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.1042, accuracy : 97.00\n",
            "iteration : 100, loss : 0.1109, accuracy : 96.86\n",
            "iteration : 150, loss : 0.1133, accuracy : 96.86\n",
            "iteration : 200, loss : 0.1176, accuracy : 96.67\n",
            "iteration : 250, loss : 0.1194, accuracy : 96.59\n",
            "iteration : 300, loss : 0.1191, accuracy : 96.61\n",
            "iteration : 350, loss : 0.1215, accuracy : 96.58\n",
            "Epoch : 133, training loss : 0.1218, training accuracy : 96.57, test loss : 0.2310, test accuracy : 93.73\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.1016, accuracy : 97.17\n",
            "iteration : 100, loss : 0.1121, accuracy : 96.87\n",
            "iteration : 150, loss : 0.1131, accuracy : 96.77\n",
            "iteration : 200, loss : 0.1139, accuracy : 96.75\n",
            "iteration : 250, loss : 0.1161, accuracy : 96.73\n",
            "iteration : 300, loss : 0.1175, accuracy : 96.66\n",
            "iteration : 350, loss : 0.1178, accuracy : 96.67\n",
            "Epoch : 134, training loss : 0.1183, training accuracy : 96.65, test loss : 0.2365, test accuracy : 93.66\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.1187, accuracy : 96.81\n",
            "iteration : 100, loss : 0.1216, accuracy : 96.70\n",
            "iteration : 150, loss : 0.1183, accuracy : 96.83\n",
            "iteration : 200, loss : 0.1177, accuracy : 96.74\n",
            "iteration : 250, loss : 0.1177, accuracy : 96.71\n",
            "iteration : 300, loss : 0.1194, accuracy : 96.68\n",
            "iteration : 350, loss : 0.1205, accuracy : 96.64\n",
            "Epoch : 135, training loss : 0.1200, training accuracy : 96.65, test loss : 0.2498, test accuracy : 93.41\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.1197, accuracy : 96.59\n",
            "iteration : 100, loss : 0.1137, accuracy : 96.81\n",
            "iteration : 150, loss : 0.1148, accuracy : 96.78\n",
            "iteration : 200, loss : 0.1154, accuracy : 96.75\n",
            "iteration : 250, loss : 0.1186, accuracy : 96.70\n",
            "iteration : 300, loss : 0.1194, accuracy : 96.63\n",
            "iteration : 350, loss : 0.1213, accuracy : 96.58\n",
            "Epoch : 136, training loss : 0.1206, training accuracy : 96.61, test loss : 0.2329, test accuracy : 93.75\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.1095, accuracy : 96.80\n",
            "iteration : 100, loss : 0.1135, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1139, accuracy : 96.65\n",
            "iteration : 200, loss : 0.1131, accuracy : 96.68\n",
            "iteration : 250, loss : 0.1136, accuracy : 96.69\n",
            "iteration : 300, loss : 0.1141, accuracy : 96.68\n",
            "iteration : 350, loss : 0.1146, accuracy : 96.66\n",
            "Epoch : 137, training loss : 0.1143, training accuracy : 96.66, test loss : 0.2476, test accuracy : 93.48\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.1257, accuracy : 96.48\n",
            "iteration : 100, loss : 0.1217, accuracy : 96.57\n",
            "iteration : 150, loss : 0.1202, accuracy : 96.55\n",
            "iteration : 200, loss : 0.1217, accuracy : 96.53\n",
            "iteration : 250, loss : 0.1201, accuracy : 96.58\n",
            "iteration : 300, loss : 0.1190, accuracy : 96.57\n",
            "iteration : 350, loss : 0.1192, accuracy : 96.53\n",
            "Epoch : 138, training loss : 0.1198, training accuracy : 96.52, test loss : 0.2493, test accuracy : 93.58\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.1073, accuracy : 97.05\n",
            "iteration : 100, loss : 0.1043, accuracy : 97.07\n",
            "iteration : 150, loss : 0.1111, accuracy : 96.82\n",
            "iteration : 200, loss : 0.1160, accuracy : 96.66\n",
            "iteration : 250, loss : 0.1156, accuracy : 96.73\n",
            "iteration : 300, loss : 0.1149, accuracy : 96.75\n",
            "iteration : 350, loss : 0.1145, accuracy : 96.75\n",
            "Epoch : 139, training loss : 0.1142, training accuracy : 96.77, test loss : 0.2296, test accuracy : 93.87\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.1022, accuracy : 97.03\n",
            "iteration : 100, loss : 0.1030, accuracy : 97.02\n",
            "iteration : 150, loss : 0.1070, accuracy : 96.90\n",
            "iteration : 200, loss : 0.1059, accuracy : 96.93\n",
            "iteration : 250, loss : 0.1103, accuracy : 96.83\n",
            "iteration : 300, loss : 0.1130, accuracy : 96.72\n",
            "iteration : 350, loss : 0.1144, accuracy : 96.71\n",
            "Epoch : 140, training loss : 0.1151, training accuracy : 96.68, test loss : 0.2354, test accuracy : 93.73\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.1156, accuracy : 96.45\n",
            "iteration : 100, loss : 0.1129, accuracy : 96.62\n",
            "iteration : 150, loss : 0.1132, accuracy : 96.66\n",
            "iteration : 200, loss : 0.1130, accuracy : 96.68\n",
            "iteration : 250, loss : 0.1140, accuracy : 96.66\n",
            "iteration : 300, loss : 0.1139, accuracy : 96.67\n",
            "iteration : 350, loss : 0.1144, accuracy : 96.69\n",
            "Epoch : 141, training loss : 0.1142, training accuracy : 96.73, test loss : 0.2426, test accuracy : 93.48\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0996, accuracy : 97.19\n",
            "iteration : 100, loss : 0.1036, accuracy : 97.12\n",
            "iteration : 150, loss : 0.1064, accuracy : 96.95\n",
            "iteration : 200, loss : 0.1098, accuracy : 96.80\n",
            "iteration : 250, loss : 0.1100, accuracy : 96.80\n",
            "iteration : 300, loss : 0.1127, accuracy : 96.69\n",
            "iteration : 350, loss : 0.1136, accuracy : 96.67\n",
            "Epoch : 142, training loss : 0.1137, training accuracy : 96.67, test loss : 0.2354, test accuracy : 93.91\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0958, accuracy : 97.30\n",
            "iteration : 100, loss : 0.1032, accuracy : 97.12\n",
            "iteration : 150, loss : 0.1059, accuracy : 96.98\n",
            "iteration : 200, loss : 0.1099, accuracy : 96.86\n",
            "iteration : 250, loss : 0.1112, accuracy : 96.84\n",
            "iteration : 300, loss : 0.1127, accuracy : 96.78\n",
            "iteration : 350, loss : 0.1117, accuracy : 96.80\n",
            "Epoch : 143, training loss : 0.1115, training accuracy : 96.80, test loss : 0.2259, test accuracy : 94.20\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.1070, accuracy : 96.86\n",
            "iteration : 100, loss : 0.1055, accuracy : 96.97\n",
            "iteration : 150, loss : 0.1099, accuracy : 96.89\n",
            "iteration : 200, loss : 0.1067, accuracy : 97.00\n",
            "iteration : 250, loss : 0.1085, accuracy : 96.97\n",
            "iteration : 300, loss : 0.1090, accuracy : 96.94\n",
            "iteration : 350, loss : 0.1089, accuracy : 96.95\n",
            "Epoch : 144, training loss : 0.1096, training accuracy : 96.93, test loss : 0.2393, test accuracy : 93.67\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.1056, accuracy : 97.16\n",
            "iteration : 100, loss : 0.1037, accuracy : 97.08\n",
            "iteration : 150, loss : 0.1074, accuracy : 96.98\n",
            "iteration : 200, loss : 0.1051, accuracy : 97.03\n",
            "iteration : 250, loss : 0.1038, accuracy : 97.06\n",
            "iteration : 300, loss : 0.1053, accuracy : 97.01\n",
            "iteration : 350, loss : 0.1080, accuracy : 96.94\n",
            "Epoch : 145, training loss : 0.1085, training accuracy : 96.93, test loss : 0.2362, test accuracy : 93.67\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.1110, accuracy : 96.86\n",
            "iteration : 100, loss : 0.1055, accuracy : 96.95\n",
            "iteration : 150, loss : 0.1037, accuracy : 97.04\n",
            "iteration : 200, loss : 0.1046, accuracy : 97.01\n",
            "iteration : 250, loss : 0.1062, accuracy : 96.98\n",
            "iteration : 300, loss : 0.1078, accuracy : 96.89\n",
            "iteration : 350, loss : 0.1087, accuracy : 96.83\n",
            "Epoch : 146, training loss : 0.1079, training accuracy : 96.84, test loss : 0.2324, test accuracy : 93.80\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0971, accuracy : 97.30\n",
            "iteration : 100, loss : 0.0971, accuracy : 97.17\n",
            "iteration : 150, loss : 0.1049, accuracy : 97.05\n",
            "iteration : 200, loss : 0.1069, accuracy : 96.99\n",
            "iteration : 250, loss : 0.1076, accuracy : 96.96\n",
            "iteration : 300, loss : 0.1092, accuracy : 96.91\n",
            "iteration : 350, loss : 0.1116, accuracy : 96.80\n",
            "Epoch : 147, training loss : 0.1119, training accuracy : 96.79, test loss : 0.2396, test accuracy : 93.70\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.1063, accuracy : 96.98\n",
            "iteration : 100, loss : 0.1046, accuracy : 97.10\n",
            "iteration : 150, loss : 0.1063, accuracy : 96.96\n",
            "iteration : 200, loss : 0.1080, accuracy : 96.93\n",
            "iteration : 250, loss : 0.1080, accuracy : 96.99\n",
            "iteration : 300, loss : 0.1070, accuracy : 96.97\n",
            "iteration : 350, loss : 0.1081, accuracy : 96.91\n",
            "Epoch : 148, training loss : 0.1077, training accuracy : 96.90, test loss : 0.2333, test accuracy : 93.88\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.1032, accuracy : 97.11\n",
            "iteration : 100, loss : 0.1030, accuracy : 97.17\n",
            "iteration : 150, loss : 0.1028, accuracy : 97.14\n",
            "iteration : 200, loss : 0.1059, accuracy : 97.07\n",
            "iteration : 250, loss : 0.1058, accuracy : 97.01\n",
            "iteration : 300, loss : 0.1068, accuracy : 96.98\n",
            "iteration : 350, loss : 0.1086, accuracy : 96.93\n",
            "Epoch : 149, training loss : 0.1083, training accuracy : 96.93, test loss : 0.2421, test accuracy : 93.81\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.1112, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1122, accuracy : 96.75\n",
            "iteration : 150, loss : 0.1058, accuracy : 96.88\n",
            "iteration : 200, loss : 0.1020, accuracy : 97.00\n",
            "iteration : 250, loss : 0.1040, accuracy : 96.93\n",
            "iteration : 300, loss : 0.1038, accuracy : 96.96\n",
            "iteration : 350, loss : 0.1065, accuracy : 96.89\n",
            "Epoch : 150, training loss : 0.1074, training accuracy : 96.86, test loss : 0.2411, test accuracy : 93.77\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0988, accuracy : 97.36\n",
            "iteration : 100, loss : 0.1036, accuracy : 97.12\n",
            "iteration : 150, loss : 0.1040, accuracy : 97.08\n",
            "iteration : 200, loss : 0.1054, accuracy : 97.06\n",
            "iteration : 250, loss : 0.1061, accuracy : 96.99\n",
            "iteration : 300, loss : 0.1054, accuracy : 97.00\n",
            "iteration : 350, loss : 0.1045, accuracy : 97.01\n",
            "Epoch : 151, training loss : 0.1046, training accuracy : 97.01, test loss : 0.2358, test accuracy : 93.75\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0897, accuracy : 97.47\n",
            "iteration : 100, loss : 0.0963, accuracy : 97.30\n",
            "iteration : 150, loss : 0.0977, accuracy : 97.18\n",
            "iteration : 200, loss : 0.1018, accuracy : 97.07\n",
            "iteration : 250, loss : 0.1019, accuracy : 97.12\n",
            "iteration : 300, loss : 0.1010, accuracy : 97.15\n",
            "iteration : 350, loss : 0.1019, accuracy : 97.09\n",
            "Epoch : 152, training loss : 0.1023, training accuracy : 97.07, test loss : 0.2317, test accuracy : 93.87\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.1093, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1062, accuracy : 97.13\n",
            "iteration : 150, loss : 0.1017, accuracy : 97.19\n",
            "iteration : 200, loss : 0.1021, accuracy : 97.12\n",
            "iteration : 250, loss : 0.1028, accuracy : 97.06\n",
            "iteration : 300, loss : 0.1035, accuracy : 97.04\n",
            "iteration : 350, loss : 0.1029, accuracy : 97.05\n",
            "Epoch : 153, training loss : 0.1031, training accuracy : 97.05, test loss : 0.2339, test accuracy : 93.89\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0996, accuracy : 97.11\n",
            "iteration : 100, loss : 0.1008, accuracy : 97.09\n",
            "iteration : 150, loss : 0.1002, accuracy : 97.11\n",
            "iteration : 200, loss : 0.1012, accuracy : 97.09\n",
            "iteration : 250, loss : 0.1030, accuracy : 97.03\n",
            "iteration : 300, loss : 0.1032, accuracy : 97.02\n",
            "iteration : 350, loss : 0.1032, accuracy : 97.00\n",
            "Epoch : 154, training loss : 0.1039, training accuracy : 96.98, test loss : 0.2515, test accuracy : 93.61\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.1008, accuracy : 97.11\n",
            "iteration : 100, loss : 0.1030, accuracy : 97.11\n",
            "iteration : 150, loss : 0.1015, accuracy : 97.11\n",
            "iteration : 200, loss : 0.1025, accuracy : 97.10\n",
            "iteration : 250, loss : 0.1046, accuracy : 97.05\n",
            "iteration : 300, loss : 0.1040, accuracy : 97.03\n",
            "iteration : 350, loss : 0.1037, accuracy : 97.05\n",
            "Epoch : 155, training loss : 0.1036, training accuracy : 97.05, test loss : 0.2433, test accuracy : 93.76\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.1026, accuracy : 97.16\n",
            "iteration : 100, loss : 0.0980, accuracy : 97.22\n",
            "iteration : 150, loss : 0.0976, accuracy : 97.31\n",
            "iteration : 200, loss : 0.0982, accuracy : 97.29\n",
            "iteration : 250, loss : 0.0980, accuracy : 97.26\n",
            "iteration : 300, loss : 0.0992, accuracy : 97.22\n",
            "iteration : 350, loss : 0.1001, accuracy : 97.20\n",
            "Epoch : 156, training loss : 0.1005, training accuracy : 97.19, test loss : 0.2378, test accuracy : 93.76\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0861, accuracy : 97.44\n",
            "iteration : 100, loss : 0.0921, accuracy : 97.31\n",
            "iteration : 150, loss : 0.0994, accuracy : 97.12\n",
            "iteration : 200, loss : 0.0984, accuracy : 97.09\n",
            "iteration : 250, loss : 0.0991, accuracy : 97.11\n",
            "iteration : 300, loss : 0.0995, accuracy : 97.14\n",
            "iteration : 350, loss : 0.1002, accuracy : 97.12\n",
            "Epoch : 157, training loss : 0.1007, training accuracy : 97.12, test loss : 0.2299, test accuracy : 93.87\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.1051, accuracy : 96.91\n",
            "iteration : 100, loss : 0.0954, accuracy : 97.16\n",
            "iteration : 150, loss : 0.1025, accuracy : 96.96\n",
            "iteration : 200, loss : 0.1036, accuracy : 96.95\n",
            "iteration : 250, loss : 0.1044, accuracy : 96.97\n",
            "iteration : 300, loss : 0.1048, accuracy : 96.99\n",
            "iteration : 350, loss : 0.1037, accuracy : 97.03\n",
            "Epoch : 158, training loss : 0.1025, training accuracy : 97.08, test loss : 0.2310, test accuracy : 94.04\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0881, accuracy : 97.44\n",
            "iteration : 100, loss : 0.0948, accuracy : 97.28\n",
            "iteration : 150, loss : 0.0953, accuracy : 97.32\n",
            "iteration : 200, loss : 0.0969, accuracy : 97.25\n",
            "iteration : 250, loss : 0.0965, accuracy : 97.32\n",
            "iteration : 300, loss : 0.0954, accuracy : 97.37\n",
            "iteration : 350, loss : 0.0949, accuracy : 97.37\n",
            "Epoch : 159, training loss : 0.0956, training accuracy : 97.33, test loss : 0.2373, test accuracy : 93.93\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0942, accuracy : 97.33\n",
            "iteration : 100, loss : 0.0935, accuracy : 97.45\n",
            "iteration : 150, loss : 0.0943, accuracy : 97.41\n",
            "iteration : 200, loss : 0.0967, accuracy : 97.27\n",
            "iteration : 250, loss : 0.0986, accuracy : 97.20\n",
            "iteration : 300, loss : 0.0967, accuracy : 97.26\n",
            "iteration : 350, loss : 0.0963, accuracy : 97.29\n",
            "Epoch : 160, training loss : 0.0963, training accuracy : 97.30, test loss : 0.2443, test accuracy : 93.85\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0984, accuracy : 97.06\n",
            "iteration : 100, loss : 0.0957, accuracy : 97.10\n",
            "iteration : 150, loss : 0.0951, accuracy : 97.23\n",
            "iteration : 200, loss : 0.0956, accuracy : 97.27\n",
            "iteration : 250, loss : 0.0980, accuracy : 97.22\n",
            "iteration : 300, loss : 0.0991, accuracy : 97.18\n",
            "iteration : 350, loss : 0.1005, accuracy : 97.17\n",
            "Epoch : 161, training loss : 0.0998, training accuracy : 97.19, test loss : 0.2394, test accuracy : 93.92\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0850, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0890, accuracy : 97.47\n",
            "iteration : 150, loss : 0.0911, accuracy : 97.33\n",
            "iteration : 200, loss : 0.0915, accuracy : 97.34\n",
            "iteration : 250, loss : 0.0934, accuracy : 97.31\n",
            "iteration : 300, loss : 0.0958, accuracy : 97.27\n",
            "iteration : 350, loss : 0.0974, accuracy : 97.25\n",
            "Epoch : 162, training loss : 0.0971, training accuracy : 97.25, test loss : 0.2494, test accuracy : 93.65\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.1015, accuracy : 97.11\n",
            "iteration : 100, loss : 0.0959, accuracy : 97.29\n",
            "iteration : 150, loss : 0.0952, accuracy : 97.24\n",
            "iteration : 200, loss : 0.0955, accuracy : 97.22\n",
            "iteration : 250, loss : 0.0947, accuracy : 97.28\n",
            "iteration : 300, loss : 0.0938, accuracy : 97.26\n",
            "iteration : 350, loss : 0.0935, accuracy : 97.29\n",
            "Epoch : 163, training loss : 0.0938, training accuracy : 97.29, test loss : 0.2466, test accuracy : 93.99\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0835, accuracy : 97.56\n",
            "iteration : 100, loss : 0.0792, accuracy : 97.73\n",
            "iteration : 150, loss : 0.0844, accuracy : 97.53\n",
            "iteration : 200, loss : 0.0867, accuracy : 97.45\n",
            "iteration : 250, loss : 0.0884, accuracy : 97.44\n",
            "iteration : 300, loss : 0.0899, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0910, accuracy : 97.36\n",
            "Epoch : 164, training loss : 0.0915, training accuracy : 97.34, test loss : 0.2478, test accuracy : 93.67\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0813, accuracy : 97.42\n",
            "iteration : 100, loss : 0.0860, accuracy : 97.48\n",
            "iteration : 150, loss : 0.0861, accuracy : 97.49\n",
            "iteration : 200, loss : 0.0859, accuracy : 97.54\n",
            "iteration : 250, loss : 0.0866, accuracy : 97.55\n",
            "iteration : 300, loss : 0.0884, accuracy : 97.49\n",
            "iteration : 350, loss : 0.0891, accuracy : 97.47\n",
            "Epoch : 165, training loss : 0.0896, training accuracy : 97.44, test loss : 0.2557, test accuracy : 93.54\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0839, accuracy : 97.55\n",
            "iteration : 100, loss : 0.0837, accuracy : 97.59\n",
            "iteration : 150, loss : 0.0845, accuracy : 97.58\n",
            "iteration : 200, loss : 0.0868, accuracy : 97.59\n",
            "iteration : 250, loss : 0.0892, accuracy : 97.52\n",
            "iteration : 300, loss : 0.0900, accuracy : 97.47\n",
            "iteration : 350, loss : 0.0913, accuracy : 97.44\n",
            "Epoch : 166, training loss : 0.0917, training accuracy : 97.44, test loss : 0.2409, test accuracy : 93.90\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0880, accuracy : 97.44\n",
            "iteration : 100, loss : 0.0887, accuracy : 97.44\n",
            "iteration : 150, loss : 0.0884, accuracy : 97.48\n",
            "iteration : 200, loss : 0.0876, accuracy : 97.50\n",
            "iteration : 250, loss : 0.0876, accuracy : 97.48\n",
            "iteration : 300, loss : 0.0904, accuracy : 97.41\n",
            "iteration : 350, loss : 0.0900, accuracy : 97.42\n",
            "Epoch : 167, training loss : 0.0903, training accuracy : 97.41, test loss : 0.2434, test accuracy : 94.02\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0910, accuracy : 97.36\n",
            "iteration : 100, loss : 0.0881, accuracy : 97.43\n",
            "iteration : 150, loss : 0.0889, accuracy : 97.41\n",
            "iteration : 200, loss : 0.0885, accuracy : 97.49\n",
            "iteration : 250, loss : 0.0868, accuracy : 97.53\n",
            "iteration : 300, loss : 0.0876, accuracy : 97.51\n",
            "iteration : 350, loss : 0.0896, accuracy : 97.44\n",
            "Epoch : 168, training loss : 0.0906, training accuracy : 97.43, test loss : 0.2449, test accuracy : 94.06\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0882, accuracy : 97.55\n",
            "iteration : 100, loss : 0.0893, accuracy : 97.45\n",
            "iteration : 150, loss : 0.0887, accuracy : 97.51\n",
            "iteration : 200, loss : 0.0886, accuracy : 97.52\n",
            "iteration : 250, loss : 0.0905, accuracy : 97.43\n",
            "iteration : 300, loss : 0.0902, accuracy : 97.44\n",
            "iteration : 350, loss : 0.0890, accuracy : 97.48\n",
            "Epoch : 169, training loss : 0.0890, training accuracy : 97.47, test loss : 0.2545, test accuracy : 93.71\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0792, accuracy : 97.73\n",
            "iteration : 100, loss : 0.0805, accuracy : 97.67\n",
            "iteration : 150, loss : 0.0851, accuracy : 97.53\n",
            "iteration : 200, loss : 0.0855, accuracy : 97.48\n",
            "iteration : 250, loss : 0.0871, accuracy : 97.43\n",
            "iteration : 300, loss : 0.0862, accuracy : 97.46\n",
            "iteration : 350, loss : 0.0858, accuracy : 97.47\n",
            "Epoch : 170, training loss : 0.0871, training accuracy : 97.45, test loss : 0.2469, test accuracy : 93.80\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0820, accuracy : 97.80\n",
            "iteration : 100, loss : 0.0799, accuracy : 97.77\n",
            "iteration : 150, loss : 0.0808, accuracy : 97.68\n",
            "iteration : 200, loss : 0.0808, accuracy : 97.62\n",
            "iteration : 250, loss : 0.0823, accuracy : 97.62\n",
            "iteration : 300, loss : 0.0825, accuracy : 97.62\n",
            "iteration : 350, loss : 0.0852, accuracy : 97.58\n",
            "Epoch : 171, training loss : 0.0855, training accuracy : 97.56, test loss : 0.2548, test accuracy : 93.59\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0941, accuracy : 97.34\n",
            "iteration : 100, loss : 0.0918, accuracy : 97.39\n",
            "iteration : 150, loss : 0.0871, accuracy : 97.51\n",
            "iteration : 200, loss : 0.0838, accuracy : 97.62\n",
            "iteration : 250, loss : 0.0833, accuracy : 97.61\n",
            "iteration : 300, loss : 0.0846, accuracy : 97.59\n",
            "iteration : 350, loss : 0.0844, accuracy : 97.58\n",
            "Epoch : 172, training loss : 0.0846, training accuracy : 97.58, test loss : 0.2426, test accuracy : 93.92\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0775, accuracy : 97.81\n",
            "iteration : 100, loss : 0.0768, accuracy : 97.83\n",
            "iteration : 150, loss : 0.0775, accuracy : 97.74\n",
            "iteration : 200, loss : 0.0771, accuracy : 97.77\n",
            "iteration : 250, loss : 0.0779, accuracy : 97.73\n",
            "iteration : 300, loss : 0.0802, accuracy : 97.68\n",
            "iteration : 350, loss : 0.0821, accuracy : 97.63\n",
            "Epoch : 173, training loss : 0.0828, training accuracy : 97.61, test loss : 0.2453, test accuracy : 94.03\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0729, accuracy : 97.88\n",
            "iteration : 100, loss : 0.0815, accuracy : 97.70\n",
            "iteration : 150, loss : 0.0799, accuracy : 97.78\n",
            "iteration : 200, loss : 0.0807, accuracy : 97.72\n",
            "iteration : 250, loss : 0.0808, accuracy : 97.75\n",
            "iteration : 300, loss : 0.0829, accuracy : 97.69\n",
            "iteration : 350, loss : 0.0832, accuracy : 97.68\n",
            "Epoch : 174, training loss : 0.0832, training accuracy : 97.66, test loss : 0.2514, test accuracy : 93.82\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0745, accuracy : 97.77\n",
            "iteration : 100, loss : 0.0749, accuracy : 97.71\n",
            "iteration : 150, loss : 0.0784, accuracy : 97.66\n",
            "iteration : 200, loss : 0.0788, accuracy : 97.67\n",
            "iteration : 250, loss : 0.0825, accuracy : 97.58\n",
            "iteration : 300, loss : 0.0836, accuracy : 97.58\n",
            "iteration : 350, loss : 0.0840, accuracy : 97.54\n",
            "Epoch : 175, training loss : 0.0844, training accuracy : 97.54, test loss : 0.2454, test accuracy : 94.02\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0844, accuracy : 97.56\n",
            "iteration : 100, loss : 0.0776, accuracy : 97.80\n",
            "iteration : 150, loss : 0.0759, accuracy : 97.86\n",
            "iteration : 200, loss : 0.0780, accuracy : 97.80\n",
            "iteration : 250, loss : 0.0790, accuracy : 97.76\n",
            "iteration : 300, loss : 0.0789, accuracy : 97.73\n",
            "iteration : 350, loss : 0.0788, accuracy : 97.71\n",
            "Epoch : 176, training loss : 0.0790, training accuracy : 97.71, test loss : 0.2496, test accuracy : 93.91\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0757, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0784, accuracy : 97.63\n",
            "iteration : 150, loss : 0.0807, accuracy : 97.61\n",
            "iteration : 200, loss : 0.0817, accuracy : 97.62\n",
            "iteration : 250, loss : 0.0800, accuracy : 97.68\n",
            "iteration : 300, loss : 0.0806, accuracy : 97.68\n",
            "iteration : 350, loss : 0.0807, accuracy : 97.66\n",
            "Epoch : 177, training loss : 0.0804, training accuracy : 97.66, test loss : 0.2490, test accuracy : 93.97\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0782, accuracy : 97.84\n",
            "iteration : 100, loss : 0.0744, accuracy : 97.94\n",
            "iteration : 150, loss : 0.0733, accuracy : 97.92\n",
            "iteration : 200, loss : 0.0763, accuracy : 97.79\n",
            "iteration : 250, loss : 0.0804, accuracy : 97.67\n",
            "iteration : 300, loss : 0.0806, accuracy : 97.66\n",
            "iteration : 350, loss : 0.0807, accuracy : 97.68\n",
            "Epoch : 178, training loss : 0.0809, training accuracy : 97.65, test loss : 0.2479, test accuracy : 93.91\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0663, accuracy : 98.02\n",
            "iteration : 100, loss : 0.0701, accuracy : 98.00\n",
            "iteration : 150, loss : 0.0744, accuracy : 97.89\n",
            "iteration : 200, loss : 0.0760, accuracy : 97.84\n",
            "iteration : 250, loss : 0.0762, accuracy : 97.84\n",
            "iteration : 300, loss : 0.0757, accuracy : 97.87\n",
            "iteration : 350, loss : 0.0765, accuracy : 97.81\n",
            "Epoch : 179, training loss : 0.0781, training accuracy : 97.77, test loss : 0.2526, test accuracy : 93.77\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0733, accuracy : 97.72\n",
            "iteration : 100, loss : 0.0723, accuracy : 97.84\n",
            "iteration : 150, loss : 0.0697, accuracy : 97.92\n",
            "iteration : 200, loss : 0.0724, accuracy : 97.89\n",
            "iteration : 250, loss : 0.0744, accuracy : 97.85\n",
            "iteration : 300, loss : 0.0737, accuracy : 97.90\n",
            "iteration : 350, loss : 0.0745, accuracy : 97.88\n",
            "Epoch : 180, training loss : 0.0745, training accuracy : 97.88, test loss : 0.2464, test accuracy : 94.09\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0684, accuracy : 98.00\n",
            "iteration : 100, loss : 0.0683, accuracy : 98.03\n",
            "iteration : 150, loss : 0.0709, accuracy : 97.97\n",
            "iteration : 200, loss : 0.0714, accuracy : 97.97\n",
            "iteration : 250, loss : 0.0711, accuracy : 97.97\n",
            "iteration : 300, loss : 0.0737, accuracy : 97.91\n",
            "iteration : 350, loss : 0.0754, accuracy : 97.86\n",
            "Epoch : 181, training loss : 0.0757, training accuracy : 97.85, test loss : 0.2529, test accuracy : 93.79\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0855, accuracy : 97.77\n",
            "iteration : 100, loss : 0.0795, accuracy : 97.77\n",
            "iteration : 150, loss : 0.0807, accuracy : 97.72\n",
            "iteration : 200, loss : 0.0794, accuracy : 97.73\n",
            "iteration : 250, loss : 0.0784, accuracy : 97.78\n",
            "iteration : 300, loss : 0.0780, accuracy : 97.80\n",
            "iteration : 350, loss : 0.0779, accuracy : 97.80\n",
            "Epoch : 182, training loss : 0.0778, training accuracy : 97.81, test loss : 0.2446, test accuracy : 93.97\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0673, accuracy : 98.16\n",
            "iteration : 100, loss : 0.0690, accuracy : 98.00\n",
            "iteration : 150, loss : 0.0728, accuracy : 97.91\n",
            "iteration : 200, loss : 0.0736, accuracy : 97.91\n",
            "iteration : 250, loss : 0.0740, accuracy : 97.91\n",
            "iteration : 300, loss : 0.0733, accuracy : 97.92\n",
            "iteration : 350, loss : 0.0731, accuracy : 97.90\n",
            "Epoch : 183, training loss : 0.0732, training accuracy : 97.89, test loss : 0.2410, test accuracy : 94.10\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0718, accuracy : 97.92\n",
            "iteration : 100, loss : 0.0717, accuracy : 97.95\n",
            "iteration : 150, loss : 0.0713, accuracy : 98.01\n",
            "iteration : 200, loss : 0.0710, accuracy : 98.03\n",
            "iteration : 250, loss : 0.0719, accuracy : 98.01\n",
            "iteration : 300, loss : 0.0728, accuracy : 97.92\n",
            "iteration : 350, loss : 0.0743, accuracy : 97.85\n",
            "Epoch : 184, training loss : 0.0746, training accuracy : 97.84, test loss : 0.2478, test accuracy : 94.15\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0620, accuracy : 98.39\n",
            "iteration : 100, loss : 0.0689, accuracy : 98.13\n",
            "iteration : 150, loss : 0.0708, accuracy : 98.03\n",
            "iteration : 200, loss : 0.0721, accuracy : 97.97\n",
            "iteration : 250, loss : 0.0722, accuracy : 97.98\n",
            "iteration : 300, loss : 0.0735, accuracy : 97.91\n",
            "iteration : 350, loss : 0.0742, accuracy : 97.88\n",
            "Epoch : 185, training loss : 0.0743, training accuracy : 97.87, test loss : 0.2516, test accuracy : 93.89\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0688, accuracy : 98.06\n",
            "iteration : 100, loss : 0.0676, accuracy : 98.04\n",
            "iteration : 150, loss : 0.0685, accuracy : 98.01\n",
            "iteration : 200, loss : 0.0694, accuracy : 97.98\n",
            "iteration : 250, loss : 0.0710, accuracy : 97.92\n",
            "iteration : 300, loss : 0.0716, accuracy : 97.94\n",
            "iteration : 350, loss : 0.0716, accuracy : 97.93\n",
            "Epoch : 186, training loss : 0.0720, training accuracy : 97.92, test loss : 0.2530, test accuracy : 93.92\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0580, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0651, accuracy : 98.18\n",
            "iteration : 150, loss : 0.0651, accuracy : 98.15\n",
            "iteration : 200, loss : 0.0671, accuracy : 98.10\n",
            "iteration : 250, loss : 0.0681, accuracy : 98.07\n",
            "iteration : 300, loss : 0.0697, accuracy : 98.02\n",
            "iteration : 350, loss : 0.0688, accuracy : 98.03\n",
            "Epoch : 187, training loss : 0.0695, training accuracy : 98.01, test loss : 0.2538, test accuracy : 93.89\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0633, accuracy : 97.89\n",
            "iteration : 100, loss : 0.0676, accuracy : 97.91\n",
            "iteration : 150, loss : 0.0689, accuracy : 97.96\n",
            "iteration : 200, loss : 0.0674, accuracy : 97.98\n",
            "iteration : 250, loss : 0.0684, accuracy : 97.95\n",
            "iteration : 300, loss : 0.0690, accuracy : 97.93\n",
            "iteration : 350, loss : 0.0694, accuracy : 97.93\n",
            "Epoch : 188, training loss : 0.0700, training accuracy : 97.93, test loss : 0.2470, test accuracy : 94.26\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0638, accuracy : 98.16\n",
            "iteration : 100, loss : 0.0662, accuracy : 98.12\n",
            "iteration : 150, loss : 0.0639, accuracy : 98.16\n",
            "iteration : 200, loss : 0.0664, accuracy : 98.04\n",
            "iteration : 250, loss : 0.0677, accuracy : 97.99\n",
            "iteration : 300, loss : 0.0683, accuracy : 98.01\n",
            "iteration : 350, loss : 0.0695, accuracy : 97.97\n",
            "Epoch : 189, training loss : 0.0697, training accuracy : 97.97, test loss : 0.2586, test accuracy : 93.81\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0587, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0583, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0589, accuracy : 98.34\n",
            "iteration : 200, loss : 0.0611, accuracy : 98.28\n",
            "iteration : 250, loss : 0.0629, accuracy : 98.23\n",
            "iteration : 300, loss : 0.0658, accuracy : 98.12\n",
            "iteration : 350, loss : 0.0683, accuracy : 98.07\n",
            "Epoch : 190, training loss : 0.0680, training accuracy : 98.08, test loss : 0.2538, test accuracy : 93.88\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0593, accuracy : 98.25\n",
            "iteration : 100, loss : 0.0618, accuracy : 98.18\n",
            "iteration : 150, loss : 0.0630, accuracy : 98.20\n",
            "iteration : 200, loss : 0.0646, accuracy : 98.11\n",
            "iteration : 250, loss : 0.0649, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0653, accuracy : 98.11\n",
            "iteration : 350, loss : 0.0665, accuracy : 98.05\n",
            "Epoch : 191, training loss : 0.0665, training accuracy : 98.05, test loss : 0.2551, test accuracy : 94.02\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0629, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0622, accuracy : 98.26\n",
            "iteration : 150, loss : 0.0614, accuracy : 98.26\n",
            "iteration : 200, loss : 0.0642, accuracy : 98.16\n",
            "iteration : 250, loss : 0.0658, accuracy : 98.08\n",
            "iteration : 300, loss : 0.0643, accuracy : 98.14\n",
            "iteration : 350, loss : 0.0662, accuracy : 98.08\n",
            "Epoch : 192, training loss : 0.0663, training accuracy : 98.08, test loss : 0.2525, test accuracy : 94.08\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0549, accuracy : 98.30\n",
            "iteration : 100, loss : 0.0566, accuracy : 98.26\n",
            "iteration : 150, loss : 0.0616, accuracy : 98.16\n",
            "iteration : 200, loss : 0.0614, accuracy : 98.17\n",
            "iteration : 250, loss : 0.0628, accuracy : 98.13\n",
            "iteration : 300, loss : 0.0640, accuracy : 98.09\n",
            "iteration : 350, loss : 0.0636, accuracy : 98.14\n",
            "Epoch : 193, training loss : 0.0635, training accuracy : 98.15, test loss : 0.2513, test accuracy : 94.15\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0553, accuracy : 98.33\n",
            "iteration : 100, loss : 0.0571, accuracy : 98.33\n",
            "iteration : 150, loss : 0.0602, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0604, accuracy : 98.26\n",
            "iteration : 250, loss : 0.0626, accuracy : 98.21\n",
            "iteration : 300, loss : 0.0631, accuracy : 98.18\n",
            "iteration : 350, loss : 0.0629, accuracy : 98.19\n",
            "Epoch : 194, training loss : 0.0632, training accuracy : 98.19, test loss : 0.2571, test accuracy : 94.10\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0655, accuracy : 98.14\n",
            "iteration : 100, loss : 0.0590, accuracy : 98.26\n",
            "iteration : 150, loss : 0.0583, accuracy : 98.33\n",
            "iteration : 200, loss : 0.0601, accuracy : 98.28\n",
            "iteration : 250, loss : 0.0608, accuracy : 98.25\n",
            "iteration : 300, loss : 0.0604, accuracy : 98.23\n",
            "iteration : 350, loss : 0.0623, accuracy : 98.19\n",
            "Epoch : 195, training loss : 0.0625, training accuracy : 98.19, test loss : 0.2602, test accuracy : 93.85\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0563, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0572, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0604, accuracy : 98.34\n",
            "iteration : 200, loss : 0.0600, accuracy : 98.34\n",
            "iteration : 250, loss : 0.0621, accuracy : 98.24\n",
            "iteration : 300, loss : 0.0632, accuracy : 98.22\n",
            "iteration : 350, loss : 0.0619, accuracy : 98.24\n",
            "Epoch : 196, training loss : 0.0616, training accuracy : 98.24, test loss : 0.2536, test accuracy : 94.03\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0547, accuracy : 98.42\n",
            "iteration : 100, loss : 0.0569, accuracy : 98.37\n",
            "iteration : 150, loss : 0.0611, accuracy : 98.20\n",
            "iteration : 200, loss : 0.0607, accuracy : 98.20\n",
            "iteration : 250, loss : 0.0604, accuracy : 98.19\n",
            "iteration : 300, loss : 0.0604, accuracy : 98.19\n",
            "iteration : 350, loss : 0.0612, accuracy : 98.18\n",
            "Epoch : 197, training loss : 0.0612, training accuracy : 98.18, test loss : 0.2597, test accuracy : 93.92\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0586, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0590, accuracy : 98.35\n",
            "iteration : 150, loss : 0.0590, accuracy : 98.38\n",
            "iteration : 200, loss : 0.0565, accuracy : 98.43\n",
            "iteration : 250, loss : 0.0592, accuracy : 98.33\n",
            "iteration : 300, loss : 0.0607, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0601, accuracy : 98.29\n",
            "Epoch : 198, training loss : 0.0604, training accuracy : 98.28, test loss : 0.2626, test accuracy : 93.87\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0527, accuracy : 98.58\n",
            "iteration : 100, loss : 0.0514, accuracy : 98.52\n",
            "iteration : 150, loss : 0.0539, accuracy : 98.39\n",
            "iteration : 200, loss : 0.0561, accuracy : 98.31\n",
            "iteration : 250, loss : 0.0565, accuracy : 98.35\n",
            "iteration : 300, loss : 0.0575, accuracy : 98.31\n",
            "iteration : 350, loss : 0.0588, accuracy : 98.27\n",
            "Epoch : 199, training loss : 0.0594, training accuracy : 98.27, test loss : 0.2576, test accuracy : 94.13\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0565, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0533, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0565, accuracy : 98.30\n",
            "iteration : 200, loss : 0.0597, accuracy : 98.21\n",
            "iteration : 250, loss : 0.0606, accuracy : 98.19\n",
            "iteration : 300, loss : 0.0595, accuracy : 98.23\n",
            "iteration : 350, loss : 0.0589, accuracy : 98.24\n",
            "Epoch : 200, training loss : 0.0588, training accuracy : 98.25, test loss : 0.2537, test accuracy : 94.14\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0508, accuracy : 98.59\n",
            "iteration : 100, loss : 0.0498, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0515, accuracy : 98.54\n",
            "iteration : 200, loss : 0.0515, accuracy : 98.54\n",
            "iteration : 250, loss : 0.0529, accuracy : 98.48\n",
            "iteration : 300, loss : 0.0533, accuracy : 98.47\n",
            "iteration : 350, loss : 0.0541, accuracy : 98.47\n",
            "Epoch : 201, training loss : 0.0547, training accuracy : 98.44, test loss : 0.2684, test accuracy : 93.94\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0560, accuracy : 98.52\n",
            "iteration : 100, loss : 0.0543, accuracy : 98.53\n",
            "iteration : 150, loss : 0.0527, accuracy : 98.55\n",
            "iteration : 200, loss : 0.0533, accuracy : 98.52\n",
            "iteration : 250, loss : 0.0547, accuracy : 98.50\n",
            "iteration : 300, loss : 0.0558, accuracy : 98.46\n",
            "iteration : 350, loss : 0.0564, accuracy : 98.42\n",
            "Epoch : 202, training loss : 0.0561, training accuracy : 98.42, test loss : 0.2604, test accuracy : 94.08\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0535, accuracy : 98.30\n",
            "iteration : 100, loss : 0.0518, accuracy : 98.46\n",
            "iteration : 150, loss : 0.0547, accuracy : 98.37\n",
            "iteration : 200, loss : 0.0552, accuracy : 98.36\n",
            "iteration : 250, loss : 0.0557, accuracy : 98.34\n",
            "iteration : 300, loss : 0.0556, accuracy : 98.32\n",
            "iteration : 350, loss : 0.0569, accuracy : 98.29\n",
            "Epoch : 203, training loss : 0.0565, training accuracy : 98.31, test loss : 0.2634, test accuracy : 94.08\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0582, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0544, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0552, accuracy : 98.38\n",
            "iteration : 200, loss : 0.0542, accuracy : 98.43\n",
            "iteration : 250, loss : 0.0545, accuracy : 98.44\n",
            "iteration : 300, loss : 0.0548, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0552, accuracy : 98.40\n",
            "Epoch : 204, training loss : 0.0556, training accuracy : 98.37, test loss : 0.2612, test accuracy : 94.04\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0456, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0512, accuracy : 98.52\n",
            "iteration : 150, loss : 0.0500, accuracy : 98.55\n",
            "iteration : 200, loss : 0.0496, accuracy : 98.59\n",
            "iteration : 250, loss : 0.0500, accuracy : 98.57\n",
            "iteration : 300, loss : 0.0498, accuracy : 98.60\n",
            "iteration : 350, loss : 0.0509, accuracy : 98.56\n",
            "Epoch : 205, training loss : 0.0513, training accuracy : 98.54, test loss : 0.2759, test accuracy : 93.86\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0527, accuracy : 98.53\n",
            "iteration : 100, loss : 0.0481, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0498, accuracy : 98.62\n",
            "iteration : 200, loss : 0.0513, accuracy : 98.57\n",
            "iteration : 250, loss : 0.0500, accuracy : 98.62\n",
            "iteration : 300, loss : 0.0508, accuracy : 98.61\n",
            "iteration : 350, loss : 0.0518, accuracy : 98.54\n",
            "Epoch : 206, training loss : 0.0522, training accuracy : 98.52, test loss : 0.2699, test accuracy : 94.03\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0545, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0482, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0515, accuracy : 98.57\n",
            "iteration : 200, loss : 0.0522, accuracy : 98.52\n",
            "iteration : 250, loss : 0.0517, accuracy : 98.53\n",
            "iteration : 300, loss : 0.0520, accuracy : 98.53\n",
            "iteration : 350, loss : 0.0513, accuracy : 98.52\n",
            "Epoch : 207, training loss : 0.0516, training accuracy : 98.52, test loss : 0.2690, test accuracy : 94.00\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0482, accuracy : 98.55\n",
            "iteration : 100, loss : 0.0455, accuracy : 98.70\n",
            "iteration : 150, loss : 0.0458, accuracy : 98.70\n",
            "iteration : 200, loss : 0.0493, accuracy : 98.59\n",
            "iteration : 250, loss : 0.0500, accuracy : 98.57\n",
            "iteration : 300, loss : 0.0496, accuracy : 98.59\n",
            "iteration : 350, loss : 0.0493, accuracy : 98.62\n",
            "Epoch : 208, training loss : 0.0495, training accuracy : 98.62, test loss : 0.2612, test accuracy : 94.19\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0481, accuracy : 98.55\n",
            "iteration : 100, loss : 0.0446, accuracy : 98.72\n",
            "iteration : 150, loss : 0.0466, accuracy : 98.71\n",
            "iteration : 200, loss : 0.0469, accuracy : 98.73\n",
            "iteration : 250, loss : 0.0497, accuracy : 98.64\n",
            "iteration : 300, loss : 0.0509, accuracy : 98.57\n",
            "iteration : 350, loss : 0.0511, accuracy : 98.56\n",
            "Epoch : 209, training loss : 0.0512, training accuracy : 98.54, test loss : 0.2696, test accuracy : 94.02\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0479, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0449, accuracy : 98.71\n",
            "iteration : 150, loss : 0.0459, accuracy : 98.69\n",
            "iteration : 200, loss : 0.0459, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0461, accuracy : 98.73\n",
            "iteration : 300, loss : 0.0450, accuracy : 98.76\n",
            "iteration : 350, loss : 0.0473, accuracy : 98.69\n",
            "Epoch : 210, training loss : 0.0482, training accuracy : 98.67, test loss : 0.2699, test accuracy : 94.09\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0422, accuracy : 98.61\n",
            "iteration : 100, loss : 0.0447, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0469, accuracy : 98.52\n",
            "iteration : 200, loss : 0.0475, accuracy : 98.58\n",
            "iteration : 250, loss : 0.0472, accuracy : 98.59\n",
            "iteration : 300, loss : 0.0468, accuracy : 98.58\n",
            "iteration : 350, loss : 0.0473, accuracy : 98.58\n",
            "Epoch : 211, training loss : 0.0471, training accuracy : 98.60, test loss : 0.2700, test accuracy : 94.22\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0463, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0455, accuracy : 98.62\n",
            "iteration : 150, loss : 0.0473, accuracy : 98.56\n",
            "iteration : 200, loss : 0.0455, accuracy : 98.61\n",
            "iteration : 250, loss : 0.0452, accuracy : 98.66\n",
            "iteration : 300, loss : 0.0458, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0460, accuracy : 98.67\n",
            "Epoch : 212, training loss : 0.0464, training accuracy : 98.65, test loss : 0.2781, test accuracy : 93.99\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0424, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0433, accuracy : 98.73\n",
            "iteration : 150, loss : 0.0418, accuracy : 98.79\n",
            "iteration : 200, loss : 0.0437, accuracy : 98.77\n",
            "iteration : 250, loss : 0.0437, accuracy : 98.75\n",
            "iteration : 300, loss : 0.0438, accuracy : 98.76\n",
            "iteration : 350, loss : 0.0439, accuracy : 98.75\n",
            "Epoch : 213, training loss : 0.0442, training accuracy : 98.75, test loss : 0.2692, test accuracy : 94.06\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0467, accuracy : 98.77\n",
            "iteration : 100, loss : 0.0412, accuracy : 98.90\n",
            "iteration : 150, loss : 0.0398, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0415, accuracy : 98.81\n",
            "iteration : 250, loss : 0.0437, accuracy : 98.74\n",
            "iteration : 300, loss : 0.0452, accuracy : 98.71\n",
            "iteration : 350, loss : 0.0456, accuracy : 98.69\n",
            "Epoch : 214, training loss : 0.0452, training accuracy : 98.69, test loss : 0.2656, test accuracy : 94.28\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0389, accuracy : 98.95\n",
            "iteration : 100, loss : 0.0392, accuracy : 98.87\n",
            "iteration : 150, loss : 0.0403, accuracy : 98.89\n",
            "iteration : 200, loss : 0.0408, accuracy : 98.86\n",
            "iteration : 250, loss : 0.0423, accuracy : 98.78\n",
            "iteration : 300, loss : 0.0427, accuracy : 98.76\n",
            "iteration : 350, loss : 0.0433, accuracy : 98.73\n",
            "Epoch : 215, training loss : 0.0440, training accuracy : 98.70, test loss : 0.2779, test accuracy : 94.00\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0417, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0423, accuracy : 98.72\n",
            "iteration : 150, loss : 0.0423, accuracy : 98.74\n",
            "iteration : 200, loss : 0.0423, accuracy : 98.78\n",
            "iteration : 250, loss : 0.0429, accuracy : 98.74\n",
            "iteration : 300, loss : 0.0427, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0425, accuracy : 98.75\n",
            "Epoch : 216, training loss : 0.0427, training accuracy : 98.75, test loss : 0.2727, test accuracy : 93.97\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0451, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0412, accuracy : 98.85\n",
            "iteration : 150, loss : 0.0424, accuracy : 98.78\n",
            "iteration : 200, loss : 0.0403, accuracy : 98.88\n",
            "iteration : 250, loss : 0.0417, accuracy : 98.80\n",
            "iteration : 300, loss : 0.0407, accuracy : 98.83\n",
            "iteration : 350, loss : 0.0421, accuracy : 98.78\n",
            "Epoch : 217, training loss : 0.0426, training accuracy : 98.77, test loss : 0.2676, test accuracy : 94.28\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0415, accuracy : 98.75\n",
            "iteration : 100, loss : 0.0435, accuracy : 98.72\n",
            "iteration : 150, loss : 0.0429, accuracy : 98.73\n",
            "iteration : 200, loss : 0.0451, accuracy : 98.68\n",
            "iteration : 250, loss : 0.0442, accuracy : 98.68\n",
            "iteration : 300, loss : 0.0429, accuracy : 98.72\n",
            "iteration : 350, loss : 0.0434, accuracy : 98.67\n",
            "Epoch : 218, training loss : 0.0438, training accuracy : 98.67, test loss : 0.2811, test accuracy : 93.98\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0393, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0385, accuracy : 98.91\n",
            "iteration : 150, loss : 0.0383, accuracy : 98.91\n",
            "iteration : 200, loss : 0.0385, accuracy : 98.92\n",
            "iteration : 250, loss : 0.0370, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0383, accuracy : 98.92\n",
            "iteration : 350, loss : 0.0392, accuracy : 98.91\n",
            "Epoch : 219, training loss : 0.0394, training accuracy : 98.90, test loss : 0.2740, test accuracy : 94.23\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0361, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0347, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0362, accuracy : 98.93\n",
            "iteration : 200, loss : 0.0353, accuracy : 98.95\n",
            "iteration : 250, loss : 0.0381, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0388, accuracy : 98.84\n",
            "iteration : 350, loss : 0.0400, accuracy : 98.80\n",
            "Epoch : 220, training loss : 0.0397, training accuracy : 98.80, test loss : 0.2791, test accuracy : 94.24\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0340, accuracy : 98.88\n",
            "iteration : 100, loss : 0.0345, accuracy : 98.89\n",
            "iteration : 150, loss : 0.0371, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0373, accuracy : 98.84\n",
            "iteration : 250, loss : 0.0409, accuracy : 98.76\n",
            "iteration : 300, loss : 0.0412, accuracy : 98.78\n",
            "iteration : 350, loss : 0.0415, accuracy : 98.75\n",
            "Epoch : 221, training loss : 0.0409, training accuracy : 98.78, test loss : 0.2764, test accuracy : 94.06\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0369, accuracy : 98.88\n",
            "iteration : 100, loss : 0.0375, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0381, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0371, accuracy : 98.90\n",
            "iteration : 250, loss : 0.0373, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0378, accuracy : 98.86\n",
            "iteration : 350, loss : 0.0384, accuracy : 98.85\n",
            "Epoch : 222, training loss : 0.0384, training accuracy : 98.85, test loss : 0.2755, test accuracy : 94.13\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0384, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0363, accuracy : 98.95\n",
            "iteration : 150, loss : 0.0371, accuracy : 98.91\n",
            "iteration : 200, loss : 0.0370, accuracy : 98.92\n",
            "iteration : 250, loss : 0.0382, accuracy : 98.90\n",
            "iteration : 300, loss : 0.0382, accuracy : 98.90\n",
            "iteration : 350, loss : 0.0379, accuracy : 98.88\n",
            "Epoch : 223, training loss : 0.0380, training accuracy : 98.87, test loss : 0.2721, test accuracy : 94.19\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0374, accuracy : 98.92\n",
            "iteration : 100, loss : 0.0368, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0349, accuracy : 98.93\n",
            "iteration : 200, loss : 0.0352, accuracy : 98.96\n",
            "iteration : 250, loss : 0.0353, accuracy : 98.94\n",
            "iteration : 300, loss : 0.0353, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0359, accuracy : 98.95\n",
            "Epoch : 224, training loss : 0.0360, training accuracy : 98.94, test loss : 0.2717, test accuracy : 94.49\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0322, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0336, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0369, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0362, accuracy : 98.97\n",
            "iteration : 250, loss : 0.0359, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0361, accuracy : 98.96\n",
            "iteration : 350, loss : 0.0352, accuracy : 98.97\n",
            "Epoch : 225, training loss : 0.0348, training accuracy : 98.98, test loss : 0.2938, test accuracy : 93.99\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0380, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0368, accuracy : 99.00\n",
            "iteration : 150, loss : 0.0360, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0357, accuracy : 99.03\n",
            "iteration : 250, loss : 0.0354, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0359, accuracy : 98.98\n",
            "iteration : 350, loss : 0.0356, accuracy : 98.96\n",
            "Epoch : 226, training loss : 0.0352, training accuracy : 98.98, test loss : 0.2742, test accuracy : 94.29\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0371, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0340, accuracy : 99.08\n",
            "iteration : 150, loss : 0.0312, accuracy : 99.15\n",
            "iteration : 200, loss : 0.0323, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0319, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0327, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0330, accuracy : 99.08\n",
            "Epoch : 227, training loss : 0.0328, training accuracy : 99.07, test loss : 0.2835, test accuracy : 94.03\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0328, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0334, accuracy : 98.99\n",
            "iteration : 150, loss : 0.0320, accuracy : 99.07\n",
            "iteration : 200, loss : 0.0321, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0317, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0325, accuracy : 99.08\n",
            "iteration : 350, loss : 0.0326, accuracy : 99.06\n",
            "Epoch : 228, training loss : 0.0327, training accuracy : 99.07, test loss : 0.2800, test accuracy : 94.39\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0311, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0305, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0327, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0332, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0340, accuracy : 98.99\n",
            "iteration : 300, loss : 0.0358, accuracy : 98.96\n",
            "iteration : 350, loss : 0.0354, accuracy : 98.96\n",
            "Epoch : 229, training loss : 0.0357, training accuracy : 98.96, test loss : 0.2747, test accuracy : 94.20\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0288, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0294, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0302, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0311, accuracy : 99.12\n",
            "iteration : 250, loss : 0.0309, accuracy : 99.10\n",
            "iteration : 300, loss : 0.0329, accuracy : 99.05\n",
            "iteration : 350, loss : 0.0331, accuracy : 99.05\n",
            "Epoch : 230, training loss : 0.0334, training accuracy : 99.05, test loss : 0.2820, test accuracy : 94.18\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0314, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0316, accuracy : 99.00\n",
            "iteration : 150, loss : 0.0311, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0319, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0314, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0307, accuracy : 99.11\n",
            "iteration : 350, loss : 0.0309, accuracy : 99.09\n",
            "Epoch : 231, training loss : 0.0312, training accuracy : 99.09, test loss : 0.2871, test accuracy : 94.11\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0317, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0317, accuracy : 99.03\n",
            "iteration : 150, loss : 0.0325, accuracy : 99.02\n",
            "iteration : 200, loss : 0.0314, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0307, accuracy : 99.07\n",
            "iteration : 300, loss : 0.0299, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0303, accuracy : 99.08\n",
            "Epoch : 232, training loss : 0.0304, training accuracy : 99.09, test loss : 0.2875, test accuracy : 94.07\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0366, accuracy : 98.89\n",
            "iteration : 100, loss : 0.0353, accuracy : 98.97\n",
            "iteration : 150, loss : 0.0330, accuracy : 99.01\n",
            "iteration : 200, loss : 0.0333, accuracy : 98.99\n",
            "iteration : 250, loss : 0.0333, accuracy : 98.99\n",
            "iteration : 300, loss : 0.0330, accuracy : 99.02\n",
            "iteration : 350, loss : 0.0328, accuracy : 99.01\n",
            "Epoch : 233, training loss : 0.0324, training accuracy : 99.03, test loss : 0.2798, test accuracy : 94.23\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0268, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0302, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0286, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0284, accuracy : 99.16\n",
            "iteration : 250, loss : 0.0282, accuracy : 99.18\n",
            "iteration : 300, loss : 0.0284, accuracy : 99.18\n",
            "iteration : 350, loss : 0.0284, accuracy : 99.19\n",
            "Epoch : 234, training loss : 0.0290, training accuracy : 99.17, test loss : 0.2877, test accuracy : 94.38\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0262, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0277, accuracy : 99.18\n",
            "iteration : 150, loss : 0.0277, accuracy : 99.20\n",
            "iteration : 200, loss : 0.0274, accuracy : 99.20\n",
            "iteration : 250, loss : 0.0271, accuracy : 99.22\n",
            "iteration : 300, loss : 0.0266, accuracy : 99.24\n",
            "iteration : 350, loss : 0.0265, accuracy : 99.25\n",
            "Epoch : 235, training loss : 0.0264, training accuracy : 99.26, test loss : 0.2826, test accuracy : 94.32\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0290, accuracy : 99.08\n",
            "iteration : 100, loss : 0.0294, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0280, accuracy : 99.20\n",
            "iteration : 200, loss : 0.0280, accuracy : 99.22\n",
            "iteration : 250, loss : 0.0283, accuracy : 99.20\n",
            "iteration : 300, loss : 0.0281, accuracy : 99.21\n",
            "iteration : 350, loss : 0.0271, accuracy : 99.22\n",
            "Epoch : 236, training loss : 0.0270, training accuracy : 99.21, test loss : 0.2889, test accuracy : 94.21\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0242, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0252, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0249, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0258, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0263, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0261, accuracy : 99.23\n",
            "iteration : 350, loss : 0.0259, accuracy : 99.27\n",
            "Epoch : 237, training loss : 0.0264, training accuracy : 99.24, test loss : 0.2812, test accuracy : 94.22\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0258, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0262, accuracy : 99.30\n",
            "iteration : 150, loss : 0.0280, accuracy : 99.28\n",
            "iteration : 200, loss : 0.0273, accuracy : 99.24\n",
            "iteration : 250, loss : 0.0262, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0253, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0263, accuracy : 99.25\n",
            "Epoch : 238, training loss : 0.0264, training accuracy : 99.25, test loss : 0.2927, test accuracy : 94.25\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0202, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0241, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0229, accuracy : 99.37\n",
            "iteration : 200, loss : 0.0238, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0236, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0238, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0242, accuracy : 99.34\n",
            "Epoch : 239, training loss : 0.0246, training accuracy : 99.32, test loss : 0.2832, test accuracy : 94.32\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0252, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0259, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0246, accuracy : 99.26\n",
            "iteration : 200, loss : 0.0256, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0253, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0249, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0256, accuracy : 99.23\n",
            "Epoch : 240, training loss : 0.0258, training accuracy : 99.22, test loss : 0.2852, test accuracy : 94.35\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0208, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0239, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0251, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0248, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0240, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0246, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0249, accuracy : 99.31\n",
            "Epoch : 241, training loss : 0.0249, training accuracy : 99.30, test loss : 0.2897, test accuracy : 94.22\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0248, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0228, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0231, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0239, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0237, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0240, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0240, accuracy : 99.29\n",
            "Epoch : 242, training loss : 0.0239, training accuracy : 99.29, test loss : 0.2853, test accuracy : 94.30\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0270, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0257, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0257, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0252, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0239, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0236, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0235, accuracy : 99.29\n",
            "Epoch : 243, training loss : 0.0236, training accuracy : 99.28, test loss : 0.2913, test accuracy : 94.25\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0178, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0214, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0232, accuracy : 99.30\n",
            "iteration : 200, loss : 0.0231, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0221, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0226, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0228, accuracy : 99.33\n",
            "Epoch : 244, training loss : 0.0231, training accuracy : 99.33, test loss : 0.2903, test accuracy : 94.26\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0175, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0185, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0198, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0198, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0205, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0208, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0214, accuracy : 99.38\n",
            "Epoch : 245, training loss : 0.0217, training accuracy : 99.37, test loss : 0.3014, test accuracy : 94.30\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0232, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0212, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0210, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0211, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0205, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0200, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0198, accuracy : 99.45\n",
            "Epoch : 246, training loss : 0.0198, training accuracy : 99.45, test loss : 0.2923, test accuracy : 94.42\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0252, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0248, accuracy : 99.27\n",
            "iteration : 150, loss : 0.0229, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0224, accuracy : 99.33\n",
            "iteration : 250, loss : 0.0215, accuracy : 99.37\n",
            "iteration : 300, loss : 0.0216, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0221, accuracy : 99.35\n",
            "Epoch : 247, training loss : 0.0224, training accuracy : 99.35, test loss : 0.3025, test accuracy : 94.18\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0171, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0169, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0179, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0191, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0197, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0191, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0187, accuracy : 99.47\n",
            "Epoch : 248, training loss : 0.0191, training accuracy : 99.47, test loss : 0.2924, test accuracy : 94.36\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0156, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0160, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0192, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0184, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0195, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0200, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0206, accuracy : 99.40\n",
            "Epoch : 249, training loss : 0.0204, training accuracy : 99.41, test loss : 0.2922, test accuracy : 94.37\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0175, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0184, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0196, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0200, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0199, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0193, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0196, accuracy : 99.44\n",
            "Epoch : 250, training loss : 0.0195, training accuracy : 99.44, test loss : 0.2971, test accuracy : 94.13\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0188, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0162, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0160, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0158, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0161, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0175, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0181, accuracy : 99.47\n",
            "Epoch : 251, training loss : 0.0181, training accuracy : 99.47, test loss : 0.3012, test accuracy : 94.20\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0178, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0190, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0189, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0182, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0179, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0189, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0188, accuracy : 99.46\n",
            "Epoch : 252, training loss : 0.0190, training accuracy : 99.45, test loss : 0.2938, test accuracy : 94.34\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0201, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0196, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0210, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0195, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0195, accuracy : 99.45\n",
            "iteration : 300, loss : 0.0193, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0189, accuracy : 99.47\n",
            "Epoch : 253, training loss : 0.0193, training accuracy : 99.46, test loss : 0.2977, test accuracy : 94.34\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0180, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0168, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0177, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0172, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0177, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0171, accuracy : 99.51\n",
            "Epoch : 254, training loss : 0.0172, training accuracy : 99.51, test loss : 0.2941, test accuracy : 94.37\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0186, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0176, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0171, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0163, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0159, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0164, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0169, accuracy : 99.52\n",
            "Epoch : 255, training loss : 0.0169, training accuracy : 99.51, test loss : 0.2996, test accuracy : 94.35\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0149, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0145, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0151, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0153, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0153, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0147, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0154, accuracy : 99.57\n",
            "Epoch : 256, training loss : 0.0156, training accuracy : 99.56, test loss : 0.2989, test accuracy : 94.24\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0153, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0141, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0150, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0155, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0158, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0158, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0157, accuracy : 99.58\n",
            "Epoch : 257, training loss : 0.0155, training accuracy : 99.59, test loss : 0.3002, test accuracy : 94.37\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0183, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0175, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0158, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0158, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0155, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0157, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0163, accuracy : 99.52\n",
            "Epoch : 258, training loss : 0.0162, training accuracy : 99.53, test loss : 0.2969, test accuracy : 94.36\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0180, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0171, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0158, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0163, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0152, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0154, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0155, accuracy : 99.58\n",
            "Epoch : 259, training loss : 0.0157, training accuracy : 99.56, test loss : 0.2963, test accuracy : 94.40\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0146, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0151, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0152, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0150, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0153, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0157, accuracy : 99.56\n",
            "Epoch : 260, training loss : 0.0159, training accuracy : 99.57, test loss : 0.3068, test accuracy : 94.26\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0143, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0148, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0150, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0155, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0149, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0146, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0148, accuracy : 99.60\n",
            "Epoch : 261, training loss : 0.0146, training accuracy : 99.60, test loss : 0.2958, test accuracy : 94.44\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0125, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0123, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0129, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0131, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0137, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0140, accuracy : 99.61\n",
            "Epoch : 262, training loss : 0.0139, training accuracy : 99.61, test loss : 0.2997, test accuracy : 94.51\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0126, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0134, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0137, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0134, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0133, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0135, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0136, accuracy : 99.64\n",
            "Epoch : 263, training loss : 0.0134, training accuracy : 99.65, test loss : 0.2924, test accuracy : 94.54\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0163, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0130, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0139, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0137, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0136, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0137, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0137, accuracy : 99.62\n",
            "Epoch : 264, training loss : 0.0134, training accuracy : 99.63, test loss : 0.2960, test accuracy : 94.40\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0100, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0105, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0112, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0110, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0113, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0118, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.73\n",
            "Epoch : 265, training loss : 0.0119, training accuracy : 99.72, test loss : 0.2993, test accuracy : 94.41\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0106, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0129, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0134, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0134, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0131, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0132, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0133, accuracy : 99.63\n",
            "Epoch : 266, training loss : 0.0132, training accuracy : 99.64, test loss : 0.3057, test accuracy : 94.25\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0113, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0113, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0118, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0121, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0124, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0127, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0125, accuracy : 99.67\n",
            "Epoch : 267, training loss : 0.0124, training accuracy : 99.68, test loss : 0.3159, test accuracy : 94.41\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0136, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0130, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0133, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0135, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0129, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0130, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0137, accuracy : 99.65\n",
            "Epoch : 268, training loss : 0.0138, training accuracy : 99.64, test loss : 0.3084, test accuracy : 94.35\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0127, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0120, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0115, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0116, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0115, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0114, accuracy : 99.71\n",
            "Epoch : 269, training loss : 0.0117, training accuracy : 99.70, test loss : 0.3033, test accuracy : 94.45\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0131, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0136, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0126, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0120, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0117, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0121, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0119, accuracy : 99.69\n",
            "Epoch : 270, training loss : 0.0118, training accuracy : 99.70, test loss : 0.2999, test accuracy : 94.41\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0126, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0103, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0108, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.72\n",
            "Epoch : 271, training loss : 0.0111, training accuracy : 99.72, test loss : 0.3051, test accuracy : 94.37\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0117, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0118, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0125, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0126, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0122, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0119, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0116, accuracy : 99.69\n",
            "Epoch : 272, training loss : 0.0116, training accuracy : 99.68, test loss : 0.2995, test accuracy : 94.35\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0116, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0106, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0110, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0114, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0114, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0113, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.70\n",
            "Epoch : 273, training loss : 0.0111, training accuracy : 99.71, test loss : 0.3001, test accuracy : 94.35\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0111, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0110, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0108, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0105, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0108, accuracy : 99.72\n",
            "Epoch : 274, training loss : 0.0108, training accuracy : 99.72, test loss : 0.3020, test accuracy : 94.49\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0099, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0098, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0099, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0099, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0100, accuracy : 99.76\n",
            "Epoch : 275, training loss : 0.0100, training accuracy : 99.76, test loss : 0.3047, test accuracy : 94.30\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0117, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0117, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0107, accuracy : 99.71\n",
            "Epoch : 276, training loss : 0.0107, training accuracy : 99.72, test loss : 0.3001, test accuracy : 94.53\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0101, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0096, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0104, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0104, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0103, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0103, accuracy : 99.75\n",
            "Epoch : 277, training loss : 0.0101, training accuracy : 99.76, test loss : 0.2962, test accuracy : 94.54\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0097, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0087, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0102, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0101, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0103, accuracy : 99.73\n",
            "Epoch : 278, training loss : 0.0104, training accuracy : 99.73, test loss : 0.3030, test accuracy : 94.37\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0114, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0112, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0112, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0102, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0100, accuracy : 99.74\n",
            "Epoch : 279, training loss : 0.0100, training accuracy : 99.74, test loss : 0.2991, test accuracy : 94.43\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0118, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0118, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0109, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0102, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0097, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0093, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0093, accuracy : 99.77\n",
            "Epoch : 280, training loss : 0.0095, training accuracy : 99.77, test loss : 0.3035, test accuracy : 94.43\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0093, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0089, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.79\n",
            "Epoch : 281, training loss : 0.0090, training accuracy : 99.79, test loss : 0.3054, test accuracy : 94.34\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0079, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0092, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0092, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0090, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0092, accuracy : 99.78\n",
            "Epoch : 282, training loss : 0.0093, training accuracy : 99.78, test loss : 0.2971, test accuracy : 94.46\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0093, accuracy : 99.75\n",
            "Epoch : 283, training loss : 0.0091, training accuracy : 99.76, test loss : 0.3043, test accuracy : 94.36\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0093, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0090, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0093, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0089, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0097, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0096, accuracy : 99.75\n",
            "Epoch : 284, training loss : 0.0098, training accuracy : 99.75, test loss : 0.3032, test accuracy : 94.53\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0122, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0105, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0099, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0089, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0093, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0096, accuracy : 99.75\n",
            "Epoch : 285, training loss : 0.0096, training accuracy : 99.75, test loss : 0.3068, test accuracy : 94.38\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0094, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0104, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0095, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0097, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0094, accuracy : 99.75\n",
            "Epoch : 286, training loss : 0.0094, training accuracy : 99.75, test loss : 0.3057, test accuracy : 94.35\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0084, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0096, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0089, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0087, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0088, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0091, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0093, accuracy : 99.76\n",
            "Epoch : 287, training loss : 0.0091, training accuracy : 99.76, test loss : 0.3028, test accuracy : 94.44\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0080, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0083, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0089, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0087, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0087, accuracy : 99.79\n",
            "Epoch : 288, training loss : 0.0087, training accuracy : 99.79, test loss : 0.3041, test accuracy : 94.36\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0079, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0084, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0085, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0086, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0086, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.79\n",
            "Epoch : 289, training loss : 0.0087, training accuracy : 99.79, test loss : 0.3118, test accuracy : 94.34\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0074, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0085, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0082, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0079, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0081, accuracy : 99.79\n",
            "Epoch : 290, training loss : 0.0081, training accuracy : 99.79, test loss : 0.3035, test accuracy : 94.35\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0088, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0091, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0087, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0087, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0082, accuracy : 99.81\n",
            "Epoch : 291, training loss : 0.0082, training accuracy : 99.81, test loss : 0.3063, test accuracy : 94.40\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0092, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0087, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0083, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0084, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0084, accuracy : 99.82\n",
            "Epoch : 292, training loss : 0.0083, training accuracy : 99.82, test loss : 0.3057, test accuracy : 94.38\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0071, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0081, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0083, accuracy : 99.79\n",
            "Epoch : 293, training loss : 0.0082, training accuracy : 99.79, test loss : 0.3013, test accuracy : 94.45\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0077, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0079, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0080, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0080, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0082, accuracy : 99.81\n",
            "Epoch : 294, training loss : 0.0082, training accuracy : 99.82, test loss : 0.3063, test accuracy : 94.40\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0090, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0085, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0085, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0087, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0085, accuracy : 99.81\n",
            "Epoch : 295, training loss : 0.0085, training accuracy : 99.80, test loss : 0.3133, test accuracy : 94.26\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0072, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0078, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0086, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0086, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0086, accuracy : 99.76\n",
            "Epoch : 296, training loss : 0.0084, training accuracy : 99.76, test loss : 0.2977, test accuracy : 94.41\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0073, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0093, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0090, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.74\n",
            "Epoch : 297, training loss : 0.0091, training accuracy : 99.74, test loss : 0.3059, test accuracy : 94.40\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0080, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0092, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0096, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0094, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0089, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0090, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0089, accuracy : 99.77\n",
            "Epoch : 298, training loss : 0.0089, training accuracy : 99.78, test loss : 0.2989, test accuracy : 94.54\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0091, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0090, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0089, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0086, accuracy : 99.78\n",
            "Epoch : 299, training loss : 0.0086, training accuracy : 99.79, test loss : 0.2985, test accuracy : 94.49\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0081, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0080, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0083, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0080, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0078, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0081, accuracy : 99.82\n",
            "Epoch : 300, training loss : 0.0082, training accuracy : 99.81, test loss : 0.3086, test accuracy : 94.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "id": "iUQVIKR-X3v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a5483b-8dcc-480a-f9e7-4b0e09e149f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.26001455211171914, 95.02919483712354)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : Weight decay coefficient = 5e-4\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7CNz1iabSB21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "65e28c26-7db9-4f2b-fecf-540d8a0f5ed1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : Exponential\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3436792e-aec7-4517-8384-471e58f94206"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU5dn/8c9FCCQsIhBANgWs4i4Coj513wrUaq27on20FrVqa6u2rtTq01+1rUtt6y5a61Z3qWJFLe6ioKKsAq4EZFf2JSTX74/rDJmEJITAZBLm+3695jVz9vvMJPd17uXcx9wdERHJXU2ynQAREckuBQIRkRynQCAikuMUCEREcpwCgYhIjlMgEBHJcQoEIlInZnaamY2q5brXmNmDmU6T1I0CgdSamX1hZodnOx2NgZkdbGZlZras0mu/bKetLsysh5m5mTVNzXP3h9z9yGymSzaPphteRWTLYGZN3X1tPR5ytrt3q8fjidSJSgSyycysuZndYmazk9ctZtY8WVZkZs+Z2bdmtsjM3jCzJsmy35jZLDNbamafmNlh1ey/0MxuNLMvzWyxmb2ZzDvYzIorrbuu1JJURzxhZg+a2RLgCjNbaWbt0tbfy8wWmFl+Mn2WmU0xs2/M7EUz2y4D31c7Mys2sx8k063MbIaZnZFM329md5jZS8l381p6Oszsf8xsbPJdjDWz/0lb9qqZXWdmbyXbjjKzorTl+5rZ28nv8ZGZHVzLbV9P3r9NlWzM7H/N7M207f9iZjPNbImZvW9mB2zu704yQ4FANocrgX2BPsCewADgqmTZxUAx0AHoBFwBuJn1Bi4A9nb31sD3gC+q2f+fgX7A/wDtgF8DZbVM2zHAE8DWwJ+Ad4Dj0pafCjzh7iVmdkySvh8l6X0DeKS6HZvZx2Z2ai3TsY67LwLOAu42s47AzcB4d38gbbXTgOuAImA88FByzHbA88CtQHvgJuB5M2tf6ZzOBDoCzYBLkm27Jtv+H/E9XgI8aWYdNrQtcGDyvrW7t3L3d6o4tbHE30A74GHgcTMrqP03I9miQCCbw2nAte4+z93nA78DTk+WlQCdge3cvcTd3/AY4KoUaA7sYmb57v6Fu39aecdJ6eEs4BfuPsvdS939bXdfXcu0vePuz7h7mbuvJDKoU5J9G3ByMg/gXOAP7j4lqUL6f0Cf6koF7r6Huz9c1bJEl+TKO/3VMtl2FPA48AowGDin0rbPu/vryXleCexnZt2B7wPT3f2f7r7W3R8BpgI/SNv2PneflpzvY0TmDDAEGOnuI5Pv4yVgXHL8DW27Qe7+oLsvTNJ1I/H79q7t9pI9CgSyOXQBvkyb/jKZB3EVPgMYZWafmdllAO4+A7gIuAaYZ2aPmlkX1lcEFADrBYlamllp+kkiU+1MXOWWEVf+ANsBf0ll2sAiwICudTz2bHffutJredryu4DdgPvdfWF16Xb3ZUlaurD+d00ynZ7GOWmfVwCtks/bASekByZgfyJQb2jbDTKzS5JqtcXJvtsQv580cAoEsjnMJjKZlG2Tebj7Une/2N17AUcDv0q1Bbj7w+6+f7KtAzdUse8FwCpg+yqWLQdapCbMLI+o0klXYXhdd/8GGAWcRFSDPOrlQ/DOBM6plHEXuvvbG/wGNlKS1ruAB4Cfmdl3Kq3SPW3dVkR1y2zW/64hvu9ZtTjsTOCflc6vpbtfX4ttaxymOGkP+DVwItDW3bcGFhOBVBo4BQLZWPlmVpD2akrUo19lZh2SxsVhwIMAZnaUmX0nqYZZTFQJlZlZbzM71KJReRWwkirq/d29DBgO3GRmXcwsL2mobA5MAwrM7PtJY+9VRHXEhjwMnAEcT3m1EMAdwOVmtmuS9jZmdsLGf0W1cgWRuZ5FlJoeSIJDymAz29/MmhFtBWPcfSYwEtjRzE41s6ZmdhKwC/BcLY75IPADM/te8j0WWDS416Zn03zi9+lVzfLWwNpkvaZmNgzYqhb7lQZAgUA21kgi0069riEaH8cBHwMTgA+SeQA7AC8Dy4iG2tvcfTSRYV9PXPHPIRonL6/mmJck+x1LVJHcADRx98XAz4B7iCvi5UTD9IaMSNI1x90/Ss1096eTfT9q0ctoIjCoup2Y2SQzO62G43Sx9e8jOM7M+gG/As5w99LkmA5clrbtw8Bvk/PtR9Tvk1QhHUU0wi8krsKPcvcFGzrpJJCkGsTnEyWES6lFPuDuK4DfA28l1Ur7VlrlReA/RHD+kgjulavlpIEyPZhGpGExs/uBYne/akPrimwOKhGIiOQ4BQIRkRynqiERkRynEoGISI5rdIPOFRUVeY8ePbKdDBGRRuX9999f4O6V77MBGmEg6NGjB+PGjct2MkREGhUzq3xH+jqqGhIRyXEKBCIiOU6BQEQkxzW6NgIRkbooKSmhuLiYVatWZTspGVVQUEC3bt3Iz8+v9TYKBCKSE4qLi2ndujU9evQgxkDc8rg7CxcupLi4mJ49e9Z6O1UNiUhOWLVqFe3bt99igwCAmdG+ffuNLvUoEIhIztiSg0BKXc4xZwLBpEkwbBjMm5ftlIiINCw5EwimTIHrrlMgEJHs+Pbbb7nttts2ervBgwfz7bffZiBF5XImEDRJzrRsvWdgiYhkXnWBYO3atTVuN3LkSLbeeutMJQvIoV5DCgQikk2XXXYZn376KX369CE/P5+CggLatm3L1KlTmTZtGj/84Q+ZOXMmq1at4he/+AVDhw4FyofVWbZsGYMGDWL//ffn7bffpmvXrjz77LMUFhZuctoUCEQk51x0EYwfv3n32acP3HJL9cuvv/56Jk6cyPjx43n11Vf5/ve/z8SJE9d18xw+fDjt2rVj5cqV7L333hx33HG0b9++wj6mT5/OI488wt13382JJ57Ik08+yZAhQzY57QoEIiJZMGDAgAp9/W+99VaefvppAGbOnMn06dPXCwQ9e/akT58+APTr148vvvhis6RFgUBEck5NV+71pWXLlus+v/rqq7z88su88847tGjRgoMPPrjKewGaN2++7nNeXh4rV67cLGlRY7GISD1o3bo1S5curXLZ4sWLadu2LS1atGDq1KmMGTOmXtOmEoGISD1o37493/3ud9ltt90oLCykU6dO65YNHDiQO+64g5133pnevXuz77771mvaMhYIzGw4cBQwz913q2G9vYF3gJPd/YlMpUeBQESy7eGHH65yfvPmzXnhhReqXJZqBygqKmLixInr5l9yySWbLV2ZrBq6HxhY0wpmlgfcAIzKYDoABQIRkepkLBC4++vAog2sdiHwJJDx+30VCEREqpa1xmIz6wocC9xei3WHmtk4Mxs3f/78Oh1PgUBEpGrZ7DV0C/Abd99g1uzud7l7f3fv36FDhzodTIFARKRq2ew11B94NBkytQgYbGZr3f2ZTBxMgUBEpGpZCwTuvu6WOjO7H3guU0EAFAhERKqTsaohM3uE6Bba28yKzewnZnaumZ2bqWPWRIFARLKprsNQA9xyyy2sWLFiM6eoXCZ7DZ3i7p3dPd/du7n7ve5+h7vfUcW6/5vJewhAgUBEsqshBwLdWSwiUg/Sh6E+4ogj6NixI4899hirV6/m2GOP5Xe/+x3Lly/nxBNPpLi4mNLSUq6++mrmzp3L7NmzOeSQQygqKmL06NGbPW0KBCKSe7IwDnX6MNSjRo3iiSee4L333sPdOfroo3n99deZP38+Xbp04fnnnwdiDKI2bdpw0003MXr0aIqKijZvmhMadE5EpJ6NGjWKUaNGsddee9G3b1+mTp3K9OnT2X333XnppZf4zW9+wxtvvEGbNm3qJT0qEYhI7snyONTuzuWXX84555yz3rIPPviAkSNHctVVV3HYYYcxbNiwjKdHJQIRkXqQPgz19773PYYPH86yZcsAmDVrFvPmzWP27Nm0aNGCIUOGcOmll/LBBx+st20mqEQgIlIP0oehHjRoEKeeeir77bcfAK1ateLBBx9kxowZXHrppTRp0oT8/Hxuvz1G4Bk6dCgDBw6kS5cuGWksNnff7DvNpP79+/u4ceM2ertp06B3b3joITj11AwkTEQatClTprDzzjtnOxn1oqpzNbP33b1/VeurakhEJMcpEIiI5DgFAhHJGY2tKrwu6nKOCgQikhMKCgpYuHDhFh0M3J2FCxdSUFCwUdup15CI5IRu3bpRXFxMXR9u1VgUFBTQrVu3jdpGgUBEckJ+fj49e/bc8Io5SFVDIiI5ToFARCTHKRCIiOQ4BQIRkRynQCAikuMy+czi4WY2z8wmVrP8NDP72MwmmNnbZrZnptICCgQiItXJZIngfmBgDcs/Bw5y992B64C7MpgWBQIRkWpk7D4Cd3/dzHrUsPzttMkxwMbdAbGRFAhERKrWUNoIfgK8UN1CMxtqZuPMbFxd7wpUIBARqVrWA4GZHUIEgt9Ut4673+Xu/d29f4cOHep0HAUCEZGqZXWICTPbA7gHGOTuCzN5LAUCEZGqZa1EYGbbAk8Bp7v7tEwfT4FARKRqGSsRmNkjwMFAkZkVA78F8gHc/Q5gGNAeuM3MANZW9xi1zZOeeFcgEBGpKJO9hk7ZwPKzgbMzdfzKzOKlQCAiUlHWG4vrU5MmCgQiIpUpEIiI5DgFAhGRHKdAICKS4xQIRERynAKBiEiOUyAQEclxCgQiIjlOgUBEJMcpEIiI5DgFAhGRHKdAICKS4xQIRERynAKBiEiOUyAQEclxCgQiIjlOgUBEJMcpEIiI5LiMBQIzG25m88xsYjXLzcxuNbMZZvaxmfXNVFpSFAhERNaXyRLB/cDAGpYPAnZIXkOB2zOYFkCBQESkKhkLBO7+OrCohlWOAR7wMAbY2sw6Zyo9oEAgIlKVbLYRdAVmpk0XJ/PWY2ZDzWycmY2bP39+nQ+oQCAisr5G0Vjs7ne5e39379+hQ4c670eBQERkfdkMBLOA7mnT3ZJ5GaNAICKyvmwGghHAGUnvoX2Bxe7+dSYPqEAgIrK+ppnasZk9AhwMFJlZMfBbIB/A3e8ARgKDgRnACuDMTKUlRYFARGR9GQsE7n7KBpY7cH6mjl8VBQIRkfU1isbizUWBQERkfQoEIiI5ToFARCTHKRCIiOQ4BQIRkRynQCAikuMUCEREcpwCgYhIjlMgEBHJcQoEIiI5ToFARCTHKRCIiOQ4BQIRkRynQCAikuMUCEREclzuBIJ33+XC906naM3sbKdERKRByZ1AUFzMAV8+yNYl87OdEhGRBiWjgcDMBprZJ2Y2w8wuq2L5tmY22sw+NLOPzWxwxhLTogUAzUpXZuwQIiKNUcYCgZnlAX8HBgG7AKeY2S6VVrsKeMzd9wJOBm7LVHooLAQUCEREKstkiWAAMMPdP3P3NcCjwDGV1nFgq+RzGyBzFfipQFCmQCAikq5WgcDMWppZk+TzjmZ2tJnlb2CzrsDMtOniZF66a4AhZlYMjAQurFWq6yKpGiooXZGxQ4iINEa1LRG8DhSYWVdgFHA6cP9mOP4pwP3u3g0YDPwzFXDSmdlQMxtnZuPmz69jY29SImiuEoGISAW1DQTm7iuAHwG3ufsJwK4b2GYW0D1tulsyL91PgMcA3P0doAAoqrwjd7/L3fu7e/8OHTrUMsmVqGpIRKRKtQ4EZrYfcBrwfDIvbwPbjAV2MLOeZtaMaAweUWmdr4DDkgPsTASCzPTvTKqGVCIQEamotoHgIuBy4Gl3n2RmvYDRNW3g7muBC4AXgSlE76BJZnatmR2drHYx8FMz+wh4BPhfd/e6nMgGJSWCgjK1EYiIpGtam5Xc/TXgNYCkDn+Bu/+8FtuNJBqB0+cNS/s8GfjuxiS4zpo3pwyjQCUCEZEKattr6GEz28rMWgITgclmdmlmk7aZmbG2aYGqhkREKqlt1dAu7r4E+CHwAtCT6DnUqKxp2oICV9WQiEi62gaC/OS+gR8CI9y9hLgZrFEpaVqoqiERkUpqGwjuBL4AWgKvm9l2wJJMJSpT1uYXUuAKBCIi6WrbWHwrcGvarC/N7JDMJClzSpq2UCAQEamkto3FbczsptTdvWZ2I1E6aFRK8gspVBuBiEgFta0aGg4sBU5MXkuA+zKVqExR1ZCIyPpqVTUEbO/ux6VN/87MxmciQZlUkt+CAuZkOxkiIg1KbUsEK81s/9SEmX0XaHSX1mvzC2mBqoZERNLVtkRwLvCAmbVJpr8BfpyZJGVOaX4hLVmJO5hlOzUiIg1DbXsNfQTsaWZbJdNLzOwi4ONMJm5zK2nWgkJWUlYGeRsaMk9EJEds1BPK3H1JcocxwK8ykJ6MKs0vXBcIREQkbMqjKhtd5craZtFGoEAgIlJuUwJBoxtiojS/kHzWUrZmbbaTIiLSYNTYRmBmS6k6wzegMCMpyqDS5vFwmrLlK6F16yynRkSkYagxELj7FpVbljaL2OUrVgJb1KmJiNTZplQNNTqpQFC2TPcSiIik5FYgSKqGokQgIiKQ4UBgZgPN7BMzm2Fml1WzzolmNtnMJpnZw5lMz7pAsGx5Jg8jItKo1PbO4o1mZnnA34EjgGJgrJmNSJ5TnFpnB+By4Lvu/o2ZdcxUegDWFrSKD8sVCEREUjJZIhgAzHD3z9x9DfAocEyldX4K/N3dvwFw93kZTA+lBTFytkoEIiLlMhkIugIz06aLk3npdgR2NLO3zGyMmQ2sakdmNjT1LIT58+fXOUGlhUmJYNmyOu9DRGRLk+3G4qbADsDBwCnA3Wa2deWV3P0ud+/v7v07dOhQ54MpEIiIrC+TgWAW0D1tulsyL10xMMLdS9z9c2AaERgyoqwweaiaAoGIyDqZDARjgR3MrKeZNQNOBkZUWucZojSAmRURVUWfZSpB60oEaiwWEVknY4HA3dcCFwAvAlOAx9x9kplda2ZHJ6u9CCw0s8nAaOBSd1+YqTTRrBklNMWWq0QgIpKSse6jAO4+EhhZad6wtM9ODGddL0NaN8kzltFKgUBEJE22G4vrVZMmsJyWCgQiImlyLhAsoxW2Qm0EIiIpuRkIVCIQEVkn5wLBclrSRIFARGSdnAsEUTWkQCAikpKTgaDJSrURiIik5FQgaN066TWkO4tFRNbJqUDQsaOqhkREKsvJQJC3UoFARCQlpwJBUVESCMrWwsSJ2U6OiEiDkFOBoGlToEUyAunuu8Nbb2U1PSIiDUFOBQKAvDatyiemTs1eQkREGoicCwRtW5WUT3z+efYSIiLSQORcIFjcbdfyienTs5cQEZEGIucCwaJdD2CbNith4ECYMSPbyRERybqcCwQdO8LcxQWU9tohSgTu2U6SiEhW5WQgAFja6TuwdCnMm5fdBImIZFnOBYJOneJ97ta948OHH2YvMSIiDUBGA4GZDTSzT8xshpldVsN6x5mZm1n/TKYH4vYBgDfzDoJ27eCeezJ9SBGRBi1jgcDM8oC/A4OAXYBTzGyXKtZrDfwCeDdTaUnXq1fk/+98WABnngnPPAOzZ9fHoUVEGqRMlggGADPc/TN3XwM8ChxTxXrXATcAqzKYlnXMYMAAeO894KyzoLQUnn22Pg4tItIgZTIQdAVmpk0XJ/PWMbO+QHd3f76mHZnZUDMbZ2bj5s+fv8kJGzAAJk2CZd13hh494D//2eR9iog0VllrLDazJsBNwMUbWtfd73L3/u7ev0OHDpt87H32gbIyePsdg0GD4JVXYPXqTd6viEhjlMlAMAvonjbdLZmX0hrYDXjVzL4A9gVG1EeD8SGHQMuW8NRTxI1ly5fDm29m+rAiIg1SJgPBWGAHM+tpZs2Ak4ERqYXuvtjdi9y9h7v3AMYAR7v7uAymCYDCQvj+9yMQlB50KDRrpuohEclZGQsE7r4WuAB4EZgCPObuk8zsWjM7OlPHra3jj4f58+GND1vBAQfACy9kO0kiIlmR0TYCdx/p7ju6+/bu/vtk3jB3H1HFugfXR2kgZdCgKBk88UQyMWkSzJy5we1ERLY0OXdncUqrVpH/P/kklA0+KmY+8UR2EyUikgU5GwggqofmzIHX5vSG/v3hH//IdpJEROpdTgeCY46Btm3httuAH/8YPvooXiIiOSSnA0GLFnD22fD00/DVPifEzIcfjjqjyZOzmzgRkXqS04EA4MILoXlzOO+aTvhuu8HNN0dX0n/9K9tJE5GGoLQUlizJdioyKucDQffucP31MHIkTOl8GJQkzzR+6614nzMnbkMWkdzjDieeCN26lecJm8vixfDb38KXX1acX1wMxx23/vHefRdWZWZINvNG9oSu/v37+7hxm7eXaVkZHHwwdHn/3zy64ugoIjRtChMmxHClBxwAL70Et98eDQs9e27W44vIZjB/fjT6TZoUGXf79jF/wYKoB27RouL6paXQpEmMRHnRRTBrFjz6KOTlxXJ3uPZauOYaaNMmhqG55hro0iVGIrj2Wli5Mq4ip02D88+HDh1iuk8fuP/+2OeFF8bdq0uXwnPPxdOxBg6EF1+E99+H7baDrbaKfOeoo2Jo/OJi2HPPGAvt2GNjXJz+/eH00yMfqgMze9/dqx65wd0b1atfv36eCdOnu29dsNKf7Hmxl914kzu4Dx0a7+B+8MHx/rOfVb2Du+92f++9jKRNZIs0e/bGb1NaWvX86dPdW7Vy33tv96ZN3Xff3X3RIvdrr3UvLHTfZRf3X/3K/bLL3A84wL1tW/fmzd27dnU/4YTy//Nevdz79XN/6in3I46Ieaed5j5njvvRR5evV/mVl+een+/evXv5vPz8ODa4N2kS6TrmGPf994957du7X3ede8eO7oceGmkH9x12iLSm73/rrd2LityLi+v8dQPjvJp8NesZ+8a+MhUI3N3/8pf4Rh67uTh+uLw89223dT/ppPIfpEcP95KSihsuXRrrHnVUxtImskV5+ml3s/UvnkpL3V9+2X3ECPdVq9z/8x/3uXPjn3P0aPfOnd1vvDGCyFVXud97r/vhh7t36uReUBD/ox07VsxEv/9995YtIyM2i0z1nHPcL77YfeDAyJCPO8795JPj/71ly9iuU6c4Vnrweftt98cecx8/3v23v3W/7Tb3CRMiPWef7d6tm/udd0YG/+WX7m+84X7gge7vvltxP0uWuJeVVTz3srL4PlascF+92v3MM+N7uvpq91NPjWNvAgWCWiotjYuFNm3cF55wjq8rFUyfHn9kffqU/3ENHVr+w778csxr3Xr9IFGV1avjJbKlWLOm/PPatfE+b577Aw/E3/qnn1bM+I49Nv5nDjssMvK//9191qzykjdEpgyRgadn7GbuzZqVT7drF1fvzzwTV/LTprn/+9+Rgb7yShxvxgz3mTPjtXBh1eeQSt/777v/85/uK1du/u8pixQINsL06XGB0CVvji/o2c/9zTdjwYIF7l99VfEP8uabY9k115TPGzOm5gOsXRtFwyOOyOh5iGxWd90V1aJLl1acv3ZtXEl37hyZ/f/9X1xJ3XtveVVKUVG8d+8epeYHHohqmfz8mJ/K1Js1i6qUO+5wvympnj399Lhq/+Mf3Xv2jCrYM890v/RS9ylT3B95xP2LL7LznTQyCgQbad68uDDJy3P/85/LL3Dc3f3118tX2G67WHj44fEZ3H//e/d33ol/ipQPPnCfPNn9k0/czzrL19UpLl6c8XMRqZObboqqj9WrI6NN1XW3bBlX87/8ZXzOy4v5hYVxBZ+fH1UvqQuj/fePDPy666KKNXWVD5Gp9+vnPnas++OPu++zj/urr5anYd68rJ3+lkiBoA6WLHH/wQ/iG/rlL6tY4YknYuHw4fFPcOGF7v37u++2W1QR7bVXFDVLSty32SYaq3r0iG369Yv3ESM2PmEzZkRxuk8f9+XLN377kpIoyXzzzcZvK5lTmyrFTTFiRNRlp5s82f3nP4+r91tucd9++8i4n3mmYgafunC5/373c8+N9jOIevUrrnB/9NG4+Nljj6hX//rr2N9xx0V9d+XzHDNGHSuyQIGgjsrK3C+4IL6lX/+6UoP9mjVxpWMWr0mTomEpvepo6NDYMH3eww9HI1hhYZQO1qyJ6cpF7ur89Kfl+3rxxZj32WfuV15Z3sj09NPldbaVe1k89VRse+WVm/z9bJTRo93vu2/9BrLKysoiCmfDjBlRSistjTrCZcvqvq9p0+K7rk5JifvUqXG+V10V9dwzZsSyUaPc99wzrqJ32ikaJhcvjnr0BQviCnrSpMjYzz/f/YUX4m9t3LjYftEi9xtuiCqb7343LlJSmXmrVpFhn3VWVMWYlf89HXhg1ItCtIlddFE0Uv7yl+5/+1t52p96Kkq+VTV2prcVSIOiQLAJVq6M/6dU76877khb+M478c91wgkxPWtW/GPtuGN0AUv9g3XoEBl/27blDVBHHeXr6k+32SbqWKdNi/aGZs3cW7RwHzDA/Xe/cx882P2556L+tXNn90GDogj+61/HP99BB8W+brkl6lIh1nnllehB8eqrUZ96/fXu3/uer+sRUVWDdWmp+//7f3E1d999UbI5/vhoH0k3f36cy8cfRwZU0xXtq6+W1wOfc06cZ1WWL49iWOvWNdf7VhVM1qxxf/LJ9a9AFyyI0tjIkRXnf/xxxWPMnBlVHccf777vvpHWU06JZdOn19zVcenSyJzTA8f//E/s48YbK647YULMGzIklrdpU/53cswxkYG3bVvx4mH33d179y7PoKvrwtiunftf/xrvqa6Qu+wSn7/znahXP//8CA6tWkUd/uzZ7sOGRZApK4vv6+qr4+pftigKBJvBZ59Fm5VZdD2+9tokP5o8uWIVzZ//HFdoS5dGZnn11dGgdccd7v/4R/l6334b3dCOPTZ2XFQUAaNJE/cjjyyvg63qH/7+++Pqzcz9xBPLM4Fttonua6m+zKlifYcOFa/8dt013ocMcf/88+i//Pjjka5UCSbVwFdYGOno1i3O5fnn3X/0o9hfqutdar9HHRXnfeSR0WZyxRVRTZAKjOedF+fXrJn77bfHle4117i/9FIc++KLY7/5+fG9XHtt9K9Ob0t54YU4v1Sj/Jo17m+95f6b30QaTjwxMtvSUvf//jcyVogquyuuiCLeAQeUp/nKK2Pd9C7Cqcw3P9/9wQfjO+jY0f2jj+Iq+VZ36ToAABUPSURBVKijojR2+umRmaaCfocOEYzGjvV1jaPg/uMfR6AcNSqCXOoYJ58cJbzbb6/Yb3zXXd1fey0uAm67LeZtv31k1ocfHvMeesj91lvdP/wwrvifeCJKqBBB46OP4vtJVQVWrhaSnKNAsJksXx4XVKkL8CFD3CdOrP4el40yaVJkAN27R5Bwjwzl1lvdn302ivJHHx2Z09y5UW2QukIcNiz6GLdqFfOeeaa8Cmnw4Hg/44zoUrfffpEpXHVVZMqpjLxVK/fLL4/P550XV7c//3lk0uPHR5BJZVRNmkQGnZq+4oq40k8FJDP3Qw6J91QweuyxOKdZs8rTlFoGkZm1bh1X4VdfXTFTPvTQ2O8pp5S3s7RtG2nq1Kl8vV69yj+nGu8hSlepdLdpE1UjN9wQNwqB+847x/tPfxrFvr59IyNNbb/HHhH9U1fa6Vfl+fnRA+a226I6B2IfrVtHFU3qXPr1i/l77BGZ9h/+ULEXwvLl8R19/fX6fxszZ264Ss09SngjRsRxRSpRINjMSksjH031fsvLi3z20083MSisXVtzvfTKlREw3KMe/V//qvhPP39+ZPZlZZGxPPRQZA5z51a9v2HD4gSOPNK9S5f4fPjhVdfzrlkTV+bXXhvVVEuXxhXwQQeVr/OXv0TVSirT//jjuDrdc8+KmV5JSXkPkqlTy+uwIRoRy8qiL/f48dEwDlFXngpaf/pTBLTTT4/SyT33xLksWhQ38Pzxj7Hu8cfH+rffHtVq775b8ZxWrYquiIccEvsoK4sSz9SpsfzUU+NqfsmSCAwtWkTJaNGiqDZ7+umYn9rvmjXRzfKSS6KnWMrNN0eQP+OM8iAvUs9qCgQZHWvIzAYCfwHygHvc/fpKy38FnA2sBeYDZ7n7l+vtKE0mxhqqq9mz41HH778Pd94ZYxZtuy0MHgxHHhnDnXznOzH8SYNUUhIPYzjppEjkm2/GmCatWtVu+y++iHWLiqpfZ+3aOE5h4frLSkvLx3V5/fUYr+Xssyuu8+GH8Ic/RDrbtIkxWGoz1pN7jCEza1aMDWNWu3OqyUcfQUEB9O696fsSqWc1jTWUsUBgZnnANOAIoBgYC5zi7pPT1jkEeNfdV5jZecDB7n5STfttSIEg3aRJkY+OGBGDBi5eHPO32Qb++EfYffcYP6pNm82TJ4mIbIyaAkHTDB53ADDD3T9LEvEocAywLhC4++i09ccAQzKYnozaddd4nXNOXASPHh2DIV51FZxxRvl6+flwyCFw1lmxXuvWsMMOsPPO2Uu7iOS2TAaCrsDMtOliYJ8a1v8J8EJVC8xsKDAUYNttt91c6cuYpk3hiCPi8/HHw9Sp8SoujuqkO++EUaPK12/eHA48EObOhZ/9DAYMgG++gf33h2bNsnMOIpI7MhkIas3MhgD9gYOqWu7udwF3QVQN1WPSNlmzZrDHHvFKufjiCAotW0aGf/758M47UTI499yK2zdvHlXce+0VVejNmkVpYqedooppjz3gtdci8Lz3Hhx6aAyxLiJSW5kMBLOA7mnT3ZJ5FZjZ4cCVwEHuvjqD6WkwOneOV8qYMfHgoTZtos3088+j7fbDD+O5FxMmxOfdd4evvoILLijftqAgti0sjHUPOgh+9KN4hsWECfEsjm++iWdbdE9+jdWrI8CIiEBmG4ubEo3FhxEBYCxwqrtPSltnL+AJYKC7T6/NfhtqY3F9cYfp06NEMWVKNE4femg8FOngg+Huu+Hrr9ffrlkzGDIEli+Hxx6Lhx+ddlp06Pn882jIHjAgHqxUUycgEWmcstJrKDnwYOAWovvocHf/vZldS/RnHWFmLwO7A6ms6yt3P7qmfeZ6IKiNzz+HsWOhb99okDaDm2+G++6L6qjjj4cPPohurxBP9Fu4sHz7fv3g00+j1HLUUVGqmJWU5Q46KBq7n38+usoecUTsX9VRIg1b1gJBJigQ1F1ZWcUM+6uvogqqdeuoPho3LgLIU09Fe8X8+dH+UFoaj1ktLY3Hv6bLy4tSyv77w377RQApKIj7J95/Px7FeuedEYA++yxKIOqGL1L/FAikzlasiF5QzZpFIBk9OnpA7bknjB8fvaDKyuChh6LXU8+esGQJzJkTJYq5cyPYtGgBM5M+ZIcdFkHm229hl12idDJ4MJxwQpRO9tknnvM9ZUo8C7w294+JSM0UCCTjysqiZJC6Ubi0NEofL78Mjz4aJYE+fSLzHzEiGqtXr472jkGD4MUXYdmy2Ha33eCTT2IbgOOOi+61TZtGwOnSJdpFdtgh2kWaNoi+byINmwKBNEilpdF4vdVWUTX10Ufw9tsxbMd3vxtVTWPGwD33lFdJ7bprlDhSpYtu3aLk0bx5NHZ37gzz5kVV1pAhcWf3woXw7ruw777Qrl32zlckmxQIpNH76qtoBD/ggGic/uKLaIN4+OHoPrt4cbRvlJTE3dslJTEMUo8eMHlylFi23hp+/etYd/vtYc2aCDoHHQS/+lV5aUZkS6RAIDmhtDTaFtq0iSqnP/0p2ir69o37Kv72t7iju0mTCAwQJYY5c6KaaautoqRx9NFRhdWrF/z4x7H+NttonChp3BQIRBKTJkHXrnEfRqtWsN128PTTcMstUb3Uti08/ng0jq9ZU3Hbrl2ju+zee0eV1H//G91zjz022il22SUat0UaIgUCkY3wxhvR1jB7dtw/4R6f330XXnkl2jMgMv8mTcoDRpMm0Xjdr1/c1Ld0afR4GjQoek+ZxU18ZWVRPaX2CqlPCgQim0lZWWTqM2dG7yWIHk5lZRFA/vWvaMvo0iWqmqZNi95RKYceGttOnx7dZE88MbrNdu9e3r4hkgkKBCL1KPVMHIgeSxMmxP0R//533FzXo0d0h33++RhDCqKqqbg42iLy82Ncqb33Ln917Bj7Xb06btgT2VgKBCIN1PTp0Sbx4Yew447RcL1mTUxPnhyZP0QgWLw4lp19dgSWpUvjRryBAyPwfPVVBBT1fpKqKBCINELLlsVd12PHxl3W7drBokVw771RamjWLO7D6N49Sgrz5kVJonPnqKraZ5/yUWdLS6MhWzff5S4FApEtyKJF0f7gHj2eHnkkurbutBM8+GBUHZlFEEl1k4UYqnyvvaL3U//+cdNemzYRIDp00MCBWzoFApEctGBB3DC3ZElk8tOmxcCCc+bAjBkV183Lg06dyp+VceCB0bBdXBylj759o/1CGq9sPbNYRLKoqAhOP73qZZ9+GqPBfvNNVCl9/XX56/PP4bnnKq5fUBBdY0tL467sDh3i8/HHx7AfeqRq46YSgYisZ8yY6Obaq1cM4XHnnTEWVEFBtFcsXx7VT6WlUWLYaado0F6xIj5vv31st2ZNPH61adMIOrvtpiqobFHVkIhsNiUlkcEvXRp3V0+YAB9/HFVRLVpEoJg7NzL8Jk3i7uuUtm2jjaJPn2jYzs+PZ1mUlsYQ5337RsmjefMINi1bZu00tziqGhKRzSY/P14tW8Kpp1a9zurVEQRmzoRnn40AUVAA77wT7Q4vvxwN21UpLIzuscXF0ah95pkxYGAqiHTqFNVZhYXRnqFgselUIhCReucejdjLl8fQ42VlMbLshx9GkJg3L8aBGjECJk6seV+tW0d7yM47x/Aen3wSQ3t07hzBp3fvaMNYsCAavHffvbxnVS7J5jOLBwJ/IZ5ZfI+7X19peXPgAaAfsBA4yd2/qGmfCgQiucM9MnaINoZZs6LXU1FRlDrmzIkG7nnzog0j9VS7BQsqdp2trE2buPmuefMIEqn31q2jraNTp/K7uVNjS61YEYGmadMIIk2bxr0d7dpFYFm6NLr1Nm0arzZtyhvR3aOKbO3a8uX1HYiyUjVkZnnA34EjgGJgrJmNcPfJaav9BPjG3b9jZicDNwAnZSpNItK4mEXjc20tXx5VRvPmxXRJSTxatbQ0gsf06fDll3EX9ty5EUzWrIn3pUujOuq112JokMqaNq3Y3lEbBQVx7NTT9tI1a1b+SgWi/PwIGqWl8UqlrVmzGC33vPPgkks2Lg21kck2ggHADHf/DMDMHgWOAdIDwTHANcnnJ4C/mZl5Y6uvEpEGIdVekH7PQ/fu5Z/7V3k9vL61a6NU4R5tE6WlMX/69PKSxpo1UVpYtAhWroyMeunSWF5SEs+0WLIk7tFIZfKpYLJmTfUvs9gmL688QKxZE3ead+266d9RVTIZCLoCM9Omi4F9qlvH3dea2WKgPbAgfSUzGwoMBdh2220zlV4RESAy7KpuoOvTp/7TUh8aRY9ed7/L3fu7e/8OevKHiMhmlclAMAtIK5TRLZlX5Tpm1hRoQzQai4hIPclkIBgL7GBmPc2sGXAyMKLSOiOAHyefjwf+q/YBEZH6lbE2gqTO/wLgRaL76HB3n2Rm1wLj3H0EcC/wTzObASwigoWIiNSjjN5Z7O4jgZGV5g1L+7wKOCGTaRARkZo1isZiERHJHAUCEZEcp0AgIpLjGt2gc2Y2H/iyjpsXUelmtUZM59Iw6VwaJp0LbOfuVd6I1egCwaYws3HVDbrU2OhcGiadS8Okc6mZqoZERHKcAoGISI7LtUBwV7YTsBnpXBomnUvDpHOpQU61EYiIyPpyrUQgIiKVKBCIiOS4nAkEZjbQzD4xsxlmdlm207OxzOwLM5tgZuPNbFwyr52ZvWRm05P3ttlOZ1XMbLiZzTOziWnzqky7hVuT3+ljM+ubvZSvr5pzucbMZiW/zXgzG5y27PLkXD4xs+9lJ9XrM7PuZjbazCab2SQz+0Uyv9H9LjWcS2P8XQrM7D0z+yg5l98l83ua2btJmv+VjOiMmTVPpmcky3vU6cDuvsW/iNFPPwV6Ac2Aj4Bdsp2ujTyHL4CiSvP+CFyWfL4MuCHb6awm7QcCfYGJG0o7MBh4ATBgX+DdbKe/FudyDXBJFevukvytNQd6Jn+Dedk+hyRtnYG+yefWwLQkvY3ud6nhXBrj72JAq+RzPvBu8n0/BpyczL8DOC/5/DPgjuTzycC/6nLcXCkRrHt+sruvAVLPT27sjgH+kXz+B/DDLKalWu7+OjHMeLrq0n4M8ICHMcDWZta5flK6YdWcS3WOAR5199Xu/jkwg/hbzDp3/9rdP0g+LwWmEI+ObXS/Sw3nUp2G/Lu4uy9LJvOTlwOHEs91h/V/l9Tv9QRwmJnZxh43VwJBVc9PztBjoDPGgVFm9n7yDGeATu7+dfJ5DtApO0mrk+rS3lh/qwuSKpPhaVV0jeJckuqEvYirz0b9u1Q6F2iEv4uZ5ZnZeGAe8BJRYvnW3dcmq6Snt8Jz34HUc983Sq4Egi3B/u7eFxgEnG9mB6Yv9CgbNsq+wI057Ynbge2BPsDXwI3ZTU7tmVkr4EngIndfkr6ssf0uVZxLo/xd3L3U3fsQj/cdAOyU6WPmSiCozfOTGzR3n5W8zwOeJv5A5qaK58n7vOylcKNVl/ZG91u5+9zkn7cMuJvyaoYGfS5mlk9knA+5+1PJ7Eb5u1R1Lo31d0lx92+B0cB+RFVc6kFi6endLM99z5VAUJvnJzdYZtbSzFqnPgNHAhOp+MznHwPPZieFdVJd2kcAZyS9VPYFFqdVVTRIlerKjyV+G4hzOTnp2dET2AF4r77TV5WkHvleYIq735S2qNH9LtWdSyP9XTqY2dbJ50LgCKLNYzTxXHdY/3fZ9Oe+Z7uVvL5eRK+HaUR925XZTs9Gpr0X0cvhI2BSKv1EXeArwHTgZaBdttNaTfofIYrmJUT95k+qSzvRa+Lvye80Aeif7fTX4lz+maT14+Qfs3Pa+lcm5/IJMCjb6U9L1/5Etc/HwPjkNbgx/i41nEtj/F32AD5M0jwRGJbM70UEqxnA40DzZH5BMj0jWd6rLsfVEBMiIjkuV6qGRESkGgoEIiI5ToFARCTHKRCIiOQ4BQIRkRynQCANlpm5md2YNn2JmV2zmfZ9v5kdv+E1N/k4J5jZFDMbneljVTru/5rZ3+rzmNJ4KRBIQ7Ya+JGZFWU7IenS7vCsjZ8AP3X3QzKVHpFNpUAgDdla4vmsv6y8oPIVvZktS94PNrPXzOxZM/vMzK43s9OSMd4nmNn2abs53MzGmdk0Mzsq2T7PzP5kZmOTwcrOSdvvG2Y2AphcRXpOSfY/0cxuSOYNI252utfM/lTFNpemHSc17nwPM5tqZg8lJYknzKxFsuwwM/swOc5wM2uezN/bzN62GMP+vdRd6EAXM/uPxbMF/ph2fvcn6ZxgZut9t5J7NubKRiQb/g58nMrIamlPYGdiuOjPgHvcfYDFA0suBC5K1utBjD+zPTDazL4DnEEMn7B3ktG+ZWajkvX7Art5DF28jpl1AW4A+gHfEKPE/tDdrzWzQ4kx8cdV2uZIYmiDAcRduyOSgQS/AnoDP3H3t8xsOPCzpJrnfuAwd59mZg8A55nZbcC/gJPcfayZbQWsTA7ThxiJczXwiZn9FegIdHX33ZJ0bL0R36tsoVQikAbNYxTJB4Cfb8RmYz3GqF9NDCOQysgnEJl/ymPuXubu04mAsRMxjtMZFsMAv0sMubBDsv57lYNAYm/gVXef7zEU8EPEA2xqcmTy+hD4IDl26jgz3f2t5PODRKmiN/C5u09L5v8jOUZv4Gt3HwvxfXn5cMWvuPtid19FlGK2S86zl5n91cwGAhVGHJXcpBKBNAa3EJnlfWnz1pJcyJhZE+LJcymr0z6XpU2XUfFvvvL4Kk5cnV/o7i+mLzCzg4HldUt+lQz4g7vfWek4PapJV12kfw+lQFN3/8bM9gS+B5wLnAicVcf9yxZCJQJp8Nx9EfGovp+kzf6CqIoBOJp4ktPGOsHMmiTtBr2IAcheJKpc8gHMbMdkxNeavAccZGZFZpYHnAK8toFtXgTOshhDHzPramYdk2Xbmtl+yedTgTeTtPVIqq8ATk+O8QnQ2cz2TvbTuqbG7KThvYm7PwlcRVR3SY5TiUAaixuBC9Km7waeNbOPgP9Qt6v1r4hMfCvgXHdfZWb3ENVHHyTDG89nA48AdfevzewyYqhgA5539xqHBHf3UWa2M/BOHIZlwBDiyv0T4uFDw4kqnduTtJ0JPJ5k9GOJZ9WuMbOTgL8mwxavBA6v4dBdgfuSUhTA5TWlU3KDRh8VaUCSqqHnUo25IvVBVUMiIjlOJQIRkRynEoGISI5TIBARyXEKBCIiOU6BQEQkxykQiIjkuP8P3ScGneqKmf0AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_exp = {train_loss_list}\") \n",
        "print(f\"train_acc_list_exp = {train_acc_list}\")\n",
        "print(f\"test_loss_list_exp = {test_loss_list}\")\n",
        "print(f\"test_acc_list_exp = {test_acc_list}\")"
      ],
      "metadata": {
        "id": "3eiY3bTlWipW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01644761-084c-46ca-a0b1-cae868783581"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_exp = [1.4802386524877573, 0.5145602391825782, 0.4270873037860969, 0.3911595640143728, 0.3666531684275888, 0.3449077741482717, 0.3271806535278232, 0.31609291052269095, 0.30408265799041684, 0.29365021432560634, 0.28160075124524797, 0.2770334360439603, 0.26884961707724464, 0.2641961875202533, 0.25375436896554177, 0.2502837141112583, 0.2470766868048567, 0.23852058467745457, 0.23988390942091542, 0.23230066057546997, 0.2278682370051782, 0.22565480602304464, 0.22504844034953816, 0.2194394195467476, 0.2184035734028674, 0.21561946675464067, 0.21413457837854297, 0.21124843120332656, 0.21243113762923696, 0.20603898459374098, 0.20471800083350036, 0.20373490892936222, 0.20350655851769578, 0.20246598469774899, 0.1970768437334677, 0.1986252810896897, 0.19743034299109685, 0.19816612322034874, 0.19047524803254987, 0.19524715533909112, 0.19178811626301873, 0.1882726750904467, 0.19070912812783467, 0.18814858636720394, 0.18752294930258417, 0.1876332449012494, 0.18488309697972405, 0.1866223762634647, 0.1871858422712582, 0.18362822461378606, 0.18218793275798886, 0.18321568313935585, 0.17826895162382422, 0.18373284461658176, 0.17807404935198426, 0.1832454036466959, 0.17776847138072094, 0.17643137354918612, 0.17395331942776676, 0.17643668946777255, 0.17554401157065458, 0.17678272510124093, 0.17290620969199552, 0.16925092947575943, 0.16994382533923721, 0.17394802385189023, 0.17083977395037647, 0.16816641190430012, 0.16901177336489606, 0.17049305643734894, 0.1686860634806518, 0.16724471368279237, 0.16594552272945884, 0.16281739412365245, 0.16591493490588696, 0.16363012161721704, 0.1613770334055107, 0.1623856468898494, 0.16301845652139607, 0.162652157650892, 0.1592392718626393, 0.15867647268180926, 0.16240244100329676, 0.15764614089636944, 0.15857719487654484, 0.1579075544810069, 0.15415261916028775, 0.15517796557850955, 0.15625821325554434, 0.15458142586799495, 0.15438464393985626, 0.1572210744003132, 0.15048929238262862, 0.14945686451060985, 0.1505291250440971, 0.15305787657556658, 0.14894246368550348, 0.147898023902643, 0.15114493573180382, 0.14610998743897693, 0.15086569631002783, 0.1484981526729214, 0.1449999701039901, 0.14578966223975506, 0.14083756948631954, 0.14295084347448697, 0.14506198940319098, 0.14481029806905968, 0.14044388525470647, 0.13931026286508655, 0.13716323383980328, 0.14206144404484006, 0.14102018442822667, 0.13867486305635796, 0.13811369409385896, 0.1344010781062651, 0.13408899530364568, 0.1343176202560828, 0.13219609678417524, 0.13159260965459715, 0.13269671327295665, 0.13183512560377114, 0.12735726753487012, 0.12682472022002952, 0.12942703321656884, 0.1311747801154448, 0.12721967680990534, 0.1226911582961315, 0.12565258846474373, 0.12294719365695467, 0.12425805079601807, 0.12707497591410227, 0.12184429030895717, 0.11833101225799823, 0.11996741157762081, 0.12062076490806532, 0.11433951715184099, 0.11977466401267989, 0.11421744397217988, 0.11513779362345614, 0.11420854406688756, 0.11369123289659418, 0.11147084164869818, 0.10956912912014458, 0.10847692516680332, 0.10791541966369886, 0.11187728162006877, 0.1076874076956656, 0.10829882281340235, 0.10742072247190851, 0.10462035163530328, 0.10231590462308428, 0.10307718535789306, 0.10392249831399782, 0.10356237866277214, 0.10049116989057562, 0.10070552909620571, 0.10252807007188881, 0.0956033191373597, 0.09626625927523545, 0.09980903398548442, 0.09713503368814705, 0.09384269924546645, 0.0914715800489877, 0.08955621895276838, 0.09167020563995289, 0.090305259370505, 0.09060482793616328, 0.08901907593203673, 0.08711527488610851, 0.08550666729467833, 0.08462664417589341, 0.08277387312313082, 0.0831775262416782, 0.08437745708992407, 0.07903462006309168, 0.08039322875561268, 0.08088428793136827, 0.07809663470834494, 0.07452205053169714, 0.07565905422547727, 0.07784699146360724, 0.07322079027046356, 0.07456694824272782, 0.07434099188786213, 0.07195496950349026, 0.06945457162340563, 0.06995407763495032, 0.06968635793139295, 0.06799854681420779, 0.0665388950500082, 0.06632828803324117, 0.06345813088421899, 0.06316864148266917, 0.06253555392074149, 0.06164193874588463, 0.0611763400048381, 0.06035886153573711, 0.05943589109351965, 0.05881024452638093, 0.05468251183250449, 0.0560565908366264, 0.05653399054899933, 0.05562049048248588, 0.051311384115448895, 0.052225344276767435, 0.05155897112739926, 0.049526256553293645, 0.0512430043146519, 0.04822691060415896, 0.047050642753196606, 0.04643308828764047, 0.04417288107519873, 0.04520953228056108, 0.04404221094140233, 0.04265529490744841, 0.042559341516164015, 0.04379070952893071, 0.03938076019963517, 0.03966092142758532, 0.04086068194664934, 0.03843482981150753, 0.03796145588356426, 0.03604420552757091, 0.03480926173332786, 0.035225326502645204, 0.032808877683565184, 0.03266389738152505, 0.03565281066162698, 0.03340759336365372, 0.031172389340609674, 0.030425866454685747, 0.03243158847853279, 0.028973121385324777, 0.026384176789886422, 0.026978798047197744, 0.026420692340937774, 0.0263953241601657, 0.024609150329029293, 0.025792538209220138, 0.02489618113110483, 0.02391088762519967, 0.023590878200172107, 0.023072245570535704, 0.021719697244505735, 0.01978167379306513, 0.0224422716837235, 0.01906526322807198, 0.02043990451882929, 0.0194835271951326, 0.01808247663810102, 0.018989686450673657, 0.019306088054456765, 0.017153792940761622, 0.016933396825223636, 0.015568477072599054, 0.015502154842576865, 0.016233396913596307, 0.015708776872332502, 0.01585502114947027, 0.014598894385724433, 0.013940555258366332, 0.013421826192990531, 0.013425068274836957, 0.011932213828257893, 0.013165008134731069, 0.012398852705501201, 0.013773291333588622, 0.011650380652604831, 0.011782062857671765, 0.011110670982890316, 0.011588883696175698, 0.011087752127802874, 0.010806514879342845, 0.009987936503137777, 0.01071135072052721, 0.010115013357193833, 0.010417424021213035, 0.010015223428940142, 0.009483683681138224, 0.008984847573833514, 0.009285728387196414, 0.009138723183864965, 0.009764598564845498, 0.00961481691192997, 0.009357751404228027, 0.009121658366620237, 0.00870354351181244, 0.008720945385802124, 0.008100192143321845, 0.008175653403910094, 0.008284241586373387, 0.008199483759103085, 0.008166708315275913, 0.008518998447406865, 0.008448232254922026, 0.009054402698987343, 0.008894852471574484, 0.008632234484777561, 0.008205045137029434]\n",
            "train_acc_list_exp = [48.277395447326626, 83.64002117522499, 86.79724722075171, 88.12493382742191, 88.89147697194282, 89.55214399152992, 89.9947061937533, 90.69772366331392, 90.96029645314981, 91.36050820539968, 91.55955532027528, 91.68660667019587, 92.08046585494971, 92.15457914240339, 92.65643197458974, 92.62466913710958, 92.80254102699841, 93.1011116993118, 93.08840656431974, 93.11593435680254, 93.28110111169931, 93.37850714663843, 93.46956061408153, 93.68131286394917, 93.59661196400212, 93.65166754896771, 93.93118051879301, 93.87612493382743, 93.82953943885654, 94.09846479618847, 94.14716781365802, 94.02858655373214, 94.13022763366861, 94.16410799364743, 94.31445209105347, 94.212811011117, 94.20434092112228, 94.25939650608788, 94.53255690841715, 94.46056114346214, 94.500794070937, 94.56855479089465, 94.40974060349392, 94.58125992588671, 94.53255690841715, 94.62361037586024, 94.77183695076761, 94.66807834833246, 94.64690312334568, 94.67866596082584, 94.7443091582848, 94.71042879830598, 94.96029645314981, 94.68290100582318, 94.88406564319746, 94.70619375330863, 94.8268925357332, 94.91371095817892, 94.92429857067232, 94.99417681312865, 94.94759131815776, 94.89253573319216, 95.142403388036, 95.1148755955532, 95.13816834303864, 94.97511911064055, 95.08946532556908, 95.23133933298041, 95.18475383800953, 95.00264690312335, 95.13393329804128, 95.06617257808364, 95.18475383800953, 95.32874536791954, 95.26521969295923, 95.21651667548967, 95.358390682901, 95.29274748544204, 95.39862361037586, 95.3223928004235, 95.47908946532557, 95.358390682901, 95.3499205929063, 95.42826892535733, 95.46638433033351, 95.43462149285337, 95.57014293276866, 95.4219163578613, 95.45579671784013, 95.63366860772896, 95.55532027527792, 95.5214399152991, 95.74166225516146, 95.71836950767602, 95.66543144520911, 95.5849655902594, 95.67178401270513, 95.75860243515088, 95.6569613552144, 95.80518793012176, 95.6929592376919, 95.71836950767602, 95.84542085759661, 95.8644785600847, 95.95764955002647, 95.92165166754897, 95.84965590259397, 95.78613022763366, 96.01694017998942, 95.98941238750662, 96.02117522498676, 95.97670725251456, 95.93859184753838, 96.09317098994177, 96.00211752249868, 96.11858125992589, 96.14822657490735, 96.15669666490207, 96.22022233986236, 96.18845950238222, 96.2075172048703, 96.16516675489677, 96.35997882477501, 96.38538909475913, 96.26257278983589, 96.2350449973531, 96.36421386977237, 96.56961355214399, 96.3790365272631, 96.54843832715723, 96.38750661725781, 96.3430386447856, 96.56749602964531, 96.65431445209106, 96.64584436209634, 96.6056114346215, 96.66490206458444, 96.52302805717311, 96.76654314452091, 96.68395976707252, 96.72631021704606, 96.67337215457914, 96.80254102699841, 96.92535733192165, 96.93170989941768, 96.84277395447327, 96.79195341450503, 96.9020645844362, 96.929592376919, 96.86394917946004, 97.01429327686607, 97.07146638433034, 97.05452620434092, 96.98041291688725, 97.05029115934357, 97.19428268925357, 97.12228692429856, 97.07993647432504, 97.32556908417152, 97.30227633668608, 97.18581259925887, 97.25357331921651, 97.28533615669666, 97.3446267866596, 97.43991529910005, 97.44415034409741, 97.41450502911593, 97.43144520910535, 97.46744309158285, 97.45262043409211, 97.56484912652196, 97.57755426151402, 97.60508205399682, 97.66225516146109, 97.53732133403918, 97.7067231339333, 97.66013763896241, 97.65378507146639, 97.76601376389624, 97.87612493382743, 97.8507146638433, 97.80836421386977, 97.89306511381683, 97.83800952885125, 97.86553732133405, 97.91847538380095, 98.0052938062467, 97.92906299629433, 97.97141344626786, 98.07517204870302, 98.04552673372154, 98.08152461619905, 98.14505029115935, 98.18951826363156, 98.1852832186342, 98.24033880359978, 98.18316569613552, 98.27633668607729, 98.26786659608258, 98.24669137109582, 98.44150344097406, 98.41821069348862, 98.30598200105877, 98.37374272101641, 98.54314452091053, 98.52196929592377, 98.51561672842774, 98.61725780836422, 98.53890947591319, 98.66807834833246, 98.59820010587613, 98.64690312334568, 98.74854420328217, 98.68925357331922, 98.70407623080995, 98.74642668078349, 98.76548438327157, 98.67019587083112, 98.89677077818952, 98.80359978824775, 98.77818951826363, 98.854420328216, 98.86924298570672, 98.94123875066173, 98.97511911064055, 98.97511911064055, 99.07252514557968, 99.06617257808364, 98.96029645314981, 99.04711487559555, 99.09158284806776, 99.08523028057174, 99.02593965060879, 99.16569613552144, 99.26098464796189, 99.214399152991, 99.2419269454738, 99.2503970354685, 99.31604023292748, 99.22498676548439, 99.30333509793542, 99.29062996294336, 99.28427739544733, 99.32874536791954, 99.36897829539438, 99.44944415034409, 99.34568554790894, 99.46638433033351, 99.40709370037057, 99.43673901535203, 99.47273689782953, 99.44944415034409, 99.46003176283747, 99.50661725780836, 99.51085230280572, 99.56167284277396, 99.58708311275808, 99.5299100052938, 99.56379036527264, 99.56802541026998, 99.59978824775013, 99.61461090524087, 99.65272631021705, 99.62519851773425, 99.7204870301747, 99.63578613022763, 99.67813658020117, 99.64002117522499, 99.69931180518793, 99.69719428268925, 99.72472207517205, 99.68448914769719, 99.70566437268396, 99.7204870301747, 99.76283748014822, 99.71625198517734, 99.75860243515088, 99.72683959767072, 99.74377977766014, 99.76707252514558, 99.78613022763366, 99.78189518263632, 99.7564849126522, 99.75224986765484, 99.75224986765484, 99.75436739015352, 99.7649550026469, 99.79460031762838, 99.7924827951297, 99.7924827951297, 99.81154049761778, 99.82424563260984, 99.79460031762838, 99.82001058761249, 99.80307040762308, 99.7649550026469, 99.73954473266278, 99.77766013763896, 99.78613022763366, 99.81365802011646]\n",
            "test_loss_list_exp = [0.8332259766027039, 0.6243491267748907, 0.4589659714815663, 0.4804437569543427, 0.427488262965983, 0.4090569673800001, 0.3808782717906961, 0.35566168862814995, 0.33532728992548644, 0.3060293437949583, 0.3283534451734786, 0.2954925663184886, 0.28179204358043625, 0.30891463029034, 0.3015652174750964, 0.28739879868340257, 0.30414057471880723, 0.2780561326254232, 0.2854650225125107, 0.2774708117632305, 0.2591885347649747, 0.2489084253343297, 0.26453685285706147, 0.2969295278775926, 0.2676425111644408, 0.24623633818883522, 0.2657030882569505, 0.2502379405367024, 0.2519026789814234, 0.24595809209288336, 0.2657459572907172, 0.24606278589835354, 0.26561722005991373, 0.26659162207415293, 0.27839325434144807, 0.24987868000479305, 0.24812665541528486, 0.24487176610558642, 0.2487423630321727, 0.2408910162162547, 0.23539556405853992, 0.24755567443721435, 0.24770869559371003, 0.24718565263730638, 0.239624624816226, 0.24662797083603402, 0.2383566615445649, 0.2449228444374075, 0.23434106121752776, 0.2598630274627723, 0.2621555033998162, 0.23332552688525005, 0.23622890707908892, 0.24026584826117636, 0.2526375870494282, 0.23738020169092158, 0.2334751014177706, 0.2508628443796553, 0.2439220617068749, 0.23279762205978236, 0.24812749104903026, 0.23510865852528928, 0.24088096268036785, 0.2441782963772615, 0.2372785997120481, 0.2591664406160514, 0.24067971285651713, 0.2356143380497016, 0.22471256245512003, 0.23980964548593642, 0.22926432496922858, 0.22651415298163308, 0.23500783566166372, 0.24451460265645794, 0.24563370374780075, 0.238621749385607, 0.24734771609598516, 0.24317575092701352, 0.2357527948002897, 0.23691468079592667, 0.2317277501318969, 0.2389378704167172, 0.2415773031100923, 0.22682394105575832, 0.24305828332024462, 0.23989992117618814, 0.24055047181672326, 0.2310189416851191, 0.2320305799663651, 0.24022304533305122, 0.23773051294333794, 0.23404084142370551, 0.22795141963105575, 0.24373077162924936, 0.2519711878004612, 0.24464675623412227, 0.24204467719092088, 0.23641339631057254, 0.22852404189168238, 0.23696348417148577, 0.23560348832431963, 0.2293583228636314, 0.22483952014761813, 0.2419880603750547, 0.24878904088308998, 0.23500451945937148, 0.23449834377742282, 0.23925709554596858, 0.22304683715543328, 0.23272172509528258, 0.23005880462918796, 0.2274422672230239, 0.24031657513742352, 0.23025121483221359, 0.23231310611042907, 0.23103023649138563, 0.23504440533910312, 0.23217533609154178, 0.24353400623316274, 0.23417713490369566, 0.23423893881194732, 0.228971739591775, 0.23540909750861863, 0.22729575031382196, 0.24729683045663087, 0.23187032206824013, 0.23728142879611136, 0.2368104361983783, 0.23822332604550847, 0.24225895598019456, 0.24053165952072425, 0.23070352232339336, 0.23100480022748895, 0.23646043049281135, 0.2497626494835405, 0.2328693575280554, 0.2476191526169286, 0.24926081999186792, 0.22955395642887144, 0.23540200713072337, 0.24256359033432662, 0.23535256659356402, 0.22585722259884955, 0.23929224061030968, 0.2362090051356776, 0.2323842137759807, 0.23957762155024445, 0.23328206293723164, 0.24211033893858686, 0.24111823025433457, 0.2357599771234627, 0.23172222019410602, 0.23386546501926347, 0.2514914059828894, 0.2433166304037121, 0.2378106768261276, 0.22987670018611586, 0.230959424767278, 0.23733513216104576, 0.24432600466717108, 0.23943918337132417, 0.24938077077853912, 0.24655740848724164, 0.2478333150456641, 0.25572608141045944, 0.24093728490612087, 0.24338949404145574, 0.24488756612089335, 0.2545141223452839, 0.24692704464655882, 0.2547990579669382, 0.2425614733468084, 0.24526120959689804, 0.2513910967254025, 0.24536202011593417, 0.24960096059914896, 0.24901917171390617, 0.24787598632860416, 0.25261163421194344, 0.24638482389569866, 0.2529066298522201, 0.24463685903259935, 0.2410413659926431, 0.2477684386403245, 0.2515937814708142, 0.25298880261606443, 0.2537755832423036, 0.24700629240011468, 0.25862112336372045, 0.2538327400974345, 0.25513705633142414, 0.25248319325128604, 0.25129741044970705, 0.25710856879823935, 0.2602397995252235, 0.2535753774401896, 0.25966272229219184, 0.26258349593947916, 0.2575982420467863, 0.25366368794850275, 0.26840075843182265, 0.2604089367623423, 0.26335741690926107, 0.26115994502370266, 0.27586783806556, 0.2699058622340946, 0.2690262606038767, 0.26124058914023873, 0.2695645693068703, 0.269948998539179, 0.2699922961274199, 0.2781037180410588, 0.26918452950742316, 0.2656496547746892, 0.27793413524826366, 0.2727473574854872, 0.26761945906807394, 0.28111935063612226, 0.2739635939691581, 0.2791407521741063, 0.27640176702327296, 0.27548765678269166, 0.2720885994460653, 0.2716641928087555, 0.29375703590830754, 0.2741913403402649, 0.2834579655352761, 0.2800045997799173, 0.2746915383556602, 0.28199352176093, 0.2871078792502921, 0.2875168366430729, 0.2797919814200962, 0.2876746460211043, 0.2826278824541791, 0.2888666771662732, 0.28122600651912244, 0.2926978058565189, 0.2832188007111351, 0.2851658707082856, 0.2897379945543613, 0.2852640647404626, 0.29130700086334754, 0.29031389254127066, 0.301364694575907, 0.29234155917576715, 0.30245087251943703, 0.29240411739138994, 0.2921989212068273, 0.29711635103997064, 0.30123716481395213, 0.29383554357085745, 0.2976747955944316, 0.2940556710385078, 0.29956718912238584, 0.2989022258906534, 0.3001505616760137, 0.29694770219023614, 0.296321282619793, 0.30678381798241067, 0.2958103663415885, 0.29974740378412545, 0.29243658106847137, 0.29601444117724895, 0.2992933266522253, 0.3056794012914978, 0.3159245145963688, 0.3084262056714472, 0.3033023014774217, 0.2999487395635715, 0.305078714501624, 0.29950604752144394, 0.3001469790424202, 0.3019553049303153, 0.3047376123220459, 0.30005093909087865, 0.2961901391984201, 0.3029952965421127, 0.2991081877867235, 0.3034919787508746, 0.30540474100659293, 0.29712304374312654, 0.304328731279455, 0.30321371049492385, 0.3067686218019648, 0.30573491976760764, 0.30279137281810536, 0.304144932066693, 0.3118132336941712, 0.3035398056803673, 0.3062666428193231, 0.3056575841106036, 0.30131836995190264, 0.3063342159285265, 0.31325177424678613, 0.2976547159327596, 0.3059330361475255, 0.2988617525655119, 0.29846939126796584, 0.3085705796804498]\n",
            "test_acc_list_exp = [73.40580823601721, 79.92086662569146, 86.0479409956976, 85.25276582667486, 87.02750460971113, 87.60755992624462, 88.40273509526736, 89.31699446834665, 89.8740012292563, 90.96496619545175, 90.20052243392747, 91.4259373079287, 91.59496004917025, 91.09557467732022, 91.30301167793485, 91.72172710510141, 91.09557467732022, 91.82928703134604, 91.69483712354025, 91.82160417947142, 92.55147510755992, 92.82805777504609, 92.3939766441303, 91.19161032575292, 92.22111247695145, 92.82037492317149, 92.24031960663798, 92.8242163491088, 92.70513214505225, 92.94330055316533, 92.41318377381684, 92.88952059004302, 92.42086662569146, 92.1980639213276, 92.02519975414874, 92.71665642286416, 92.86647203441917, 92.96250768285188, 92.83958205285802, 93.14305470190534, 93.32744314689613, 92.98939766441303, 93.01244622003688, 93.0278119237861, 93.2160417947142, 92.97019053472648, 93.29287031346036, 93.04317762753534, 93.25829748002458, 92.65903503380454, 92.59373079287032, 93.4119545175169, 93.1238475722188, 93.28134603564843, 92.85494775660726, 93.31207744314689, 93.42347879532882, 93.00476336816226, 93.08927473878303, 93.51951444376152, 93.00860479409957, 93.36585740626921, 93.20835894283958, 92.98555623847572, 93.39274738783037, 92.60141364474492, 93.18146896127843, 93.35049170251997, 93.68469575906576, 93.27750460971113, 93.51951444376152, 93.6578057775046, 93.51951444376152, 93.1238475722188, 92.98555623847572, 93.27366318377382, 93.15457897971727, 93.38890596189306, 93.45421020282728, 93.32360172095882, 93.44268592501537, 93.38122311001844, 93.4618930547019, 93.59634296250768, 93.42347879532882, 93.34665027658266, 93.31976029502151, 93.7077443146896, 93.42347879532882, 93.34665027658266, 93.49646588813768, 93.48110018438844, 93.6578057775046, 93.37354025814382, 93.1238475722188, 93.1737861094038, 93.24677320221267, 93.45421020282728, 93.71542716656423, 93.39658881376766, 93.5041487400123, 93.58097725875845, 93.9881684081131, 93.26982175783651, 93.0201290719115, 93.61555009219423, 93.52335586969883, 93.39274738783037, 93.91518131530424, 93.71926859250154, 93.86140135218193, 93.77688998156115, 93.37354025814382, 93.69237861094038, 93.6578057775046, 93.80762138905962, 93.49262446220037, 93.66548862937923, 93.40427166564228, 93.6578057775046, 93.68469575906576, 93.66548862937923, 93.61939151813154, 94.08420405654579, 93.59634296250768, 93.65012292563, 93.6539643515673, 93.61939151813154, 93.61170866625692, 93.48878303626306, 93.58097725875845, 93.73847572218807, 93.73079287031346, 93.66164720344192, 93.4119545175169, 93.75, 93.48110018438844, 93.58097725875845, 93.86524277811924, 93.73079287031346, 93.47725875845114, 93.90749846342962, 94.19944683466503, 93.66933005531654, 93.66933005531654, 93.79609711124769, 93.700061462815, 93.88060848186846, 93.80762138905962, 93.76920712968654, 93.75, 93.86524277811924, 93.88829133374308, 93.6078672403196, 93.75768285187462, 93.76152427781193, 93.87292562999386, 94.0419483712354, 93.93054701905348, 93.85371850030731, 93.91902274124155, 93.6539643515673, 93.9881684081131, 93.66548862937923, 93.53872157344806, 93.89981561155501, 94.01505838967425, 94.06115550092194, 93.7077443146896, 93.799938537185, 93.59250153657038, 93.91902274124155, 94.02658266748617, 93.82298709280884, 94.01505838967425, 93.90749846342962, 93.96511985248924, 93.91133988936693, 93.76536570374923, 94.0918869084204, 93.78841425937308, 93.96896127842655, 94.09956976029503, 94.14566687154272, 93.8921327596804, 93.91518131530424, 93.8921327596804, 94.26090964966195, 93.80762138905962, 93.88060848186846, 94.01889981561156, 94.07652120467118, 94.14566687154272, 94.10341118623234, 93.8460356484327, 94.02658266748617, 93.91902274124155, 93.86908420405655, 94.12645974185618, 94.1379840196681, 93.93822987092808, 94.07652120467118, 94.08420405654579, 94.0381069452981, 93.85755992624462, 94.02658266748617, 94.00353411186232, 94.18792255685311, 94.02274124154886, 94.0918869084204, 94.21865396435157, 93.99200983405039, 94.06499692685925, 94.28011677934849, 93.99969268592501, 93.96896127842655, 94.28011677934849, 93.97664413030117, 94.22633681622618, 94.24170251997542, 94.05731407498463, 94.13414259373079, 94.1917639827904, 94.49139520590043, 93.99200983405039, 94.28779963122311, 94.03042409342348, 94.38767670559312, 94.19944683466503, 94.1840811309158, 94.11109403810694, 94.07267977873387, 94.23017824216349, 94.38383527965581, 94.32237246465888, 94.21481253841426, 94.22249539028887, 94.24938537185002, 94.31853103872157, 94.34926244622004, 94.21865396435157, 94.29932390903504, 94.25322679778733, 94.26090964966195, 94.29548248309773, 94.41840811309157, 94.17639827904118, 94.36078672403197, 94.37231100184388, 94.13414259373079, 94.19944683466503, 94.34157959434542, 94.34157959434542, 94.36846957590657, 94.35310387215735, 94.24170251997542, 94.37231100184388, 94.36462814996926, 94.39920098340504, 94.25706822372464, 94.43761524277812, 94.50676090964966, 94.53749231714812, 94.40304240934235, 94.40688383527966, 94.24554394591273, 94.41072526121697, 94.35310387215735, 94.45298094652735, 94.41072526121697, 94.36846957590657, 94.35310387215735, 94.34926244622004, 94.48755377996312, 94.30316533497235, 94.5259680393362, 94.54133374308543, 94.36846957590657, 94.4299323909035, 94.4299323909035, 94.34157959434542, 94.45682237246466, 94.36462814996926, 94.5259680393362, 94.38383527965581, 94.34542102028273, 94.43761524277812, 94.35694529809466, 94.34157959434542, 94.35310387215735, 94.39920098340504, 94.3761524277812, 94.44913952059004, 94.39920098340504, 94.25706822372464, 94.40688383527966, 94.40304240934235, 94.53749231714812, 94.49139520590043, 94.39920098340504]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list_01 = [2.3849518770770977, 2.2417600981911345, 2.2415069635644516, 2.2409557133186153, 2.2419579268147953, 2.240125378942102, 2.240931454066662, 2.2417839348800785, 2.242252948807507, 2.241471426273749, 2.241231945472035, 2.2413479971691843, 2.241036834432504, 2.2407813569717616, 2.2415969093963706]\n",
        "train_acc_list_01 = [18.56855479089465, 18.617257808364215, 18.746426680783483, 18.60243515087348, 18.61937533086289, 18.886183165696135, 18.60243515087348, 18.598200105876124, 18.604552673372154, 18.740074113287452, 18.731604023292746, 18.814187400741133, 18.848067760719957, 18.82901005823187, 18.752779248279513]\n",
        "test_loss_list_01 = [2.2387831538331273, 2.241503697984359, 2.241926829020182, 2.240557459055209, 2.2406100852816713, 2.252836311564726, 2.2468298743752873, 2.2446437150824305, 2.2425780202828203, 2.240177970306546, 2.243702617346072, 2.2441832843948815, 2.253918958645241, 2.245230858232461, 2.2435202879064224]\n",
        "test_acc_list_01 = [18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 14.27858020897357, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463]\n",
        "train_loss_list_001 = [2.2888378896687414, 2.232192633274771, 1.4893088792236193, 0.5001831288098643, 0.3830500937251218, 0.32867970416539405, 0.29770232389774426, 0.27895135900919354, 0.2627035691970732, 0.24814978033950336, 0.23438045485475198, 0.2243682525668364, 0.21301924496848731, 0.20818865677811266, 0.1958482328166322]\n",
        "train_acc_list_001 = [18.50926416093171, 18.968766543144522, 47.80307040762308, 84.23292747485442, 88.15881418740074, 89.9142403388036, 91.04923239809423, 91.63790365272631, 92.08893594494441, 92.58020116463737, 93.03758602435151, 93.36791953414505, 93.715193223928, 93.84012705134992, 94.35044997353097]\n",
        "test_loss_list_001 = [2.2451091665847627, 2.2302937355695986, 0.684039752711268, 0.4374444575286379, 0.39873581839834943, 0.36398082489476485, 0.3313831433507742, 0.3210300371854329, 0.2847569804346445, 0.29315854806233854, 0.27853756525791157, 0.26896954926789973, 0.2596830692069203, 0.26028463620619446, 0.2491153629460171]\n",
        "test_acc_list_001 = [18.88060848186847, 19.053472649047325, 78.20759065765212, 86.33988936693301, 87.90334972341734, 88.90596189305471, 90.08143822987093, 90.81515058389674, 91.70636140135218, 91.54118008604794, 91.8638598647818, 92.2480024585126, 92.43239090350338, 92.50537799631223, 92.82037492317149]\n",
        "train_loss_list_0001 = [1.8565612323885041, 0.5532636212785715, 0.4003713336094285, 0.3402645644860539, 0.309145948165639, 0.2848975209968523, 0.262982070708501, 0.24855145600026216, 0.2386487888167221, 0.2278864942390098, 0.21327269485164788, 0.20556094100683686, 0.19517452138549268, 0.18913837452608395, 0.18232752032436653]\n",
        "train_acc_list_0001 = [34.7993647432504, 82.35468501852831, 87.65272631021705, 89.51614610905241, 90.6723133933298, 91.51932239280042, 92.26892535733192, 92.59925886712546, 92.97617787188989, 93.34674430915828, 93.69401799894123, 93.97353096876654, 94.31868713605083, 94.53255690841715, 94.71254632080466]\n",
        "test_loss_list_0001 = [1.0861167063315709, 0.46979708385233787, 0.41340532643245714, 0.3364271931350231, 0.3262409484561752, 0.34040282589986043, 0.28803076817854945, 0.2942232322678262, 0.2779932311169949, 0.27514038454083833, 0.2478603608906269, 0.2512748020463714, 0.26197264781769586, 0.2462065773194327, 0.24985540344142446]\n",
        "test_acc_list_0001 = [64.00583896742471, 85.58312845728334, 87.41933005531654, 89.81637984019667, 89.970036877689, 89.99308543331284, 91.59111862323294, 91.35295021511985, 91.82928703134604, 92.03672403196066, 92.76275353411187, 92.83574062692071, 92.90488629379226, 92.98555623847572, 92.87031346035648]\n",
        "train_loss_list_00001 = [1.2440541174192092, 0.4583125376927497, 0.36052036624613815, 0.3151768069603256, 0.2875107263081119, 0.26835051384883196, 0.25187577066948097, 0.23390740957767336, 0.22118305101950317, 0.21190600004299545, 0.20555683115350845, 0.1952596850249018, 0.18612936185546683, 0.18178928815090883, 0.17480877608181986]\n",
        "train_acc_list_00001 = [57.18581259925887, 85.53943885653786, 88.79618845950239, 90.25304393859184, 91.214399152991, 92.01058761249338, 92.36421386977237, 93.09899417681312, 93.43991529910005, 93.80412916887242, 93.86765484383271, 94.25092641609317, 94.500794070937, 94.64478560084702, 94.82477501323451]\n",
        "test_loss_list_00001 = [0.5935118007017117, 0.43328142947718207, 0.3668157114994292, 0.34575528556517526, 0.3590214093964474, 0.3134572241893586, 0.2903973020467104, 0.28062032824199573, 0.26905518266208034, 0.26447016406146917, 0.27132851287138227, 0.26421160440818936, 0.25491809147391836, 0.24584030433028353, 0.2630316608895858]\n",
        "test_acc_list_00001 = [80.93500307314075, 86.63567916410571, 88.97510755992624, 89.3438844499078, 88.99431468961278, 90.58466502765826, 91.54502151198525, 91.56422864167179, 92.02135832821143, 92.25184388444991, 91.97910264290104, 92.37861094038107, 92.72433927473878, 92.96250768285188, 92.59757221880763]\n",
        "\n",
        "\n",
        "train_loss_list_const = [1.5944857293674293, 0.4869092239677745, 0.3710624326454592, 0.3296906750215101, 0.29884549410039496, 0.27146996971633697, 0.2604337949053382, 0.24417246496532022, 0.23419785655045575, 0.22457996957751147, 0.21173857996457315, 0.20202396321425917, 0.1943116241051414, 0.18745123495052501, 0.1774632519257424, 0.17214223601650902, 0.16552220173886797, 0.15473239550866733, 0.15404348591276948, 0.14213740165398372, 0.13690419029192066, 0.13265049481779578, 0.12643728150322348, 0.1223953123176647, 0.11572830584960256, 0.10872174589453028, 0.10607895330301306, 0.10143619665729645, 0.09846852844464908, 0.09155762590831373, 0.09176684167932689, 0.08320119946962853, 0.07973214024854547, 0.07931727142424037, 0.06983993944351063, 0.07091535234995448, 0.0671700671211713, 0.06470882359558974, 0.061619590856841586, 0.061459206172131346, 0.057121708876198427, 0.053374216585170206, 0.0498615506466044, 0.05073856983490308, 0.04987424655961312, 0.046790412364042994, 0.04711244383266544, 0.04263703334889801, 0.043034394194061555, 0.04126133985860339, 0.03625233271121373, 0.03783284285014904, 0.0379838087573284, 0.03416189216178284, 0.03383261713717981, 0.03370409071676403, 0.033968668481206325, 0.03165312097212274, 0.03038872497441464, 0.02841417055683044, 0.029356409022635036, 0.02889881345480024, 0.028939039615515447, 0.02568263415692672, 0.027656220918051838, 0.026301205131101617, 0.023402816848356156, 0.024891200900357974, 0.022950206206843942, 0.022271349775869285, 0.0250707899006071, 0.022344395426495713, 0.019562920480307198, 0.022191016989943715, 0.02281398802152065, 0.0203801996838415, 0.020550807893653537, 0.02117716848076041, 0.019771566791103416, 0.022764958889409142, 0.017198871523580392, 0.02078044665365311, 0.020109122048629333, 0.01588759385191313, 0.01774437841210012, 0.020397339600326397, 0.01534856222032823, 0.016435430918308794, 0.01711455919842304, 0.01770936636729633, 0.018040290616609177, 0.01564757464108749, 0.015251886521385311, 0.01534154344099917, 0.015427147975282334, 0.01419120683843651, 0.01811075882562171, 0.016642653122089984, 0.01305790530750528, 0.013949767823940267, 0.014656124457392462, 0.013457846312966957, 0.014343252129156173, 0.013850259383991146, 0.012460459422866397, 0.012413865320129272, 0.013889214684304953, 0.01221698466477806, 0.015144580592092777, 0.010421825547786688, 0.01315658220278792, 0.014559990600058128, 0.012665790851758906, 0.01143317199561781, 0.011717887349175301, 0.011158559258571676, 0.012634062059679534, 0.011741106513152792, 0.012515452659958159, 0.011751638104695717, 0.011457235534427129, 0.011438692509360596, 0.012512708038015468, 0.008602326916451698, 0.010682771495822786, 0.011196336384384224, 0.009268063090769713, 0.009998142540840234, 0.012729091897495166, 0.010267185127698763, 0.010566324319321028, 0.010713477439735606, 0.010119398856430022, 0.012654226244056406, 0.009628543746825273, 0.0102562899387879, 0.010169452542220153, 0.011113815507671713, 0.01246779066452764, 0.007518461719419533, 0.007389902458939484, 0.011104748547043264, 0.01135712010794771, 0.008941290669380526, 0.009924649224834431, 0.009838336234147059, 0.007612355080635734, 0.009282812758648675, 0.008423602285464519, 0.009065308554876006, 0.010365516194014629, 0.006115070432431249, 0.006490228478146479, 0.011122480412586441, 0.00872685812082587, 0.008791860558152852, 0.009787340952780856, 0.008418811355189983, 0.007423892029066022, 0.009898100130695444, 0.008828991230531168, 0.007722080036888794, 0.009378696144602659, 0.00829515335842403, 0.006554383687879633, 0.006126663065066873, 0.010910632581322311, 0.007716069375264186, 0.006358072765228986, 0.007697185452836498, 0.008877029283653514, 0.008559577807589543, 0.00707696825589556, 0.00850764569837624, 0.0062317561151193695, 0.0066259507287960455, 0.008618629247509826, 0.007878521594295732, 0.007498281048782551, 0.006598806819186967, 0.006230976371933824, 0.006141664580167378, 0.007923200858393221, 0.008045455975570557, 0.00792612456981016, 0.007601726206049413, 0.007419711852838272, 0.004584900550783639, 0.00685006011318293, 0.008050930358640324, 0.008370204849807444, 0.005507358266772813, 0.006857401374734981, 0.008172955556791895, 0.004244687181667273, 0.008633549227632837, 0.008705449396538356, 0.0067560189635043925, 0.006710546794999935, 0.00550614756518684, 0.0069180619830691474, 0.00835907400430254, 0.005712247417545953, 0.006098340349643973, 0.007154704712266156, 0.0061693518397019, 0.005750073719051822, 0.005536363746094307, 0.006909997050809464, 0.006392437509654407, 0.004530645426771153, 0.00935107147158372, 0.005991038718238898, 0.004413133404240297, 0.007152540924743997, 0.006123794079590131, 0.004265024197029019, 0.007488072243164834, 0.006849384789138319, 0.006199823599835201, 0.004939858833603685, 0.006542191565922548, 0.006134184821023406, 0.006127372573951168, 0.006120713498504846, 0.005705225819374859, 0.0054501006014126396, 0.00476737067947397, 0.005204442173390228, 0.006888031104115721, 0.005438639611901732, 0.005198591940753804, 0.006226397409455738, 0.00398913007948585, 0.006725981141281091, 0.0057350191533619786, 0.004547678196042624, 0.003760278201966155, 0.004862335098237806, 0.0053206211415940214, 0.004511028374038128, 0.006714381331292853, 0.006507630908786355, 0.004150821972775074, 0.005154698122662017, 0.005662354828146746, 0.004606740834664833, 0.004731273036621596, 0.004916682220897782, 0.0048261530498232945, 0.006001339212270287, 0.007263619651073496, 0.0038986454153652154, 0.005036929771996826, 0.005049693082181207, 0.00644224065052205, 0.004578751418381182, 0.004686958738437559, 0.003585059260631736, 0.003101582141557616, 0.007221638428928184, 0.005000114876925785, 0.0051700827908760325, 0.005254589704386773, 0.00406367993565528, 0.0036232792127412505, 0.006171727926178867, 0.005875237415135579, 0.00392443083374783, 0.006561909474143628, 0.0037487958143322068, 0.004443046452993911, 0.005447318903908573, 0.004017829913798502, 0.00472008602112813, 0.0055074588290326305, 0.005279580161203713, 0.0036886348879267314, 0.0035813810519778975, 0.004883374933941502, 0.004846986711028166, 0.004054715485154408, 0.004949977601104352, 0.0046826732979178415, 0.004384392334953691, 0.004187341836835158, 0.004243570542040143, 0.005868256121943751, 0.00557879406272601, 0.0037290168971954394, 0.0024883511193469366, 0.004939023949055762, 0.004748034648033541, 0.0038421762496379727, 0.003486280636854575, 0.0036575751641310756, 0.00670746078328728, 0.004645855993902198, 0.004636629221823842, 0.004005611049817734]\n",
        "train_acc_list_const = [44.03176283748015, 84.49761778718899, 88.61196400211752, 89.87400741132875, 91.01958708311275, 91.79460031762838, 92.2350449973531, 92.80889359449444, 93.03546850185283, 93.37427210164108, 93.80412916887242, 94.0052938062467, 94.38009528851244, 94.6278454208576, 94.88618316569614, 95.01323451561673, 95.1868713605082, 95.66543144520911, 95.57226045526734, 95.91953414505029, 96.01905770248808, 96.16093170989942, 96.35150873478031, 96.54632080465855, 96.74113287453679, 96.86183165696136, 96.96347273689783, 97.10746426680784, 97.21757543673901, 97.32133403917416, 97.3276866066702, 97.63896241397565, 97.72154579142403, 97.7342509264161, 97.88883006881949, 97.87400741132875, 97.96506087877184, 98.09211222869243, 98.24245632609846, 98.17257808364214, 98.28692429857067, 98.34833245103229, 98.44997353096876, 98.49444150344097, 98.49655902593965, 98.54102699841185, 98.55796717840127, 98.67443091582848, 98.63419798835362, 98.74219163578613, 98.87559555320276, 98.77607199576495, 98.78030704076231, 98.9793541556379, 98.94970884065643, 98.9433562731604, 98.90947591318158, 98.98782424563261, 98.96453149814717, 99.08099523557438, 99.05770248808894, 99.05134992059291, 99.0428798305982, 99.1784012705135, 99.13393329804128, 99.1148755955532, 99.20381154049761, 99.18051879301217, 99.27580730545262, 99.23980942297511, 99.1784012705135, 99.25674960296453, 99.3499205929063, 99.26310217046056, 99.2503970354685, 99.32874536791954, 99.28215987294865, 99.27157226045527, 99.29486500794071, 99.24404446797247, 99.42615140285865, 99.2779248279513, 99.31180518793012, 99.46214928533615, 99.40921122286925, 99.36262572789836, 99.49602964531498, 99.43673901535203, 99.42615140285865, 99.40921122286925, 99.41132874536792, 99.47697194282689, 99.49602964531498, 99.49814716781366, 99.47273689782953, 99.5044997353097, 99.39015352038115, 99.45156167284277, 99.57649550026468, 99.55320275277924, 99.5404976177872, 99.5659078877713, 99.53414505029116, 99.54685018528322, 99.58073054526204, 99.5849655902594, 99.49391212281631, 99.58920063525674, 99.51720487030175, 99.66543144520911, 99.5574377977766, 99.56167284277396, 99.56802541026998, 99.60402329274748, 99.62519851773425, 99.6019057702488, 99.5934356802541, 99.62519851773425, 99.60402329274748, 99.61461090524087, 99.61461090524087, 99.63790365272631, 99.6019057702488, 99.70566437268396, 99.66119640021175, 99.64002117522499, 99.69507676019057, 99.68025410269983, 99.58920063525674, 99.6569613552144, 99.62519851773425, 99.6569613552144, 99.66754896770779, 99.54261514028586, 99.69719428268925, 99.63366860772896, 99.67178401270513, 99.60825833774484, 99.6019057702488, 99.75436739015352, 99.75436739015352, 99.59555320275278, 99.62731604023293, 99.6929592376919, 99.65484383271573, 99.66966649020645, 99.7289571201694, 99.71836950767602, 99.69084171519323, 99.67601905770249, 99.66119640021175, 99.79671784012704, 99.78824775013234, 99.62519851773425, 99.71836950767602, 99.72683959767072, 99.68872419269455, 99.73107464266808, 99.73107464266808, 99.67601905770249, 99.70989941768131, 99.73319216516676, 99.68025410269983, 99.7204870301747, 99.78824775013234, 99.80518793012176, 99.64849126521969, 99.73107464266808, 99.79883536262572, 99.74377977766014, 99.69719428268925, 99.69719428268925, 99.77130757014294, 99.71625198517734, 99.7924827951297, 99.784012705135, 99.70354685018528, 99.75860243515088, 99.73319216516676, 99.79036527263102, 99.78189518263632, 99.79036527263102, 99.73530968766543, 99.69719428268925, 99.75013234515616, 99.72472207517205, 99.76071995764956, 99.84330333509793, 99.784012705135, 99.73530968766543, 99.69084171519323, 99.82212811011117, 99.76919004764426, 99.73319216516676, 99.85812599258867, 99.70989941768131, 99.70142932768661, 99.76071995764956, 99.78824775013234, 99.8284806776072, 99.7924827951297, 99.7204870301747, 99.8009528851244, 99.81577554261514, 99.75436739015352, 99.7924827951297, 99.81365802011646, 99.8009528851244, 99.77554261514028, 99.77977766013764, 99.85389094759132, 99.67601905770249, 99.81789306511382, 99.86871360508205, 99.74377977766014, 99.78189518263632, 99.84965590259397, 99.77130757014294, 99.78824775013234, 99.82001058761249, 99.81365802011646, 99.77130757014294, 99.78189518263632, 99.79883536262572, 99.80730545262044, 99.82636315510852, 99.79036527263102, 99.83059820010588, 99.81154049761778, 99.7564849126522, 99.8094229751191, 99.8284806776072, 99.81154049761778, 99.88353626257279, 99.75224986765484, 99.79671784012704, 99.85389094759132, 99.88565378507147, 99.83059820010588, 99.80730545262044, 99.84118581259926, 99.77130757014294, 99.81154049761778, 99.85177342509265, 99.83271572260455, 99.8094229751191, 99.8284806776072, 99.86236103758603, 99.8369507676019, 99.82424563260984, 99.8009528851244, 99.78824775013234, 99.84330333509793, 99.82636315510852, 99.8284806776072, 99.78189518263632, 99.85812599258867, 99.85177342509265, 99.89624139756485, 99.89835892006353, 99.77130757014294, 99.84118581259926, 99.81577554261514, 99.8369507676019, 99.85600847008999, 99.86659608258337, 99.81154049761778, 99.79671784012704, 99.87718369507677, 99.79883536262572, 99.85812599258867, 99.85389094759132, 99.83906829010058, 99.88353626257279, 99.84753838009529, 99.8369507676019, 99.83906829010058, 99.88565378507147, 99.87506617257809, 99.83271572260455, 99.84330333509793, 99.85812599258867, 99.86659608258337, 99.8369507676019, 99.8284806776072, 99.84753838009529, 99.85177342509265, 99.7924827951297, 99.79671784012704, 99.88141874007411, 99.92376919004765, 99.86024351508735, 99.84753838009529, 99.85600847008999, 99.87506617257809, 99.87718369507677, 99.79671784012704, 99.86024351508735, 99.85177342509265, 99.87083112758073]\n",
        "test_loss_list_const = [0.9403148495099124, 0.46971940738605517, 0.34306909633325594, 0.3384028169892582, 0.3164549315823059, 0.31764499748162195, 0.282772644879479, 0.27239831069520876, 0.2788072020618939, 0.2605946859454407, 0.2569786167758353, 0.25151387514436946, 0.23040011815507622, 0.23389688687508597, 0.23323347488893012, 0.22432381940969065, 0.23885389055837603, 0.22965874502837075, 0.23512107754747072, 0.23369459731175618, 0.22183155821745887, 0.2491116562639089, 0.23399925456546686, 0.25359821337841304, 0.23499618255186314, 0.22481991891183106, 0.2324665375966944, 0.23745987874766192, 0.24356824685545528, 0.24952035153503804, 0.23930090873082185, 0.25436064657554325, 0.25367995643732594, 0.26609678615761156, 0.2672480665622096, 0.2746706861893044, 0.25824960023529975, 0.26025373894063863, 0.2791316908072023, 0.26918739248432366, 0.2763673086997633, 0.2997446244436444, 0.2916372886438872, 0.3003798500981693, 0.2806282534501424, 0.2931029381979184, 0.29610180335265457, 0.3087221756942716, 0.32797835285172744, 0.30948590080929445, 0.31205729929292025, 0.3264701991711798, 0.31612680869761345, 0.3249107254029927, 0.3218845636081681, 0.3128845189679779, 0.3158182442261308, 0.3337518404621412, 0.34533743259003935, 0.32409434777447116, 0.3321887091642209, 0.3324712138604738, 0.33405348602864965, 0.3346349073199592, 0.3316200801226146, 0.35435040097446274, 0.3272651647738017, 0.3605334343612377, 0.3315225986252521, 0.36446072681642633, 0.37520757036320135, 0.3792235794586732, 0.3674935271379118, 0.3611337201596767, 0.35125305392213313, 0.3664919300844856, 0.3579704279956572, 0.358177753384499, 0.37499817195074525, 0.3516562903372973, 0.36848902222061275, 0.37098927200570997, 0.37174441270968495, 0.3886868230103716, 0.36849470644751015, 0.3531273581982389, 0.3660184144471571, 0.37421249364520986, 0.3748011891273599, 0.3667490633350669, 0.3643053767691348, 0.37450089440772344, 0.37444961906465535, 0.3669995936729452, 0.37039543743080955, 0.38952702733085437, 0.39265704210208474, 0.3686957350274658, 0.3655954713695774, 0.39540933398529887, 0.37815322328870205, 0.3858970995974161, 0.37387897242682383, 0.37425000457020074, 0.38926156392941874, 0.3786647673787586, 0.37843504400156874, 0.4000659274078869, 0.38383779517721894, 0.38883355182513374, 0.38116612786189746, 0.39886108679952575, 0.4039541971048011, 0.4085461224575399, 0.4040325807553588, 0.40590017215878355, 0.38623136059180196, 0.38693335685221586, 0.41144327609343273, 0.3980357802459313, 0.4007313655798926, 0.40366758222636934, 0.38523547610669745, 0.42097171660804866, 0.4246416387294291, 0.40921079142786126, 0.4052840040850581, 0.4231068941344525, 0.38739459854824576, 0.4194799537094785, 0.41747657055327414, 0.4164134435739149, 0.3939945743985328, 0.393845445202554, 0.4172966992665155, 0.4208380446276244, 0.408174136686641, 0.43686796033608855, 0.3842315211042981, 0.4022488394277353, 0.41533097460427704, 0.4123304054533661, 0.3947430606748836, 0.4344836208585869, 0.42245670334509045, 0.4120090134880122, 0.4084896664774301, 0.4208405370477076, 0.4342218170389898, 0.4342225538966173, 0.414541769489719, 0.42016579197290554, 0.43672865715023934, 0.3970657474470927, 0.4120631425404081, 0.4072342333657777, 0.4361969401840778, 0.40728934426043256, 0.4082725720443562, 0.43000999789721533, 0.4488575828864294, 0.4490402939628956, 0.42708765610358584, 0.4119907574152903, 0.4171651719850214, 0.46127884272559017, 0.424621216852364, 0.42663686249551236, 0.443727373143238, 0.4234333980655042, 0.4163527505554478, 0.41962003758446514, 0.43725314623146666, 0.4135830312939909, 0.4485339165719993, 0.438206387026345, 0.42548432996423513, 0.4139385414218493, 0.422614497990877, 0.44258191624619797, 0.5020544356643679, 0.42181872726217207, 0.45136614169414135, 0.42586187040889817, 0.4617275644605066, 0.42785005136758236, 0.4181436715811929, 0.43456211805745376, 0.456493051905258, 0.4419752404130265, 0.439949865836431, 0.458016514120733, 0.45355927485370023, 0.4266304138993077, 0.4774111132959233, 0.43147677636942733, 0.42984621565533326, 0.4257584124226888, 0.41347907219703, 0.44227188734301165, 0.44509480125270784, 0.42359101505694435, 0.45620652116542937, 0.4532003666402078, 0.4544208156419735, 0.4288948582868804, 0.45772276510365817, 0.4652134206776014, 0.4481767811538542, 0.45044724102926387, 0.45808432965228957, 0.4832801581086481, 0.4330355538119215, 0.450099578350965, 0.44405912482818843, 0.4245691280629413, 0.47918898603130206, 0.4441465862088508, 0.4232571859679678, 0.42079631317699073, 0.46119900758140814, 0.4447093260310152, 0.45832808096619215, 0.4615354106998911, 0.46208279893970955, 0.44396297247651234, 0.44644354116719437, 0.4640259245666219, 0.466397661642701, 0.46452038604811785, 0.4498248620908342, 0.4491576017337103, 0.465446551038208, 0.47035092626716574, 0.4713521593012938, 0.4597488452506927, 0.4512224776633814, 0.48416280633240355, 0.4734927043455708, 0.46729338517887337, 0.46055449181357305, 0.44595291037751617, 0.45059747884606977, 0.4550898942740305, 0.4553583972247354, 0.484807043427638, 0.47273430763972085, 0.45941312947109636, 0.4590342458538419, 0.4751177701332113, 0.47533442642466694, 0.45637929342760175, 0.4544383191900766, 0.4463540438526109, 0.45517611408414427, 0.4642113188975582, 0.44581558580930325, 0.4676268981386195, 0.46861990691874833, 0.47670730448090565, 0.4613685188541079, 0.46828112538502203, 0.47280027833310706, 0.44121663249097764, 0.4606796685479554, 0.5259090004415781, 0.48354477134040175, 0.469667333146265, 0.4692415113266393, 0.45400641179944884, 0.47124824484846756, 0.4692198993460111, 0.44249545135900525, 0.4556816854969571, 0.4647824307538423, 0.47515893393360514, 0.44086019356972445, 0.47874322776481804, 0.48824521492911027, 0.4832109012732319, 0.45331234720138397, 0.46669590011166007, 0.4659509803990231, 0.4592359331253843, 0.4794528412395248, 0.4734935767528619, 0.4946094546649678, 0.47707713397183255, 0.4516702479244593, 0.44164761501893984, 0.48553112554637823, 0.4603754159190929, 0.4560155915005096, 0.48063205852739366, 0.4870615708185177, 0.503982417563926, 0.4785489882890354, 0.44539653152848285, 0.4539821001874539, 0.47518135870204253]\n",
        "test_acc_list_const = [71.6080208973571, 85.50245851259987, 89.67040565457899, 89.58589428395821, 90.46173939766442, 90.5577750460971, 91.50276582667486, 92.0520897357099, 91.96757836508912, 92.47464658881377, 92.6820835894284, 92.88952059004302, 93.56177012907192, 93.4580516287646, 93.66548862937923, 93.80377996312231, 93.34665027658266, 93.71158574062692, 93.5041487400123, 93.57329440688383, 94.06499692685925, 93.33896742470804, 93.72311001843885, 92.94330055316533, 93.68853718500307, 94.18792255685311, 93.96127842655194, 93.91518131530424, 93.82682851874615, 94.03042409342348, 94.1917639827904, 93.77304855562384, 93.9420712968654, 93.68853718500307, 94.08420405654579, 93.77304855562384, 93.91518131530424, 94.14566687154272, 93.93822987092808, 94.14566687154272, 94.06499692685925, 93.73079287031346, 93.8921327596804, 94.07267977873387, 94.01889981561156, 94.22633681622618, 93.86140135218193, 93.90749846342962, 93.97664413030117, 94.07652120467118, 94.03042409342348, 93.97280270436386, 94.21865396435157, 94.07267977873387, 94.09956976029503, 94.21865396435157, 94.30700676090964, 93.88444990780577, 94.06883835279656, 94.30316533497235, 94.23017824216349, 94.19944683466503, 94.05731407498463, 94.36846957590657, 94.26475107559926, 94.04578979717272, 94.31468961278426, 94.13030116779349, 94.3262138905962, 94.06499692685925, 93.98432698217579, 94.34926244622004, 94.3262138905962, 94.10341118623234, 94.35694529809466, 93.99969268592501, 94.21481253841426, 94.41840811309157, 94.13414259373079, 94.23017824216349, 94.39920098340504, 94.10341118623234, 94.0381069452981, 94.36846957590657, 94.44529809465274, 94.29548248309773, 94.39920098340504, 94.2839582052858, 94.34542102028273, 94.51444376152428, 94.34926244622004, 94.1418254456054, 94.45682237246466, 94.5221266133989, 94.41072526121697, 94.45298094652735, 93.9958512599877, 94.24938537185002, 94.57974800245852, 94.1418254456054, 94.18792255685311, 94.34926244622004, 94.48371235402581, 94.39535955746773, 94.39151813153042, 94.39535955746773, 94.41072526121697, 94.1418254456054, 94.24554394591273, 94.39535955746773, 94.49907805777505, 94.3262138905962, 94.22633681622618, 94.34542102028273, 94.42224953902888, 94.27627535341118, 94.46450522433928, 94.1840811309158, 94.1917639827904, 94.50291948371235, 94.09956976029503, 94.27243392747388, 94.38383527965581, 94.40304240934235, 94.10725261216963, 94.36462814996926, 94.52980946527352, 94.36078672403197, 94.50676090964966, 94.1917639827904, 94.25706822372464, 94.31084818684695, 94.43761524277812, 94.36462814996926, 94.42224953902888, 94.21097111247695, 94.38383527965581, 94.18023970497849, 94.21865396435157, 94.5259680393362, 94.53365089121081, 94.21865396435157, 94.56822372464659, 94.21865396435157, 94.31468961278426, 94.41072526121697, 94.50291948371235, 94.35310387215735, 94.51060233558697, 94.26090964966195, 94.3338967424708, 94.4299323909035, 94.40688383527966, 94.66425937307929, 94.41072526121697, 94.39535955746773, 94.23786109403811, 94.44145666871543, 94.30316533497235, 94.3761524277812, 94.04578979717272, 94.36846957590657, 94.3300553165335, 94.44145666871543, 94.64505224339274, 93.99200983405039, 94.53365089121081, 94.52980946527352, 94.3799938537185, 94.38767670559312, 94.54133374308543, 94.36078672403197, 94.44913952059004, 94.28779963122311, 94.48755377996312, 94.51828518746159, 94.4721880762139, 94.45682237246466, 94.39920098340504, 94.51444376152428, 94.10341118623234, 94.61432083589429, 94.41840811309157, 94.21865396435157, 94.46450522433928, 94.44913952059004, 94.4798709280885, 94.61432083589429, 94.22633681622618, 94.63352796558083, 94.51060233558697, 94.44145666871543, 94.5221266133989, 94.50291948371235, 93.93438844499079, 94.4721880762139, 94.48371235402581, 94.63736939151813, 94.5759065765212, 94.61047940995698, 94.39920098340504, 94.56822372464659, 94.21481253841426, 94.59895513214505, 94.32237246465888, 94.59127228027043, 94.25322679778733, 94.4299323909035, 94.54517516902274, 94.3338967424708, 94.40688383527966, 94.08420405654579, 94.5259680393362, 94.59895513214505, 94.6681007990166, 94.61047940995698, 94.44529809465274, 94.48371235402581, 94.48755377996312, 94.67962507682851, 94.36846957590657, 94.09572833435772, 94.35694529809466, 94.43377381684081, 94.55669944683467, 94.61047940995698, 94.36462814996926, 94.6220036877689, 94.69499078057775, 94.39535955746773, 94.47602950215119, 94.43761524277812, 94.3300553165335, 94.68730792870313, 94.56822372464659, 94.45298094652735, 94.61816226183159, 94.40304240934235, 94.48371235402581, 94.62968653964352, 94.54517516902274, 94.51828518746159, 94.60663798401967, 94.55669944683467, 94.50676090964966, 94.52980946527352, 94.59511370620774, 94.46066379840197, 94.40304240934235, 94.49139520590043, 94.4299323909035, 94.34157959434542, 94.6681007990166, 94.58358942839583, 94.61047940995698, 94.34926244622004, 94.57974800245852, 94.50676090964966, 94.71035648432698, 94.70651505838967, 94.5221266133989, 94.3761524277812, 94.50291948371235, 94.59127228027043, 94.52980946527352, 94.41456668715428, 94.53749231714812, 94.29932390903504, 94.5720651505839, 94.68730792870313, 94.61816226183159, 94.55285802089736, 94.6681007990166, 94.70651505838967, 94.59127228027043, 94.40304240934235, 94.69114935464044, 94.68730792870313, 94.5221266133989, 94.45682237246466, 94.51828518746159, 94.57974800245852, 94.61432083589429, 94.6220036877689, 94.34542102028273, 94.53749231714812, 94.40688383527966, 94.49523663183774, 94.42609096496619, 94.6757836508912, 94.42224953902888, 94.43377381684081, 94.56438229870928, 94.65273509526736, 94.49139520590043, 94.4721880762139, 94.3338967424708, 94.56054087277197, 94.70651505838967, 94.45682237246466]\n",
        "train_loss_list_cosine = [1.4922631353059113, 0.4829037673670425, 0.3722163860390826, 0.33177220926375245, 0.30060175129876227, 0.2739472426776964, 0.2571369293057499, 0.24421742562517565, 0.23246381484315323, 0.22585582048670064, 0.2115350698794776, 0.2027042203842786, 0.19336028488953586, 0.18796989869901803, 0.17635167322467335, 0.17350608562712425, 0.16392932351688705, 0.15824737119157786, 0.15103116432623812, 0.14280527628211148, 0.13911624957976465, 0.12778460819445814, 0.12645664817696503, 0.12117257786202963, 0.11404008718162048, 0.11024573898614261, 0.10301367604352918, 0.09962807324661957, 0.09590673247266429, 0.09140933892338822, 0.08890839097617563, 0.08312069913650028, 0.07912878103322049, 0.07539688910895247, 0.0703017507051307, 0.06730901317161112, 0.06545131962347604, 0.06006853946239806, 0.05934957680948644, 0.055661115047034776, 0.05501487654656535, 0.04808283364715756, 0.048917624036854686, 0.04978224897863177, 0.04599738403836765, 0.043476243917943865, 0.04154425250810538, 0.044289368833225914, 0.039924735993999975, 0.038134503976484824, 0.03573395804436448, 0.03633099462935413, 0.03407486775469762, 0.03466019954098273, 0.03175460545031914, 0.030751291346965928, 0.02861118235254312, 0.026954070920645837, 0.026345375887917098, 0.027870583739497655, 0.02564559653584358, 0.02595152790995118, 0.02664908538916373, 0.017370514487667858, 0.02682043477352759, 0.02288949422978072, 0.023653404380517905, 0.021177587269342865, 0.01825827019425427, 0.020935813946993946, 0.02142309610224594, 0.021006980622225117, 0.017297085196421504, 0.020975425832167893, 0.016085938927383088, 0.019187606733535582, 0.016529617174421272, 0.017658664713450265, 0.017461861724943822, 0.01473946469002416, 0.013888732157426767, 0.018813401251478592, 0.018245284999345322, 0.013429369967483517, 0.013213716673361565, 0.014179550147890478, 0.013165565478723676, 0.013442610755386424, 0.011938090494525969, 0.013952531346849035, 0.012777711567658235, 0.01361946394184095, 0.01415981548655886, 0.0132249127186142, 0.008202303269795845, 0.013768190907590946, 0.01187117434464689, 0.011072834069390408, 0.011375907309212407, 0.008056683960140176, 0.010813868758558904, 0.010755819991023294, 0.011055263281323623, 0.009854590676922565, 0.009897416544746738, 0.007694702545891319, 0.010974907935630917, 0.009073475042950686, 0.00957946586983009, 0.008521491170432235, 0.009181163735296882, 0.008033073926435217, 0.0075308985053745, 0.007736907337884813, 0.008905222457779166, 0.0075535059801524005, 0.008956405532912782, 0.00652504011866664, 0.007371700082288889, 0.007105531808951359, 0.009608815764438242, 0.006718999051766881, 0.006360923469938631, 0.006009158956909616, 0.005984226889401126, 0.006414240613540077, 0.008046470651133564, 0.006813095370703897, 0.004941387222356937, 0.006961080895634556, 0.0061504495415253034, 0.0044804993527444515, 0.005129817030924186, 0.0051584241011990275, 0.005904302868943308, 0.007040096893751609, 0.006497695825108162, 0.006231063241578372, 0.0045172427992303645, 0.004859256595798965, 0.005357292924210756, 0.0036115553579709734, 0.004059441841032762, 0.003976077818785122, 0.004686579279045762, 0.0047765986038821135, 0.0052203105924466305, 0.004452093922426263, 0.00366650621972908, 0.004813907496713725, 0.003619882242262452, 0.003379426909955734, 0.003642032724296165, 0.003996376050219234, 0.0027063930590780893, 0.0038246032934270147, 0.0035842454059730284, 0.0031291756938454703, 0.003011805406823518, 0.004053137657470737, 0.003489878551598058, 0.002363212944410382, 0.003003648613003568, 0.002955618500260455, 0.002723948648240625, 0.0037992216377342554, 0.0022372855783796795, 0.002972214323449715, 0.0029076020603400095, 0.002062946104120749, 0.0018581666937362376, 0.002313281514427136, 0.0030319557007007047, 0.0023327526890917524, 0.001145549262753536, 0.0016048090303144983, 0.0012414230531554533, 0.0032581396387386516, 0.0029362018629135516, 0.001846583708184242, 0.0012200965248589121, 0.001480849612214921, 0.002236391388236972, 0.0015825234965880586, 0.0009787926420108245, 0.001177633173575748, 0.0017891079141621141, 0.001687879517755926, 0.0027874750058864543, 0.0015881228617325382, 0.0013229923813940598, 0.0013549324787301051, 0.0013446073458035835, 0.0015000026920124656, 0.0012523066383357765, 0.000858102498131121, 0.001035546338261591, 0.0019881639752974524, 0.0016324728715111814, 0.001434380197954095, 0.0009871888154247414, 0.0008098082351434123, 0.0006027996978315856, 0.0008483088922217938, 0.0007788997386943644, 0.0014305688407895846, 0.0007751793022755421, 0.0005314110757599325, 0.0008008613197451418, 0.0005263145778683319, 0.0005657160187561332, 0.0008550505100606119, 0.0004061993359216422, 0.0008238756363850807, 0.0004022425374623086, 0.0007475109217440491, 0.0008010522570415786, 0.0008719455779264424, 0.0008102537067081855, 0.0006836875085969626, 0.000508847326879101, 0.00048076220441707043, 0.0003452522178527452, 0.0005887742460472115, 0.0007575123468680768, 0.0007316459814930954, 0.0006069562958639393, 0.00040482217438505993, 0.000413289390024444, 0.0002512041357034269, 0.00035321101188540014, 0.0002289985919775472, 0.00031013075728369315, 0.00048718760862263036, 0.00024815363102110633, 0.0002127035124524638, 0.00040036594692248164, 0.00022304668872061623, 0.0002539508311412804, 0.00033001078329677927, 0.0005046333238317957, 0.0002472649033615957, 0.00045928622355630194, 0.00022344069632032276, 0.00026002821110843074, 0.00015903297622683538, 0.00040398000405654787, 0.0002816041986039678, 0.00024018690979448482, 0.00029882328487923265, 0.00034577957207554183, 0.00028206157859465874, 0.00017730721445880266, 0.00011439717115794378, 0.00023043857095496602, 0.00013369225500067483, 0.00022097409422235014, 0.00013705560435689807, 0.000107482878806249, 0.0002181373778419168, 9.028549420162597e-05, 8.780925534434591e-05, 0.00015708037552040194, 0.00013996531600239778, 0.00015848159901898015, 9.64823931015096e-05, 0.00021168788567095436, 0.000136050276398834, 0.00010472044780222292, 0.0002206477002030788, 0.00013934975601720095, 0.00010074895917807085, 9.32180454911494e-05, 0.0001472607405713623, 0.00011569483834266491, 7.036176643037457e-05, 0.0001351957850342979, 0.000159376835863944, 0.00010674569268919185, 0.0001185344251126515, 8.718404802724013e-05, 0.00011900450702660869, 0.00012125460894170754, 0.00011230571605721275, 0.00010454212349596384, 0.0001487989837777661, 6.830416546227417e-05, 6.393169394098347e-05, 0.00020005663507281302, 0.00010431355027100938, 0.00017086659031600928, 7.99108349451789e-05, 0.00011386882648142751, 9.478551902827757e-05, 0.00010300004370941282, 8.344318782375742e-05, 0.0001274786575664539, 8.525987476745569e-05, 7.452874058836112e-05, 0.00011550953129777401]\n",
        "train_acc_list_cosine = [47.73742721016411, 84.54632080465855, 88.4425622022234, 89.96717840127052, 90.86712546320804, 91.64849126521969, 92.33245103229221, 92.79195341450503, 93.21757543673901, 93.32980412916888, 93.78295394388566, 94.04129168872419, 94.3928004235045, 94.53679195341451, 94.81206987824245, 95.05134992059291, 95.20169401799895, 95.48755955532027, 95.65484383271573, 95.92165166754897, 95.9640021175225, 96.39385918475384, 96.32186341979883, 96.55479089465325, 96.74748544203283, 96.74748544203283, 97.07146638433034, 97.24722075172049, 97.19640021175225, 97.32980412916888, 97.39332980412917, 97.62413975648491, 97.67496029645315, 97.79777660137638, 97.90365272631021, 97.96717840127052, 98.07940709370037, 98.21492853361568, 98.19163578613023, 98.32715722604553, 98.32292218104817, 98.55584965590259, 98.57702488088935, 98.43938591847538, 98.58973001588141, 98.68713605082054, 98.66807834833246, 98.61302276336686, 98.80359978824775, 98.78454208575967, 98.83748014822658, 98.80783483324511, 98.86077289571202, 98.89253573319216, 98.9433562731604, 99.02805717310747, 99.05982001058761, 99.13816834303864, 99.14452091053468, 99.0979354155638, 99.13393329804128, 99.1148755955532, 99.10217046056114, 99.45579671784013, 99.15510852302806, 99.22710428798305, 99.22498676548439, 99.32451032292218, 99.39650608787719, 99.32662784542086, 99.27368978295394, 99.3774483853891, 99.41132874536792, 99.29698253043938, 99.45367919534145, 99.3668607728957, 99.47061937533087, 99.43885653785071, 99.43250397035469, 99.51932239280042, 99.5404976177872, 99.33509793541556, 99.3774483853891, 99.5299100052938, 99.54473266278454, 99.5129698253044, 99.57014293276866, 99.57014293276866, 99.6294335627316, 99.48967707781895, 99.55320275277924, 99.5659078877713, 99.53202752779248, 99.58073054526204, 99.73742721016411, 99.53838009528852, 99.61461090524087, 99.6294335627316, 99.63366860772896, 99.7289571201694, 99.63155108523029, 99.64213869772367, 99.61884595023822, 99.71201694017999, 99.69084171519323, 99.74589730015882, 99.63790365272631, 99.72260455267337, 99.69719428268925, 99.72260455267337, 99.66754896770779, 99.70354685018528, 99.73954473266278, 99.72683959767072, 99.67601905770249, 99.73319216516676, 99.71625198517734, 99.80518793012176, 99.7289571201694, 99.77130757014294, 99.6929592376919, 99.7924827951297, 99.77766013763896, 99.784012705135, 99.80730545262044, 99.78613022763366, 99.76071995764956, 99.79036527263102, 99.83906829010058, 99.76707252514558, 99.7924827951297, 99.86659608258337, 99.84118581259926, 99.8284806776072, 99.79883536262572, 99.77977766013764, 99.79671784012704, 99.79671784012704, 99.83059820010588, 99.84965590259397, 99.8369507676019, 99.86871360508205, 99.89412387506617, 99.87083112758073, 99.85177342509265, 99.85177342509265, 99.81365802011646, 99.85389094759132, 99.87294865007941, 99.84118581259926, 99.87294865007941, 99.89200635256749, 99.88353626257279, 99.87294865007941, 99.90682901005823, 99.87506617257809, 99.87506617257809, 99.89624139756485, 99.90047644256221, 99.85812599258867, 99.88988883006881, 99.91953414505029, 99.90894653255691, 99.89412387506617, 99.90047644256221, 99.88988883006881, 99.92165166754897, 99.88777130757015, 99.90047644256221, 99.92588671254632, 99.93223928004235, 99.91953414505029, 99.88565378507147, 99.91953414505029, 99.95764955002647, 99.94917946003176, 99.95341450502912, 99.90471148755955, 99.91529910005293, 99.94070937003706, 99.95764955002647, 99.95764955002647, 99.93435680254103, 99.94706193753309, 99.9724722075172, 99.96611964002118, 99.9364743250397, 99.94282689253573, 99.91953414505029, 99.94706193753309, 99.94917946003176, 99.96188459502382, 99.95341450502912, 99.94917946003176, 99.95129698253044, 99.96823716251986, 99.97458973001588, 99.9364743250397, 99.9555320275278, 99.95129698253044, 99.9640021175225, 99.97458973001588, 99.97458973001588, 99.9640021175225, 99.97882477501324, 99.94706193753309, 99.97458973001588, 99.97882477501324, 99.9724722075172, 99.98094229751192, 99.98305982001058, 99.96188459502382, 99.98305982001058, 99.97035468501853, 99.98517734250926, 99.97458973001588, 99.95976707252514, 99.9640021175225, 99.9640021175225, 99.97670725251456, 99.98729486500794, 99.97670725251456, 99.98305982001058, 99.97670725251456, 99.97882477501324, 99.97882477501324, 99.97670725251456, 99.98305982001058, 99.98729486500794, 99.98517734250926, 99.98941238750662, 99.98941238750662, 99.98941238750662, 99.98729486500794, 99.98941238750662, 99.9915299100053, 99.98094229751192, 99.9915299100053, 99.98517734250926, 99.98094229751192, 99.98729486500794, 99.98941238750662, 99.98094229751192, 99.98941238750662, 99.98305982001058, 99.99364743250398, 99.98517734250926, 99.98729486500794, 99.98941238750662, 99.9915299100053, 99.98094229751192, 99.98305982001058, 99.9915299100053, 99.99576495500264, 99.98517734250926, 99.9915299100053, 99.98941238750662, 99.99576495500264, 99.99576495500264, 99.98941238750662, 99.99364743250398, 100.0, 99.9915299100053, 99.9915299100053, 99.98941238750662, 99.99576495500264, 99.98729486500794, 99.99576495500264, 99.99364743250398, 99.9915299100053, 99.99364743250398, 99.99364743250398, 99.99576495500264, 99.98941238750662, 99.9915299100053, 99.99788247750132, 99.98941238750662, 99.9915299100053, 99.99788247750132, 99.99576495500264, 99.99576495500264, 99.99364743250398, 99.9915299100053, 99.99576495500264, 99.99364743250398, 99.99364743250398, 99.99788247750132, 100.0, 99.98941238750662, 99.99576495500264, 99.99364743250398, 99.99576495500264, 99.9915299100053, 99.99576495500264, 99.99364743250398, 99.99576495500264, 99.9915299100053, 99.99788247750132, 99.99576495500264, 99.99576495500264]\n",
        "test_loss_list_cosine = [0.8128060245630788, 0.43769084055926283, 0.3670157931160693, 0.3468363919094497, 0.320717461845454, 0.29874755628407, 0.2880513122414841, 0.27089574385215254, 0.2712526289636598, 0.25813622246770296, 0.27607545629143715, 0.2508813175281473, 0.23157318663217275, 0.23963026514313385, 0.23259784851004095, 0.21799716919514478, 0.2553474395992417, 0.23297278822271847, 0.23411084861293727, 0.24059130224015782, 0.22228851220479198, 0.23491562519441633, 0.23474602972832964, 0.24284341398115253, 0.2531394691699568, 0.2231898330809439, 0.23778781842659502, 0.2260036576612323, 0.2408501879476449, 0.2531653588601187, 0.2437402953521586, 0.25621635543511195, 0.25155149374668506, 0.25073284860335143, 0.24820336959708264, 0.2527083886933385, 0.265060295743466, 0.2638632501977697, 0.27953584988911945, 0.28330178079469237, 0.2898024614425559, 0.285278165183377, 0.28805743263322203, 0.29073836449898927, 0.3027702011898452, 0.3096775676069014, 0.30173745603921515, 0.3021223259468873, 0.30385272365574745, 0.3038429137702812, 0.31642839670473455, 0.31861640270068947, 0.3238279426963452, 0.3290770745598802, 0.3349987664123011, 0.3213929162525079, 0.33029763011590524, 0.34525904689422426, 0.3447993094028503, 0.330581444653445, 0.321795464146371, 0.3393226447423883, 0.33919910747813536, 0.36219359443102983, 0.34984895151437206, 0.34508059543155717, 0.3312660858683361, 0.3478064838449891, 0.37045160054649207, 0.36637233636871563, 0.3615635437069132, 0.34741060503338483, 0.36895488022698786, 0.3510999241478595, 0.3899413192069487, 0.3609655104285362, 0.3731252212287383, 0.38873775874940203, 0.3712333329387155, 0.37048297439354894, 0.39439879441816433, 0.373478519492874, 0.36712522848564033, 0.3914665420420979, 0.38469330903471394, 0.3913668092230664, 0.38145966287337096, 0.37784753624788103, 0.38290000464036766, 0.3881754014622785, 0.39466715339279057, 0.38420211229765533, 0.3928751589150588, 0.3871026979576723, 0.39617241206852827, 0.42028651262323063, 0.38088225707521334, 0.3810242618087168, 0.38620046643978534, 0.41002833922667536, 0.40598774530157883, 0.4145270490324965, 0.4107616166036357, 0.39369509952124576, 0.3983155651949346, 0.4124191829971239, 0.40144294117778245, 0.4075972944425017, 0.41423957538791, 0.4235737093589634, 0.4338786544680011, 0.41868019625818464, 0.4211389536400983, 0.42915419865783083, 0.43854167406428973, 0.44444839669135855, 0.4239410953870153, 0.4140438513154639, 0.4214930013826519, 0.4469692508014394, 0.42807763744620425, 0.4140391830473627, 0.40559402319109616, 0.4296429092794949, 0.447968863443855, 0.4537148197985017, 0.4160180985562357, 0.42932759126757875, 0.4655364652474721, 0.44017627959450084, 0.4476152099646153, 0.45807484502666723, 0.4661675389518267, 0.46650424552624864, 0.4483312015951264, 0.43905310305839806, 0.4307253006571794, 0.4221340075886224, 0.43024291792957514, 0.4534414633743319, 0.439005637879246, 0.455770535284982, 0.4492703357884916, 0.46906485925337266, 0.4463952654680493, 0.4687405661497192, 0.4440763243108842, 0.4453616064936653, 0.466629167502819, 0.4585120371433304, 0.45512742501017, 0.4547710154278606, 0.46565503765847166, 0.47993873200361054, 0.46249571734783695, 0.47604353346076667, 0.4764781709778689, 0.4798887720623729, 0.47362343249294686, 0.477798045156341, 0.475341152007162, 0.46233028874677773, 0.47649588846765895, 0.4905768512090778, 0.49597619045023605, 0.4826731762720966, 0.46256263993963526, 0.4635071344308409, 0.4836937281412675, 0.4737923650753538, 0.47685492698641896, 0.49306667890107514, 0.5079087016615542, 0.47638218596294596, 0.47670910018496215, 0.5044073596980203, 0.5164688170211864, 0.5278736670528922, 0.4981932266542286, 0.4995226736783105, 0.5136372742845732, 0.47614785384697217, 0.5164846121424845, 0.4969261799825985, 0.504426286427998, 0.5065735648545966, 0.5321676667674682, 0.5085760319283615, 0.5058678923307133, 0.505933805387102, 0.5226816843154237, 0.5136628734705714, 0.49541927424862103, 0.5014571096729853, 0.5036171923942782, 0.4995590589264883, 0.5177521720453275, 0.5068038706028578, 0.5099713094623796, 0.4981300873846254, 0.5104206896560523, 0.5262805088377539, 0.5344062875499767, 0.5306157725908812, 0.5357992586351055, 0.5134817360370767, 0.5160781835849562, 0.5250964877469575, 0.5374465012448091, 0.5358220952946473, 0.5378625886514783, 0.5421092164064711, 0.5452210189199403, 0.5411839835076392, 0.5549019162050065, 0.5455894065546054, 0.5472353847934773, 0.5522625445497825, 0.5455132025983367, 0.538114388518985, 0.5509232937326363, 0.536594347077577, 0.5530879087015694, 0.5591187921909652, 0.5578411847065368, 0.5505201403601163, 0.564152417302716, 0.5473365803345052, 0.5458330702511411, 0.5650019636080947, 0.5671965913600562, 0.5513512968934853, 0.5562908629117552, 0.5620296227371356, 0.5617975970529312, 0.5581122653555426, 0.5675084015614736, 0.5657756982859699, 0.5526409040846606, 0.5721038452220634, 0.5690990948042942, 0.5608643038853474, 0.5609114988775556, 0.560604001846692, 0.5697500870121397, 0.5728723204822517, 0.5595448732640886, 0.5570606883866632, 0.5764513230818671, 0.556780993596048, 0.5647389884220948, 0.5543315944429624, 0.5563175826808255, 0.5460904273889301, 0.5574481183030716, 0.552725079560689, 0.5607348622220075, 0.5605667959076955, 0.5626120614498515, 0.5649992083380507, 0.5637176143309083, 0.5681553194373526, 0.5555440176634446, 0.5632415316604059, 0.5714907293874478, 0.5667051952526284, 0.5770144956528812, 0.5655391789224072, 0.57731314366444, 0.5691993516018413, 0.5740196594384079, 0.5696190810367446, 0.5625503671834332, 0.5508576962696251, 0.5676857517648708, 0.575283505166333, 0.5721206341997958, 0.5749247564988977, 0.5699750101832929, 0.580029735533411, 0.5632244463092374, 0.5753009883919731, 0.5712149130710054, 0.5708004219147066, 0.5698616763567734, 0.5771289975888495, 0.577879674463332, 0.5692181185472245, 0.591953951887102, 0.5678756476480368, 0.5756764801849118, 0.578166978474816, 0.5680953554498652, 0.5808629620039616, 0.5650976837804431, 0.5722748214018294, 0.5711645722097042, 0.5751072838172024, 0.5794820262486681, 0.5856684064892047]\n",
        "test_acc_list_cosine = [75.21511985248924, 86.48970497848802, 88.72157344806392, 89.39766441303011, 90.33113091579594, 91.21850030731407, 91.4259373079287, 92.23647818070067, 92.30562384757222, 92.6859250153657, 91.92532267977873, 92.7819606637984, 93.44652735095268, 93.21988322065151, 93.53872157344806, 93.85371850030731, 92.85494775660726, 93.56561155500921, 93.33512599877075, 93.48494161032575, 93.96896127842655, 93.75768285187462, 93.72311001843885, 93.5579287031346, 93.30439459127228, 94.19560540872772, 93.9459127228027, 94.2340196681008, 93.98048555623848, 93.700061462815, 94.24554394591273, 93.68085433312845, 93.82298709280884, 94.05731407498463, 94.24938537185002, 94.29164105716042, 93.83835279655808, 94.26090964966195, 94.03042409342348, 93.97280270436386, 94.00353411186232, 94.06883835279656, 94.13414259373079, 94.00737553779963, 94.12261831591887, 93.93822987092808, 94.16871542716656, 93.9958512599877, 93.96127842655194, 94.22249539028887, 93.93438844499079, 93.91518131530424, 94.0918869084204, 94.01121696373694, 94.15719114935465, 94.29164105716042, 94.31853103872157, 94.01889981561156, 94.15719114935465, 93.88060848186846, 94.54133374308543, 94.34542102028273, 94.14950829748003, 94.09572833435772, 94.36078672403197, 94.21481253841426, 94.3262138905962, 94.27627535341118, 93.92286416717886, 94.05731407498463, 94.09572833435772, 94.41456668715428, 94.24554394591273, 94.31853103872157, 94.24170251997542, 94.06115550092194, 94.0419483712354, 93.95359557467732, 94.30316533497235, 94.42224953902888, 94.1917639827904, 94.02274124154886, 94.43761524277812, 94.12645974185618, 94.20712968653964, 94.40304240934235, 94.35694529809466, 94.29164105716042, 94.48371235402581, 94.13030116779349, 94.31853103872157, 94.3338967424708, 94.41840811309157, 94.66041794714198, 94.48755377996312, 94.09956976029503, 94.26090964966195, 94.34157959434542, 94.38767670559312, 94.4798709280885, 94.38767670559312, 94.36078672403197, 94.16871542716656, 94.64505224339274, 94.36846957590657, 94.48371235402581, 94.28011677934849, 94.46450522433928, 94.28779963122311, 94.36078672403197, 94.35694529809466, 94.55285802089736, 94.35310387215735, 94.48755377996312, 94.34926244622004, 94.47602950215119, 94.64889366933005, 94.46450522433928, 94.44145666871543, 94.54133374308543, 94.52980946527352, 94.54901659496005, 94.69883220651506, 94.58743085433314, 94.16871542716656, 94.41456668715428, 94.56438229870928, 94.5759065765212, 94.21097111247695, 94.54517516902274, 94.44529809465274, 94.46834665027659, 94.27627535341118, 94.34157959434542, 94.71035648432698, 94.49139520590043, 94.60663798401967, 94.75645359557468, 94.74108789182544, 94.66041794714198, 94.59895513214505, 94.66041794714198, 94.57974800245852, 94.57974800245852, 94.71035648432698, 94.54901659496005, 94.7679778733866, 94.7679778733866, 94.56438229870928, 94.67194222495391, 94.66425937307929, 94.74492931776275, 94.79486785494775, 94.44529809465274, 94.73724646588813, 94.43761524277812, 94.72956361401353, 94.72572218807622, 94.74877074370006, 94.66041794714198, 94.4798709280885, 94.80639213275968, 94.61432083589429, 94.50676090964966, 94.70651505838967, 94.70651505838967, 94.59895513214505, 94.78334357713584, 94.76413644744929, 94.79870928088506, 94.69883220651506, 94.59511370620774, 94.42609096496619, 94.87169637369392, 94.86785494775661, 94.74492931776275, 94.74108789182544, 94.29932390903504, 94.86785494775661, 94.84864781807006, 94.68346650276582, 94.94852489244008, 94.69499078057775, 94.82175783650891, 94.86017209588199, 94.89090350338046, 94.6258451137062, 94.82559926244622, 94.83328211432084, 94.90242778119237, 94.88322065150584, 94.89090350338046, 94.96004917025199, 94.87553779963122, 94.9139520590043, 94.91779348494161, 94.99462200368777, 94.94852489244008, 94.85248924400737, 94.83712354025815, 94.90626920712968, 94.76029502151198, 94.78334357713584, 94.99078057775046, 94.88706207744315, 95.02151198524892, 94.97925629993854, 94.97541487400123, 94.87553779963122, 94.97541487400123, 94.88706207744315, 94.76413644744929, 94.84096496619546, 94.9638905961893, 94.96004917025199, 95.01382913337432, 94.99846342962508, 94.91011063306699, 94.92163491087892, 94.93315918869084, 94.9139520590043, 95.12907191149354, 95.03687768899816, 94.8640135218193, 94.82559926244622, 94.98693915181315, 94.90626920712968, 94.95236631837739, 94.98309772587585, 94.92931776275353, 94.74877074370006, 95.11754763368162, 95.02151198524892, 95.15596189305471, 94.98693915181315, 94.89858635525508, 94.99462200368777, 94.98693915181315, 95.07145052243392, 94.9638905961893, 94.97925629993854, 94.9638905961893, 94.98309772587585, 94.99462200368777, 94.95236631837739, 95.059926244622, 95.01767055931161, 95.11754763368162, 95.02919483712354, 95.13291333743085, 95.09834050399509, 95.109864781807, 95.159803318992, 95.16748617086662, 94.98693915181315, 95.22126613398893, 95.06376767055932, 95.12138905961893, 95.1521204671174, 95.04071911493547, 95.16364474492931, 95.17132759680393, 95.09834050399509, 95.1521204671174, 95.08681622618316, 95.059926244622, 95.20974185617702, 95.059926244622, 95.02151198524892, 95.14443761524278, 95.109864781807, 95.12907191149354, 95.17901044867855, 95.20974185617702, 95.17132759680393, 95.17132759680393, 95.19437615242778, 95.02151198524892, 95.17132759680393, 95.12907191149354, 95.19437615242778, 95.19821757836509, 95.12907191149354, 95.09449907805778, 95.109864781807, 95.24431468961278, 95.12907191149354, 95.16748617086662, 95.01382913337432, 95.22894898586355, 95.06760909649662, 95.11370620774431, 95.21742470805162, 95.159803318992, 95.22894898586355, 95.14059618930547, 95.109864781807, 95.14827904118009, 95.13675476336816, 95.09449907805778]\n",
        "train_loss_list_step = [1.5044204033810271, 0.49208608810817644, 0.37830603914209177, 0.33356322168415475, 0.30051283241529775, 0.2731129997548695, 0.25931339448464275, 0.24751879757416603, 0.23341218238762076, 0.22551919120114025, 0.21245744068246225, 0.20461816869696303, 0.19351729513385754, 0.18888680500591673, 0.1790498158513369, 0.17313903582794718, 0.16563977062863708, 0.15938490460599017, 0.1541340289319434, 0.14407484958246147, 0.13983443950815252, 0.13245242257630277, 0.12638695052443805, 0.12125968426304458, 0.11788717211605249, 0.11147532753424269, 0.10493720971996869, 0.10114793143257862, 0.0986312177905505, 0.09325535373060603, 0.06156983597173802, 0.050082290781091464, 0.04566037622696864, 0.042090283896077454, 0.037587769078418896, 0.03525592588446025, 0.034040165755286976, 0.031216950466235478, 0.030938032096463855, 0.029107897011765093, 0.02691679777953582, 0.024817724369887797, 0.02327188092970408, 0.022008026162917072, 0.021083637177603455, 0.020199724932410693, 0.020445302296316296, 0.01811823503557229, 0.016885934063002497, 0.016305384605991728, 0.015734232263797315, 0.01582022506202922, 0.01498541162761943, 0.013855328222454038, 0.012501976389670114, 0.011526370763357194, 0.012135067156394237, 0.011748958516110058, 0.01063293406628148, 0.010577014861956521, 0.009060478683824204, 0.00822005160518879, 0.007903504668233972, 0.0073660259777244665, 0.007953700653634155, 0.006759841606805863, 0.0069558188274406844, 0.007026840372753726, 0.006894602971896024, 0.0063807274687209935, 0.006584282455781946, 0.006663436767551165, 0.006154599042466102, 0.006103539849245962, 0.006048463034234704, 0.00614482418276391, 0.006322457585395453, 0.006028014029933677, 0.006245754332408476, 0.00581059281254909, 0.005709030747089944, 0.005923201890116833, 0.00537758718227695, 0.005807922808844647, 0.005243748521360034, 0.004974289045318631, 0.005323306708443634, 0.005775554073696229, 0.005093053230784864, 0.005536427160324011, 0.0052108629591284105, 0.004749226128792285, 0.005127630246161776, 0.005065120122888249, 0.004685268356975264, 0.005243820296808683, 0.0049121043785080205, 0.005093204319377071, 0.005370734221114888, 0.005008859195898547, 0.005249464887712999, 0.004638611593474268, 0.004728934781005616, 0.005035177127017733, 0.004871168427265863, 0.005197190184529169, 0.004969428170602996, 0.004836069228517858, 0.005308753678475464, 0.004877718586694237, 0.0046394178739258, 0.004715173941094246, 0.005071522070231865, 0.005326596872854554, 0.004782932753934018, 0.00452435254143787, 0.005128751716483132, 0.004405792564614916, 0.005072576416472445, 0.004683375086275378, 0.004745950062742184, 0.005199869099014116, 0.004980171865863766, 0.004840858398257126, 0.0046297258561474545, 0.004515150684752434, 0.005211585615480793, 0.0046955638231178315, 0.004485690257187711, 0.004824536544108968, 0.004908301922912914, 0.005288558260384356, 0.004324457162535601, 0.004497020012851448, 0.004832310812420976, 0.004629586771150122, 0.00469785524222389, 0.0051049072852895816, 0.004760202388700317, 0.00467975922823293, 0.004756802660750659, 0.004577458516745399, 0.0046352100029977686, 0.0045855785586362005, 0.0045302773612948275, 0.00527981271794404, 0.004842230399004512, 0.005128007588957292, 0.0047336897144657575, 0.00452429031405685, 0.004871112167589785, 0.004707888855899686, 0.004987144301444176, 0.004576647475924656, 0.0050210763291275344, 0.004869554478370365, 0.005145938421950075, 0.0049324849194394005, 0.005007630027365255, 0.004694439974090723, 0.005161294266647788, 0.005086089323139447, 0.0045723088694465674, 0.0049315163145030455, 0.005043683056833177, 0.004782498019489051, 0.005208640262013842, 0.004906343821498318, 0.004504839811582297, 0.004765471186647242, 0.004636792729359163, 0.0048650642755604795, 0.00440384292277286, 0.005331020985203442, 0.004309070902481564, 0.005132160800841049, 0.0048202084057818, 0.0042395592619208135, 0.004579358937877841, 0.004854094481356265, 0.004861267090400903, 0.004641431140822869, 0.004752275634260134, 0.005280723348089562, 0.0049866622946490744, 0.004713922302353166, 0.0049851976106240516, 0.0049522482596579456, 0.0047414097434517405, 0.005198805679831853, 0.005367297160049249, 0.004801289511232152, 0.004936103088396973, 0.004513620775553264, 0.005461517685133368, 0.005038318795404084, 0.004621399451298942, 0.004861979541443869, 0.004728190580562456, 0.00424972596940481, 0.004389661508080736, 0.004675524760544651, 0.004678315578823907, 0.005468495250993464, 0.004904287783485815, 0.004628283689890138, 0.004957924133196958, 0.004207717516179686, 0.004497190112673026, 0.0054004669855449725, 0.004738672002041044, 0.004832978636525801, 0.004593934473861766, 0.00428500973297249, 0.004800252157330026, 0.004579809123577946, 0.0047766751335504105, 0.004927973183699149, 0.003962443585616099, 0.005313499298157754, 0.005448946752569696, 0.004626204171514537, 0.00486521731347835, 0.00464669697920995, 0.00473364783223967, 0.0051715146905747635, 0.004559630797677459, 0.004807513805560901, 0.004414798407140251, 0.0055058369074104515, 0.004735360263094689, 0.00523965430700253, 0.004729446927025576, 0.0046202187999946675, 0.004473174700221971, 0.005083680975845281, 0.00473606128646736, 0.004358955291625135, 0.004462744230992882, 0.00445035025452959, 0.004661599569843557, 0.004881452044679059, 0.004753526896741532, 0.004433994836329803, 0.004789544894031266, 0.004920090539939323, 0.005374149948520697, 0.004787621462456112, 0.004773729829575745, 0.0044936602159881545, 0.004565336937149469, 0.004793149001421886, 0.004954056282359589, 0.00447947610881707, 0.004911790105014495, 0.004962967739427566, 0.004856595992693377, 0.0049429370942105965, 0.004495315723300048, 0.005116432885005868, 0.004663471084030195, 0.005105866048162296, 0.0048063792890729005, 0.004996042520567828, 0.0048235019935209174, 0.0052012274302681025, 0.004979272351725164, 0.004979745390991236, 0.005170751338667516, 0.004858217863982788, 0.004663840651548111, 0.005053441917670759, 0.004584236903877065, 0.004780888221675258, 0.004612757356412744, 0.004703525326821731, 0.005128545914503725, 0.005097627184756964, 0.004888794860495936, 0.004546376856262884, 0.0049637655482588335, 0.004738951851973477, 0.005256142429007358, 0.004507868475276601, 0.004676521270572799, 0.0042853716939203765, 0.004519075942008067, 0.004505532296253924, 0.004830542344546013, 0.004257976696643534, 0.004718432969060035, 0.004490681710655531, 0.004793198622190861, 0.004690485046958793, 0.004381172440937899, 0.004597538895479108, 0.0050219146489527905, 0.004799934776746264, 0.004984567811975109, 0.004093347564194513]\n",
        "train_acc_list_step = [47.394388565378506, 84.38962413975648, 88.32609846479619, 89.65802011646373, 90.93912122816305, 91.76071995764956, 92.30492323980943, 92.69878242456326, 93.1371095817893, 93.283218634198, 93.84859714134463, 93.97988353626258, 94.4203282159873, 94.56643726839597, 94.83324510322922, 95.06405505558496, 95.14452091053468, 95.38168343038645, 95.61461090524087, 95.86871360508205, 95.94282689253573, 96.23928004235044, 96.33456855479089, 96.51667548967708, 96.58231868713605, 96.79618845950239, 96.97829539438857, 97.11805187930122, 97.2006352567496, 97.23451561672843, 98.32927474854421, 98.71254632080466, 98.81630492323981, 98.94970884065643, 99.04499735309687, 99.05558496559026, 99.12969825304394, 99.22075172048703, 99.16993118051879, 99.24616199047115, 99.32027527792482, 99.33721545791424, 99.38168343038645, 99.40285865537321, 99.45156167284277, 99.46426680783483, 99.43885653785071, 99.49814716781366, 99.51932239280042, 99.55320275277924, 99.58073054526204, 99.56379036527264, 99.56379036527264, 99.6209634727369, 99.65907887771307, 99.68237162519851, 99.61884595023822, 99.62731604023293, 99.69507676019057, 99.72260455267337, 99.73742721016411, 99.7564849126522, 99.76919004764426, 99.79883536262572, 99.7649550026469, 99.83906829010058, 99.80307040762308, 99.81154049761778, 99.81154049761778, 99.83271572260455, 99.8009528851244, 99.82424563260984, 99.8369507676019, 99.82424563260984, 99.83906829010058, 99.83483324510323, 99.8369507676019, 99.84965590259397, 99.82636315510852, 99.83059820010588, 99.83906829010058, 99.84118581259926, 99.86236103758603, 99.85600847008999, 99.86236103758603, 99.87083112758073, 99.8369507676019, 99.8284806776072, 99.84753838009529, 99.85600847008999, 99.85389094759132, 99.87083112758073, 99.85812599258867, 99.86024351508735, 99.88353626257279, 99.85600847008999, 99.87506617257809, 99.85600847008999, 99.86236103758603, 99.84753838009529, 99.85600847008999, 99.87930121757543, 99.86236103758603, 99.8644785600847, 99.85177342509265, 99.86024351508735, 99.85389094759132, 99.85812599258867, 99.84330333509793, 99.86024351508735, 99.87718369507677, 99.86871360508205, 99.85812599258867, 99.84753838009529, 99.86659608258337, 99.87083112758073, 99.85389094759132, 99.88988883006881, 99.86024351508735, 99.87718369507677, 99.86659608258337, 99.84118581259926, 99.87083112758073, 99.85812599258867, 99.88777130757015, 99.88988883006881, 99.84542085759661, 99.87294865007941, 99.87083112758073, 99.85177342509265, 99.86024351508735, 99.86236103758603, 99.87506617257809, 99.8644785600847, 99.86024351508735, 99.86659608258337, 99.86871360508205, 99.85600847008999, 99.87083112758073, 99.87294865007941, 99.87294865007941, 99.88353626257279, 99.86236103758603, 99.87930121757543, 99.87930121757543, 99.86024351508735, 99.86024351508735, 99.83483324510323, 99.8644785600847, 99.89200635256749, 99.87083112758073, 99.84965590259397, 99.84965590259397, 99.88141874007411, 99.84753838009529, 99.87506617257809, 99.85389094759132, 99.85812599258867, 99.86236103758603, 99.85600847008999, 99.85600847008999, 99.84118581259926, 99.87930121757543, 99.86024351508735, 99.85812599258867, 99.86871360508205, 99.85389094759132, 99.86236103758603, 99.87718369507677, 99.85812599258867, 99.87083112758073, 99.85600847008999, 99.87506617257809, 99.87506617257809, 99.88777130757015, 99.85812599258867, 99.85600847008999, 99.88777130757015, 99.86659608258337, 99.87718369507677, 99.86659608258337, 99.86659608258337, 99.88141874007411, 99.85177342509265, 99.85600847008999, 99.87718369507677, 99.86236103758603, 99.87930121757543, 99.87506617257809, 99.85177342509265, 99.84118581259926, 99.86236103758603, 99.85812599258867, 99.87506617257809, 99.82636315510852, 99.85600847008999, 99.87718369507677, 99.88353626257279, 99.87294865007941, 99.89624139756485, 99.88353626257279, 99.87718369507677, 99.87083112758073, 99.84542085759661, 99.8644785600847, 99.88141874007411, 99.85600847008999, 99.89412387506617, 99.88141874007411, 99.85177342509265, 99.85389094759132, 99.87930121757543, 99.88565378507147, 99.88353626257279, 99.86871360508205, 99.86871360508205, 99.8644785600847, 99.86871360508205, 99.89624139756485, 99.84330333509793, 99.84542085759661, 99.88353626257279, 99.86236103758603, 99.87930121757543, 99.87506617257809, 99.87083112758073, 99.87294865007941, 99.87294865007941, 99.86871360508205, 99.84330333509793, 99.86659608258337, 99.84542085759661, 99.87506617257809, 99.8644785600847, 99.89200635256749, 99.84542085759661, 99.86871360508205, 99.87506617257809, 99.88141874007411, 99.87083112758073, 99.87083112758073, 99.86659608258337, 99.86659608258337, 99.86871360508205, 99.88565378507147, 99.87083112758073, 99.84753838009529, 99.88777130757015, 99.87718369507677, 99.88353626257279, 99.87083112758073, 99.86236103758603, 99.87718369507677, 99.88777130757015, 99.84965590259397, 99.85600847008999, 99.86236103758603, 99.85812599258867, 99.87718369507677, 99.8644785600847, 99.86024351508735, 99.87506617257809, 99.8644785600847, 99.8644785600847, 99.87930121757543, 99.85177342509265, 99.86024351508735, 99.85177342509265, 99.84118581259926, 99.86871360508205, 99.87718369507677, 99.86024351508735, 99.88988883006881, 99.8644785600847, 99.88141874007411, 99.87506617257809, 99.85177342509265, 99.86871360508205, 99.87506617257809, 99.87506617257809, 99.87930121757543, 99.86871360508205, 99.85812599258867, 99.88988883006881, 99.88353626257279, 99.89412387506617, 99.87930121757543, 99.86659608258337, 99.86659608258337, 99.88988883006881, 99.87083112758073, 99.87506617257809, 99.87083112758073, 99.87718369507677, 99.87930121757543, 99.87083112758073, 99.84330333509793, 99.87718369507677, 99.87294865007941, 99.88565378507147]\n",
        "test_loss_list_step = [0.7521707053278007, 0.41922322909037274, 0.3967930445191907, 0.35680990534670215, 0.3250766685049908, 0.30643289908766747, 0.27803952721696273, 0.26461896110399097, 0.26403681638047977, 0.2668783114309989, 0.2609864034708224, 0.25131638298797254, 0.24851202544774495, 0.24535505795011334, 0.23515790532909187, 0.23713360856488055, 0.2406249167215006, 0.24633904404061682, 0.22870721204169825, 0.23081522924350759, 0.23210598513776182, 0.23380596216256713, 0.23264140811036615, 0.2492445185597913, 0.24672274596477842, 0.23322659500819795, 0.2332557491848574, 0.2325650899324055, 0.2574915187433362, 0.23942345477567584, 0.21860766188953729, 0.22490006466122234, 0.22599033247588166, 0.2330661828027052, 0.23813083913980745, 0.2410290090677639, 0.25265564182408007, 0.25388093285408675, 0.25897152009694013, 0.2653597001095905, 0.267472947769634, 0.2757880237756991, 0.28352711534164116, 0.2891207381023788, 0.2937827925471699, 0.3016136303891008, 0.29683313752506296, 0.3059604737939605, 0.31242154799766986, 0.313900034299449, 0.3219801964347853, 0.33177046875889393, 0.3355082166837711, 0.33843776536192377, 0.34364481102309974, 0.3455076480613035, 0.34563379169569586, 0.35321818167051555, 0.35429473890576, 0.35551488774317297, 0.36010582940470354, 0.35364405492631096, 0.3636273516743791, 0.36364757276012327, 0.35665451670887277, 0.3549169595261999, 0.3703063836905594, 0.36219835961146246, 0.3605355029998749, 0.3578162646861564, 0.36774474406140106, 0.3633771698702784, 0.3653042366746448, 0.3691165424956411, 0.36504066177625577, 0.3715900047225695, 0.377250660302154, 0.3705682765610297, 0.3703488810170515, 0.37351591648606985, 0.37677567382343113, 0.37160208523638694, 0.3709423184650494, 0.3721590631769276, 0.3763345616011351, 0.3800408639488559, 0.37680745827874135, 0.3771674718214747, 0.3848353975142042, 0.3798498407590623, 0.3773468048494382, 0.3819653425073507, 0.3785710880149375, 0.37396970703654614, 0.37354924148131236, 0.38423583746029466, 0.3775230319622685, 0.36990976165614875, 0.3731101816017911, 0.3812584620443921, 0.3834761079017292, 0.38286398570327196, 0.3858671422372116, 0.3799955192816389, 0.3770175949378195, 0.3851937071338077, 0.38235137271968755, 0.38205398502303106, 0.38025479872400564, 0.3843826920378442, 0.38355313671533675, 0.3811237645043316, 0.37951354816665545, 0.3756716687428564, 0.38028262046110983, 0.38163883488296585, 0.38113248269712807, 0.38293040138395396, 0.38826611508414444, 0.3821931903378344, 0.3927168071534777, 0.3814124613087259, 0.38783683029788674, 0.384523262598497, 0.3777104146395098, 0.3834854347001323, 0.37351655306331083, 0.37643494215958256, 0.38091917734081837, 0.3816218973667014, 0.38416977602915436, 0.3851156278796421, 0.3824646536297366, 0.37491797317988146, 0.3818769982979432, 0.37861419655382633, 0.3842346754938583, 0.37655236199498177, 0.37527537754024654, 0.37987153332077844, 0.3830455071018899, 0.3794875234803733, 0.38347619624041457, 0.39262797689868834, 0.37537328259763764, 0.3836479137067263, 0.3833252173236699, 0.37210422399563386, 0.387080483238998, 0.3719306717816211, 0.37743640282446994, 0.3764378684846794, 0.37271541687568616, 0.37936363588361177, 0.3804472682075392, 0.371879458409168, 0.38374935142586336, 0.3781576618488294, 0.3732976428844838, 0.3825277503210065, 0.37921793430167083, 0.38190348801550034, 0.3811836604795912, 0.3839620574760963, 0.3847266497711341, 0.3755092674263698, 0.38182204500680755, 0.3782161524446279, 0.383978412504874, 0.3785792852560168, 0.37529283202728075, 0.38586872027200814, 0.37986113718144743, 0.3781509461410928, 0.385330743640296, 0.3866525995362477, 0.3780062812672672, 0.38378071400574315, 0.38093069549102115, 0.38920295537065935, 0.38671026251041424, 0.3797818386251582, 0.3798417299438049, 0.3740828680422376, 0.38145504804218516, 0.38200287843196123, 0.3875016655645096, 0.38280217152308016, 0.38698648213974984, 0.3808485120260978, 0.38649215569317924, 0.3831135995238654, 0.3805198727321683, 0.38389426964682105, 0.38486584148132336, 0.3807648518761876, 0.3849117392856701, 0.38554816227406263, 0.38357163151251333, 0.3802410184734446, 0.3815109624947403, 0.382021243195506, 0.3853015545938237, 0.377494075719048, 0.38711871966427447, 0.3774435311857173, 0.3905071147407095, 0.3806642469603057, 0.38146250298721535, 0.3838621310104488, 0.39097126777849944, 0.38649382303450625, 0.3796885114930132, 0.38229777593183895, 0.3814927838633166, 0.37877432031410874, 0.37605211639082897, 0.37402715621625676, 0.3860787514177169, 0.3734693876285033, 0.3757060868665576, 0.3836308258823028, 0.3818031575019453, 0.3754675344144012, 0.3817046451035376, 0.3788763102984019, 0.38524902579100695, 0.3789480691410455, 0.383543092720941, 0.39350814419780294, 0.3787268260402568, 0.3791520138837251, 0.3838299135951435, 0.3793616016559741, 0.3767985271874304, 0.3790097143866268, 0.37912912847583785, 0.38894346707007466, 0.3742955804382469, 0.38342078903909116, 0.3848988146238102, 0.37459168269061577, 0.38242888645561157, 0.3820874952601598, 0.38645922241951614, 0.37736037103276626, 0.37371054134450343, 0.3787742013148233, 0.3864014833873394, 0.38398269388605566, 0.3823276355716528, 0.38692381239368345, 0.38753888685731036, 0.37855833295878827, 0.37346514290673477, 0.37298866356814314, 0.3816173101830132, 0.3767242920311058, 0.3841047031056209, 0.38750297252965327, 0.369205586767445, 0.3753658247490724, 0.38275528518373475, 0.3800859667141648, 0.3870932775020015, 0.37551862878414494, 0.3840912418421723, 0.3726249620291021, 0.37941730108486454, 0.38384662088298915, 0.37976392629720707, 0.37775822057772207, 0.38208118103006306, 0.3800909294399853, 0.37907081163104844, 0.3812881442672555, 0.3761177863743083, 0.38076628101350485, 0.37963548230518607, 0.3693312196718419, 0.37269937157557875, 0.3810739266987452, 0.37098004634254705, 0.38030854867332997, 0.3809964715083148, 0.37681868622152537, 0.3750186215574835, 0.3816620926098788, 0.3736031973351012, 0.38245252669588026, 0.3819549302798787, 0.38419706475756626, 0.3735217913966991, 0.3787470100191878, 0.37715579814040195, 0.3867923977976555, 0.3815506379960068, 0.3814866198135503, 0.3853403181933305, 0.3877864530602214]\n",
        "test_acc_list_step = [75.81054087277197, 86.9660417947142, 87.58451137062077, 89.28242163491088, 90.1160110633067, 91.05331899200984, 91.73325138291334, 92.37092808850646, 92.390135218193, 92.24031960663798, 92.37092808850646, 92.9778733866011, 93.1737861094038, 93.04701905347265, 93.41963736939152, 93.37738168408113, 93.3159188690842, 93.0278119237861, 93.7077443146896, 93.72695144437616, 93.68853718500307, 93.78073140749846, 93.73847572218807, 93.26213890596189, 93.30055316533497, 93.93438844499079, 94.06883835279656, 94.11877688998156, 93.6040258143823, 94.0419483712354, 94.87553779963122, 94.83712354025815, 94.81023355869699, 94.87169637369392, 94.91779348494161, 94.89474492931777, 94.86785494775661, 94.84096496619546, 94.71419791026429, 94.56054087277197, 94.75645359557468, 94.73724646588813, 94.64889366933005, 94.64889366933005, 94.69499078057775, 94.72956361401353, 94.6757836508912, 94.74492931776275, 94.6220036877689, 94.65273509526736, 94.54901659496005, 94.53749231714812, 94.63352796558083, 94.61816226183159, 94.61432083589429, 94.5221266133989, 94.64121081745544, 94.56054087277197, 94.74492931776275, 94.61047940995698, 94.59127228027043, 94.75645359557468, 94.75645359557468, 94.66425937307929, 94.73340503995082, 94.6757836508912, 94.53365089121081, 94.64505224339274, 94.66041794714198, 94.70267363245236, 94.6258451137062, 94.75645359557468, 94.80255070682237, 94.67962507682851, 94.58358942839583, 94.54517516902274, 94.6258451137062, 94.7180393362016, 94.66425937307929, 94.6681007990166, 94.61432083589429, 94.78718500307313, 94.76029502151198, 94.68730792870313, 94.64121081745544, 94.62968653964352, 94.69114935464044, 94.58743085433314, 94.58358942839583, 94.64889366933005, 94.69114935464044, 94.70651505838967, 94.70267363245236, 94.66041794714198, 94.69883220651506, 94.61432083589429, 94.71419791026429, 94.82559926244622, 94.69883220651506, 94.72572218807622, 94.49523663183774, 94.75261216963737, 94.61047940995698, 94.64505224339274, 94.64889366933005, 94.66041794714198, 94.70267363245236, 94.55669944683467, 94.65657652120467, 94.6681007990166, 94.77566072526122, 94.85633066994468, 94.66041794714198, 94.78334357713584, 94.79102642901044, 94.63352796558083, 94.78718500307313, 94.64505224339274, 94.56438229870928, 94.73340503995082, 94.61047940995698, 94.68730792870313, 94.70267363245236, 94.61432083589429, 94.67194222495391, 94.77181929932391, 94.73724646588813, 94.64505224339274, 94.69499078057775, 94.65273509526736, 94.69883220651506, 94.6757836508912, 94.71035648432698, 94.69883220651506, 94.67194222495391, 94.66425937307929, 94.6757836508912, 94.76029502151198, 94.75645359557468, 94.70267363245236, 94.64505224339274, 94.74108789182544, 94.54133374308543, 94.54901659496005, 94.74877074370006, 94.67962507682851, 94.67194222495391, 94.67194222495391, 94.69883220651506, 94.8140749846343, 94.64121081745544, 94.7180393362016, 94.74492931776275, 94.78334357713584, 94.65273509526736, 94.5759065765212, 94.61816226183159, 94.61047940995698, 94.76413644744929, 94.79870928088506, 94.67962507682851, 94.69114935464044, 94.72572218807622, 94.60663798401967, 94.5720651505839, 94.75261216963737, 94.80255070682237, 94.70267363245236, 94.66041794714198, 94.66425937307929, 94.69114935464044, 94.72188076213891, 94.68730792870313, 94.55285802089736, 94.75645359557468, 94.64505224339274, 94.7180393362016, 94.53749231714812, 94.72572218807622, 94.65657652120467, 94.6258451137062, 94.61432083589429, 94.72188076213891, 94.77566072526122, 94.62968653964352, 94.66041794714198, 94.54901659496005, 94.6757836508912, 94.66425937307929, 94.55669944683467, 94.7679778733866, 94.74877074370006, 94.68346650276582, 94.63352796558083, 94.5759065765212, 94.61047940995698, 94.66425937307929, 94.71419791026429, 94.59895513214505, 94.61816226183159, 94.72956361401353, 94.7180393362016, 94.67194222495391, 94.68346650276582, 94.59127228027043, 94.76029502151198, 94.69499078057775, 94.66041794714198, 94.6681007990166, 94.69499078057775, 94.64889366933005, 94.67962507682851, 94.78718500307313, 94.67962507682851, 94.77950215119853, 94.6757836508912, 94.74492931776275, 94.84096496619546, 94.73340503995082, 94.72188076213891, 94.69883220651506, 94.61047940995698, 94.5720651505839, 94.72572218807622, 94.70651505838967, 94.6681007990166, 94.6258451137062, 94.64121081745544, 94.65657652120467, 94.48371235402581, 94.6757836508912, 94.73724646588813, 94.59895513214505, 94.70651505838967, 94.76029502151198, 94.75645359557468, 94.76413644744929, 94.6220036877689, 94.72188076213891, 94.7180393362016, 94.66425937307929, 94.6681007990166, 94.62968653964352, 94.71419791026429, 94.68346650276582, 94.69883220651506, 94.69114935464044, 94.70267363245236, 94.60663798401967, 94.67194222495391, 94.55285802089736, 94.59895513214505, 94.66041794714198, 94.61432083589429, 94.59895513214505, 94.73724646588813, 94.63352796558083, 94.73724646588813, 94.77566072526122, 94.60279655808236, 94.76413644744929, 94.74108789182544, 94.65273509526736, 94.58743085433314, 94.59511370620774, 94.80255070682237, 94.61047940995698, 94.71035648432698, 94.65273509526736, 94.6681007990166, 94.75261216963737, 94.72188076213891, 94.61816226183159, 94.65657652120467, 94.79870928088506, 94.64121081745544, 94.66425937307929, 94.69883220651506, 94.64889366933005, 94.83712354025815, 94.69883220651506, 94.71419791026429, 94.77181929932391, 94.63352796558083, 94.74492931776275, 94.69883220651506, 94.61816226183159, 94.73340503995082, 94.73724646588813, 94.67194222495391, 94.72956361401353, 94.69114935464044, 94.77181929932391, 94.65657652120467, 94.68346650276582, 94.60663798401967, 94.64889366933005, 94.72188076213891, 94.70651505838967, 94.70267363245236]\n",
        "train_loss_list_linear = [1.0505396097010067, 0.41031305952285363, 0.34015994405924144, 0.3038505317194029, 0.27671615290771007, 0.2539030461090044, 0.24293845730422312, 0.2313264879753919, 0.22205986383403867, 0.21279181394635177, 0.20017173882260877, 0.19267895106014196, 0.18516845405505603, 0.18218227440546844, 0.17281363284604012, 0.16757936995264475, 0.1608044776490064, 0.1526946394343363, 0.1503949542921087, 0.14460325269861435, 0.1379334318267338, 0.13074936200347212, 0.12652073063834654, 0.12361395870888137, 0.11955785878043027, 0.11495751238915171, 0.10918548092407586, 0.10549298947964741, 0.10151599690843081, 0.09746526491886312, 0.09452159306524084, 0.09095732697353857, 0.08772212413056352, 0.08348307219462667, 0.08420775942554884, 0.07539896097104885, 0.07707164484154443, 0.07315899130855473, 0.07151561531902047, 0.06873627238071506, 0.06587666986752452, 0.060232834865876696, 0.059981180607343754, 0.06175297403930204, 0.05708458852451951, 0.05296678836066225, 0.057111382709063976, 0.053133565748266894, 0.05266129370491073, 0.050845057070684346, 0.04849550221115351, 0.0469664877891379, 0.04695639163596419, 0.04652371517443136, 0.04273645039004221, 0.042981587787635805, 0.040151388551610795, 0.03779632822411049, 0.04038962408859879, 0.04276739005321198, 0.035970243897294085, 0.0389290425395164, 0.034595519276827996, 0.034754975707721986, 0.03550548981976138, 0.034968214722219, 0.033513369056052934, 0.033747713374943145, 0.03146642715614011, 0.03088276023026253, 0.03096695340504784, 0.030255960882325888, 0.029922214430077934, 0.026458163596317426, 0.030674233368527256, 0.026803990912770092, 0.028383832357679988, 0.029045142340322395, 0.026921882543196714, 0.025758919344589033, 0.027054627095336514, 0.026907914471629563, 0.02701804478293749, 0.026367548554233428, 0.0261913293705708, 0.02126370853306549, 0.023870387598386333, 0.02343241305678558, 0.026290576351723865, 0.024439686884867598, 0.02106365902915024, 0.022621461096346544, 0.022915108798849105, 0.026723837544939667, 0.021479926218290063, 0.022599881692684752, 0.02025709556021947, 0.019150306689010323, 0.0245601841217654, 0.02191897199554167, 0.01751039288937257, 0.019330726730524307, 0.019164910448808477, 0.024538845911642237, 0.015059424295682626, 0.020382445553086728, 0.022040061389306198, 0.018767614002140155, 0.019355695793985044, 0.019107737408797596, 0.017818392037986942, 0.017417942697963446, 0.021768966118750215, 0.017869346996114355, 0.017951215525590914, 0.01640004834187341, 0.01777675135988997, 0.019044938084980482, 0.015788522874557873, 0.022552578193564623, 0.019296770346089745, 0.014811130384146363, 0.015027894850487148, 0.018469126043487265, 0.018098258266041212, 0.014053442152699729, 0.015989337644331865, 0.018125206405116463, 0.016060621967932922, 0.014774189744400855, 0.01385672290374157, 0.018077892058706895, 0.01727216446030504, 0.014268980369684513, 0.015724211079724697, 0.016640492273927766, 0.014277730624145401, 0.016971004403142326, 0.014619164786088283, 0.016830329990812937, 0.01374343925469295, 0.012524507506886185, 0.01549116191725552, 0.013598875393053052, 0.0151209402418879, 0.014543565561276227, 0.01439005304061679, 0.01173475706005888, 0.016134371224896974, 0.011917180406560032, 0.012485065904832735, 0.0135999512963786, 0.01591276152397482, 0.011587380154317354, 0.009772636422980987, 0.01344329923276031, 0.015756404174912624, 0.014378462400149205, 0.014014234783006126, 0.012014912663851542, 0.01611480384725613, 0.012844832076313767, 0.013578993814619104, 0.011672752483787808, 0.014098815763512592, 0.012935566670426325, 0.015032568787632202, 0.012224024389351091, 0.014372231721552152, 0.011108738588597336, 0.011174887407980897, 0.016397042357398143, 0.007919598232576864, 0.011932518933157513, 0.012465056306042874, 0.012968409797498517, 0.012325529049170838, 0.011099239793170491, 0.011867815633636468, 0.012848886264346762, 0.012182228031996032, 0.011562556203482023, 0.010714672763688681, 0.013554639555182596, 0.012572727286869529, 0.01000095399527329, 0.011995266647266443, 0.011573533052920085, 0.012627172824654724, 0.013339461832399402, 0.010677528724291735, 0.011543188395016422, 0.010650319310586942, 0.009674460366150607, 0.010641892690030602, 0.012230279986791725, 0.01145689182891605, 0.01050705676828164, 0.00970218700554869, 0.011088927940358716, 0.010147759896973753, 0.0144252521625949, 0.009280225094541092, 0.0071248399373703905, 0.013807516299508516, 0.01255990872180107, 0.011576532107942895, 0.008876261175843192, 0.009988300831966212, 0.00985363164915767, 0.01084158176923064, 0.011923880645647825, 0.010890684340816107, 0.011154608579179295, 0.010417346872992216, 0.00848533340091224, 0.009280508183635208, 0.012399288785308195, 0.0072954312946646145, 0.007857306191068237, 0.013135696996073112, 0.007985612870672613, 0.009203406998578557, 0.011825267453099425, 0.009498726522558978, 0.010254976841146922, 0.012558721663513307, 0.007996204799592626, 0.007471766532690587, 0.01177365344033834, 0.010632048703338191, 0.01163114502161057, 0.008471069070895856, 0.008329393120841747, 0.010546008776983772, 0.007213423388353836, 0.009344507342992069, 0.01120725078885267, 0.008723668579122892, 0.008438797396795254, 0.009862698597215867, 0.011641287900165775, 0.009182740705942006, 0.00702334287046593, 0.010164535000019891, 0.008744396335386742, 0.009129313499368456, 0.008084669785307412, 0.010208096987843342, 0.011367271971373962, 0.007117051272343865, 0.008831601460626934, 0.00922884266564445, 0.009587360389017516, 0.008383878850809764, 0.007664171105706518, 0.010088089144474219, 0.007183663350923383, 0.010528553138330638, 0.010388069153280813, 0.008873608166863086, 0.007134666277026189, 0.00842848956719368, 0.008079520347828268, 0.009280271344362845, 0.008955260842407897, 0.010437573963957794, 0.007464409461843186, 0.00868264229136562, 0.008687793158964795, 0.008216140737795655, 0.007891892086514802, 0.007512410591053153, 0.008665417990315836, 0.008976994133145053, 0.008162402152780846, 0.008810677293791461, 0.006835815719566462, 0.007850608918254282, 0.010575828147088042, 0.008934358710317107, 0.0076927311314263745, 0.007455576667292384, 0.005771527193367357, 0.008088972039366073, 0.011438270648105747, 0.005829402718917685, 0.009437662637821316, 0.006043440636937449, 0.007854135620251515, 0.009180552848274084, 0.010004485826416422, 0.007954190094466415, 0.0069836680795018575, 0.006469878575636705, 0.008808609063802835, 0.007791922163181544, 0.006387679697369947, 0.009282423594551585, 0.007415674569260415]\n",
        "train_acc_list_linear = [64.05293806246691, 87.14875595553202, 89.55002646903124, 90.75913181577555, 91.62308099523557, 92.33456855479089, 92.87030174695606, 93.13922710428798, 93.25780836421387, 93.82106934886183, 94.23822128110112, 94.36315510852303, 94.60667019587083, 94.74007411328745, 95.01111699311805, 95.09158284806776, 95.25674960296453, 95.67178401270513, 95.61461090524087, 95.74377977766014, 95.9915299100053, 96.22869242985706, 96.29645314981472, 96.45526733721546, 96.42985706723134, 96.65431445209106, 96.86818422445738, 96.87241926945474, 97.08628904182108, 97.16040232927475, 97.2641609317099, 97.3361566966649, 97.35309687665432, 97.51191106405506, 97.51191106405506, 97.76601376389624, 97.7342509264161, 97.88035997882477, 97.82953943885654, 97.96717840127052, 98.00317628374802, 98.16622551614611, 98.13658020116463, 98.11540497617787, 98.1937533086289, 98.37374272101641, 98.19798835362626, 98.44150344097406, 98.33562731604023, 98.41821069348862, 98.43303335097936, 98.4923239809423, 98.5643197458973, 98.4923239809423, 98.66807834833246, 98.64902064584436, 98.69348861831656, 98.76971942826893, 98.68925357331922, 98.64690312334568, 98.82053996823716, 98.74219163578613, 98.81842244573849, 98.86500794070938, 98.88194812069878, 98.78877713075701, 98.90100582318688, 98.90312334568554, 98.96664902064585, 98.97723663313923, 98.98358920063525, 99.0428798305982, 99.01111699311805, 99.12122816304924, 98.98570672313393, 99.10852302805718, 99.09158284806776, 99.05134992059291, 99.1233456855479, 99.1868713605082, 99.0873478030704, 99.10217046056114, 99.08523028057174, 99.13181577554262, 99.11064055055584, 99.33721545791424, 99.22710428798305, 99.17628374801482, 99.0979354155638, 99.20169401799895, 99.2694547379566, 99.2419269454738, 99.25463208046585, 99.08099523557438, 99.26733721545791, 99.2419269454738, 99.30968766543144, 99.3223928004235, 99.1868713605082, 99.25463208046585, 99.40921122286925, 99.36262572789836, 99.3499205929063, 99.16357861302276, 99.52355743779778, 99.31815775542616, 99.2779248279513, 99.37109581789306, 99.3499205929063, 99.37321334039174, 99.36050820539968, 99.42826892535733, 99.22075172048703, 99.41979883536263, 99.3943885653785, 99.46426680783483, 99.39015352038115, 99.33509793541556, 99.47697194282689, 99.28427739544733, 99.36050820539968, 99.5574377977766, 99.5044997353097, 99.41556379036527, 99.38803599788248, 99.5489677077819, 99.49179460031763, 99.37956590788777, 99.47697194282689, 99.5214399152991, 99.55108523028058, 99.42403388035999, 99.45156167284277, 99.51720487030175, 99.47273689782953, 99.42403388035999, 99.5299100052938, 99.41132874536792, 99.48755955532027, 99.46638433033351, 99.53202752779248, 99.57014293276866, 99.49179460031763, 99.53626257278984, 99.50026469031233, 99.50026469031233, 99.5299100052938, 99.61884595023822, 99.45579671784013, 99.59978824775013, 99.55108523028058, 99.55532027527792, 99.47273689782953, 99.62519851773425, 99.67813658020117, 99.57437797776602, 99.45579671784013, 99.5299100052938, 99.50873478030704, 99.61672842773955, 99.46214928533615, 99.59555320275278, 99.57014293276866, 99.6209634727369, 99.53414505029116, 99.58920063525674, 99.5044997353097, 99.57014293276866, 99.53626257278984, 99.58073054526204, 99.6209634727369, 99.46850185283219, 99.72260455267337, 99.58920063525674, 99.55955532027528, 99.56379036527264, 99.60825833774484, 99.64849126521969, 99.60402329274748, 99.5574377977766, 99.60825833774484, 99.61884595023822, 99.63790365272631, 99.56802541026998, 99.58284806776072, 99.66754896770779, 99.58708311275808, 99.62519851773425, 99.57226045526734, 99.59767072525146, 99.64213869772367, 99.60614081524616, 99.63366860772896, 99.66331392271043, 99.65907887771307, 99.58920063525674, 99.61884595023822, 99.67813658020117, 99.66119640021175, 99.62731604023293, 99.68237162519851, 99.57437797776602, 99.66543144520911, 99.80307040762308, 99.5574377977766, 99.5659078877713, 99.63366860772896, 99.69931180518793, 99.66966649020645, 99.67178401270513, 99.63578613022763, 99.59767072525146, 99.63790365272631, 99.66119640021175, 99.64425622022235, 99.72683959767072, 99.68660667019587, 99.61249338274219, 99.7734250926416, 99.7480148226575, 99.56379036527264, 99.74589730015882, 99.67813658020117, 99.62731604023293, 99.69931180518793, 99.65484383271573, 99.59767072525146, 99.7649550026469, 99.73319216516676, 99.61037586024352, 99.65060878771837, 99.62731604023293, 99.72260455267337, 99.73107464266808, 99.66543144520911, 99.76071995764956, 99.71413446267867, 99.62731604023293, 99.74166225516146, 99.71201694017999, 99.67178401270513, 99.58708311275808, 99.69507676019057, 99.73954473266278, 99.68025410269983, 99.71836950767602, 99.70566437268396, 99.73107464266808, 99.65272631021705, 99.61037586024352, 99.79460031762838, 99.71201694017999, 99.68872419269455, 99.70778189518263, 99.7564849126522, 99.7564849126522, 99.66543144520911, 99.76919004764426, 99.64213869772367, 99.66966649020645, 99.71836950767602, 99.76071995764956, 99.73319216516676, 99.75224986765484, 99.6929592376919, 99.67813658020117, 99.65907887771307, 99.72472207517205, 99.68872419269455, 99.71413446267867, 99.72472207517205, 99.76071995764956, 99.7924827951297, 99.69719428268925, 99.68448914769719, 99.73107464266808, 99.73742721016411, 99.76283748014822, 99.75224986765484, 99.66543144520911, 99.70354685018528, 99.7564849126522, 99.76283748014822, 99.79460031762838, 99.72260455267337, 99.64002117522499, 99.79036527263102, 99.69084171519323, 99.8009528851244, 99.75860243515088, 99.73107464266808, 99.6569613552144, 99.75013234515616, 99.77977766013764, 99.79883536262572, 99.70142932768661, 99.69507676019057, 99.7734250926416, 99.70989941768131, 99.7480148226575]\n",
        "test_loss_list_linear = [0.6001535044873462, 0.4386981564993952, 0.32313791202271686, 0.3436826935001448, 0.3047397654576629, 0.28500792238057826, 0.26378655251042515, 0.2683426228297107, 0.27285771396960695, 0.2532431707516605, 0.25363739693135606, 0.25782927618745494, 0.2381599084200228, 0.24098271157081222, 0.23853215436432876, 0.22876223501767598, 0.2459365823762674, 0.24074384005849853, 0.23645990466078123, 0.23522190126937395, 0.23930488064812094, 0.23977934446770185, 0.22856533881642072, 0.2503986795633739, 0.2523015177096514, 0.2286284007497278, 0.2522817155821066, 0.23435633048853455, 0.25106761294106644, 0.23668714861075082, 0.25767872133748787, 0.2464016547937896, 0.24990028208669493, 0.25964026233437015, 0.2509775684014255, 0.25643125852095144, 0.25149472640352505, 0.2636343431750349, 0.2690670864802657, 0.2728121512952973, 0.2877597091719508, 0.2628066184400928, 0.289077399882908, 0.2637001336643509, 0.2718517802947876, 0.30822780637034014, 0.2853905560412243, 0.3029168626914422, 0.2886001042948634, 0.30125397385335434, 0.2918206257928236, 0.31290997323744435, 0.3016124471894228, 0.3364011060409978, 0.30226883032888757, 0.30097986143264993, 0.3160975992350894, 0.324009220079318, 0.2896068644545534, 0.28848243194321793, 0.30809056048518885, 0.31027250127026845, 0.31317287329219134, 0.3204711799728958, 0.323861251740406, 0.30745547297684583, 0.313345615642474, 0.31225476020435783, 0.32664194748755176, 0.3649349614393477, 0.3289772248781268, 0.34484266127715363, 0.34519970382326376, 0.3562348909566508, 0.34551431653181125, 0.3507686852495752, 0.3557186407749267, 0.34874562685396154, 0.3626312471414898, 0.32565479080977977, 0.3328241787111277, 0.3502718099739914, 0.35965223179436195, 0.33991847547026827, 0.32748869253212914, 0.3568636128751963, 0.366436490642966, 0.36107295554350405, 0.34370773008056715, 0.343383331070928, 0.3387923375794701, 0.33830804925631075, 0.3589262347485797, 0.3675877243079537, 0.3818419510498643, 0.35536084654649686, 0.35281344309595286, 0.3716038850629154, 0.35846942082485733, 0.34315043032242387, 0.36869288876871853, 0.3778895241226636, 0.38406652862242624, 0.352233542257226, 0.3620971898712656, 0.37449508531055614, 0.37493528568131085, 0.36601132105159406, 0.36861560567665624, 0.3717369966793294, 0.369726698087784, 0.3781300349087984, 0.35664984070714195, 0.3536169459751132, 0.406203392372631, 0.3737115380445532, 0.36973171939562055, 0.37459768973948326, 0.40706704461983606, 0.3881958369896108, 0.37344259142364356, 0.3670044747710812, 0.3696818688847855, 0.37015472998952165, 0.38189552233134416, 0.3749138325744984, 0.3891030573742647, 0.3678013856029686, 0.36681280497863306, 0.3926687809620418, 0.4037954283184281, 0.4014408321065061, 0.3803745116673264, 0.38791271775741787, 0.37617705127808687, 0.3729255223537193, 0.36608954668775495, 0.3800407153572522, 0.37864029130843635, 0.3903196998415332, 0.39593674976597815, 0.4233312764953749, 0.41837985076092404, 0.4037898524353902, 0.39368393412772934, 0.40711697678574743, 0.3831657852119237, 0.39936033349630295, 0.4122653530234946, 0.4055917443908459, 0.391819414573119, 0.3998463283493823, 0.3856644867325896, 0.38932094629853964, 0.45811930652159977, 0.4117749379582557, 0.3846923142269838, 0.38948512334814844, 0.3886060610632686, 0.4141315968488069, 0.39312611405244646, 0.40893315564037536, 0.3917342342217179, 0.4033093715232669, 0.40686407040658534, 0.402124198153615, 0.3760559221218322, 0.41189432279298116, 0.3945868524777539, 0.4064665962668026, 0.4281328501380688, 0.3944055610610282, 0.4145982712644207, 0.39619766517231864, 0.4008463170269833, 0.4100334781368135, 0.3959438812681565, 0.4048078361277779, 0.3898382809506181, 0.3940078625638111, 0.41857934771699135, 0.40016016322553305, 0.42573420590191496, 0.4076968614815497, 0.40349324788971275, 0.41680151768320917, 0.4404344489323158, 0.4126104597257925, 0.41489401694464806, 0.39322311091510687, 0.39995222047482637, 0.4001541677862406, 0.40168759471955984, 0.4003755842317261, 0.4116448443930815, 0.3964004161512004, 0.40526396525092423, 0.40040231844885094, 0.436148803023731, 0.42100324081804824, 0.42921065237811384, 0.4016612854548821, 0.4237121128860642, 0.439825722326835, 0.40309608828586835, 0.4022408921143734, 0.40526854241376414, 0.4139130270638156, 0.42377193091327653, 0.4159748113506437, 0.43089511207140546, 0.41587188575124623, 0.42511460325662415, 0.42877695572507735, 0.4061464395730154, 0.4108063448129185, 0.4237177971266575, 0.4147245909142144, 0.4100258317867331, 0.44727529514599224, 0.40763759292552576, 0.4096112189969669, 0.4398777195818576, 0.4397024883450392, 0.42169872507014694, 0.4525160300362782, 0.4039532243748944, 0.42747145336048276, 0.41558541071500776, 0.4243580105614063, 0.4347197997529863, 0.40417124533697085, 0.4213481634563091, 0.42768024219492196, 0.41689553120922224, 0.42403014976640835, 0.43499788091353636, 0.40865986559576556, 0.39625785907949596, 0.41788156942793114, 0.42710175029203, 0.39860118992453186, 0.40182735008534554, 0.43774937425612237, 0.4086107819651564, 0.42035614554861594, 0.42495340735231546, 0.42730405055943477, 0.41829457241749646, 0.4210991954211803, 0.39986408456210415, 0.43789072635163573, 0.4252349973893633, 0.4179426681058591, 0.4189265777229094, 0.42167082347327334, 0.4152061831133038, 0.4278368429582128, 0.42154261065354826, 0.39703031598279875, 0.42471260858663157, 0.4301079974747172, 0.4101321147526523, 0.4378960633869557, 0.412081252308745, 0.4338206772089881, 0.4061669551530013, 0.4237026169088067, 0.4324107744557527, 0.4283796655671561, 0.4480068746077664, 0.42207027151815446, 0.4178310304186216, 0.42660820346289113, 0.423226847276822, 0.44835605301127274, 0.4472741484733335, 0.4282900923771747, 0.4323618656535651, 0.4345184702519784, 0.4196194001455225, 0.4023636024691822, 0.4263699218977754, 0.45100487594940136, 0.4451090616046214, 0.4251364004779972, 0.4428678105822673, 0.42934233425459, 0.4463761615374025, 0.42917647955519167, 0.43650849757935195, 0.40754444616865, 0.4072841090405835, 0.43068673907249583, 0.42376545924857695, 0.4177722467438263, 0.41679786920105794, 0.4319006550381435, 0.42252690807057947, 0.4652784176572573]\n",
        "test_acc_list_linear = [81.16548862937923, 86.56653349723418, 90.2581438229871, 89.47065150583897, 90.86893054701905, 91.77934849416103, 92.38629379225569, 92.26336816226183, 92.23263675476336, 92.60525507068223, 92.71665642286416, 92.73586355255071, 93.29287031346036, 93.27366318377382, 93.4081130915796, 93.58481868469576, 93.13921327596803, 93.3620159803319, 93.6078672403196, 93.46957590657652, 93.5540872771973, 93.73847572218807, 93.77688998156115, 93.21220036877689, 93.30055316533497, 94.06883835279656, 93.46957590657652, 94.04963122311001, 93.73847572218807, 93.8959741856177, 93.44652735095268, 93.91518131530424, 93.73847572218807, 93.6578057775046, 93.9881684081131, 93.98432698217579, 93.95743700061463, 93.98432698217579, 93.8921327596804, 93.92670559311617, 93.68469575906576, 94.3338967424708, 93.6040258143823, 94.10341118623234, 94.14950829748003, 93.79609711124769, 94.17255685310387, 93.64244007375538, 93.9958512599877, 93.56945298094652, 94.00353411186232, 93.67317148125385, 93.96127842655194, 93.50799016594961, 93.99969268592501, 94.3300553165335, 93.97664413030117, 93.78457283343577, 94.16871542716656, 94.10341118623234, 94.16103257529196, 94.16487400122925, 94.05731407498463, 94.11877688998156, 94.12645974185618, 94.16487400122925, 94.17639827904118, 94.17639827904118, 94.18792255685311, 94.02658266748617, 94.1379840196681, 93.90365703749232, 93.80762138905962, 93.93054701905348, 94.00353411186232, 93.98432698217579, 93.97280270436386, 94.06499692685925, 93.99969268592501, 94.25706822372464, 94.41840811309157, 94.01505838967425, 93.78457283343577, 94.14566687154272, 94.25706822372464, 94.04963122311001, 93.8959741856177, 93.97280270436386, 94.13030116779349, 94.41072526121697, 94.20712968653964, 94.31084818684695, 93.77304855562384, 93.85755992624462, 93.78073140749846, 94.21865396435157, 94.3262138905962, 93.96896127842655, 94.20328826060233, 94.43377381684081, 94.19944683466503, 94.17639827904118, 93.82298709280884, 94.3761524277812, 94.31084818684695, 94.13030116779349, 94.18023970497849, 94.27627535341118, 94.14950829748003, 94.30700676090964, 94.41456668715428, 94.35310387215735, 94.10341118623234, 94.3262138905962, 93.75, 94.23786109403811, 94.28779963122311, 93.94975414874001, 93.8921327596804, 93.83066994468346, 94.08420405654579, 94.37231100184388, 94.17255685310387, 94.44913952059004, 94.14950829748003, 94.3761524277812, 94.22249539028887, 94.21865396435157, 94.53365089121081, 94.14566687154272, 94.16487400122925, 93.91133988936693, 93.98432698217579, 94.2839582052858, 94.19944683466503, 94.21481253841426, 94.3338967424708, 93.98432698217579, 94.18792255685311, 94.03042409342348, 94.27627535341118, 93.83066994468346, 94.12261831591887, 94.02274124154886, 94.13030116779349, 94.0880454824831, 94.52980946527352, 94.0880454824831, 94.02658266748617, 94.3262138905962, 94.31084818684695, 94.42609096496619, 94.39535955746773, 94.42224953902888, 93.99969268592501, 94.16871542716656, 94.35310387215735, 94.23017824216349, 94.26475107559926, 94.39535955746773, 94.49139520590043, 94.26090964966195, 94.51444376152428, 94.40304240934235, 94.15719114935465, 94.16103257529196, 94.41840811309157, 94.17639827904118, 94.31853103872157, 94.31853103872157, 94.00737553779963, 94.3338967424708, 94.34926244622004, 94.3338967424708, 94.24554394591273, 94.51060233558697, 94.39151813153042, 94.27243392747388, 94.43377381684081, 94.3262138905962, 94.23017824216349, 94.24170251997542, 93.86908420405655, 94.13030116779349, 94.29164105716042, 94.35694529809466, 94.19560540872772, 94.3799938537185, 94.3338967424708, 94.25322679778733, 94.2839582052858, 94.2340196681008, 94.26859250153657, 94.69114935464044, 94.16487400122925, 94.46834665027659, 94.27243392747388, 94.36846957590657, 94.10725261216963, 94.43377381684081, 94.23786109403811, 94.46450522433928, 94.32237246465888, 94.28779963122311, 94.11493546404425, 94.16871542716656, 94.4299323909035, 94.30700676090964, 94.19560540872772, 94.46066379840197, 94.17639827904118, 94.53365089121081, 94.16871542716656, 94.29164105716042, 94.29548248309773, 94.49523663183774, 94.25706822372464, 94.10341118623234, 94.64889366933005, 94.30316533497235, 94.42609096496619, 94.44529809465274, 94.19944683466503, 94.1917639827904, 94.44529809465274, 94.16103257529196, 94.3338967424708, 94.54901659496005, 94.4721880762139, 94.31084818684695, 94.39920098340504, 94.49139520590043, 94.3761524277812, 94.45298094652735, 94.31468961278426, 94.45682237246466, 94.30316533497235, 94.55285802089736, 94.4299323909035, 94.40688383527966, 94.34542102028273, 94.4299323909035, 94.49139520590043, 94.27243392747388, 94.47602950215119, 94.26090964966195, 94.24554394591273, 94.54517516902274, 94.3300553165335, 94.29548248309773, 94.61432083589429, 94.11877688998156, 94.40304240934235, 94.27243392747388, 94.59511370620774, 94.2839582052858, 94.39151813153042, 94.39535955746773, 94.21865396435157, 94.49523663183774, 94.24554394591273, 94.51060233558697, 94.59127228027043, 94.43377381684081, 94.31853103872157, 94.46834665027659, 94.43761524277812, 94.43761524277812, 94.37231100184388, 94.34157959434542, 94.00353411186232, 94.38383527965581, 94.58358942839583, 94.5759065765212, 94.41840811309157, 94.07652120467118, 94.09956976029503, 94.3799938537185, 94.30700676090964, 94.29932390903504, 94.38383527965581, 94.5259680393362, 94.26859250153657, 94.35310387215735, 94.36462814996926, 94.37231100184388, 94.38767670559312, 94.35694529809466, 94.36846957590657, 94.34542102028273, 94.40304240934235, 94.28779963122311, 94.46834665027659, 94.56054087277197, 94.62968653964352, 94.35310387215735, 94.43761524277812, 94.39535955746773, 94.26475107559926, 93.79609711124769]\n",
        "train_loss_list_exp = [1.5108453227575556, 0.45569133027620756, 0.38955149030297753, 0.38201055714108434, 0.3791554563736851, 0.37846633355791975, 0.37789585104156637, 0.37833547551780533, 0.37885424543202406, 0.3798585463830126, 0.3801606303146538, 0.3785690202864851, 0.380058984244419, 0.38191306041831247, 0.3755665107309657, 0.37845069464790787, 0.3810173268240642, 0.37906573632060675, 0.38025176412044825, 0.37710853672124506, 0.37910957913088605, 0.3795646985129612, 0.37768419939004955, 0.37915716253645054, 0.3763544963304266, 0.37615088787343764, 0.3767496614356028, 0.3780435439209305, 0.37970296291477956, 0.3768931365190806, 0.3782231926433439, 0.37863624968179843, 0.3801447201146666, 0.38008787327504095, 0.3784985868184547, 0.3775513489152681, 0.3797982176387213, 0.378436125835106, 0.37775709227656284, 0.3784049479618951, 0.3812676381047179, 0.377607957215167, 0.37778531927564923, 0.3785970114110931, 0.3763440459320539, 0.37659880587563604, 0.38260392961786366, 0.3788121801404772, 0.3782783241937477, 0.37851579224837184, 0.37789375260270386, 0.37802613358995135, 0.3780238080800064, 0.38144587775879113, 0.3779912972595634, 0.3791698661600025, 0.3766854087996289, 0.37804330437163997, 0.3786587099718854, 0.37654282059772876, 0.3787634488609102, 0.3799068733524824, 0.37713607115958764, 0.37751451530430696, 0.3801226066299247, 0.37911422368956776, 0.3805197463610631, 0.37700172774190827, 0.3788724349847008, 0.3784166157326401, 0.38057625192775313, 0.37780569122251134, 0.38091700400924944, 0.3794297932528545, 0.3814112742338077, 0.37960589101644066, 0.37835461256626823, 0.37888884483798735, 0.3795582245439695, 0.37849634607148364, 0.37682519744082194, 0.37698636250444223, 0.3814956169464401, 0.3775139360570003, 0.379286357057773, 0.37696852730864755, 0.3763741816123973, 0.37921798265561824, 0.3787178158921601, 0.3787029756682352, 0.37783306570557074, 0.3772085670452454, 0.377733004771597, 0.3765660253401371, 0.3802021978912638, 0.37975252381346736, 0.3792028047528047, 0.3785102287039847, 0.3774594428739574, 0.3806928960773034, 0.3792232088441771, 0.37827566011649805, 0.38008303063994825, 0.3778988100325835, 0.3803016372974003, 0.3791544923614357, 0.38108806501882186, 0.37818956795100594, 0.37843327103106955, 0.38068289166382013, 0.38067130910025704, 0.37755737811084683, 0.37938234137325755, 0.3798758971497295, 0.37914535011540906, 0.37674440685811084, 0.37839110369281714, 0.3803737539505248, 0.3769939240966709, 0.37618244623104086, 0.3802966764625818, 0.376914295521855, 0.38093100847590583, 0.3785027291797364, 0.38015424458153524, 0.3799492670753138, 0.3770360491140102, 0.3781222916311688, 0.37643939103214397, 0.37902817707559283, 0.37899113222350916, 0.3788869072428241, 0.37866583002130516, 0.38024979207896925, 0.37998249028433306, 0.37665241209633626, 0.3791340853221371, 0.3768656629776244, 0.3781943121738227, 0.37956079872966136, 0.3790555945660687, 0.3767165967280949, 0.3785593254860178, 0.3785436793071468, 0.3786095209157241, 0.37977585394369556, 0.37895426606421223, 0.37726105127715803, 0.3811360982456181, 0.3777372095239195, 0.37985739951857384, 0.3775850888190231, 0.37942486108964696, 0.3774096647575296, 0.37866538746893247, 0.37956950698441605, 0.37765448986676325, 0.3765703885535884, 0.37493878506063444, 0.3787174464322041, 0.3790939331539278, 0.3798395115024029, 0.3802033216972661, 0.3778447469237051, 0.37922620373528176, 0.3788863822696655, 0.3776389633010073, 0.37965354569720705, 0.37847416466329153, 0.38210083584636856, 0.3769269231858292, 0.376283608316406, 0.3791458204105941, 0.3800827085891067, 0.3800524467940576, 0.379729083156198, 0.3786869500145357, 0.37784998191566, 0.37798632447150987, 0.37959555187199495, 0.3772686913326827, 0.37737370587299834, 0.3774894487647829, 0.3773925192149351, 0.37866038096144916, 0.3799699583673865, 0.37930802181161194, 0.37929151229419034, 0.37872534558217374, 0.38051237880699035, 0.3785012537224829, 0.37907243408969427, 0.3786347356188265, 0.3772817448306536, 0.37789186423386983, 0.377803510807071, 0.37891541296227516, 0.3768928705596019, 0.3764786839808229, 0.37735731827049723, 0.37620092403436417, 0.37594923223583354, 0.37946950848186567, 0.37934725721515616, 0.3799315985383057, 0.37742675118006985, 0.3800657158137014, 0.38043200723362486, 0.3790072623468673, 0.37812678907621844, 0.380248889528962, 0.3795400029679301, 0.37716066687895355, 0.37926767851279036, 0.3811982564002195, 0.3785968935344277, 0.37829239452434427, 0.3791591095003655, 0.37750050801087204, 0.3755794913788152, 0.37707439347657407, 0.3798365030023787, 0.37901620768757693, 0.3787338199815776, 0.3768027005237616, 0.378330920527621, 0.3773879486774688, 0.37904529988281127, 0.3793277707364824, 0.37877471587522243, 0.3773436505135482, 0.38000925620235404, 0.37747299065434836, 0.3783611831707037, 0.38008849536823386, 0.3802644786069064, 0.37820831300604957, 0.37898807813158525, 0.37832132050500006, 0.37651883493755567, 0.3780955279745707, 0.3794448867157546, 0.37681427719147226, 0.37846876878725483, 0.37892088152690306, 0.3757547806433546, 0.3790287462272618, 0.3771775669764051, 0.37960463952081314, 0.38023487892415786, 0.3773157599818739, 0.3790636116734688, 0.37847773605568946, 0.3790109474969104, 0.3778621018094422, 0.3795850692483468, 0.38052020777208695, 0.3777503734681664, 0.38023589062819957, 0.37878056504539037, 0.3778544519813403, 0.3769152445117956, 0.37817029428837423, 0.3794189135879682, 0.3814815942268708, 0.3767595124357761, 0.376037456559618, 0.3787438214067521, 0.37781988387185383, 0.3780568046621514, 0.3789186481295562, 0.3792363849031893, 0.3795603767723895, 0.38027571985715125, 0.37710057130350977, 0.37790428061633896, 0.3758348726887044, 0.37957349484205893, 0.3801340398184329, 0.37570041014250055, 0.37696901714898706, 0.3780564808748602, 0.37865450675409984, 0.37989892315896867, 0.37623435913062675, 0.3776249932402841, 0.3777484252120098, 0.37681428954853274, 0.3777670302203677, 0.37880285431537525, 0.3775242522641572, 0.37820892516513505, 0.37701137581976446, 0.378751233704691, 0.37953954490865793, 0.3773617081364319, 0.3809620241324107, 0.37960273697770386, 0.3808248356428896, 0.37698043551709914]\n",
        "train_acc_list_exp = [47.21651667548968, 85.66437268395977, 87.928004235045, 88.11858125992589, 88.25410269984118, 88.07623080995235, 88.23928004235044, 88.18422445738486, 88.14187400741133, 88.09740603493913, 88.06564319745897, 88.14399152991001, 88.0084700899947, 87.9915299100053, 88.28163049232398, 88.06776071995765, 88.09105346744309, 88.11434621492853, 87.96188459502382, 88.11222869242985, 88.12916887241927, 88.13975648491265, 88.2435150873478, 88.16728427739545, 88.09105346744309, 88.20116463737428, 88.20328215987296, 88.01482265749074, 88.0084700899947, 88.20116463737428, 88.15034409740603, 88.22445738485972, 88.04870301746956, 88.06776071995765, 88.28586553732133, 88.19269454737956, 88.11011116993119, 88.18422445738486, 88.20539968237162, 88.08046585494971, 88.071995764955, 88.23292747485442, 88.23292747485442, 88.15881418740074, 88.26680783483324, 88.32398094229751, 88.06140815246162, 88.18210693488618, 88.17363684489148, 88.15881418740074, 88.13975648491265, 88.15034409740603, 88.12281630492323, 88.01694017998942, 88.19481206987824, 88.18634197988354, 88.3070407623081, 88.16093170989942, 88.16516675489677, 88.19481206987824, 88.25622022233986, 88.12493382742191, 88.1355214399153, 88.16093170989942, 88.09105346744309, 88.24563260984648, 88.11858125992589, 88.10799364743251, 88.09317098994177, 88.09528851244045, 88.0359978824775, 88.17787188988883, 88.10375860243515, 88.1355214399153, 88.08470089994707, 88.09105346744309, 88.23928004235044, 88.12916887241927, 88.10375860243515, 88.26045526733722, 88.24775013234516, 88.16093170989942, 88.05717310746427, 88.11858125992589, 88.15034409740603, 88.24563260984648, 88.14822657490735, 88.05293806246691, 88.1630492323981, 88.12281630492323, 88.32821598729487, 88.21386977236634, 88.23928004235044, 88.26892535733192, 88.23928004235044, 87.91529910005293, 88.05293806246691, 88.16093170989942, 88.26892535733192, 88.10587612493383, 88.13128639491795, 88.28374801482266, 88.21810481736368, 88.20328215987296, 88.18422445738486, 88.09740603493913, 88.00635256749602, 88.08681842244575, 88.10375860243515, 88.09528851244045, 88.06140815246162, 88.16093170989942, 88.10799364743251, 88.12493382742191, 88.08681842244575, 88.29433562731604, 88.08258337744839, 88.0084700899947, 88.17363684489148, 88.27527792482795, 88.10587612493383, 88.12069878242457, 87.98941238750662, 88.22022233986236, 88.12916887241927, 88.12281630492323, 88.14399152991001, 88.16516675489677, 88.14399152991001, 88.09528851244045, 88.18845950238222, 88.19692959237692, 88.15246161990471, 88.06564319745897, 88.12069878242457, 88.25833774483854, 88.15246161990471, 88.25622022233986, 88.11646373742721, 88.10375860243515, 88.11434621492853, 88.18422445738486, 88.14610905240868, 88.27316040232928, 88.15881418740074, 88.16728427739545, 88.13763896241397, 88.08681842244575, 88.03811540497618, 88.15457914240339, 88.15034409740603, 88.28586553732133, 88.13975648491265, 88.29010058231869, 88.25622022233986, 88.21810481736368, 88.22445738485972, 88.27316040232928, 88.2710428798306, 88.11222869242985, 88.02541026998412, 88.02541026998412, 88.08258337744839, 88.14610905240868, 88.09740603493913, 88.18634197988354, 88.18634197988354, 88.17998941238751, 88.11646373742721, 88.06140815246162, 88.05293806246691, 88.2710428798306, 88.071995764955, 88.22233986236104, 88.09317098994177, 88.12493382742191, 88.18634197988354, 88.11434621492853, 88.21810481736368, 88.00211752249868, 88.11858125992589, 88.27527792482795, 88.30280571731075, 88.24563260984648, 88.15246161990471, 88.12493382742191, 88.11434621492853, 88.02117522498676, 88.24563260984648, 88.06987824245633, 88.23716251985178, 88.0635256749603, 88.14399152991001, 88.22445738485972, 88.19269454737956, 88.26045526733722, 88.1630492323981, 88.1990471148756, 88.14399152991001, 88.27527792482795, 88.24563260984648, 88.32609846479619, 88.02117522498676, 88.15669666490207, 88.071995764955, 88.16728427739545, 88.15034409740603, 88.12281630492323, 88.15881418740074, 88.17787188988883, 88.06987824245633, 88.08470089994707, 88.11646373742721, 88.13763896241397, 88.1355214399153, 88.18845950238222, 88.18845950238222, 88.14399152991001, 88.25410269984118, 88.22445738485972, 88.17998941238751, 88.07623080995235, 88.14187400741133, 88.09952355743779, 88.31127580730545, 88.19481206987824, 88.2265749073584, 88.10164107993647, 88.12705134992059, 88.1905770248809, 88.11434621492853, 88.13128639491795, 88.22233986236104, 88.17998941238751, 88.13128639491795, 88.12705134992059, 88.24563260984648, 88.16940179989412, 88.06987824245633, 88.24986765484384, 88.12493382742191, 88.15457914240339, 88.22869242985706, 88.06987824245633, 88.08470089994707, 88.23928004235044, 88.07411328745368, 88.215987294865, 88.15881418740074, 88.14822657490735, 88.12069878242457, 88.10799364743251, 88.27527792482795, 88.00423504499736, 88.17363684489148, 88.18422445738486, 88.1355214399153, 88.18634197988354, 88.08893594494441, 88.11011116993119, 88.09105346744309, 88.2265749073584, 88.20539968237162, 88.13763896241397, 88.11646373742721, 88.29221810481737, 88.20116463737428, 88.18845950238222, 88.14399152991001, 88.0359978824775, 88.15246161990471, 88.14399152991001, 88.07834833245103, 88.30068819481207, 88.21386977236634, 88.1715193223928, 88.34515616728427, 87.99788247750132, 87.91106405505559, 88.15034409740603, 88.15669666490207, 88.1630492323981, 88.13975648491265, 88.08470089994707, 88.3705664372684, 88.22869242985706, 88.1355214399153, 88.17787188988883, 88.14610905240868, 88.18210693488618, 88.27527792482795, 88.17363684489148, 88.15034409740603, 88.26469031233457, 88.14187400741133, 88.26045526733722, 88.08681842244575, 88.09952355743779, 88.16728427739545, 88.12916887241927]\n",
        "test_loss_list_exp = [0.7599668520338395, 0.41391250622623105, 0.392150557216476, 0.3908533724207504, 0.3910430484980929, 0.3915113131059151, 0.3893689420439449, 0.39278720669886646, 0.3913472319642703, 0.3898209324654411, 0.3919214537622882, 0.3930370972729197, 0.3875842901567618, 0.3887199363579937, 0.3931986349178295, 0.3915082841527228, 0.39034265929869577, 0.38962281656031517, 0.39410267273585003, 0.3923792628680958, 0.3913208600498882, 0.3940731910806076, 0.3901035269978, 0.3910912872091228, 0.38958175787154364, 0.3905664257266942, 0.3923723243323027, 0.3930160653795682, 0.3910279740013328, 0.39261947053612445, 0.39096589437594603, 0.391885179076709, 0.3932027472730945, 0.39465218753206965, 0.38952960351518556, 0.3913427036182553, 0.39291733357251857, 0.38893387598149914, 0.3907097995865579, 0.39178018352272465, 0.38997077613192443, 0.38897235888768644, 0.39309656094102297, 0.3918713939686616, 0.38930126476813764, 0.3919200319431576, 0.3909892667742336, 0.3936515018782195, 0.3921337866900014, 0.3897011807444049, 0.389769053050116, 0.3902200594106141, 0.38991651070468564, 0.3911624582228707, 0.3900841569491461, 0.38808364508783116, 0.39078373330480914, 0.3911517692693308, 0.3905084923494096, 0.392090796986047, 0.3915708720245782, 0.3868667685664168, 0.3898552243469977, 0.3904008638186782, 0.38936665559224054, 0.3908520120323873, 0.3909977381574173, 0.3894340494538055, 0.3913114227938886, 0.38867319550584345, 0.3922208594340904, 0.39250611046365663, 0.3933124074748918, 0.3923281282916957, 0.3923263843445217, 0.3901771641537255, 0.39134884019400556, 0.38913627334085166, 0.39259269424513277, 0.39072176375809836, 0.3921530346806143, 0.39100976697370116, 0.39079241250075547, 0.3899397780643959, 0.3929170014373228, 0.3889102427398457, 0.38894158966985404, 0.39256049473496046, 0.3897372295020842, 0.39233241295989824, 0.39279353041567056, 0.394793822411813, 0.3923672336865874, 0.38789207575952306, 0.391120845853698, 0.3914312515042576, 0.3890334714715387, 0.38947177671042144, 0.3914659348334752, 0.39052898429480254, 0.38922284959870224, 0.3926088816541083, 0.3930806327684253, 0.392626012584158, 0.3940140974580073, 0.3908073946687521, 0.3892480945762466, 0.3914746798428835, 0.3904998555925547, 0.39119182118013796, 0.39440940546931, 0.39126060596283746, 0.39380357363352586, 0.3891267492344566, 0.3960636570027061, 0.3884448780879086, 0.39201007885675804, 0.3887008518418845, 0.3946489935704306, 0.39012787235425966, 0.3926868131201641, 0.391174876587648, 0.39185114438627283, 0.3916316426121721, 0.39595334516728625, 0.3925237079315326, 0.3923376457510041, 0.3896145831574412, 0.3903454670719072, 0.38917045835770814, 0.3901947463552157, 0.38894958538459795, 0.39138510881685745, 0.3923698270729944, 0.391075956543871, 0.3886808121905607, 0.3895190705855687, 0.3937717227666986, 0.3892171628334943, 0.3928420579462659, 0.3929640487128613, 0.39179296280239145, 0.39022867213569434, 0.3915944700585861, 0.3908092556338684, 0.38998721832153843, 0.3919305083360158, 0.38930609106433156, 0.3915616778620318, 0.39198073642510994, 0.3898476078083702, 0.3942314390750492, 0.39122209778311207, 0.39321639089315547, 0.39556199356037025, 0.39069073004465477, 0.3901906652631713, 0.3918502590089452, 0.3921655998656563, 0.3897442833027419, 0.39050330440787706, 0.39026205139417275, 0.3908781868716081, 0.39245978443353785, 0.3903481831007144, 0.39445868105280635, 0.3892170712351799, 0.390042986282531, 0.3925706706941128, 0.39200084298556925, 0.3911351076528138, 0.39240403268851487, 0.3938527383348521, 0.3900003031480546, 0.3922225654709573, 0.3932921053001694, 0.39253465655971975, 0.3926194997540876, 0.3874349548097919, 0.39200115612908903, 0.3948955665908608, 0.3898700127998988, 0.38938687602971117, 0.3864065995257275, 0.39098570473930416, 0.3909385864641152, 0.3916171494067884, 0.39381099733359676, 0.3898175518740626, 0.3911994169740116, 0.39155133265782804, 0.39230194549058, 0.39114902234252763, 0.39017920955723406, 0.3944379236622184, 0.3916288679283039, 0.39200210498244153, 0.39363550146420795, 0.3911365672361617, 0.39186333141782703, 0.39117470471297994, 0.38984077394593, 0.39264219977399883, 0.3900822729018389, 0.38898184559508864, 0.3910406724202867, 0.3907987319809549, 0.39212526841198697, 0.39124271427007284, 0.3914998725202738, 0.38839635442869336, 0.3937654222781752, 0.3902626017875531, 0.3955241435883092, 0.39323169840317146, 0.39330630157800284, 0.38655056892072454, 0.39017754106544983, 0.3952399448436849, 0.39261257144458156, 0.3932858498073092, 0.39499423879326556, 0.3910665978111473, 0.3922448316041161, 0.39132243789294185, 0.3905644398547855, 0.39066209877822916, 0.39234474681171716, 0.3916584701806891, 0.39128884439374884, 0.38904376978091165, 0.39183107588221044, 0.3900741520611679, 0.3930635173969409, 0.38995324082526506, 0.39311547854951784, 0.3887742017121876, 0.3909773443113355, 0.38996322007448064, 0.38965144855718986, 0.39268765284442436, 0.390111534545819, 0.3925885803559247, 0.39064669915858435, 0.3895316495030534, 0.3907163414154567, 0.3907886814399093, 0.39067917981860684, 0.3914143468673323, 0.39072976911477014, 0.39038341521632436, 0.39487345085716713, 0.39242782728636966, 0.39340144921751585, 0.391638440317383, 0.38982952050134245, 0.39072682591629965, 0.39341319571523103, 0.39195014886996327, 0.3917578032203749, 0.38969211536003096, 0.3894471714601797, 0.3898716885961738, 0.39170538400318106, 0.39051353675769823, 0.39179608466870647, 0.3921622397998969, 0.39439454747765673, 0.39445579307628614, 0.39132673534400325, 0.3867330071972866, 0.3894542139388767, 0.39122700530524346, 0.3883669243431559, 0.38984382444737004, 0.3917064682817927, 0.3907441727670969, 0.39336200226463525, 0.3893578349083078, 0.3913763885696729, 0.3897980788320887, 0.3937839002293699, 0.39068301957027585, 0.3921215317997278, 0.39208485865417647, 0.3916279115513259, 0.3911278141918136, 0.39544760121726524, 0.38938475297946556, 0.39043475139667005, 0.39209672136634005, 0.39414026142627584, 0.3913420901871195, 0.39028101735839654, 0.3931575333516972, 0.39237413212072614, 0.38880399651094977, 0.3887612786801422, 0.3894933131979961, 0.3917112931901333]\n",
        "test_acc_list_exp = [75.58773816840811, 87.06976029502151, 87.53073140749846, 87.8918254456054, 87.76889981561156, 87.74969268592501, 87.98786109403811, 87.76121696373694, 87.87645974185618, 87.71896127842655, 87.74969268592501, 87.79578979717272, 87.96481253841426, 87.81115550092194, 87.81499692685925, 87.80731407498463, 87.9417639827904, 87.93023970497849, 87.76505838967425, 87.78042409342348, 87.66133988936693, 87.7458512599877, 87.7458512599877, 87.85725261216963, 87.66518131530424, 87.71127842655194, 87.65365703749232, 87.66133988936693, 87.74200983405039, 87.7458512599877, 87.85725261216963, 87.89566687154272, 87.64981561155501, 87.61140135218193, 88.02243392747388, 87.85725261216963, 87.50384142593731, 87.91871542716656, 87.82652120467118, 87.80731407498463, 87.82652120467118, 87.93792255685311, 87.6920712968654, 87.72280270436386, 87.82267977873387, 87.81499692685925, 87.87645974185618, 87.66133988936693, 87.8418869084204, 87.76889981561156, 87.88030116779349, 87.78042409342348, 87.7919483712354, 87.79963122311001, 87.81883835279656, 87.8918254456054, 87.92639827904118, 87.94944683466503, 87.78426551936079, 87.8918254456054, 87.7881069452981, 88.00322679778733, 87.87645974185618, 87.7881069452981, 87.68054701905348, 87.81499692685925, 87.71896127842655, 87.82267977873387, 87.7919483712354, 87.81883835279656, 87.83036263060848, 87.78042409342348, 87.78042409342348, 87.71511985248924, 87.72664413030117, 87.82267977873387, 87.80731407498463, 87.88030116779349, 87.83036263060848, 87.84956976029503, 87.91487400122925, 87.73048555623848, 87.81115550092194, 87.73048555623848, 87.7381684081131, 87.72664413030117, 87.8918254456054, 87.66518131530424, 87.8418869084204, 87.7919483712354, 87.69975414874001, 87.79578979717272, 87.77658266748617, 87.90334972341734, 87.75353411186232, 87.83036263060848, 87.86877688998156, 87.86877688998156, 87.7919483712354, 87.84572833435772, 87.91871542716656, 87.84956976029503, 87.83036263060848, 87.80347264904732, 87.70743700061463, 87.75353411186232, 87.9840196681008, 87.70359557467732, 87.84956976029503, 87.87645974185618, 87.7381684081131, 87.81115550092194, 87.6459741856177, 87.7458512599877, 87.51920712968654, 88.05700676090964, 87.79578979717272, 87.91871542716656, 87.56914566687155, 87.82267977873387, 87.73432698217579, 87.75737553779963, 87.81115550092194, 87.85341118623234, 87.74969268592501, 87.68054701905348, 87.63444990780577, 87.83420405654579, 87.91103257529196, 87.76505838967425, 87.80731407498463, 88.04548248309773, 87.81883835279656, 87.82652120467118, 87.6920712968654, 87.76121696373694, 87.74200983405039, 87.80347264904732, 87.96481253841426, 87.8380454824831, 87.6459741856177, 87.85341118623234, 87.92255685310387, 87.68438844499079, 87.7458512599877, 87.78426551936079, 87.81883835279656, 87.98017824216349, 87.88030116779349, 87.6421327596804, 87.9340811309158, 87.65365703749232, 87.68438844499079, 87.60371850030731, 87.66133988936693, 87.6459741856177, 87.78042409342348, 87.77658266748617, 87.58451137062077, 87.97633681622618, 87.83420405654579, 87.73048555623848, 87.84572833435772, 87.75737553779963, 87.76121696373694, 87.63829133374308, 87.91103257529196, 87.75353411186232, 87.57298709280884, 87.70743700061463, 87.73048555623848, 87.77658266748617, 87.76505838967425, 88.02243392747388, 87.84572833435772, 87.73048555623848, 87.88414259373079, 87.69975414874001, 87.88030116779349, 87.88030116779349, 87.68054701905348, 87.92639827904118, 87.95712968653964, 87.91103257529196, 87.68054701905348, 87.76121696373694, 87.78426551936079, 87.67286416717886, 87.67670559311617, 87.88414259373079, 87.79578979717272, 87.66133988936693, 87.83036263060848, 87.86493546404425, 87.78426551936079, 87.67286416717886, 87.81115550092194, 87.6421327596804, 87.70359557467732, 87.71127842655194, 87.84956976029503, 87.82267977873387, 87.7919483712354, 87.71127842655194, 87.83036263060848, 87.8879840196681, 87.76121696373694, 87.73048555623848, 87.76121696373694, 87.8380454824831, 87.75353411186232, 87.72280270436386, 87.84956976029503, 87.59987707437, 87.6959127228027, 87.62292562999386, 88.03779963122311, 87.77658266748617, 87.7381684081131, 87.80731407498463, 87.56530424093424, 87.63060848186846, 87.69975414874001, 87.71896127842655, 87.89566687154272, 87.83420405654579, 87.76121696373694, 87.63060848186846, 87.79578979717272, 87.65365703749232, 87.82267977873387, 87.85341118623234, 87.85341118623234, 87.80347264904732, 87.87645974185618, 87.84572833435772, 87.96097111247695, 87.84572833435772, 87.71511985248924, 87.85725261216963, 87.78426551936079, 87.78426551936079, 87.61908420405655, 87.86493546404425, 87.96097111247695, 87.86109403810694, 87.75737553779963, 87.84956976029503, 87.7881069452981, 87.84572833435772, 87.70359557467732, 87.71127842655194, 87.65749846342962, 87.62292562999386, 87.71896127842655, 87.96865396435157, 87.97633681622618, 87.76121696373694, 87.75353411186232, 87.81115550092194, 87.8879840196681, 87.82652120467118, 87.87645974185618, 87.68438844499079, 87.8918254456054, 87.63060848186846, 87.76505838967425, 87.5960356484327, 87.66133988936693, 87.9840196681008, 87.88030116779349, 87.86877688998156, 87.77274124154886, 87.9417639827904, 87.90719114935465, 87.7881069452981, 87.69975414874001, 87.77658266748617, 87.84572833435772, 87.67286416717886, 87.80731407498463, 87.74200983405039, 87.71896127842655, 87.6459741856177, 87.68438844499079, 87.86877688998156, 87.82267977873387, 87.78426551936079, 87.72664413030117, 87.98017824216349, 87.66518131530424, 87.6959127228027, 87.75353411186232, 87.91487400122925, 87.89566687154272, 87.72280270436386, 87.91487400122925, 87.88030116779349, 87.91103257529196, 87.75353411186232]\n",
        "\n",
        "\n",
        "\n",
        "# train_loss_list_001 = [1.5218167792493924, 1.0340943300305083, 0.7934107637633911, 0.6561045421959874, 0.5800367868936862, 0.5100019117132925, 0.4574989944982072, 0.4208847186245476, 0.3881891872079228, 0.35861467820006054, 0.3339165490084944, 0.3111862796849717, 0.2948872684575498, 0.27018505394363557, 0.25883166084940823]\n",
        "# train_acc_list_001 = [44.0975, 63.055, 72.2075, 77.0575, 79.93, 82.43, 84.0575, 85.47, 86.5625, 87.675, 88.48, 89.16, 89.645, 90.5075, 90.9225]\n",
        "# test_loss_list_001 = [1.3665137306044373, 1.0809951907471766, 0.8369579171832604, 0.7580914105041118, 0.6663558483123779, 0.7236429437806334, 0.5709026199352892, 0.5137608224832559, 0.5140901245648348, 0.48385591937016836, 0.47491426897954336, 0.5022718974306614, 0.5243912806993798, 0.42129093069064466, 0.4074777747634091]\n",
        "# test_acc_list_001 = [52.13, 63.83, 71.47, 73.82, 77.34, 76.49, 80.87, 82.32, 82.77, 83.6, 84.17, 83.3, 83.55, 86.28, 86.86]\n",
        "# train_loss_list_01 = [1.8962965864723864, 1.472022865146113, 1.2269341696184664, 1.0348994015885618, 0.9050399440165144, 0.7892040218027255, 0.6947498847120486, 0.6181783355272616, 0.5774767158892208, 0.5502305545936377, 0.5206893583456167, 0.5026034010104097, 0.4774538204311944, 0.4683271567471111, 0.4492297477710742]\n",
        "# train_acc_list_01 = [31.045, 45.415, 55.22, 62.9525, 67.66, 72.095, 75.7825, 78.6325, 80.1775, 80.8325, 82.04, 82.7775, 83.69, 83.865, 84.6225]\n",
        "# test_loss_list_01 = [1.6557437422909314, 1.5662637073782426, 1.1888765285286722, 1.1435910184172136, 0.9932384845576708, 0.7845515208908275, 0.7405012536652481, 0.7023888849004915, 0.6914112718799447, 0.8937227891970284, 0.6744754122027868, 0.7125071339969393, 0.595223272148567, 0.6645651912387414, 0.5624623864511901]\n",
        "# test_acc_list_01 = [38.54, 45.39, 57.61, 59.19, 65.26, 72.6, 74.56, 76.24, 76.7, 71.16, 77.34, 75.59, 80.06, 77.85, 81.28]\n",
        "# train_loss_list_0001 = [1.6901012103016766, 1.282657652045972, 1.0717586383652002, 0.940055356619838, 0.8364242675205389, 0.7547609810821545, 0.6785155514749094, 0.6320573160061821, 0.5852954303875518, 0.5420240767466755, 0.506621429809747, 0.47925190327647393, 0.4483506758563435, 0.4286465794800189, 0.40272510780122717]\n",
        "# train_acc_list_0001 = [37.2, 53.36, 61.4225, 66.4725, 70.2, 73.2275, 76.08, 77.8025, 79.53, 80.905, 82.42, 83.1425, 84.5575, 85.195, 86.005]\n",
        "# test_loss_list_0001 = [1.4665760239468346, 1.2375031430510026, 1.0847761902628066, 1.0535234567485279, 0.8635394460038294, 0.757294207434111, 0.7295623666877988, 0.7312412850464447, 0.7336276015148887, 0.6307676260984396, 0.6266382736495778, 0.6370392079594769, 0.5392829055273081, 0.5410988666588747, 0.5530912065053288]\n",
        "# test_acc_list_0001 = [46.98, 55.76, 61.4, 63.25, 69.13, 73.23, 74.32, 74.64, 73.96, 78.38, 78.25, 78.58, 81.0, 81.49, 81.32]\n",
        "\n",
        "train_loss_list_cut = [1.613359474907287, 1.186605121571416, 0.9613291156558564, 0.8175660327981455, 0.7329503829105974, 0.6675834229198127, 0.6155491586500844, 0.5636878716298186, 0.5260912411319562, 0.5027581219094249, 0.4726218120834698, 0.44723697268543916, 0.43208082353535554, 0.41164722000828946, 0.3974420052652542, 0.3826391176103403, 0.35894755928661115, 0.3507439031863746, 0.34107909901454425, 0.32801356564124173, 0.3124787288542373, 0.31465199137457645, 0.298767308029123, 0.28487236778766584, 0.27646335530966615, 0.27320462827103587, 0.26612033452184053, 0.26308306258993025, 0.2525483318411123, 0.24124891794146822, 0.24217433363389665, 0.23168825182004477, 0.23012469901730076, 0.21693348256162942, 0.21930161923074876, 0.21653421889669217, 0.21434410071125426, 0.20718723166579256, 0.2052558541250305, 0.19716152494041303, 0.1941497164983719, 0.19097941378339792, 0.18447992457939794, 0.18228636479701477, 0.17657887242948667, 0.17477260330043282, 0.1733962548212312, 0.17061091698825168, 0.16732201631219623, 0.1637549052556483, 0.1613706673700779, 0.16051728827075457, 0.15993363755389142, 0.15627589414770993, 0.15532852677158274, 0.15280420149858007, 0.15101519260353174, 0.14880275238341037, 0.13978494027742563, 0.14830744789002803, 0.14331960949463585, 0.13924681502409256, 0.13707038418601114, 0.1308897960062225, 0.13411890785581768, 0.13409768981627002, 0.13344513029217148, 0.1273199183205827, 0.1283481349103367, 0.12471981016924968, 0.1280518789260913, 0.12418704933394639, 0.12167530884139073, 0.12545658500430684, 0.11795977773234105, 0.11789289145423962, 0.12059933257202943, 0.11666790398355491, 0.11807365684558789, 0.11512304616931338, 0.11561917723082125, 0.11491954412323217, 0.11419005636661388, 0.10912806965624944, 0.11002509410198504, 0.11631911551466766, 0.10705401468129394, 0.1092875181331326, 0.10784903160942058, 0.10965043486069186, 0.10153051761511606, 0.1083350477579493, 0.10791226423467501, 0.10326221440665828, 0.10488084647149895, 0.100699621469925, 0.1017464189983595, 0.1026811115919782, 0.10191941613587327, 0.10146693645503384, 0.0997033108370944, 0.09700418662386961, 0.09839744385653221, 0.09619725261109706, 0.0990427272757307, 0.0953601595407096, 0.09479633424157342, 0.09373613773062588, 0.09360281742228486, 0.09450248972224161, 0.0905749333885531, 0.09483718122251499, 0.094004499824188, 0.09207623065731967, 0.0874123571696468, 0.08727556101073282, 0.09273509363444468, 0.08398670141106121, 0.08792692410964935, 0.08606394986732128, 0.08853992108648387, 0.08270674364302105, 0.08553103019539922, 0.08325290489799013, 0.08070171735109613, 0.08357380274028633, 0.0855067997409132, 0.07974059663760585, 0.07902837820208301, 0.07707989068862539, 0.08178071627101768, 0.07777025827025168, 0.0837722305065622, 0.07823536036232599, 0.07849429675731986, 0.08163950307229266, 0.0772132684176151, 0.0742708618630664, 0.07653025378839086, 0.07370652056361635, 0.07428970992767488, 0.066969591103637, 0.07278624822656377, 0.0711755252714022, 0.07434361697005007, 0.0714579665521606, 0.06873778960468194, 0.06797652607099317, 0.06636746319385763, 0.06754642966004035, 0.07100995908232448, 0.07219889727584757, 0.06691185421884631, 0.06648011294047768, 0.06790807842018125, 0.06884961091755583, 0.06649817791454994, 0.06520170254853015, 0.06067689375196116, 0.06284649541583685, 0.058577289441808726, 0.06126952098277859, 0.05914597977369357, 0.06332467258952487, 0.057384233588513474, 0.055878468872473455, 0.0626550735429691, 0.05836306477245241, 0.058223563505294985, 0.05526084442453358, 0.05660795328168633, 0.0533848489447238, 0.05570016410677863, 0.052253410154685806, 0.053411352781418224, 0.05460695667971913, 0.051183629400147417, 0.053861447934287425, 0.05485730031029152, 0.05130477134769146, 0.04791999825934014, 0.0483061204941128, 0.04967576256259895, 0.045770330684848676, 0.04519799896501028, 0.04884168341720161, 0.04494558678963742, 0.04718005882141689, 0.044031581079688506, 0.04476618649699865, 0.043659136812098494, 0.04084714540554145, 0.04083325983599399, 0.04264434854375026, 0.04191593292166297, 0.03981593097694004, 0.043085993813594785, 0.040142026240416705, 0.03974097740119353, 0.037398707843162474, 0.03773854231051268, 0.035977833013172256, 0.037191637455464936, 0.03843252851401631, 0.03596398131787991, 0.03615279636307123, 0.03480513299854038, 0.034845688106557623, 0.03309281391743273, 0.03143419835645075, 0.030938323252438643, 0.031188237175833397, 0.032819104747632485, 0.03191146987844437, 0.03148931583624702, 0.030030576116479815, 0.03224378252050843, 0.03121553240239787, 0.02776832960601063, 0.02880164314531528, 0.029192425741311222, 0.030026258470062107, 0.029695636324269085, 0.027793155418494687, 0.026486221334551828, 0.026275856554293976, 0.025177531725408646, 0.025483246410664278, 0.023590462400574986, 0.025644099438033356, 0.027068594341187146, 0.02301408553766771, 0.02522345676426047, 0.023035302803220865, 0.021253975071444418, 0.02299421675913869, 0.022151207279476424, 0.02280317421388119, 0.022020146825734655, 0.019744288622618865, 0.021754962591507946, 0.02058648679832431, 0.019481258098625193, 0.018746506298904102, 0.02082187346794223, 0.019484504032284973, 0.018540539403487676, 0.018026220258518744, 0.018277281947350635, 0.019090669363150937, 0.018498932709917426, 0.016855266360646357, 0.017087985687076854, 0.01755107378292555, 0.0162497674808287, 0.016687223960023624, 0.01735119057555918, 0.016141063884457057, 0.015963702109648276, 0.01607858874766638, 0.015593683267140184, 0.017362511261907843, 0.015086121411217502, 0.014080422949388076, 0.01572396905130198, 0.01647015061964409, 0.015150130807850569, 0.014522404045821688, 0.015314155997271045, 0.014406249093742798, 0.014474769565128028, 0.014898723138633151, 0.014346211947822056, 0.013767596740370836, 0.012941224452929376, 0.01388948843457376, 0.013728754877591856, 0.012941669198511817, 0.013923830428598122, 0.01301820403204475, 0.013493596776626622, 0.014167216677105608, 0.012224345361983505, 0.013615870727791479, 0.014401413207968916, 0.013055011124228137, 0.013154573026321495, 0.013493608308438295, 0.012507462098944587, 0.012766335765792492, 0.013181844661488367, 0.013105310207699982, 0.014206890060682409, 0.01374277858861005, 0.012588148265660475, 0.013434472534368141, 0.013608296886800576, 0.012433568900153517, 0.012731193249246266, 0.012850580789437429]\n",
        "train_acc_list_cut = [40.9, 57.2025, 65.81, 71.35, 74.33, 76.565, 78.5275, 80.4, 81.7075, 82.5125, 83.585, 84.51, 85.1225, 85.4725, 85.9375, 86.5425, 87.3725, 87.685, 88.0775, 88.5075, 89.195, 88.7475, 89.465, 89.89, 90.23, 90.4575, 90.6025, 90.64, 91.0475, 91.385, 91.485, 91.8375, 91.855, 92.455, 92.2575, 92.455, 92.4025, 92.6625, 92.92, 93.085, 93.19, 93.35, 93.635, 93.515, 93.77, 93.805, 93.8825, 93.9425, 94.15, 94.27, 94.3925, 94.3325, 94.4625, 94.545, 94.6175, 94.6675, 94.66, 94.7825, 95.1875, 94.845, 94.9125, 95.1, 95.2425, 95.45, 95.28, 95.2525, 95.3375, 95.6875, 95.58, 95.74, 95.56, 95.7275, 95.7375, 95.7325, 95.96, 96.0225, 95.8975, 95.9725, 95.9975, 96.035, 95.935, 96.0, 96.1025, 96.3225, 96.3075, 96.0575, 96.32, 96.3125, 96.3075, 96.26, 96.575, 96.3125, 96.3875, 96.495, 96.4125, 96.585, 96.49, 96.455, 96.4925, 96.4725, 96.6375, 96.6125, 96.715, 96.775, 96.675, 96.7725, 96.7825, 96.7825, 96.775, 96.835, 96.9425, 96.72, 96.7375, 96.8475, 97.035, 97.0625, 96.845, 97.2175, 97.06, 97.1575, 97.055, 97.2325, 97.125, 97.2375, 97.315, 97.2125, 97.1925, 97.2775, 97.3325, 97.45, 97.25, 97.395, 97.23, 97.3725, 97.355, 97.2325, 97.4625, 97.4875, 97.4075, 97.545, 97.5525, 97.7425, 97.545, 97.6125, 97.505, 97.635, 97.7125, 97.72, 97.8225, 97.78, 97.6175, 97.5975, 97.86, 97.7825, 97.7725, 97.7525, 97.8075, 97.7875, 97.9775, 97.96, 98.1425, 97.9675, 98.0225, 97.88, 98.1, 98.2375, 97.9275, 97.995, 98.0375, 98.2175, 98.12, 98.26, 98.185, 98.2625, 98.275, 98.23, 98.285, 98.2475, 98.195, 98.36, 98.48, 98.4025, 98.315, 98.4725, 98.5175, 98.4075, 98.5175, 98.46, 98.535, 98.48, 98.6225, 98.6675, 98.72, 98.5975, 98.6575, 98.68, 98.525, 98.7025, 98.715, 98.78, 98.8, 98.845, 98.7975, 98.75, 98.835, 98.8225, 98.8975, 98.9, 98.925, 99.045, 98.995, 99.0175, 99.0025, 98.97, 98.9425, 99.075, 98.945, 98.9975, 99.07, 99.0825, 99.0525, 99.0625, 99.0075, 99.085, 99.2075, 99.19, 99.2475, 99.1775, 99.265, 99.1775, 99.1425, 99.255, 99.21, 99.215, 99.34, 99.245, 99.29, 99.28, 99.3, 99.3725, 99.3175, 99.3475, 99.4, 99.41, 99.2975, 99.4, 99.44, 99.45, 99.4325, 99.425, 99.4025, 99.4775, 99.44, 99.4275, 99.495, 99.4925, 99.4575, 99.485, 99.5075, 99.5, 99.5075, 99.425, 99.535, 99.5825, 99.4725, 99.5025, 99.5375, 99.545, 99.53, 99.58, 99.5525, 99.5425, 99.5775, 99.58, 99.635, 99.605, 99.59, 99.605, 99.6075, 99.65, 99.59, 99.58, 99.6125, 99.575, 99.5775, 99.59, 99.59, 99.585, 99.6075, 99.5975, 99.61, 99.635, 99.5625, 99.6, 99.6175, 99.5925, 99.6075, 99.6225, 99.5925, 99.615]\n",
        "test_loss_list_cut = [1.4791154242769073, 1.1658613644068754, 0.9872214477273482, 0.8314349047745331, 0.9944802085055581, 0.7577941908112055, 0.7259182379215579, 0.6789853301229356, 0.7040932129455518, 0.6710618870167793, 0.5932643009891992, 0.7190747709968422, 0.5915243693544895, 0.5677886009216309, 0.5548223072214972, 0.48938161738311187, 0.5485320487354375, 0.5049594323846358, 0.5024378439293632, 0.5253199633163742, 0.545006987037538, 0.49844539429568036, 0.4618456284456615, 0.47183522431156305, 0.45087659849396233, 0.5046738835075234, 0.4737275925618184, 0.517040255326259, 0.47144785334792316, 0.43287935023066365, 0.4623851370585116, 0.4191177538301371, 0.40598977762687055, 0.39758752437332007, 0.429098657414883, 0.49839673049842254, 0.4335803312214115, 0.4391304107406471, 0.4258851580604722, 0.4272765200349349, 0.41979367031326775, 0.3996517728023891, 0.39644326382799994, 0.4050762924966933, 0.37220431883123856, 0.41266086810751806, 0.4058349334363696, 0.40598829731911046, 0.4262330528301529, 0.4142030260608166, 0.35964680000951016, 0.3874821408262736, 0.3994222947313816, 0.4099999762411359, 0.48654232843767237, 0.40172827545600603, 0.3953292958344085, 0.4276507317642622, 0.37284702835958217, 0.3759660202113888, 0.39387373701681067, 0.3743562951118131, 0.3970871818593786, 0.41597020795828177, 0.3505105532800095, 0.3577760239190693, 0.3745738009486017, 0.36575221789034107, 0.3922628566056867, 0.3804579684628716, 0.3428110468991195, 0.38194986197012887, 0.35429555196550827, 0.361780740603616, 0.3707258914467655, 0.3518540302786646, 0.3650943885875654, 0.3918004062356828, 0.3596177604756778, 0.37847324338140365, 0.40541056722779817, 0.39005918272688417, 0.3564605245107337, 0.36085661019705517, 0.41434526443481445, 0.3446986880860751, 0.3807661754043796, 0.3560769367444364, 0.3401542149389846, 0.33212360739707947, 0.34799107928064804, 0.40249927549422543, 0.3623131317428396, 0.38307136523572705, 0.3425910604905479, 0.3611237068153635, 0.37625472587120684, 0.3761880048845388, 0.3633357567500465, 0.32559320141997516, 0.3548705181743525, 0.37774981407425073, 0.33313915548445305, 0.3662335353938839, 0.34161262233046036, 0.33970868606356125, 0.33961755310810066, 0.35591573598264137, 0.36459438993206505, 0.34952369747282586, 0.3646538981908484, 0.3669853182155875, 0.346222849586342, 0.3648104143293598, 0.35329970227012153, 0.3857896882521955, 0.3397268560491031, 0.3996561256390584, 0.344681443670128, 0.35413239726537393, 0.3316430355174632, 0.38716104249410993, 0.32942197028594683, 0.35217226400405544, 0.37594665285152723, 0.34076649818239335, 0.3449836048898818, 0.3587528810470919, 0.3401296163284326, 0.3270592946983591, 0.32152744599535493, 0.3485121142260636, 0.33158980649483355, 0.35384547361467455, 0.3337541961971718, 0.31837486522861674, 0.32609030859002586, 0.35300223284129856, 0.31153832630643363, 0.331745189390605, 0.31228505301324627, 0.32915962798685966, 0.307383563322357, 0.3250211654584619, 0.32271280979053885, 0.3312963891444327, 0.3233849070494688, 0.3239633294789097, 0.3277184535996823, 0.3104742332538472, 0.32017407990709135, 0.32736679232573207, 0.31899695456782473, 0.29834681513566, 0.3152913172033769, 0.3275818183452268, 0.33088456924203075, 0.3209677238630343, 0.32485942636864096, 0.3357626995708369, 0.3363142457755306, 0.3152533290506918, 0.3161260941171948, 0.3332944193595572, 0.3038376487697227, 0.34032498092591007, 0.31091189195838154, 0.31926076189626623, 0.31000442501110365, 0.3010354260855083, 0.30018055627617657, 0.3127445231510114, 0.30238260064698474, 0.3182603884724122, 0.31473101213385785, 0.29884613908921615, 0.31703367735011667, 0.2913209448886823, 0.2881702622280845, 0.31681924888604807, 0.29637292385855807, 0.2983416555614411, 0.29558402111258686, 0.29196765764227395, 0.3100640736048735, 0.31831720773177813, 0.29677640637264974, 0.30746520772764957, 0.2865563590503946, 0.30253688547807406, 0.28961686959749533, 0.29738174973032144, 0.29680030708071553, 0.29409151684634294, 0.2892140953601161, 0.3005977722851536, 0.2880074861872045, 0.27800113717211955, 0.2859659475993507, 0.2899065754270252, 0.28989569729642023, 0.29461942848902717, 0.27184448862754845, 0.2677759771482854, 0.2699777761214896, 0.3000830202540265, 0.2926526506301723, 0.2815253124395503, 0.2683991437094121, 0.2708052956982504, 0.2659032362737233, 0.29111536092396023, 0.27172746575331386, 0.2721299055633666, 0.27334422420097304, 0.27823255380874945, 0.2846702391022368, 0.26914313680763485, 0.2824036598299878, 0.26155152982926066, 0.28833567049307157, 0.2757935132595557, 0.28530820940114276, 0.26061007040965406, 0.2689024790932861, 0.2690693513502049, 0.2623352914859977, 0.26749783254499676, 0.2726406627629377, 0.2707079539570627, 0.2750299688947352, 0.2752766144237941, 0.26105580568502224, 0.2643242926348614, 0.2672247627302061, 0.27170711321921287, 0.28075962617427486, 0.25020905593528026, 0.24921881190583675, 0.26484523855055436, 0.2614167183637619, 0.27086676525164255, 0.2653467998474459, 0.25267240150442605, 0.2534334840653818, 0.26007933452536786, 0.2619169997640803, 0.2546714186857018, 0.2537678420732293, 0.27184398217669015, 0.24711663049610355, 0.26054487415129624, 0.2526520066246202, 0.2480248349565494, 0.24464330218638045, 0.2543100225208681, 0.2467851509587674, 0.2543815068806274, 0.2550846805112271, 0.24686935845809646, 0.25780645163753363, 0.24647484842357756, 0.2498779943849467, 0.24236657725104802, 0.25234464791756644, 0.23720906674861908, 0.2444623167378993, 0.24947976245533063, 0.24974906840656377, 0.24106483623574052, 0.24051216914306714, 0.24444433122496062, 0.240749250955974, 0.24741494844231424, 0.24492908486082585, 0.24239608881217015, 0.24886162715810764, 0.24737394035239763, 0.24470583080679556, 0.23975468323200563, 0.25973454749659647, 0.24880992423129988, 0.25671024592239644, 0.2394170629072793, 0.2508298989526833, 0.25579182813061946, 0.23148325047915494, 0.2473917234736153, 0.23667787901962858, 0.24764571838741062, 0.23395111053427564, 0.23682133947746664, 0.2453216223777095, 0.2395489558200293, 0.24663213140602352, 0.2379017510934721, 0.24195825148232375, 0.23623901610321638, 0.24765730121090443, 0.24364126761314236]\n",
        "test_acc_list_cut = [47.45, 60.0, 65.86, 71.15, 68.02, 74.04, 74.75, 76.5, 76.38, 77.35, 79.87, 75.79, 80.02, 80.57, 81.27, 83.4, 81.97, 82.87, 83.07, 82.72, 81.44, 83.43, 84.51, 84.41, 84.85, 83.96, 84.21, 82.61, 85.09, 85.42, 85.35, 86.23, 86.96, 86.92, 85.98, 84.21, 85.86, 85.85, 85.91, 86.93, 86.74, 86.97, 87.32, 87.19, 87.67, 86.55, 86.81, 87.37, 86.84, 87.3, 88.46, 87.92, 87.66, 87.19, 85.59, 87.67, 87.69, 86.81, 88.24, 88.24, 87.26, 88.01, 87.61, 87.88, 88.96, 88.76, 88.97, 88.92, 87.96, 88.36, 88.67, 87.88, 88.96, 88.67, 88.46, 88.99, 88.87, 88.21, 88.92, 88.53, 88.17, 88.26, 89.01, 88.57, 87.5, 89.68, 88.57, 88.93, 89.45, 89.58, 89.15, 88.23, 88.64, 88.35, 89.49, 89.34, 89.23, 88.47, 88.78, 89.82, 89.47, 89.04, 89.77, 88.77, 89.85, 89.69, 89.79, 89.18, 89.03, 89.57, 89.04, 89.3, 89.43, 89.17, 89.25, 88.69, 89.73, 88.08, 89.49, 89.59, 90.02, 88.41, 89.94, 89.77, 88.87, 89.91, 89.6, 89.29, 90.01, 90.18, 90.39, 89.52, 90.2, 89.82, 89.98, 90.55, 90.27, 89.84, 90.81, 90.16, 90.58, 90.17, 91.06, 90.49, 90.02, 90.36, 90.15, 90.41, 90.29, 91.05, 90.46, 90.37, 90.3, 91.15, 90.77, 90.49, 90.34, 90.44, 90.56, 90.45, 90.12, 90.36, 91.18, 90.31, 91.04, 90.24, 90.89, 90.56, 91.21, 91.14, 90.88, 90.8, 90.98, 90.69, 90.8, 91.24, 90.76, 91.31, 91.56, 90.68, 90.91, 91.29, 91.61, 91.43, 91.23, 90.98, 91.61, 91.17, 91.7, 91.2, 92.06, 91.39, 91.26, 91.56, 91.42, 91.42, 91.78, 91.94, 91.69, 91.44, 92.0, 91.36, 91.88, 92.07, 92.17, 91.33, 91.74, 91.8, 92.13, 92.09, 92.31, 91.55, 92.01, 92.26, 92.4, 91.8, 91.83, 92.3, 92.02, 92.56, 92.16, 92.24, 91.79, 92.59, 92.35, 92.7, 92.71, 92.45, 92.55, 92.34, 92.02, 92.56, 92.65, 92.35, 92.53, 92.23, 92.47, 92.65, 92.84, 92.75, 92.85, 92.58, 92.62, 92.95, 92.94, 92.61, 92.87, 92.93, 92.82, 92.37, 92.94, 92.82, 92.81, 93.03, 93.0, 92.85, 93.13, 92.88, 92.75, 93.02, 92.82, 93.14, 92.91, 93.07, 92.55, 93.2, 93.19, 93.02, 93.01, 93.03, 93.35, 93.0, 93.14, 92.89, 93.14, 93.2, 93.12, 93.15, 93.17, 93.22, 93.14, 93.1, 93.0, 93.72, 92.96, 92.94, 93.39, 92.96, 93.36, 93.22, 93.51, 93.28, 93.27, 93.3, 93.29, 93.18, 93.24, 93.64, 93.27, 93.29]\n",
        "train_loss_list_wd5e4 = [1.5027774480965952, 1.0276084239490497, 0.7975675012356938, 0.6686166104988549, 0.5772798815474343, 0.5116038408142309, 0.461431055594557, 0.4285996074493701, 0.3872021212459753, 0.36954540337998265, 0.33349453384122146, 0.3191225348760526, 0.29570154136362164, 0.2778870724736692, 0.26335706169041584, 0.2490621807809455, 0.23208605314786443, 0.22031858939522753, 0.20280856089279675, 0.1948807320465295, 0.18225760984058959, 0.17901248411058238, 0.16944760515000493, 0.15725475339034495, 0.15281064094255525, 0.14829571347552747, 0.1351967308396539, 0.1339439639982324, 0.12818257354747373, 0.1161808082328056, 0.11971007128612111, 0.11157218916728474, 0.11184166256969158, 0.1071621371736637, 0.09870970028396041, 0.09892762057221355, 0.0908012348063552, 0.08706318784933596, 0.08129636609492401, 0.0844175164608624, 0.07711375228608378, 0.0727171496318552, 0.07881306375439365, 0.06988748621207456, 0.06977271015187517, 0.07213704411785443, 0.06465452981178467, 0.059860886116259206, 0.06565443985164165, 0.055741984914904966, 0.05937281503273656, 0.054797398726851604, 0.04783525239057339, 0.057667528927778475, 0.0519976342906253, 0.05075258484479195, 0.0479780873997857, 0.051437813953047216, 0.044275228567897514, 0.05218542266202668, 0.047849885717677045, 0.04727344876809861, 0.04353566757275369, 0.05274154159564751, 0.03900954714669778, 0.038132579750217756, 0.0438798694252468, 0.04052391117319655, 0.0444194049285814, 0.03921739171041896, 0.04322019030456059, 0.03981477512677495, 0.04147126957232627, 0.03451747147622295, 0.03873391680645581, 0.03558630044438159, 0.03961444919863448, 0.03973888016243379, 0.03753592373844915, 0.03485113727642943, 0.036234460864811184, 0.03558692057875875, 0.03258687717030747, 0.03053688010224662, 0.03501797043614256, 0.02845712277083137, 0.03135093330991821, 0.03356501266074638, 0.03350056209170019, 0.03376623985473626, 0.0308699009333032, 0.02893257925968272, 0.027391545217150984, 0.0286959023021471, 0.030725791665781942, 0.026020749267517997, 0.03082717279829204, 0.02599965874775173, 0.03394242989184995, 0.03584211132750986, 0.032864981473753815, 0.03261832850271116, 0.03152383655345383, 0.028458409716550725, 0.0237263294655425, 0.02717581400335335, 0.028603503928362084, 0.02477863742285572, 0.026328867512603348, 0.02738554204796283, 0.024270188131818946, 0.02639033293170027, 0.028431153394275915, 0.0267944870994221, 0.022552848333112014, 0.027474652415665147, 0.022996818839450376, 0.02170721744931044, 0.024480172923168005, 0.025365150559651918, 0.02576148252154644, 0.024583259466499946, 0.026209578144188506, 0.022519710091368172, 0.022190634743682446, 0.017703767737207082, 0.019398964044396966, 0.02643031852926833, 0.02380849324953787, 0.02303797679733092, 0.015461873901173318, 0.016369462731820994, 0.023921750444984333, 0.024145581647188375, 0.024439221988923062, 0.021081448698151536, 0.019269814025595806, 0.01770727836546271, 0.015750449192457307, 0.018357921256341586, 0.016851160442084668, 0.016779405207331545, 0.01922887265860749, 0.01491048913590384, 0.017494613310685175, 0.020450935306655357, 0.019253915843045036, 0.01861391122789143, 0.017561238870908397, 0.01683490170342937, 0.012796001498417827, 0.016780197644567552, 0.014076976357370067, 0.0157260496032457, 0.016004126569547784, 0.017361198594353307, 0.01750588047760018, 0.014100992143118439, 0.012314881729229857, 0.008250951137283621, 0.013473853790245878, 0.010151084203766985, 0.009842516266005918, 0.010620841719269657, 0.009969146477769667, 0.011355855877097613, 0.007406337388096372, 0.008986727931512931, 0.009986997959159387, 0.009125781858848712, 0.007472109663621567, 0.008835015864224551, 0.004292508458737998, 0.005812553818874692, 0.00677099364538924, 0.0067322269050826946, 0.004625315853274061, 0.005215383676132158, 0.006344635733596671, 0.005187604376992669, 0.003839952308653345, 0.003965519353213271, 0.002669242659610467, 0.002117635275329502, 0.002775355486739308, 0.00265736014845546, 0.0022718249013076145, 0.0020042899910497447, 0.0016117119838441976, 0.0014573458997613063, 0.001228732236187917, 0.0012267742422409356, 0.0014500122589130586, 0.0014915024765362493, 0.0012808567260394986, 0.0011165388441382767, 0.0012906825464864531, 0.001357362313477245, 0.0012554813951131897, 0.0011106275753623928, 0.0011766462907799112, 0.0011652772541354281, 0.0014127067940387006, 0.0012243740158691145, 0.0011723753179798421, 0.0012344509843341149, 0.0012428820435963451, 0.0011997493186641259, 0.001336276902967451, 0.0011452813391029217, 0.0011089205116661378, 0.001165861148571673, 0.0012621701129313452, 0.0011624022613475902, 0.0012330106737020725, 0.0011809163343030425, 0.001101714661181235, 0.0011684441624340205, 0.0011685377142329615, 0.0012322912058129478, 0.0011872104088293787, 0.001143343054307119, 0.0011744529526597394, 0.001203257890125088, 0.00115107328663714, 0.0011390011661653273, 0.0012292457904782706, 0.0011242386569215443, 0.0011883258320072802, 0.0011454876748467097, 0.001156560716423066, 0.0011528850767014627, 0.001165562277152456, 0.0011236844305395365, 0.0011447015948914967, 0.0012182490931773268, 0.0011218772364628559, 0.001162500955319157, 0.0011594401151403047, 0.001198594366161587, 0.0011824657069370388, 0.001172395464897263, 0.0012272381267998333, 0.001153445542647173, 0.0012023703809961462, 0.0011459320135334262, 0.0012018852982485589, 0.0011949562754023809, 0.0011886567682230149, 0.0011700176551697043, 0.0011815206049539792, 0.0011973182879798947, 0.0012122210003242206, 0.0011632622707971392, 0.0011874910569734894, 0.0011854940763534828, 0.0011964635884451765, 0.0011862511147771734, 0.001194577806401617, 0.0012562638589407142, 0.0012219592303293534, 0.0011857493995159221, 0.0011942158280217204, 0.0011750017043601828, 0.001190489213080547, 0.0011951297786780082, 0.0011906062601321635, 0.0011730845402048442, 0.0012332784984939206, 0.0012139175384760664, 0.0011773592329021698, 0.0012054313082075394, 0.0011924652807851926, 0.0012579563214404942, 0.001199958441122926, 0.0012173140406632386, 0.0012009810876122083, 0.001210526931255806, 0.0011779946152689143, 0.0011719051533164427, 0.001181355180083432, 0.0011896568855580191, 0.0012324937115670345, 0.0011941396765940534, 0.0011866521274376386, 0.0011868935034494287, 0.0011929624456207093, 0.0012042453618922506, 0.001203572630105665, 0.001169494700063163, 0.0012129276695946893, 0.0012054597970452337, 0.0012071873559750402, 0.0011781047958621797, 0.0011885599540055584, 0.0011778895794518126, 0.0011938168243624079, 0.0011808463227607833, 0.0012280858834437763, 0.001201700613523027]\n",
        "train_acc_list_wd5e4 = [44.9675, 63.1875, 71.8725, 76.5, 79.885, 82.1125, 83.93, 85.2275, 86.5525, 87.015, 88.4975, 88.8775, 89.675, 90.22, 90.8925, 91.28, 91.83, 92.26, 92.8925, 93.18, 93.5725, 93.6975, 94.0725, 94.48, 94.7275, 94.8025, 95.185, 95.255, 95.445, 96.0025, 95.7275, 96.06, 96.1225, 96.2175, 96.5175, 96.4475, 96.815, 96.9275, 97.2, 97.03, 97.3525, 97.5325, 97.2775, 97.6, 97.6325, 97.4725, 97.8175, 97.9575, 97.6775, 98.13, 97.9825, 98.115, 98.465, 98.025, 98.2125, 98.315, 98.395, 98.265, 98.535, 98.2375, 98.3425, 98.4175, 98.59, 98.165, 98.775, 98.7575, 98.4825, 98.6875, 98.425, 98.76, 98.555, 98.6875, 98.565, 98.86, 98.715, 98.8725, 98.665, 98.645, 98.765, 98.8425, 98.845, 98.8475, 98.95, 99.0225, 98.8175, 99.0825, 98.97, 98.91, 98.885, 98.87, 98.975, 99.0475, 99.1425, 99.065, 99.0175, 99.1325, 98.9975, 99.205, 98.9175, 98.85, 98.9625, 98.9, 99.02, 99.0675, 99.295, 99.1675, 99.08, 99.225, 99.1425, 99.1, 99.2525, 99.155, 99.0975, 99.165, 99.3, 99.1275, 99.3225, 99.3125, 99.245, 99.1725, 99.1575, 99.2175, 99.1825, 99.32, 99.29, 99.48, 99.44, 99.17, 99.25, 99.32, 99.5675, 99.5, 99.275, 99.2075, 99.2025, 99.3425, 99.4, 99.5125, 99.5125, 99.495, 99.4975, 99.4925, 99.4125, 99.565, 99.465, 99.365, 99.4025, 99.4375, 99.445, 99.4825, 99.645, 99.4825, 99.62, 99.5275, 99.53, 99.43, 99.485, 99.61, 99.6675, 99.785, 99.5775, 99.705, 99.725, 99.6975, 99.69, 99.705, 99.8125, 99.765, 99.725, 99.77, 99.81, 99.785, 99.92, 99.8575, 99.8125, 99.82, 99.9025, 99.885, 99.84, 99.88, 99.935, 99.93, 99.965, 99.9775, 99.9475, 99.9625, 99.955, 99.98, 99.9875, 99.9925, 100.0, 99.995, 99.985, 99.9925, 99.9975, 100.0, 99.99, 99.9925, 99.9975, 99.9975, 99.995, 99.9925, 99.9925, 99.995, 99.9975, 99.9975, 99.9975, 99.9975, 99.99, 100.0, 100.0, 100.0, 99.9925, 100.0, 99.995, 100.0, 100.0, 99.9975, 99.9975, 99.995, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 99.9975, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 99.9975, 100.0, 99.9975, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
        "test_loss_list_wd5e4 = [1.2136726432208773, 0.9851384208172183, 0.8816822442827346, 0.9342003608051734, 0.7205261165582681, 0.7135021192363545, 0.6295640034766137, 0.6222952057289172, 0.5453219817409033, 0.6333636732041081, 0.5167707955535454, 0.5517654562298255, 0.4426909998247895, 0.41966562520099593, 0.5765619779689403, 0.43953118633620347, 0.3954082934916774, 0.399195172552821, 0.3910487050874324, 0.38879678803908674, 0.4299736600133437, 0.38120186121403415, 0.4164479757788815, 0.3728801941947092, 0.42476696719097184, 0.395566382551495, 0.39108878091166294, 0.390310141670553, 0.4194243029703068, 0.3851487408333187, 0.43473647101015983, 0.3949222198770016, 0.43027733530424817, 0.46044455280032337, 0.4314384918801392, 0.402132544525062, 0.4030761315098292, 0.4495672079958493, 0.39670435857923725, 0.3508086872251728, 0.43815861830982983, 0.4294034951849829, 0.44646918698202204, 0.4012679084192348, 0.3856019262648836, 0.38729274404954306, 0.41598545373240603, 0.4074918993666202, 0.3850251690873617, 0.36644049479237084, 0.4081271881166893, 0.3885994453596163, 0.3874774380952497, 0.38053672147702566, 0.46447082222262515, 0.3927165820628782, 0.38685785233974457, 0.3861157114181337, 0.37443527598169785, 0.3745674545629115, 0.42922606705864774, 0.3947119893906992, 0.4066375641128685, 0.4015842970413498, 0.36048868430566183, 0.35823154091080534, 0.36034491677072983, 0.3813755610321142, 0.35023685667333726, 0.3729325658347033, 0.3793654409767706, 0.3758758772400361, 0.386005234491976, 0.3741716125720664, 0.38768857264820533, 0.4233778007800066, 0.3564016471935224, 0.37897610098500795, 0.40855275943309444, 0.37117944054211244, 0.4877219747138929, 0.32491220892230166, 0.3434305104273784, 0.3840078609653666, 0.3549162511584125, 0.3900517902419537, 0.4008802462227737, 0.37963688882845864, 0.41286554225260697, 0.3738790906116932, 0.39308812431519546, 0.40336327028425434, 0.3903807069681868, 0.3731421724527697, 0.36139429652992683, 0.36114918450011485, 0.3433755166545699, 0.33576442904864684, 0.38036759027951883, 0.389648184368882, 0.3580984584515608, 0.38816530191445653, 0.35682666716696343, 0.3467950366343124, 0.3625389872283875, 0.35996044842125496, 0.3454359777366059, 0.3356767878690852, 0.37700900737243365, 0.35454515490350846, 0.33726465683194656, 0.37725700768111625, 0.37593507747861404, 0.3295244945189621, 0.3762158106399488, 0.38346752543238144, 0.3287717970677569, 0.3532322215128548, 0.3622320907402642, 0.3615872695853439, 0.3733052865047998, 0.33632627308745927, 0.3659961585757099, 0.34503319652020176, 0.3413062731299219, 0.3364775993778736, 0.3670855132462103, 0.34390894257569615, 0.35701036585282675, 0.33068783996225914, 0.3194702649606934, 0.35311298242098166, 0.35698866938488394, 0.35237887407405466, 0.38091013099573834, 0.3747274860551086, 0.3419938689168495, 0.3380954069427297, 0.36431473842527295, 0.37605810872738876, 0.3960524099537089, 0.33081852295730685, 0.36062877370586877, 0.3480613913528527, 0.37270384611962715, 0.34597224412085137, 0.3434583605090274, 0.34766560349660586, 0.3527573428199261, 0.3291342975218085, 0.3357704000759728, 0.3537662517798098, 0.35035382720488534, 0.3609619809291031, 0.36449365525306027, 0.38716287873213806, 0.36449907418293287, 0.3543851946162272, 0.3296925408553474, 0.33976211521444444, 0.36568557687952546, 0.3350432302944268, 0.35040959508358677, 0.33734601575739775, 0.35390251950372625, 0.325316196189651, 0.3150268595995782, 0.34807058489775355, 0.34181052182294147, 0.30128647698254524, 0.3265819955098478, 0.3273799891901922, 0.3255926795964, 0.3264697471374198, 0.31853930699297145, 0.3249078211905081, 0.30419793300613573, 0.31979161029375053, 0.33804947467921653, 0.30469325590360014, 0.32201941085012653, 0.32308190551739707, 0.3179449903531165, 0.29340258193544194, 0.30763082790978347, 0.3099403906660744, 0.3032598841133751, 0.30206969182325316, 0.28878650833157044, 0.298862298172486, 0.2903934564394287, 0.2809895239298857, 0.2981501730936992, 0.2927116846925096, 0.2824138938626157, 0.28358515229406234, 0.2825708462844921, 0.29461616579490374, 0.2836763092988654, 0.28226735923863666, 0.2746177826878391, 0.27155414657502236, 0.2885775946174996, 0.27818745195488387, 0.27747765494675575, 0.28425973069064225, 0.27348717754777474, 0.27310062048933176, 0.2786915644626074, 0.2682647790429713, 0.27626944993492925, 0.28479830703780623, 0.27027927178748046, 0.2733822947344448, 0.2701373880233946, 0.2778020071549506, 0.2666197860542732, 0.26562596329405336, 0.2745143292070944, 0.27233143329997606, 0.26666624231051794, 0.2696480587407758, 0.2745003360736219, 0.27249939272863954, 0.27024260172738307, 0.2686373513144783, 0.27092717569085617, 0.2656354494392872, 0.26311222001721585, 0.26525769873133187, 0.26492436928085134, 0.2699424793731563, 0.2719281587419631, 0.27291557249389115, 0.2715623058095763, 0.26247314769255964, 0.25891310165200054, 0.27840734131728545, 0.27305792734215534, 0.2638986749645275, 0.26656815414375895, 0.2615214324544502, 0.2578285336494446, 0.27850422602665575, 0.2716222282829164, 0.2712610442238518, 0.2711411705594274, 0.26657556308598457, 0.265236472498767, 0.2611617881663238, 0.27041642509306535, 0.25506745703235456, 0.2576812288806408, 0.2628553329577929, 0.2742771508195732, 0.26676151354478883, 0.26692218276896057, 0.27390673850910574, 0.2696215535360801, 0.2623596463022353, 0.2678805201585534, 0.2617303827140905, 0.2673761573254685, 0.2679485789016832, 0.26076228487529335, 0.26153764751138564, 0.2555880120283441, 0.2642273437939113, 0.25984900718248344, 0.26822723297378687, 0.2618812537645992, 0.26794138192376005, 0.2702119716832155, 0.27756816747633717, 0.2612695039450368, 0.2614873691072947, 0.2663590583148636, 0.25799272407459306, 0.2680966036417816, 0.27000968761836425, 0.2693696271015119, 0.25710588410685337, 0.26840231454447855, 0.2647050721924516, 0.2639507615019249, 0.27469288246541085, 0.2502636852898175, 0.26998129741677757, 0.2636978205623506, 0.2510043778279914, 0.26851412955718706, 0.2697306358833102, 0.26649072681423985, 0.2556019111336032, 0.2660581706256806, 0.27122338370809074, 0.2661679719067827, 0.2716638084170939, 0.2666107994280284, 0.25817724770005746]\n",
        "test_acc_list_wd5e4 = [57.49, 65.32, 68.85, 69.25, 75.11, 76.47, 79.04, 78.91, 81.38, 79.78, 83.31, 82.27, 85.39, 86.04, 81.19, 85.57, 86.87, 86.8, 87.39, 87.89, 86.55, 87.98, 87.07, 88.42, 87.53, 87.54, 88.73, 88.01, 87.66, 88.41, 87.62, 88.47, 87.7, 87.31, 87.93, 88.33, 88.35, 87.92, 88.93, 89.82, 88.12, 87.97, 88.15, 89.02, 89.51, 89.51, 88.97, 88.91, 89.73, 90.22, 89.19, 89.69, 89.73, 89.78, 87.74, 89.45, 89.75, 89.88, 89.95, 90.06, 88.65, 89.62, 89.26, 89.31, 90.25, 90.69, 90.2, 90.24, 90.68, 90.65, 90.07, 89.97, 90.12, 90.12, 89.86, 88.49, 90.59, 90.09, 89.83, 90.61, 87.36, 91.49, 90.93, 90.05, 90.69, 89.85, 89.84, 90.3, 89.98, 90.2, 90.1, 89.68, 89.81, 90.13, 90.46, 90.27, 91.08, 91.19, 90.42, 89.93, 90.76, 90.14, 90.89, 90.66, 90.76, 91.03, 91.41, 90.93, 90.67, 90.6, 91.03, 90.24, 90.46, 91.57, 90.73, 90.03, 91.58, 90.88, 90.92, 90.68, 90.39, 91.06, 90.78, 91.26, 91.24, 91.34, 90.73, 90.8, 90.85, 91.48, 91.91, 91.43, 90.71, 90.95, 90.31, 90.52, 91.39, 91.55, 90.75, 90.81, 90.03, 91.94, 90.96, 91.39, 90.87, 91.48, 91.35, 91.49, 91.32, 91.69, 92.11, 91.03, 91.27, 91.16, 91.05, 90.7, 91.13, 91.46, 91.82, 91.81, 91.49, 91.99, 91.79, 91.73, 91.2, 91.93, 92.24, 91.43, 91.47, 92.96, 92.19, 92.09, 92.48, 92.02, 92.34, 92.2, 92.68, 92.39, 91.92, 92.75, 92.45, 92.44, 92.63, 93.15, 92.81, 92.81, 92.69, 92.93, 93.25, 92.81, 93.06, 93.3, 93.25, 93.18, 93.24, 93.27, 93.09, 92.97, 93.34, 93.48, 93.5, 93.69, 92.96, 93.63, 93.47, 93.2, 93.34, 93.33, 93.45, 93.63, 93.43, 93.45, 93.43, 93.28, 93.34, 93.46, 93.25, 93.5, 93.43, 93.59, 93.52, 93.6, 93.48, 93.48, 93.64, 93.39, 93.43, 93.53, 93.86, 93.52, 93.62, 93.52, 93.53, 93.39, 93.49, 93.6, 93.69, 93.23, 93.55, 93.52, 93.32, 93.61, 93.58, 93.53, 93.55, 93.62, 93.58, 93.59, 93.54, 93.48, 93.56, 93.71, 93.59, 93.5, 93.49, 93.58, 93.58, 93.58, 93.57, 93.58, 93.4, 93.52, 93.66, 93.54, 93.58, 93.84, 93.78, 93.32, 93.76, 93.65, 93.72, 93.33, 93.67, 93.51, 93.87, 93.69, 93.5, 93.81, 93.66, 93.74, 93.39, 93.78, 93.7, 93.6, 93.74, 93.79, 93.85, 93.54, 93.78, 93.83, 93.51, 93.47, 93.65, 93.63, 93.65, 93.52, 93.58, 93.48, 93.61, 93.69]\n",
        "train_loss_list_wd1e2 = [1.5167798984545868, 1.027339467034934, 0.8097865730047987, 0.719615878769384, 0.658255658305872, 0.615522557935014, 0.5982994875207115, 0.5846719085789336, 0.5773727887164289, 0.5627464520664641, 0.5591158762145728, 0.5449530482292175, 0.5340716247550976, 0.5340199959925569, 0.5181504983109788, 0.5176402507498622, 0.5080099205810803, 0.49660853408396055, 0.49530376631992695, 0.4915612660848295, 0.4967052273856946, 0.49324704406741327, 0.49331052558490646, 0.48590693801355817, 0.4841031406443721, 0.48545389605787237, 0.4772332070734554, 0.47386771064406386, 0.47569251241394506, 0.4783006565639386, 0.4702882940967243, 0.47064500009289945, 0.47294196457908555, 0.45738911800110305, 0.4716209914928046, 0.4719133836011917, 0.4696217343068351, 0.45638318650257853, 0.46723246879090136, 0.4585420537870913, 0.45578641851489154, 0.465418814565427, 0.4614397684415689, 0.45631032715590236, 0.4486660618370714, 0.4534833081804525, 0.45102608156280394, 0.4530490816782077, 0.45389505011585957, 0.45014332449093414, 0.4494515047096216, 0.450234164349949, 0.4440193151037533, 0.44662567982658413, 0.44600409516892114, 0.4423091958125178, 0.44665925180950106, 0.4360342098120302, 0.43951448350668715, 0.44023644362394804, 0.4460902175964258, 0.43867638192999475, 0.4361597361465612, 0.433228444367552, 0.43528716051921296, 0.4372344275061696, 0.4323961653838904, 0.43725441934201664, 0.43545952991555675, 0.42910580998792436, 0.43043560903674116, 0.43121091731059286, 0.42305412998024267, 0.42636741055086397, 0.4268225592831834, 0.42100020314748293, 0.41761867097391486, 0.4185335077702428, 0.42368483900452575, 0.41616832524442826, 0.41972924578494536, 0.410687822789049, 0.4157286212562372, 0.4089399065358189, 0.40666280982022085, 0.4132289454198112, 0.40205181121064454, 0.40999404700419395, 0.4086824649819932, 0.40080371218177074, 0.4015931721788626, 0.4063145552580349, 0.4066784158110999, 0.40118428522024674, 0.4009731699483463, 0.3969333416547257, 0.39313930387314133, 0.39997971186432213, 0.3931039036653293, 0.38969586989559685, 0.3916486538351534, 0.38877204070076016, 0.38579398788773595, 0.3810211131557489, 0.3882365185803118, 0.38886873219340756, 0.3846093181984874, 0.38404670505287547, 0.3830401858392234, 0.3767934866701833, 0.37473060204959907, 0.37977902538860187, 0.37377259620843223, 0.38002860750824496, 0.36324887853651383, 0.3618837351235338, 0.36737791051308566, 0.3669434030311176, 0.3633396982099302, 0.36406448107367506, 0.3577735069365547, 0.353955817536805, 0.3599519186888259, 0.35612359438270047, 0.35251178142552175, 0.35027061312343366, 0.3470285452497653, 0.3470985202934034, 0.3467607243468586, 0.33784743396047584, 0.34203178333207823, 0.3446982724312395, 0.3408664211678429, 0.3342146307420426, 0.33859410405920715, 0.3276973366737366, 0.33356578295794537, 0.33072001608415913, 0.32800227998735043, 0.32374877627855675, 0.32380120865643597, 0.323726001829385, 0.3161135405397263, 0.32177544154298193, 0.3120566315639514, 0.31642273844431, 0.308783637401395, 0.32058241168340557, 0.31076529688728505, 0.3038082039489533, 0.2967669633678354, 0.30553271728582654, 0.29508178397870294, 0.2989721709070876, 0.2943779766654816, 0.29553699583862536, 0.28911615298769344, 0.29165190915330147, 0.29362286717746966, 0.28382395979124136, 0.2868227225999101, 0.2803722090138414, 0.27424085373505236, 0.28322451868758036, 0.2771561364777172, 0.2667812052816629, 0.2687817446578044, 0.26123926229179856, 0.2610385168474703, 0.2666228299799819, 0.2595565085784315, 0.25669008474380445, 0.2462938559798006, 0.2528521096982514, 0.24889454671654837, 0.2559791601504, 0.24216721735347194, 0.2481234513056545, 0.2393635288356973, 0.23972950325891995, 0.23211169509461133, 0.23631990731905061, 0.2304189305621595, 0.22999336732366976, 0.2226063944994451, 0.22054641979475753, 0.21703065643771388, 0.21686043295140464, 0.21233341681024137, 0.21059229460577614, 0.20723726490911204, 0.20685803997345245, 0.20887376510868438, 0.1978470858531638, 0.2038871747569535, 0.19181662209974693, 0.19145933929057166, 0.1935519773167924, 0.19051632768334673, 0.18543817138614746, 0.17970925828995415, 0.18026368427105224, 0.17701446067410917, 0.17569915164773836, 0.16774458888049323, 0.17213442190862693, 0.1631373402100211, 0.16456650209407836, 0.16205769175062545, 0.1562980950569002, 0.1515140765891098, 0.15663365386545466, 0.15088544290857955, 0.14032981417382867, 0.14692522343783715, 0.14212213777981628, 0.1392525550894463, 0.13808176975947217, 0.12566476896071968, 0.13361202100642955, 0.1256047902753749, 0.1272383151462855, 0.11795103549957275, 0.11710383291966237, 0.11266100632782561, 0.11401164303191554, 0.106656764119197, 0.10556819721961173, 0.10439886295566925, 0.10726791070387386, 0.09460471522884246, 0.0962810536590628, 0.09549028507341592, 0.09219917137259112, 0.0865226152379768, 0.08575040581651008, 0.08219950793745419, 0.07716210633992387, 0.07188480715163219, 0.08065827630650692, 0.06764008765355847, 0.06986326664186324, 0.07430443038551, 0.06353453783609997, 0.06618601354523398, 0.057043209338721375, 0.051298089039782745, 0.054104091200870445, 0.04759680214352882, 0.053709484345187394, 0.046893905104396824, 0.043303748533224905, 0.04337597922228586, 0.0364828174844527, 0.035764870218956434, 0.03372384082918731, 0.03319300545718723, 0.03330101266193885, 0.03186584125978116, 0.027693336871199715, 0.026877068286648573, 0.026169767811561166, 0.0256406356053897, 0.025631253985456005, 0.024362976106401448, 0.023579948727553264, 0.02450076937556457, 0.023537636362611296, 0.02355496631786465, 0.023737981790504136, 0.022986546093330215, 0.023408689955695748, 0.023427624023332, 0.02295914088813261, 0.023163542187156768, 0.023356065636102003, 0.023212027953217584, 0.02309029637434231, 0.022946245367296586, 0.022692412935411587, 0.02339219701842378, 0.023122945366004784, 0.023124087244843522, 0.02343619195893169, 0.022982527528660365, 0.023213101919895163, 0.02317301971248735, 0.022974888207956245, 0.023228626388806503, 0.02310882277263049, 0.023166305644396014, 0.022929255026407518, 0.02320044601377778, 0.02337465347787633, 0.02341003445224069, 0.022785558921698566, 0.023203995150213423, 0.023422795285384494, 0.023028960588355414, 0.023342699217148863]\n",
        "train_acc_list_wd1e2 = [44.33, 63.2525, 71.6725, 75.2775, 77.445, 79.1175, 80.16, 80.8725, 80.74, 81.4475, 81.53, 82.1375, 82.65, 82.6, 82.96, 83.1775, 83.415, 83.7225, 83.915, 84.0275, 83.7325, 83.8675, 83.855, 84.1725, 84.09, 84.1675, 84.56, 84.5075, 84.6425, 84.4775, 84.74, 84.7025, 84.6975, 85.115, 84.7575, 84.705, 84.82, 85.3025, 84.7525, 85.0175, 85.3575, 84.9425, 85.02, 85.27, 85.5525, 85.28, 85.3825, 85.275, 85.2875, 85.52, 85.4475, 85.18, 85.4925, 85.525, 85.6125, 85.75, 85.4725, 85.7925, 85.8875, 85.5825, 85.7075, 85.8275, 86.0, 86.11, 85.95, 85.9425, 85.93, 85.89, 85.92, 86.29, 86.0525, 86.085, 86.41, 86.3275, 86.2675, 86.5075, 86.5475, 86.4525, 86.245, 86.705, 86.37, 86.7625, 86.705, 86.8875, 86.9225, 86.7075, 87.0625, 86.7475, 87.0175, 87.1175, 87.105, 86.8725, 87.09, 87.08, 87.1425, 87.405, 87.4725, 87.09, 87.4975, 87.53, 87.6025, 87.6175, 87.61, 87.685, 87.5425, 87.6175, 87.635, 87.8375, 87.78, 87.9375, 88.0275, 87.7925, 88.2425, 87.6675, 88.5325, 88.5275, 88.3, 88.4075, 88.42, 88.435, 88.6525, 88.795, 88.54, 88.7475, 88.7525, 88.77, 88.8375, 88.9225, 88.9325, 89.41, 89.1775, 89.045, 89.1575, 89.53, 89.3225, 89.855, 89.5225, 89.5275, 89.67, 89.66, 89.7775, 89.81, 90.0825, 89.9125, 90.125, 90.0025, 90.1125, 89.94, 90.265, 90.4675, 90.795, 90.4, 90.8425, 90.7075, 90.79, 90.6825, 91.02, 91.01, 90.935, 91.225, 90.995, 91.265, 91.47, 91.1175, 91.3675, 91.805, 91.6375, 91.945, 92.08, 91.76, 91.995, 92.1175, 92.48, 92.1875, 92.3625, 92.13, 92.595, 92.445, 92.71, 92.67, 93.005, 92.7375, 92.9675, 93.0725, 93.2, 93.335, 93.5175, 93.475, 93.62, 93.7325, 93.7275, 93.7925, 93.66, 94.175, 93.9525, 94.44, 94.3725, 94.22, 94.4575, 94.51, 94.8425, 94.695, 94.77, 94.87, 95.23, 94.9475, 95.2325, 95.23, 95.28, 95.5575, 95.725, 95.595, 95.7125, 96.0975, 95.9275, 95.985, 96.155, 96.23, 96.7125, 96.4, 96.5725, 96.5825, 96.8275, 96.9275, 97.0525, 96.9475, 97.2575, 97.295, 97.4575, 97.2975, 97.66, 97.6175, 97.5925, 97.81, 98.01, 97.9825, 98.08, 98.2975, 98.5225, 98.105, 98.595, 98.47, 98.3475, 98.6925, 98.59, 99.0025, 99.1525, 99.095, 99.27, 99.0025, 99.2475, 99.4225, 99.43, 99.6275, 99.635, 99.6975, 99.715, 99.7175, 99.7575, 99.885, 99.895, 99.9175, 99.925, 99.9225, 99.9675, 99.99, 99.955, 99.9825, 99.985, 99.98, 99.9875, 99.985, 99.9875, 99.995, 99.9925, 99.995, 99.9975, 99.9975, 99.9975, 99.9975, 99.995, 99.9975, 99.9975, 99.9975, 99.9925, 99.995, 99.995, 99.9975, 99.9925, 99.9975, 99.9975, 99.9925, 99.9925, 99.9975, 100.0, 99.99, 99.9975, 99.9975, 99.9925, 99.995]\n",
        "test_loss_list_wd1e2 = [1.3725743022146104, 1.1929742784439763, 0.9703254548809196, 0.8823626479016075, 0.7912342133401316, 0.7744240081762965, 0.9433422028263913, 0.698423402218879, 0.6534588306765013, 0.8720321919344649, 0.9694693926014478, 0.7136559109144573, 1.0214628554597687, 0.8677132740805421, 0.7085553053059156, 0.6858387778076944, 0.8705045671402654, 0.7853590740433222, 0.6123643494859526, 0.8236432565918451, 0.639409672987612, 0.7271271928956237, 0.6982333724257312, 0.8511822268932681, 0.8309068830707406, 0.8227003050755851, 0.7763343317599236, 1.2531145765811582, 0.7305624854715564, 0.7311744048625608, 0.9035976242415512, 0.6800222517568854, 0.5853997695295117, 0.7430345084093795, 0.8785871493665478, 1.108451329454591, 0.6588784128050261, 0.7044716858411137, 0.6049494954604137, 0.6143702955185613, 0.7461452868920339, 0.6528882425797137, 0.8694826870024959, 0.5893921456005, 0.6651792013192479, 0.6350515420678295, 0.6344684543488901, 0.6994746138777914, 0.5725251994555509, 0.6624086785920059, 0.9637637417527694, 0.8048486015464686, 0.6040883234030083, 0.7363305929340894, 0.6205344230313844, 0.6153643851793265, 0.7306718335875982, 0.6487590738489658, 0.7805856805813464, 0.6067438793333271, 0.6581050104732755, 0.6229219368741482, 0.5560055741026432, 0.6660232819333861, 0.7725009058095231, 0.6505244955231871, 0.5968554544297955, 0.7541794965538797, 0.6514349609990663, 0.7917481374137009, 0.6845550778545911, 0.7952898657774623, 0.6756763978849484, 0.7763081943687005, 0.8057050523878653, 0.7995553084566623, 0.6522585726991484, 0.5840945632397374, 0.6991102891632274, 0.6438415571104122, 0.5860964950126938, 0.635802041126203, 0.9848449245283876, 0.7002730075317093, 0.7162983300565164, 0.637317259100419, 1.0616072612472727, 0.5819362533997886, 0.6480541802659819, 0.7091145688974405, 0.532154082497464, 0.7218736040441296, 1.2941748115080822, 0.5242264851739135, 0.5975038005581385, 0.5866859227041655, 0.7909847196144394, 0.5758454048935371, 0.613415573971181, 0.6326792579662951, 0.6269164990775192, 0.5572914639605752, 0.5726913866362994, 0.6187993967080418, 0.6928256652023219, 0.5965810301183145, 0.6376756267457069, 0.5003311121011083, 0.5492256042323534, 0.601341659509683, 0.68496092286291, 0.5872152938118463, 0.5926056911673727, 0.5051948205579685, 0.5469717281528667, 0.7418177361729779, 0.7266407209106639, 0.7612239318557933, 0.4831244179719611, 0.6568866824801964, 0.7060362685330307, 0.693530101564866, 0.5567308860489085, 0.7726584718197207, 0.7781753985187675, 0.761501886422121, 0.6479162256928939, 0.7326241222363484, 0.5910413627383075, 0.7026853682119635, 0.7172112155564224, 0.6026144582259504, 0.5826918271523488, 0.5626278546037553, 0.49854521132722684, 0.6062672617314737, 0.5234693847125089, 0.5272691702540917, 0.6541247133967243, 0.42634944221641446, 0.638887640041641, 0.49163430740561664, 0.8650434718856329, 0.6591806264617776, 0.5614948665039449, 0.5188067898720126, 0.583303042982198, 0.5776442757135705, 0.7945110775247405, 0.4383143160162093, 0.5127309507961515, 0.48725567663772196, 0.5552749682830859, 0.5127951763098753, 0.5552909845792795, 0.6396092316017875, 0.6915487266039546, 0.5918025759202016, 0.6109406363360489, 0.49616090745865543, 0.4317665135935892, 0.5233679349663891, 0.46711384986020343, 0.5448861080634443, 0.705641579401644, 0.48266834171512457, 0.4491133007067668, 0.6736038417755803, 0.41231273925757106, 0.4804556973372834, 0.49244492793384986, 0.5156307778780973, 0.5698083575013317, 0.4579203664502011, 0.5245823471606532, 0.4540955895864511, 0.4462763463394551, 0.47661840538435346, 0.3877083959081505, 0.5359865875938271, 0.47837658622596835, 0.5976610659044, 0.4057789820281765, 0.4568231581132623, 0.5033613733852966, 0.4880643072007578, 0.421935570013674, 0.3864406771674941, 0.3924215535951566, 0.4215288765822785, 0.3856588805778117, 0.39591050393219235, 0.3960021594657174, 0.4162447214881076, 0.4317766793921024, 0.43716210089152374, 0.3752851269290417, 0.3684254226428044, 0.4046018751738947, 0.4570601512736912, 0.42681945275656785, 0.3557315610254867, 0.38338644142392314, 0.45626604632486273, 0.48571689672107937, 0.4053226277420792, 0.38042039244989806, 0.40194732573213454, 0.3663549528846258, 0.40040300088592723, 0.33492799480504626, 0.34661694832994966, 0.3733026383421089, 0.31432308200039444, 0.35983454105974755, 0.3449418092075783, 0.38464838134337076, 0.34258481332018403, 0.3416274746384802, 0.3719708046203927, 0.33954131207134153, 0.3303550594969641, 0.3256681533176688, 0.3663260734911206, 0.305223887479758, 0.3600897785229019, 0.359975767286518, 0.3134645165144643, 0.31159543180013005, 0.2859196728920635, 0.3370714645031132, 0.30697164241271685, 0.3175534585231467, 0.31721039870871776, 0.3941425471743451, 0.34323022165630435, 0.2986761056169679, 0.2926558808812612, 0.298173848963991, 0.29151669446426104, 0.3160338721509221, 0.2907146306920655, 0.29263262648748445, 0.27918298995193047, 0.2787177439166021, 0.27893129094869273, 0.27565936725350876, 0.2982580420337146, 0.32113587752550465, 0.28794968552604505, 0.2873940949764433, 0.2720843562031094, 0.255550967647305, 0.24216599573817435, 0.2502805740584301, 0.2626812101542195, 0.2440197715842271, 0.24417917901956582, 0.2448149986470802, 0.2319961440148233, 0.22655500405574147, 0.2284358123058005, 0.2258316146422036, 0.22443584173540526, 0.21244820348824126, 0.21419980961687957, 0.20566311539917054, 0.20899182690095297, 0.20759060931733891, 0.20997956381002558, 0.2037774644131902, 0.20164045950845827, 0.20583347683843178, 0.209160113353518, 0.2061407360000701, 0.1988746996827518, 0.2016011231307742, 0.20003076869098446, 0.20136920108070858, 0.20208889264849167, 0.19937377427763578, 0.20227546367464186, 0.19644954659139055, 0.2079336524386949, 0.19830586704649503, 0.2019951339763931, 0.2011260839202736, 0.1998056554341618, 0.20407455923813808, 0.20218830621695216, 0.20711066649307178, 0.20228053129549267, 0.1982952310126039, 0.1989742641018916, 0.20098775002775313, 0.19563924860727938, 0.20572516359860385, 0.19656268205446534, 0.19923276595677, 0.1936172448287282]\n",
        "test_acc_list_wd1e2 = [52.13, 58.0, 66.47, 68.48, 71.93, 73.38, 68.66, 76.35, 78.12, 70.27, 67.31, 76.04, 66.26, 71.09, 76.74, 77.08, 70.56, 74.21, 79.8, 73.72, 78.4, 76.32, 76.58, 71.8, 72.87, 73.02, 74.38, 58.91, 74.56, 75.75, 69.39, 78.21, 80.51, 75.74, 71.26, 65.74, 77.96, 76.81, 80.1, 79.77, 74.87, 77.76, 72.43, 80.66, 78.24, 79.29, 79.0, 77.18, 81.23, 77.41, 70.82, 74.14, 79.96, 75.72, 79.75, 80.63, 76.93, 78.66, 73.71, 79.9, 78.5, 79.77, 82.0, 78.63, 74.67, 77.94, 80.15, 75.57, 78.33, 74.1, 77.43, 74.34, 78.18, 75.35, 74.2, 73.88, 78.85, 80.44, 76.92, 78.07, 80.31, 78.92, 68.42, 77.39, 76.56, 79.35, 67.33, 80.82, 78.55, 77.35, 82.3, 77.25, 61.02, 82.77, 79.87, 81.04, 73.94, 81.59, 79.69, 78.21, 79.9, 81.84, 81.13, 80.0, 77.16, 80.57, 79.35, 83.74, 81.48, 80.41, 78.15, 80.5, 80.9, 83.64, 82.31, 75.42, 76.71, 75.99, 84.67, 79.01, 76.87, 77.9, 81.65, 74.49, 73.92, 75.98, 78.22, 76.52, 80.6, 77.42, 76.86, 80.4, 80.66, 81.63, 84.08, 80.38, 83.18, 82.73, 78.42, 86.07, 78.92, 84.8, 74.23, 78.45, 82.33, 83.06, 80.78, 81.68, 75.94, 85.72, 83.08, 84.02, 81.86, 83.38, 81.98, 79.64, 79.1, 80.78, 80.36, 83.85, 86.0, 82.75, 85.08, 82.98, 77.55, 84.19, 85.7, 77.95, 86.42, 84.35, 83.96, 82.9, 81.53, 85.4, 83.22, 85.75, 85.89, 84.67, 87.57, 83.16, 85.08, 81.54, 87.43, 85.54, 83.96, 84.48, 86.54, 87.69, 87.78, 86.26, 87.73, 87.31, 87.28, 86.94, 86.55, 86.06, 88.29, 88.32, 87.19, 85.53, 86.85, 89.14, 88.15, 85.86, 84.75, 87.47, 88.24, 87.6, 88.28, 87.78, 89.53, 89.36, 88.74, 90.36, 88.87, 89.5, 87.99, 89.19, 89.71, 88.6, 89.51, 90.06, 90.36, 88.83, 90.54, 89.04, 89.2, 90.63, 90.62, 91.59, 90.18, 90.87, 90.09, 90.93, 88.63, 89.71, 91.42, 91.72, 91.15, 91.61, 90.94, 91.63, 91.49, 92.14, 92.34, 92.5, 92.22, 91.41, 90.91, 92.27, 92.05, 92.77, 93.17, 93.48, 93.25, 92.79, 93.44, 93.49, 93.54, 93.83, 93.98, 93.84, 94.0, 94.24, 94.31, 94.42, 94.67, 94.34, 94.28, 94.61, 94.58, 94.66, 94.53, 94.5, 94.65, 94.62, 94.71, 94.73, 94.69, 94.61, 94.78, 94.59, 94.83, 94.68, 94.68, 94.76, 94.76, 94.77, 94.61, 94.75, 94.64, 94.51, 94.78, 94.81, 94.72, 94.81, 94.72, 94.92, 94.66, 94.96]\n",
        "train_loss_list_300const = [1.5072595094339536, 1.0231038059670323, 0.7837633473423723, 0.6642771163306678, 0.5742667439265754, 0.5122711343315843, 0.4693711296723673, 0.42363618176204326, 0.38468483099922207, 0.3563146945386649, 0.32989333312922775, 0.3088601354402475, 0.28732080238695723, 0.2773427833050204, 0.255741254994854, 0.2355493075502947, 0.22032112675829055, 0.20569888635660513, 0.19303267375348857, 0.18455393645710078, 0.1760997775430306, 0.1610161360388937, 0.15303532645915643, 0.14379756043132502, 0.13295495837998275, 0.12522198601414603, 0.12198948780425821, 0.10830174501949606, 0.1100872532187845, 0.1019453627054398, 0.10001775390876178, 0.0945176584318804, 0.09056858165552631, 0.08527834246905086, 0.07428395165243563, 0.073358194616894, 0.0680886913840763, 0.06761494047416094, 0.06709703737006972, 0.05811852629716023, 0.05694502567985473, 0.054410113292499285, 0.048847372924831635, 0.05006321483610061, 0.04818616545470521, 0.04477997207657074, 0.04385583504540518, 0.044935471408021524, 0.04291497548661245, 0.03747522448019955, 0.03720155920232304, 0.03781872331246603, 0.03907512170084106, 0.03808571144822426, 0.0337266662997155, 0.03255701906303652, 0.02515740589887356, 0.024542150663780255, 0.02793147737166276, 0.02593546934959928, 0.02561265908809492, 0.027039424086304994, 0.02195272859767639, 0.022481467933937037, 0.023805265842651288, 0.019805631663177174, 0.020343415882660094, 0.022202910110800743, 0.021919588551681024, 0.025648440866278835, 0.024350768256822405, 0.02123939903258099, 0.01514708600285948, 0.013058545996253482, 0.016306638918867626, 0.01685327520953289, 0.016574702121484014, 0.016640731977996603, 0.01489181877258933, 0.015860306949298176, 0.014099853219945703, 0.013995581963445438, 0.016321878165777286, 0.013422961495113753, 0.010429043487289373, 0.015911166273230783, 0.011775680992100685, 0.014917363881686935, 0.018574517294552517, 0.012787643851754746, 0.014939473080523192, 0.01176600590211186, 0.012982209259662748, 0.008858771788844534, 0.009820060386200072, 0.009505460298391053, 0.011240429890194399, 0.010675670454989353, 0.010158317344160542, 0.006973291840534449, 0.006880928409012844, 0.005811905768775168, 0.007408292713742807, 0.007451252999362914, 0.010354434164527717, 0.01092628369954178, 0.012062771040645908, 0.007832180905609038, 0.010386954512029251, 0.010358608426182522, 0.00795716502255174, 0.0063932851670936985, 0.006447436372195698, 0.004587642372207018, 0.007233876957458231, 0.007980881170325053, 0.010550779263485601, 0.012090449684582942, 0.009726078081678914, 0.00813099715300611, 0.006632155526443176, 0.00597371292983393, 0.004589174477389631, 0.003584162614903578, 0.005510094879151764, 0.007690454193111211, 0.006682628281595362, 0.005014813251454181, 0.005320725576812532, 0.007166650853292368, 0.007646939404855167, 0.0068286928395223144, 0.008263683283372191, 0.007336255545465137, 0.007655256824561777, 0.007321817508736698, 0.00431125137086996, 0.003226296991330126, 0.004760576777890367, 0.0063299430897483565, 0.004795740673675748, 0.0033784159969536777, 0.0035024787487254625, 0.005299788615376221, 0.003612780912293712, 0.0032062702902157835, 0.004884716233539565, 0.005524865538374664, 0.0067742976795348445, 0.00882896750121604, 0.005893147790118473, 0.00848282681003614, 0.007084418307778205, 0.0041311039470466075, 0.003654746470935981, 0.0033575089104424215, 0.0021545471597365076, 0.003389105463073925, 0.004264044310640164, 0.004854566317398292, 0.0023904619371291056, 0.002829549484107492, 0.002053513441469564, 0.002324710492194019, 0.0021624633618179965, 0.00212872851480438, 0.0023564651491906493, 0.003220773860256611, 0.002329243483711235, 0.0029083483295660978, 0.001973968918878853, 0.00209470545457433, 0.0020779970437868503, 0.0028365065618229167, 0.003357957487961407, 0.0030591108087931595, 0.002803605341979417, 0.0023873494445828206, 0.0023525519992746557, 0.002546498180350455, 0.0016468311933363184, 0.004213925738022168, 0.003313322557223379, 0.003231858396061877, 0.002629125782985243, 0.0029141006389791914, 0.003536610023096286, 0.004142602702752726, 0.005321672680347896, 0.005746499752380254, 0.0031256999963618884, 0.0032810929011951122, 0.0043974452997825285, 0.005239838635538117, 0.004797415686162696, 0.0032062243664227707, 0.003533768229311691, 0.0029152913773755726, 0.0041634554793835415, 0.0021060470147562338, 0.005407320675629354, 0.006459823155465985, 0.004450791821244248, 0.004911506875814143, 0.003201270101376409, 0.0062131848153570805, 0.003254528715726523, 0.003693553237162894, 0.00231178058878833, 0.002128929863538825, 0.0034310214809528308, 0.0021012531412071865, 0.0037686275719023775, 0.0026402074050687757, 0.0015023963813803039, 0.0016959904317550368, 0.0023601827229728445, 0.0016248808494453031, 0.001744074698825616, 0.0017098960009427912, 0.001745167300260726, 0.001709676564420346, 0.0014354304178802375, 0.0017766182493312826, 0.0017726632421619605, 0.005174195695061486, 0.00404562306744882, 0.0019462628384505156, 0.0021156957895648327, 0.003270384036677395, 0.004819947180601103, 0.004801801557994203, 0.003892041585816488, 0.003026418740098223, 0.0016867717583173738, 0.00243665445885507, 0.001924227454584904, 0.0032565365431517818, 0.0025015898762403913, 0.004080375838660543, 0.0034671720829839744, 0.002242059777764631, 0.00174158359747937, 0.0023047459024574768, 0.0018280270336568435, 0.003222634576515772, 0.002665815578360469, 0.002171202788719592, 0.004139460606425585, 0.005407126804135614, 0.003192231660575842, 0.0023893227233684547, 0.0017126825603354112, 0.0012934664354602478, 0.0024260497488599003, 0.0017768104415345862, 0.001707909157712362, 0.0025417737575718987, 0.0026618135311662788, 0.0013963878702470945, 0.001963627433890095, 0.0015268292372885813, 0.0010931521974983068, 0.0014422832873635296, 0.0014909466068760181, 0.0013153712134022825, 0.0010111237624123271, 0.0009827128673119666, 0.0005205708235784471, 0.0008319928409817137, 0.0007095568490024797, 0.00037128912167129697, 0.0006035467810239169, 0.0011062782132314122, 0.000675376708713361, 0.0006885260622773773, 0.00101789951790438, 0.0006217252514125575, 0.0007492425983336682, 0.0021772955004840527, 0.0016886424110349782, 0.0013806177742215066, 0.001364799225246333, 0.0013202676723717748, 0.0013149851532095538, 0.0010957021063876863, 0.0007910217270622839, 0.0011311705605573214, 0.000875898118397272, 0.0015793223269400525, 0.0013224604617411992, 0.0022564116427801696, 0.0026849516309372387, 0.0020301068258375223, 0.0006494859589330384, 0.0005210057817731313, 0.000972889763034617, 0.0010060088542230168, 0.0009481032836110347, 0.001107927744007216]\n",
        "train_acc_list_300const = [44.7525, 63.5, 72.3975, 76.605, 79.975, 82.355, 83.73, 85.3525, 86.61, 87.5825, 88.51, 89.1375, 90.01, 90.3875, 91.105, 91.8175, 92.1325, 92.8725, 93.125, 93.5025, 93.6875, 94.285, 94.705, 94.91, 95.275, 95.5425, 95.6975, 96.1225, 96.08, 96.3625, 96.4425, 96.6725, 96.75, 96.98, 97.385, 97.405, 97.635, 97.55, 97.615, 97.935, 97.9225, 98.115, 98.2575, 98.245, 98.31, 98.4675, 98.44, 98.46, 98.4925, 98.7, 98.6225, 98.6575, 98.6, 98.63, 98.765, 98.835, 99.135, 99.13, 99.0825, 99.0975, 99.1425, 99.085, 99.25, 99.21, 99.1575, 99.3575, 99.285, 99.2325, 99.23, 99.12, 99.155, 99.3075, 99.505, 99.57, 99.4, 99.43, 99.425, 99.4425, 99.495, 99.4975, 99.4875, 99.5125, 99.4175, 99.5525, 99.6225, 99.455, 99.59, 99.475, 99.355, 99.5525, 99.4975, 99.6075, 99.545, 99.7325, 99.655, 99.6675, 99.655, 99.63, 99.65, 99.7875, 99.8125, 99.82, 99.7425, 99.7475, 99.66, 99.6325, 99.55, 99.74, 99.685, 99.6475, 99.7425, 99.7775, 99.77, 99.86, 99.7625, 99.7475, 99.6, 99.5925, 99.68, 99.7275, 99.775, 99.8, 99.8475, 99.895, 99.82, 99.7325, 99.7725, 99.8375, 99.8025, 99.7625, 99.725, 99.7525, 99.705, 99.7675, 99.735, 99.72, 99.8675, 99.8975, 99.845, 99.7875, 99.8475, 99.8725, 99.8825, 99.8425, 99.865, 99.905, 99.8525, 99.805, 99.7725, 99.695, 99.8, 99.7325, 99.76, 99.8675, 99.8725, 99.9125, 99.9425, 99.9, 99.86, 99.83, 99.935, 99.8975, 99.9425, 99.9225, 99.9325, 99.9325, 99.9275, 99.8825, 99.9275, 99.8975, 99.9425, 99.92, 99.94, 99.8975, 99.895, 99.89, 99.9075, 99.9225, 99.9175, 99.91, 99.9525, 99.855, 99.885, 99.88, 99.915, 99.9, 99.8775, 99.8475, 99.8175, 99.7875, 99.905, 99.8825, 99.8775, 99.8225, 99.8675, 99.875, 99.89, 99.9025, 99.8725, 99.9275, 99.825, 99.77, 99.86, 99.815, 99.905, 99.78, 99.8975, 99.8875, 99.925, 99.9225, 99.8875, 99.9375, 99.8775, 99.91, 99.96, 99.9425, 99.9175, 99.9525, 99.945, 99.9425, 99.945, 99.935, 99.96, 99.9375, 99.9275, 99.8525, 99.8525, 99.945, 99.935, 99.8825, 99.8275, 99.825, 99.8725, 99.8875, 99.9425, 99.9125, 99.925, 99.89, 99.9175, 99.8725, 99.8975, 99.925, 99.9475, 99.9325, 99.93, 99.8975, 99.88, 99.925, 99.8575, 99.8025, 99.895, 99.9175, 99.9475, 99.955, 99.9225, 99.94, 99.9325, 99.92, 99.8875, 99.9625, 99.925, 99.95, 99.965, 99.955, 99.9625, 99.9675, 99.9725, 99.97, 99.9925, 99.9725, 99.9825, 99.9925, 99.9825, 99.9625, 99.9875, 99.9725, 99.9625, 99.985, 99.975, 99.9375, 99.9475, 99.9525, 99.9625, 99.965, 99.97, 99.97, 99.9775, 99.9675, 99.9775, 99.9525, 99.95, 99.9225, 99.905, 99.945, 99.985, 99.985, 99.9675, 99.98, 99.9725, 99.95]\n",
        "test_loss_list_300const = [1.3109248876571655, 0.9655934632578983, 0.8187996443313889, 0.689740173047102, 0.6772998514809186, 0.6437552631655826, 0.5553377157525171, 0.5520454826234262, 0.5120236824584913, 0.5249090968053552, 0.47987785124326054, 0.45728976138030425, 0.4652932938895648, 0.49931400976603546, 0.5139825019655349, 0.46358482577378235, 0.5389019496078733, 0.4064606526229955, 0.4374082180895383, 0.41569147004356866, 0.42417793334284914, 0.43346760355973546, 0.4166975364654879, 0.3910007622045807, 0.4190592248983021, 0.4890744652174696, 0.4130742766811878, 0.44733505535729323, 0.3960334837813921, 0.4205396575263784, 0.4506959509623202, 0.45510448968108697, 0.4337734672464902, 0.4140701989961576, 0.44726073873948446, 0.43756397483469567, 0.4081488196985631, 0.4404116733164727, 0.4069605218836024, 0.4365550541802298, 0.4524993513581119, 0.4810156839180596, 0.4475790313150309, 0.43388396841061266, 0.4388974361781833, 0.4250442619565167, 0.4161135846678215, 0.4599215022370785, 0.4951787300502198, 0.5083512184740622, 0.4711726530443264, 0.5019661293754095, 0.42351165593047685, 0.4634768115946009, 0.472742423415184, 0.4849290226267863, 0.4477674431061443, 0.48274370330043986, 0.4825481849757931, 0.4942950615777245, 0.5367305686202231, 0.4768067407834379, 0.4528346563441844, 0.49772302598892887, 0.43751826391944404, 0.4484408743019345, 0.5320610732217378, 0.47107771444547025, 0.4548007876058168, 0.48090932169292544, 0.47415826705437675, 0.44294909170911284, 0.408969871039632, 0.4602267291349701, 0.4937338570627985, 0.5272085793033431, 0.5100355125680754, 0.46345121894456165, 0.4728180213442332, 0.48767814587188674, 0.4734298460468461, 0.5140062019794802, 0.48423475234568875, 0.4522600043800813, 0.4885755015324943, 0.4898544026515152, 0.48496218667000157, 0.5125900219324269, 0.4700437818146959, 0.45802032928678055, 0.49178935455370554, 0.48068239813364005, 0.4772832633196553, 0.45595825010839897, 0.4741470496865767, 0.508940930796575, 0.47970542651188525, 0.5067461744139466, 0.47261701335635364, 0.4504903811442701, 0.4822925106634068, 0.4916953529360928, 0.4976545244078093, 0.5076064607010612, 0.512105936019481, 0.5002134619634363, 0.4895322826466983, 0.46702025679847864, 0.5068643896640102, 0.5748100429773331, 0.4924963390148139, 0.49676062770282164, 0.5234567566385752, 0.49199416045146654, 0.5275697438400003, 0.5010442611160157, 0.49138097276416004, 0.5251847050989731, 0.5231706929169123, 0.4994612190919586, 0.475140321481077, 0.4973598703553405, 0.46391360763507555, 0.4987307276718224, 0.5342934448507768, 0.490154218258737, 0.5101715593209749, 0.5079800128182278, 0.4876681245580504, 0.5047554036107245, 0.4933491258681575, 0.5017587872622888, 0.5059225696928894, 0.4965471655507631, 0.5347162768999233, 0.5293986288995682, 0.48946061645504796, 0.4993787820957884, 0.5046113696468028, 0.5203786311270315, 0.5163537305367144, 0.4821115889692608, 0.554601405617557, 0.5177212712130969, 0.5428616695011719, 0.5283017215094988, 0.5123567226566846, 0.5303934999281847, 0.5816303280335439, 0.5290483341941351, 0.5672795996069908, 0.579266353309909, 0.5199603319545335, 0.4895725855721703, 0.5205931299472157, 0.5044250222323816, 0.5172668089029155, 0.48294846157107174, 0.5366536256255983, 0.5250458570220803, 0.5189918728568886, 0.5009268411918532, 0.5045282197526738, 0.5237355313346356, 0.511520393098457, 0.525018301002587, 0.510031249515618, 0.5295086546411997, 0.4966339695302746, 0.52833208498321, 0.49447107315063477, 0.5136264913444277, 0.5089094453031504, 0.5464415133376664, 0.5187408701528476, 0.5190243262656128, 0.5516869642689258, 0.5397132114519047, 0.5154607352576678, 0.5146083532820774, 0.5073600661339639, 0.5701480022148241, 0.5735466538728038, 0.5491804146691214, 0.5422320174454134, 0.5446482123453406, 0.5426935783669918, 0.5710887145015258, 0.5220382987887044, 0.5417104106915148, 0.5754493603223487, 0.533619965744924, 0.5762778842185117, 0.5338592627380467, 0.5573781338296359, 0.5375887101775483, 0.5312822029183183, 0.5451974290647085, 0.5270344963933848, 0.5519347873669637, 0.5583887022884586, 0.5747697632524031, 0.53732075910025, 0.544713698799097, 0.54281858609447, 0.5719384611407413, 0.5202449362111997, 0.5084290615742719, 0.5141259058366848, 0.5029577503475962, 0.5330851511864723, 0.5383445264040669, 0.5445871983147874, 0.5439003353254704, 0.5215786695480347, 0.5109690418349037, 0.530744302310521, 0.49466055395859704, 0.5258592778368841, 0.5579611717522899, 0.5201689512292041, 0.5321517630091196, 0.5553653376011909, 0.5559994182254695, 0.5492764958475209, 0.5437155616811559, 0.5355881104552294, 0.5277321902634222, 0.539707180819934, 0.5771049804325346, 0.5852305162933809, 0.5669170301171798, 0.5253017652261106, 0.5338942474579509, 0.5554900363653521, 0.5405757495119602, 0.5658218766315074, 0.5613152086734772, 0.5465129902468452, 0.5272626559945601, 0.5295005417134189, 0.5047299167777919, 0.49300466383559793, 0.5492192791609825, 0.553901323977905, 0.5375248759607726, 0.5450990347168113, 0.5740641949674751, 0.6136820884067801, 0.549185292064389, 0.5533378690103942, 0.5597253417289709, 0.5323915726776365, 0.5313113266720048, 0.5539356519149828, 0.5392684795056717, 0.5539341314306742, 0.5655378125890901, 0.5571187838346143, 0.5493852450877805, 0.541150254162052, 0.5641609287903279, 0.5385742417619198, 0.5644041978860204, 0.5555187009180649, 0.5374931981669197, 0.5337770737801926, 0.5295022774157645, 0.5276252516085589, 0.526966481646405, 0.5239434323356121, 0.5202406580689587, 0.5424046025057382, 0.5546739329642887, 0.5492268700765658, 0.5330030725726599, 0.5302840090250667, 0.559327697074866, 0.5229975948982601, 0.5579739920323408, 0.5483722508519511, 0.5500919239807732, 0.5255736416276497, 0.5679198548763613, 0.5679364106323146, 0.5342161246115649, 0.5500478484208071, 0.5313761402157289, 0.5704804831479169, 0.5673190015780775, 0.5638997756604907, 0.5896313563932346, 0.5433095578528657, 0.5565140822642967, 0.5584272151883645, 0.5537656983242759, 0.548184348058097, 0.5406601837352861, 0.555926272952104, 0.5514698358653467]\n",
        "test_acc_list_300const = [53.98, 65.75, 72.1, 76.14, 77.31, 77.93, 81.02, 81.31, 82.75, 82.79, 83.69, 84.8, 85.35, 83.73, 83.79, 85.27, 84.08, 86.95, 86.6, 86.76, 87.11, 87.3, 88.07, 88.24, 87.9, 86.96, 88.42, 88.0, 88.7, 88.74, 87.84, 87.77, 88.76, 88.95, 88.48, 88.71, 89.77, 88.58, 89.26, 89.81, 89.22, 88.58, 89.44, 89.89, 89.57, 89.96, 89.93, 89.62, 88.5, 89.02, 89.32, 89.09, 90.25, 89.9, 90.15, 89.83, 90.56, 89.5, 90.0, 89.84, 89.55, 90.01, 90.27, 89.89, 90.63, 90.74, 89.52, 90.6, 90.0, 90.07, 90.5, 91.04, 91.19, 90.84, 90.5, 89.92, 90.31, 90.59, 90.56, 90.44, 90.87, 90.43, 90.47, 91.12, 90.58, 90.7, 90.83, 90.28, 91.18, 91.08, 90.76, 90.93, 91.05, 91.44, 91.19, 90.73, 91.14, 90.84, 91.04, 91.48, 90.9, 91.52, 91.12, 91.39, 90.99, 90.96, 91.09, 91.44, 91.0, 90.32, 91.25, 91.42, 91.16, 91.08, 90.75, 91.24, 91.32, 90.74, 90.9, 91.13, 91.6, 91.41, 91.7, 91.22, 90.61, 91.46, 90.95, 91.5, 91.36, 91.43, 91.4, 90.91, 91.13, 91.28, 91.29, 91.08, 91.73, 91.34, 91.19, 91.15, 91.59, 91.75, 91.13, 91.5, 91.47, 91.33, 91.42, 91.52, 90.83, 91.11, 91.09, 90.95, 91.6, 91.74, 91.37, 91.74, 91.61, 91.83, 90.89, 91.51, 91.79, 91.72, 92.0, 91.68, 91.82, 91.75, 91.83, 91.66, 91.79, 91.57, 91.82, 91.86, 91.98, 91.55, 91.74, 91.64, 91.27, 91.43, 91.88, 91.69, 92.03, 91.23, 91.2, 91.33, 91.81, 91.53, 91.45, 91.15, 91.51, 90.91, 91.23, 91.95, 91.46, 91.29, 91.48, 91.44, 91.69, 91.54, 91.51, 91.46, 91.42, 91.05, 91.18, 91.19, 91.56, 91.29, 91.73, 91.76, 91.56, 91.49, 91.62, 91.53, 91.7, 91.61, 91.97, 91.83, 92.04, 91.91, 91.65, 91.62, 91.6, 91.81, 91.53, 91.51, 91.84, 91.74, 91.75, 91.74, 91.9, 91.56, 91.06, 91.42, 91.51, 91.83, 91.39, 91.63, 91.73, 91.66, 91.55, 91.7, 91.4, 91.93, 92.04, 91.7, 91.47, 91.45, 91.64, 91.32, 90.81, 91.54, 91.31, 91.47, 92.03, 91.83, 91.86, 91.87, 91.91, 91.64, 91.84, 91.63, 91.8, 91.87, 92.0, 91.71, 91.79, 91.94, 92.08, 91.91, 92.31, 92.23, 92.29, 92.38, 92.07, 92.0, 91.81, 92.24, 92.44, 92.16, 92.14, 91.99, 91.69, 91.77, 92.03, 91.96, 91.84, 92.05, 91.96, 91.89, 92.32, 91.83, 91.85, 91.69, 92.02, 91.96, 91.93, 92.11, 91.92, 92.17, 91.77, 92.18]\n",
        "train_loss_list_300cosine = [1.5071465310197287, 1.0219155532864337, 0.7846573638839843, 0.6613810150958479, 0.5711964959153732, 0.5125089824770968, 0.4690145238900718, 0.4279545375142996, 0.38901165142036476, 0.3574826151799089, 0.32995736313323243, 0.3070905926033331, 0.29151374520585177, 0.27276346844415694, 0.2528251954637016, 0.2403833875164818, 0.21865639683251945, 0.21172414217798854, 0.1913816854833795, 0.18111336490692803, 0.1749152649943821, 0.16864872572663875, 0.15266839607645527, 0.1438077868649754, 0.131705386099962, 0.12894651846002086, 0.12342969215097138, 0.11197239956773888, 0.10989574266198915, 0.10079097377058988, 0.08904699315302098, 0.09436483237856684, 0.08817791208898583, 0.08477092711939313, 0.08056267116147393, 0.07277504891490404, 0.07563522219146117, 0.07128996417795222, 0.057986204402324874, 0.06231818245218013, 0.05127317121716591, 0.05075172336099628, 0.05728455614774657, 0.05022484378014414, 0.047201051832007145, 0.04299503102446326, 0.04491299623028396, 0.03698193365499711, 0.03486268078765502, 0.03927968754864539, 0.03724308135345007, 0.03970887712738551, 0.03285741158493292, 0.028173236158518744, 0.028217624198978605, 0.029066849599247234, 0.028810474211156678, 0.026399405389732587, 0.025763797607963174, 0.022101726805115827, 0.0206387186558328, 0.023366123397712605, 0.019345413963012873, 0.024243142143205797, 0.02154286678515214, 0.01816358644301042, 0.021265280448992292, 0.015330405338271595, 0.017995145478354284, 0.014342598256486924, 0.015467149512258362, 0.016130026668673175, 0.013699235769971377, 0.011888031450363573, 0.010789586022736494, 0.010900068600833514, 0.009051817458605739, 0.010178615377595523, 0.011202018590738473, 0.014404841935349116, 0.01132253416424867, 0.010159075095953521, 0.008117529459464283, 0.009489624014945505, 0.010137052083415494, 0.008486186185057242, 0.006968282311358831, 0.007533594105837527, 0.008950318053382235, 0.008889065595822946, 0.007539918537286241, 0.0057594785649967666, 0.006437454009137117, 0.005282934063498991, 0.004347972699934572, 0.005166493265034323, 0.004321007760212431, 0.00471395535047535, 0.0032943137242381028, 0.004311655514717282, 0.003059743971269304, 0.0039670848353404575, 0.0030527470893090434, 0.003868208768115135, 0.0034133147897016363, 0.004668654355293675, 0.0059490043992545235, 0.0041872456292248, 0.0038983581991906308, 0.0037588840379914716, 0.003916245875395213, 0.0034797456036773718, 0.0029193548814658767, 0.0028640743580595858, 0.0023959889678485742, 0.0022046099687237837, 0.00213721157060805, 0.001905863403416865, 0.0018298605383502951, 0.0019732695092532088, 0.0012731390455790247, 0.0014643412322186396, 0.0014584561287045482, 0.0013225552577550437, 0.0012107872296070626, 0.0009283569928718647, 0.0009475817591004316, 0.0009087429049996629, 0.0012826446498267045, 0.0011837309814490989, 0.0007089067522449444, 0.0007332131970385923, 0.0012466143751911653, 0.0009211793556768187, 0.000789917121722373, 0.0007207691697882189, 0.0007975770474887959, 0.0006758498559978029, 0.0007351151072098131, 0.0010837652537124284, 0.000724128287233449, 0.0005810306201093711, 0.00047435187363775613, 0.0004135937659276342, 0.0007063415897789764, 0.000588239943727893, 0.0005826207090225168, 0.0003262833067903336, 0.00038663124462745416, 0.0004899335792227164, 0.00045023311883942547, 0.00032505322259691985, 0.0004240722920772437, 0.000434831311982059, 0.0005759489223388422, 0.0003049708239195287, 0.00034293277186921384, 0.00026971405763318425, 0.00031280760296981017, 0.0007162304362590164, 0.0005976533334854375, 0.00044388456952353377, 0.00032143614894833265, 0.0003041571174924992, 0.00035453209945005774, 0.00023628432779605936, 0.00022958813212190958, 0.00018880277622619602, 0.00021482868581873839, 0.0001753957263578302, 0.00041403284841927493, 0.00021361697475566445, 0.0002520289759267065, 0.00019671708842346002, 0.00019921282705291114, 0.00030435736312682684, 0.00029258786672637244, 0.00027827393721283804, 0.0002795331743087422, 0.00024584021705562536, 0.00020301578391222058, 0.00015932558792188886, 0.00035799621748254593, 0.00028251744383299383, 0.00021141435477205867, 0.00022243324366817674, 0.00020670883468984464, 0.00021545639445687408, 0.00016235478455645368, 0.00022787957680421982, 0.00016665564254608194, 0.0001477420311949244, 0.00018142510538173126, 0.00034612401611176406, 0.00015685088123029036, 0.00019157101443428062, 0.00019864877790692532, 0.00020180520096545342, 0.00012184150966041865, 0.00010955794329265264, 0.0002272417015756974, 0.00012062133251651577, 0.00024990162183020963, 0.00019493304157317115, 0.00011319477053125949, 0.0001146485327846252, 0.0001126736392463569, 0.00013831783792320434, 0.0001353729226037266, 0.0001630674412422994, 0.0001457879433627867, 0.00012657002445250193, 0.00020756496892448712, 0.0001576078423556483, 0.00016432452238864054, 0.00013326239761994157, 0.000126308732199333, 0.00011329932245867901, 9.373737983756487e-05, 0.00012264910064071485, 9.72924513225386e-05, 9.851183427198616e-05, 0.00014160107466290727, 0.00019542356384294292, 0.00010866884136099055, 0.00012103338623344784, 0.00010338596340528902, 0.00012947641171964864, 0.00016585586701756884, 0.00012378471780928916, 0.00012914159678432927, 8.873532887934217e-05, 0.00012277717895865415, 0.00010324342793536724, 0.00011429319050957435, 0.00014108263730566994, 0.00011344413554959498, 0.0001314912391754855, 0.00014366292956832224, 0.00010670398796356165, 9.395798729230545e-05, 9.690826291712445e-05, 8.27918457593515e-05, 7.49523732641198e-05, 0.000116572610751607, 0.00010861793663089236, 0.00010167052199556757, 0.0001307339418356521, 0.00010273090034757616, 0.00012697175096751327, 0.00012768910120719, 7.014640353397078e-05, 9.454710011857551e-05, 0.00011274885802570828, 0.000126904926870751, 9.48965783575255e-05, 9.193459106337249e-05, 8.93238301681306e-05, 0.00010053854063013211, 0.00010620467252830129, 0.00014532459377346714, 8.020320023135105e-05, 7.178885715230178e-05, 0.00011935103254135713, 9.011012685912722e-05, 8.608197714011839e-05, 9.837138899922847e-05, 0.00010366790116276302, 8.538921241570249e-05, 0.0001675107899030607, 7.792623628388836e-05, 6.928247518943407e-05, 8.20774587304151e-05, 0.00011248917587745207, 7.720731971381645e-05, 6.833156445040754e-05, 0.00010231887682267187, 9.609293597030557e-05, 7.292958810775106e-05, 0.00010774314482761675, 9.255914947628301e-05, 0.0001237650751620048, 9.64451171900979e-05, 0.0001169090761506665, 0.0001072614105620238, 9.686175892860217e-05, 0.00011957684801773822, 0.00011583886435356528, 0.0001275175088925282, 6.546866618231805e-05, 9.038513419941541e-05, 0.00010006681310687346, 9.007086580128472e-05, 0.00022282703456296465, 8.881661869528004e-05, 8.626907432722472e-05, 9.961434439616733e-05, 0.0001082291823644854, 0.00010509098167338485, 7.50494904148657e-05]\n",
        "train_acc_list_300cosine = [44.7275, 63.7, 72.3575, 76.9475, 80.11, 82.24, 83.6775, 85.0225, 86.4575, 87.51, 88.3775, 89.1875, 89.8275, 90.4725, 91.27, 91.6125, 92.195, 92.6575, 93.2125, 93.6125, 93.7775, 93.95, 94.65, 94.79, 95.3675, 95.5025, 95.58, 96.0475, 96.075, 96.4175, 96.79, 96.66, 96.8425, 97.0175, 97.1425, 97.4075, 97.345, 97.375, 97.945, 97.73, 98.185, 98.1775, 98.0075, 98.175, 98.36, 98.465, 98.4525, 98.72, 98.815, 98.635, 98.6625, 98.58, 98.825, 99.075, 99.0425, 98.985, 98.965, 99.1025, 99.0875, 99.26, 99.2775, 99.1925, 99.3375, 99.1475, 99.2425, 99.425, 99.2375, 99.475, 99.38, 99.505, 99.465, 99.4675, 99.5625, 99.605, 99.64, 99.6425, 99.71, 99.66, 99.625, 99.4775, 99.625, 99.6725, 99.745, 99.7, 99.67, 99.7325, 99.785, 99.7425, 99.675, 99.6725, 99.7625, 99.83, 99.7925, 99.8025, 99.8675, 99.8425, 99.8625, 99.85, 99.905, 99.8775, 99.9, 99.8825, 99.895, 99.8625, 99.9025, 99.8625, 99.7975, 99.8725, 99.87, 99.89, 99.88, 99.87, 99.9125, 99.915, 99.9375, 99.9475, 99.94, 99.945, 99.94, 99.9425, 99.9775, 99.96, 99.9575, 99.965, 99.9725, 99.975, 99.98, 99.9825, 99.9675, 99.965, 99.9925, 99.985, 99.9675, 99.975, 99.9725, 99.975, 99.98, 99.985, 99.9875, 99.9725, 99.9825, 99.9925, 99.9925, 99.9975, 99.98, 99.99, 99.9875, 99.9975, 99.995, 99.985, 99.99, 100.0, 99.9925, 99.9875, 99.9825, 100.0, 99.9975, 99.9975, 99.9925, 99.9825, 99.9875, 99.99, 99.9975, 99.9975, 99.9925, 100.0, 99.9975, 99.9975, 100.0, 100.0, 99.985, 99.9975, 99.9975, 99.995, 99.9975, 99.9975, 99.9975, 99.9975, 99.9925, 99.995, 99.9975, 100.0, 99.9875, 99.9925, 100.0, 99.995, 100.0, 99.995, 100.0, 99.9925, 100.0, 100.0, 99.9975, 99.99, 100.0, 99.9975, 99.995, 99.995, 100.0, 100.0, 99.9925, 100.0, 99.995, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 99.9975, 99.995, 99.9975, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9925, 100.0, 100.0, 100.0, 99.9975, 99.9975, 100.0, 99.9975, 100.0, 99.9975, 100.0, 99.9975, 100.0, 99.9975, 99.9975, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 99.9975, 100.0, 100.0, 99.9975, 100.0, 99.9925, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
        "test_loss_list_300cosine = [1.3360588429849358, 1.0448881915852994, 0.8597957295707509, 0.6716653414164917, 0.6908234833162042, 0.5932456066336813, 0.5461285729197007, 0.5097410641139066, 0.5401990885221506, 0.5333668193485164, 0.4556268532819386, 0.4484600973657415, 0.4620721623112884, 0.4327629621647581, 0.47597266639335245, 0.48141292925876905, 0.49725772666780255, 0.40901502231253856, 0.4323547872938687, 0.4539363452150852, 0.4590581006641629, 0.40523579641233515, 0.4201337242428261, 0.418421223005162, 0.41465936016432847, 0.43959930557993393, 0.3880773431892636, 0.397792113732688, 0.4114573869524123, 0.3806772181127645, 0.40738119260419775, 0.41724546894996983, 0.3961485907246795, 0.40635761147058463, 0.41718924026700516, 0.4624185173571864, 0.4003025793199298, 0.4194344795202907, 0.43098732470711576, 0.42882050819034817, 0.4735664082101629, 0.4482132949029343, 0.4824381698913212, 0.489483314035814, 0.42275018914590906, 0.46854421854773654, 0.46698710643037966, 0.45736848336609104, 0.48327861801732946, 0.42990076523038406, 0.4146351450228993, 0.4377061692596991, 0.4491257427991191, 0.4464895281987854, 0.5250044391502308, 0.4606155977596211, 0.48777404917946343, 0.46350483313391483, 0.45923378441152696, 0.47999939944925185, 0.4535947281725799, 0.45670295101177844, 0.4861161712604233, 0.5072417436521265, 0.45164442854591563, 0.4723219115145599, 0.465178365948834, 0.48657907800206657, 0.4763214318435403, 0.4628955114491378, 0.4531608254094667, 0.4590076025150999, 0.48202957307236105, 0.47070856392383575, 0.4524376158472858, 0.5099793840435487, 0.4644981126619291, 0.4989790280785742, 0.4910671897704088, 0.5239907608756537, 0.4774412533527688, 0.4662430659879612, 0.47549027996727183, 0.49829778188391577, 0.4727332616531396, 0.48873594582458085, 0.4645166827153556, 0.4946923785949055, 0.5139467286158211, 0.504527775924417, 0.4836999862631665, 0.4878289546770386, 0.49920391819522353, 0.48420851479602767, 0.4741327323113816, 0.4969299920374834, 0.492643963309783, 0.4669509222613105, 0.5004429721002337, 0.5008861348976062, 0.48978376954416686, 0.48975364958183676, 0.4894806569135642, 0.4939129582688778, 0.5057695177914221, 0.483693698161765, 0.49143673158899137, 0.508361778493169, 0.49146506284611136, 0.5179333079464828, 0.4985507955845398, 0.48249870953680596, 0.4882995514552804, 0.48072621575261976, 0.4871305170692975, 0.495542240859587, 0.502074130728275, 0.4848601612486417, 0.46474564952563635, 0.45505768362479876, 0.480818674534182, 0.48904268066339857, 0.48706334348343594, 0.5078222957215731, 0.48777740590180024, 0.48671044654484036, 0.4677445718759223, 0.4843851542925533, 0.47715435280830043, 0.4916676714827743, 0.48873102721534195, 0.4853220989432516, 0.495768607794484, 0.48384790348855755, 0.494273115373865, 0.520792075538937, 0.48109641991838625, 0.4784795025104209, 0.5150823619546769, 0.49928266564501994, 0.508841446871999, 0.4865191377039197, 0.49814116068278685, 0.4979177566268776, 0.509591865388653, 0.5287647111506402, 0.5030483018748367, 0.47968817134446734, 0.5014746417350406, 0.5026743098904815, 0.5065745726416383, 0.4991808756242824, 0.49776733959022956, 0.4859552224980125, 0.505850138354905, 0.507838894294787, 0.4913963109632082, 0.4985096752643585, 0.5035310268779344, 0.49438839943348606, 0.5127879598095447, 0.49104869393985484, 0.4965999275445938, 0.500093090760557, 0.5078368173747123, 0.49182856611058684, 0.4928507318225088, 0.5030448830957654, 0.4847751912436908, 0.5120558350146571, 0.4888622204336939, 0.49651806179103974, 0.49612243831912173, 0.4962354428783248, 0.513516125611112, 0.49529338731795924, 0.4980339683309386, 0.5014159933109826, 0.49898972069915337, 0.507745862573008, 0.52068718268147, 0.5003404364555697, 0.525249100938628, 0.5042423366368571, 0.5168220468714267, 0.5160682231565065, 0.5124318371467953, 0.5218248301291768, 0.4837766302914559, 0.5026627190505402, 0.5024134409201296, 0.512225415887712, 0.5186735810358313, 0.5077592483426951, 0.4826306046187123, 0.512151868848861, 0.5008148086976402, 0.4929636383924303, 0.4970756280648557, 0.5172987021977389, 0.4966934053580972, 0.5119075952451441, 0.4916618470149704, 0.5191661170389079, 0.5202390191298497, 0.5187712847432003, 0.5312336112690877, 0.5149811487409133, 0.5015346613488619, 0.4897740053225167, 0.5123436318922646, 0.4950245019001297, 0.5100116333629512, 0.517671759935874, 0.49631667703012877, 0.5063869094924082, 0.5044590200804457, 0.5116969626161116, 0.5092341914018498, 0.5033747812237921, 0.5193329711880865, 0.5127834038266653, 0.5309269350918033, 0.49558807419070716, 0.513400316049781, 0.5085111194396321, 0.510913455599471, 0.4971718424105946, 0.5032308197851423, 0.5266274438251423, 0.49809465491319005, 0.5013407922432392, 0.4893049780703798, 0.5038147557385361, 0.5071697546334206, 0.5146695088736618, 0.49518201302123976, 0.5041418398105646, 0.5182757000379925, 0.5224867431046087, 0.4978565979230253, 0.507878503939019, 0.49611272823206987, 0.5108789631837531, 0.5073691453737549, 0.5223453206163419, 0.5107548187805128, 0.5007183399004272, 0.5212629056429561, 0.49115426876122437, 0.5297577135925051, 0.5027483824310424, 0.5041316181798524, 0.4906521471618097, 0.493595665768732, 0.502438643687888, 0.4948954885896248, 0.4982064822806588, 0.5073866029328937, 0.5064849340462987, 0.5071891489662702, 0.5056039866390107, 0.5256269764673861, 0.4950309816417815, 0.5119404613594466, 0.4975051161231874, 0.4961673260866841, 0.5196594443125061, 0.4737729119914996, 0.5019824165332166, 0.4959078891367852, 0.5141448318203793, 0.49820214887208575, 0.5068159803182264, 0.4858330869221989, 0.496248337852804, 0.4940428052899204, 0.503951388069346, 0.49354483621029915, 0.4873908001788055, 0.5022429193876967, 0.50107072425794, 0.49141834751714636, 0.50169266326518, 0.4897840660584124, 0.5037467987099781, 0.501852917708928, 0.5113212121061131, 0.5046284511873994, 0.4889931109132646, 0.507928684731073, 0.4926022017681146, 0.5067250309865686, 0.510816790823695, 0.5253809748948375, 0.5128096621247786, 0.4980110616623601, 0.5035155990832969, 0.5060317163603215, 0.49634331974047646]\n",
        "test_acc_list_300cosine = [52.55, 63.48, 70.79, 76.77, 77.07, 79.66, 81.7, 82.61, 82.36, 83.15, 84.72, 85.19, 85.13, 85.7, 85.08, 84.65, 84.64, 87.17, 86.56, 86.12, 86.83, 87.48, 87.37, 87.5, 88.14, 87.33, 89.04, 88.78, 88.32, 89.27, 88.82, 88.61, 89.75, 88.78, 89.11, 88.37, 89.51, 89.38, 88.67, 89.45, 88.48, 89.32, 89.16, 88.72, 89.82, 89.02, 88.77, 89.55, 89.58, 90.05, 90.18, 90.23, 89.78, 90.12, 89.23, 89.82, 89.9, 89.95, 89.87, 90.25, 90.38, 90.39, 90.25, 89.66, 90.88, 90.49, 90.4, 90.26, 90.13, 91.0, 90.89, 90.98, 90.93, 90.94, 91.42, 90.54, 91.11, 90.89, 90.63, 90.02, 91.17, 90.81, 91.24, 90.97, 91.14, 91.35, 91.47, 91.32, 90.83, 90.87, 91.23, 90.95, 91.06, 91.39, 91.41, 90.98, 91.19, 91.65, 91.43, 90.97, 90.93, 91.36, 91.17, 91.47, 91.56, 91.59, 91.49, 90.95, 91.39, 91.46, 91.51, 91.64, 91.6, 91.65, 91.39, 91.51, 91.58, 92.1, 91.79, 91.97, 91.75, 91.74, 91.57, 91.96, 91.91, 92.0, 92.15, 92.02, 91.96, 91.79, 91.77, 92.07, 91.99, 92.41, 92.06, 91.7, 92.06, 92.22, 91.85, 91.93, 92.2, 92.1, 92.04, 91.91, 91.85, 91.94, 91.89, 92.15, 92.18, 91.94, 92.03, 92.19, 92.17, 92.45, 92.43, 91.98, 92.28, 92.25, 92.4, 92.24, 92.02, 92.17, 92.01, 92.24, 92.28, 92.14, 92.37, 92.14, 92.25, 92.3, 92.22, 92.12, 92.18, 92.11, 92.29, 92.04, 92.02, 92.2, 92.37, 92.34, 92.25, 92.27, 91.84, 92.13, 92.18, 91.97, 92.19, 91.85, 91.98, 92.21, 92.11, 92.25, 92.17, 92.13, 92.4, 92.37, 92.21, 92.18, 92.44, 92.29, 92.36, 92.11, 92.35, 92.51, 91.96, 92.18, 92.12, 92.41, 92.3, 92.57, 92.18, 92.45, 92.11, 92.13, 92.52, 92.18, 92.1, 92.1, 92.24, 92.59, 92.23, 92.27, 92.27, 92.48, 92.31, 92.38, 92.04, 92.33, 92.33, 92.15, 92.27, 92.48, 92.52, 92.61, 92.2, 92.3, 92.31, 92.55, 92.25, 92.12, 92.24, 92.12, 92.47, 92.29, 92.22, 92.34, 92.4, 92.34, 92.23, 92.56, 92.08, 92.09, 92.33, 92.22, 92.44, 92.34, 92.41, 92.41, 92.2, 92.35, 92.42, 92.43, 91.96, 92.21, 92.27, 92.41, 92.21, 92.36, 92.66, 92.32, 92.33, 92.37, 92.2, 92.16, 92.31, 92.34, 92.35, 92.43, 92.49, 92.47, 92.15, 92.23, 92.61, 92.26, 92.76, 92.44, 92.32, 92.18, 92.38, 92.38, 92.28, 92.54, 92.5, 92.4, 92.13, 92.37, 92.31, 92.37, 92.12, 92.62]\n"
      ],
      "metadata": {
        "id": "ilmYYfIElcAU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list_01)), train_acc_list_01, 'b')\n",
        "plt.plot(range(len(train_acc_list_001)), train_acc_list_001, 'r')\n",
        "plt.plot(range(len(train_acc_list_0001)), train_acc_list_0001, 'g')\n",
        "\n",
        "plt.plot(range(len(test_acc_list_01)), test_acc_list_01, color='b', linestyle='--')\n",
        "plt.plot(range(len(test_acc_list_001)), test_acc_list_001,color='r', linestyle='--')\n",
        "plt.plot(range(len(test_acc_list_0001)), test_acc_list_0001, color='g', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Combined accuracy\")\n",
        "plt.legend(['train with lr = 1e-1', 'train with lr = 1e-2','train with lr = 1e-3',\n",
        "            'test with lr = 1e-1','test with lr = 1e-2', 'test with lr = 1e-3'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ubm3Y5CSnZ0D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "0b996055-3d68-4687-e3df-b9bf0e345c17"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list_01)), train_loss_list_01, 'b')\n",
        "plt.plot(range(len(train_loss_list_001)), train_loss_list_001, 'r')\n",
        "plt.plot(range(len(train_loss_list_0001)), train_loss_list_0001, 'g')\n",
        "\n",
        "plt.plot(range(len(test_loss_list_01)), test_loss_list_01, color='b', linestyle='--')\n",
        "plt.plot(range(len(test_loss_list_001)), test_loss_list_001,color='r', linestyle='--')\n",
        "plt.plot(range(len(test_loss_list_0001)), test_loss_list_0001, color='g', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Combined loss\")\n",
        "plt.legend(['train with lr = 1e-1', 'train with lr = 1e-2','train with lr = 1e-3',\n",
        "            'test with lr = 1e-1','test with lr = 1e-2', 'test with lr = 1e-3'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QnOTaNoesBEb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7bac3918-85e2-4b76-d4a2-f41f50e74e95"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}