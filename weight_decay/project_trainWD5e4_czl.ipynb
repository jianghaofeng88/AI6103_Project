{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7be1314e5b6049eb80959c5580e1f8f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_761d78cfb0984b17911246f7ac5b14fb",
              "IPY_MODEL_92cd9afac92747998d0819e71168e7d3",
              "IPY_MODEL_e1d70bdabd674e57b244c0b346e39750"
            ],
            "layout": "IPY_MODEL_72c092bd879c48078e859b419795bdc0"
          }
        },
        "761d78cfb0984b17911246f7ac5b14fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f487c683a6ec48deb7bfa2a45d4457aa",
            "placeholder": "​",
            "style": "IPY_MODEL_26507aba632b44cc99332441fbcf97c4",
            "value": "100%"
          }
        },
        "92cd9afac92747998d0819e71168e7d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22dd70f6678944a8b416f12a2aacedf8",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_169220c8404844df847c90107e9fab25",
            "value": 182040794
          }
        },
        "e1d70bdabd674e57b244c0b346e39750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0777a3447244f199a514a699f3ea616",
            "placeholder": "​",
            "style": "IPY_MODEL_a6c174326fb44ad4b1bd49806400c9af",
            "value": " 182040794/182040794 [00:06&lt;00:00, 38778432.78it/s]"
          }
        },
        "72c092bd879c48078e859b419795bdc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f487c683a6ec48deb7bfa2a45d4457aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26507aba632b44cc99332441fbcf97c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22dd70f6678944a8b416f12a2aacedf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "169220c8404844df847c90107e9fab25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0777a3447244f199a514a699f3ea616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6c174326fb44ad4b1bd49806400c9af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a457414c0401460ca6fdbcf4d1dd29af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44489617705547408cba6ef9f5891782",
              "IPY_MODEL_ea70d7acec634550b638c19e5b7a740d",
              "IPY_MODEL_f1187874789b49198ffd3804a585a234"
            ],
            "layout": "IPY_MODEL_33ebdccc46174e05b86eae092d6bfe17"
          }
        },
        "44489617705547408cba6ef9f5891782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf37a00758134d99b337e7e3792d134a",
            "placeholder": "​",
            "style": "IPY_MODEL_88276cb3159a4294a1fe3754bb52ecbd",
            "value": "100%"
          }
        },
        "ea70d7acec634550b638c19e5b7a740d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8f4b3df58b8467fa7084a04201284ab",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccd68a1e288a46d392c5a55b19c74bfd",
            "value": 64275384
          }
        },
        "f1187874789b49198ffd3804a585a234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_269e1f40a20d49d385d9b6b797839666",
            "placeholder": "​",
            "style": "IPY_MODEL_af8fe1ff5e324bab8a1bc7cf7b08c19b",
            "value": " 64275384/64275384 [00:03&lt;00:00, 38487555.21it/s]"
          }
        },
        "33ebdccc46174e05b86eae092d6bfe17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf37a00758134d99b337e7e3792d134a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88276cb3159a4294a1fe3754bb52ecbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8f4b3df58b8467fa7084a04201284ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccd68a1e288a46d392c5a55b19c74bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "269e1f40a20d49d385d9b6b797839666": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af8fe1ff5e324bab8a1bc7cf7b08c19b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "7be1314e5b6049eb80959c5580e1f8f9",
            "761d78cfb0984b17911246f7ac5b14fb",
            "92cd9afac92747998d0819e71168e7d3",
            "e1d70bdabd674e57b244c0b346e39750",
            "72c092bd879c48078e859b419795bdc0",
            "f487c683a6ec48deb7bfa2a45d4457aa",
            "26507aba632b44cc99332441fbcf97c4",
            "22dd70f6678944a8b416f12a2aacedf8",
            "169220c8404844df847c90107e9fab25",
            "b0777a3447244f199a514a699f3ea616",
            "a6c174326fb44ad4b1bd49806400c9af",
            "a457414c0401460ca6fdbcf4d1dd29af",
            "44489617705547408cba6ef9f5891782",
            "ea70d7acec634550b638c19e5b7a740d",
            "f1187874789b49198ffd3804a585a234",
            "33ebdccc46174e05b86eae092d6bfe17",
            "bf37a00758134d99b337e7e3792d134a",
            "88276cb3159a4294a1fe3754bb52ecbd",
            "a8f4b3df58b8467fa7084a04201284ab",
            "ccd68a1e288a46d392c5a55b19c74bfd",
            "269e1f40a20d49d385d9b6b797839666",
            "af8fe1ff5e324bab8a1bc7cf7b08c19b"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "06276f65-69dd-4da7-fb0a-6798fb68c0a6"
      },
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7be1314e5b6049eb80959c5580e1f8f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a457414c0401460ca6fdbcf4d1dd29af"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "0a11f769-27c5-4664-e2e2-e5d411849f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "1949e3ed-848a-4c3a-fc0b-7ad1cfb82e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "ad69b288-30d2-4ffc-b831-e2af1f5e072c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArgupDVRwB8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ca9ea02-91b5-414d-c4aa-b9a405de7dd7"
      },
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3439, accuracy : 16.98\n",
            "iteration : 100, loss : 2.2795, accuracy : 19.23\n",
            "iteration : 150, loss : 2.1878, accuracy : 22.60\n",
            "iteration : 200, loss : 2.0178, accuracy : 28.35\n",
            "iteration : 250, loss : 1.8353, accuracy : 35.16\n",
            "iteration : 300, loss : 1.6640, accuracy : 41.48\n",
            "iteration : 350, loss : 1.5272, accuracy : 46.56\n",
            "Epoch :   1, training loss : 1.4802, training accuracy : 48.28, test loss : 0.8332, test accuracy : 73.41\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.5901, accuracy : 81.06\n",
            "iteration : 100, loss : 0.5778, accuracy : 81.60\n",
            "iteration : 150, loss : 0.5600, accuracy : 82.21\n",
            "iteration : 200, loss : 0.5468, accuracy : 82.66\n",
            "iteration : 250, loss : 0.5323, accuracy : 83.04\n",
            "iteration : 300, loss : 0.5221, accuracy : 83.37\n",
            "iteration : 350, loss : 0.5173, accuracy : 83.54\n",
            "Epoch :   2, training loss : 0.5146, training accuracy : 83.64, test loss : 0.6243, test accuracy : 79.92\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.4377, accuracy : 86.19\n",
            "iteration : 100, loss : 0.4487, accuracy : 85.96\n",
            "iteration : 150, loss : 0.4413, accuracy : 86.22\n",
            "iteration : 200, loss : 0.4343, accuracy : 86.52\n",
            "iteration : 250, loss : 0.4343, accuracy : 86.54\n",
            "iteration : 300, loss : 0.4309, accuracy : 86.67\n",
            "iteration : 350, loss : 0.4277, accuracy : 86.78\n",
            "Epoch :   3, training loss : 0.4271, training accuracy : 86.80, test loss : 0.4590, test accuracy : 86.05\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3978, accuracy : 88.00\n",
            "iteration : 100, loss : 0.3979, accuracy : 88.01\n",
            "iteration : 150, loss : 0.3946, accuracy : 87.94\n",
            "iteration : 200, loss : 0.4003, accuracy : 87.84\n",
            "iteration : 250, loss : 0.3960, accuracy : 87.95\n",
            "iteration : 300, loss : 0.3947, accuracy : 87.98\n",
            "iteration : 350, loss : 0.3916, accuracy : 88.11\n",
            "Epoch :   4, training loss : 0.3912, training accuracy : 88.12, test loss : 0.4804, test accuracy : 85.25\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.3649, accuracy : 88.64\n",
            "iteration : 100, loss : 0.3662, accuracy : 88.77\n",
            "iteration : 150, loss : 0.3699, accuracy : 88.70\n",
            "iteration : 200, loss : 0.3744, accuracy : 88.54\n",
            "iteration : 250, loss : 0.3724, accuracy : 88.69\n",
            "iteration : 300, loss : 0.3678, accuracy : 88.80\n",
            "iteration : 350, loss : 0.3673, accuracy : 88.85\n",
            "Epoch :   5, training loss : 0.3667, training accuracy : 88.89, test loss : 0.4275, test accuracy : 87.03\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.3403, accuracy : 89.92\n",
            "iteration : 100, loss : 0.3489, accuracy : 89.38\n",
            "iteration : 150, loss : 0.3532, accuracy : 89.30\n",
            "iteration : 200, loss : 0.3491, accuracy : 89.34\n",
            "iteration : 250, loss : 0.3516, accuracy : 89.31\n",
            "iteration : 300, loss : 0.3491, accuracy : 89.38\n",
            "iteration : 350, loss : 0.3470, accuracy : 89.47\n",
            "Epoch :   6, training loss : 0.3449, training accuracy : 89.55, test loss : 0.4091, test accuracy : 87.61\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.3186, accuracy : 90.02\n",
            "iteration : 100, loss : 0.3281, accuracy : 89.88\n",
            "iteration : 150, loss : 0.3321, accuracy : 89.72\n",
            "iteration : 200, loss : 0.3303, accuracy : 89.81\n",
            "iteration : 250, loss : 0.3278, accuracy : 89.89\n",
            "iteration : 300, loss : 0.3283, accuracy : 89.86\n",
            "iteration : 350, loss : 0.3276, accuracy : 89.95\n",
            "Epoch :   7, training loss : 0.3272, training accuracy : 89.99, test loss : 0.3809, test accuracy : 88.40\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.3123, accuracy : 90.56\n",
            "iteration : 100, loss : 0.3117, accuracy : 90.70\n",
            "iteration : 150, loss : 0.3172, accuracy : 90.61\n",
            "iteration : 200, loss : 0.3135, accuracy : 90.81\n",
            "iteration : 250, loss : 0.3153, accuracy : 90.79\n",
            "iteration : 300, loss : 0.3193, accuracy : 90.65\n",
            "iteration : 350, loss : 0.3172, accuracy : 90.68\n",
            "Epoch :   8, training loss : 0.3161, training accuracy : 90.70, test loss : 0.3557, test accuracy : 89.32\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.3133, accuracy : 91.05\n",
            "iteration : 100, loss : 0.3086, accuracy : 91.01\n",
            "iteration : 150, loss : 0.3074, accuracy : 90.72\n",
            "iteration : 200, loss : 0.3096, accuracy : 90.69\n",
            "iteration : 250, loss : 0.3081, accuracy : 90.71\n",
            "iteration : 300, loss : 0.3065, accuracy : 90.84\n",
            "iteration : 350, loss : 0.3048, accuracy : 90.91\n",
            "Epoch :   9, training loss : 0.3041, training accuracy : 90.96, test loss : 0.3353, test accuracy : 89.87\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2869, accuracy : 91.39\n",
            "iteration : 100, loss : 0.2918, accuracy : 91.27\n",
            "iteration : 150, loss : 0.2909, accuracy : 91.43\n",
            "iteration : 200, loss : 0.2904, accuracy : 91.44\n",
            "iteration : 250, loss : 0.2893, accuracy : 91.45\n",
            "iteration : 300, loss : 0.2912, accuracy : 91.31\n",
            "iteration : 350, loss : 0.2937, accuracy : 91.35\n",
            "Epoch :  10, training loss : 0.2937, training accuracy : 91.36, test loss : 0.3060, test accuracy : 90.96\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.2738, accuracy : 91.66\n",
            "iteration : 100, loss : 0.2820, accuracy : 91.62\n",
            "iteration : 150, loss : 0.2757, accuracy : 91.77\n",
            "iteration : 200, loss : 0.2783, accuracy : 91.69\n",
            "iteration : 250, loss : 0.2827, accuracy : 91.51\n",
            "iteration : 300, loss : 0.2801, accuracy : 91.54\n",
            "iteration : 350, loss : 0.2813, accuracy : 91.57\n",
            "Epoch :  11, training loss : 0.2816, training accuracy : 91.56, test loss : 0.3284, test accuracy : 90.20\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.2618, accuracy : 91.92\n",
            "iteration : 100, loss : 0.2702, accuracy : 91.91\n",
            "iteration : 150, loss : 0.2847, accuracy : 91.50\n",
            "iteration : 200, loss : 0.2817, accuracy : 91.67\n",
            "iteration : 250, loss : 0.2790, accuracy : 91.74\n",
            "iteration : 300, loss : 0.2774, accuracy : 91.72\n",
            "iteration : 350, loss : 0.2752, accuracy : 91.72\n",
            "Epoch :  12, training loss : 0.2770, training accuracy : 91.69, test loss : 0.2955, test accuracy : 91.43\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.2689, accuracy : 92.06\n",
            "iteration : 100, loss : 0.2749, accuracy : 91.88\n",
            "iteration : 150, loss : 0.2778, accuracy : 91.89\n",
            "iteration : 200, loss : 0.2757, accuracy : 91.94\n",
            "iteration : 250, loss : 0.2719, accuracy : 92.01\n",
            "iteration : 300, loss : 0.2709, accuracy : 92.05\n",
            "iteration : 350, loss : 0.2706, accuracy : 92.03\n",
            "Epoch :  13, training loss : 0.2688, training accuracy : 92.08, test loss : 0.2818, test accuracy : 91.59\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.2514, accuracy : 92.67\n",
            "iteration : 100, loss : 0.2629, accuracy : 92.16\n",
            "iteration : 150, loss : 0.2617, accuracy : 92.25\n",
            "iteration : 200, loss : 0.2620, accuracy : 92.23\n",
            "iteration : 250, loss : 0.2625, accuracy : 92.19\n",
            "iteration : 300, loss : 0.2638, accuracy : 92.12\n",
            "iteration : 350, loss : 0.2641, accuracy : 92.14\n",
            "Epoch :  14, training loss : 0.2642, training accuracy : 92.15, test loss : 0.3089, test accuracy : 91.10\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.2259, accuracy : 93.33\n",
            "iteration : 100, loss : 0.2353, accuracy : 93.21\n",
            "iteration : 150, loss : 0.2351, accuracy : 93.20\n",
            "iteration : 200, loss : 0.2463, accuracy : 92.84\n",
            "iteration : 250, loss : 0.2470, accuracy : 92.82\n",
            "iteration : 300, loss : 0.2500, accuracy : 92.72\n",
            "iteration : 350, loss : 0.2510, accuracy : 92.73\n",
            "Epoch :  15, training loss : 0.2538, training accuracy : 92.66, test loss : 0.3016, test accuracy : 91.30\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.2467, accuracy : 92.97\n",
            "iteration : 100, loss : 0.2483, accuracy : 92.77\n",
            "iteration : 150, loss : 0.2474, accuracy : 92.64\n",
            "iteration : 200, loss : 0.2485, accuracy : 92.63\n",
            "iteration : 250, loss : 0.2495, accuracy : 92.62\n",
            "iteration : 300, loss : 0.2496, accuracy : 92.65\n",
            "iteration : 350, loss : 0.2505, accuracy : 92.63\n",
            "Epoch :  16, training loss : 0.2503, training accuracy : 92.62, test loss : 0.2874, test accuracy : 91.72\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.2546, accuracy : 92.89\n",
            "iteration : 100, loss : 0.2470, accuracy : 93.08\n",
            "iteration : 150, loss : 0.2447, accuracy : 92.94\n",
            "iteration : 200, loss : 0.2429, accuracy : 92.95\n",
            "iteration : 250, loss : 0.2441, accuracy : 92.83\n",
            "iteration : 300, loss : 0.2457, accuracy : 92.78\n",
            "iteration : 350, loss : 0.2463, accuracy : 92.81\n",
            "Epoch :  17, training loss : 0.2471, training accuracy : 92.80, test loss : 0.3041, test accuracy : 91.10\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.2355, accuracy : 93.34\n",
            "iteration : 100, loss : 0.2449, accuracy : 93.07\n",
            "iteration : 150, loss : 0.2388, accuracy : 93.18\n",
            "iteration : 200, loss : 0.2379, accuracy : 93.19\n",
            "iteration : 250, loss : 0.2375, accuracy : 93.19\n",
            "iteration : 300, loss : 0.2380, accuracy : 93.14\n",
            "iteration : 350, loss : 0.2382, accuracy : 93.14\n",
            "Epoch :  18, training loss : 0.2385, training accuracy : 93.10, test loss : 0.2781, test accuracy : 91.83\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.2378, accuracy : 93.31\n",
            "iteration : 100, loss : 0.2319, accuracy : 93.34\n",
            "iteration : 150, loss : 0.2358, accuracy : 93.16\n",
            "iteration : 200, loss : 0.2356, accuracy : 93.18\n",
            "iteration : 250, loss : 0.2347, accuracy : 93.18\n",
            "iteration : 300, loss : 0.2362, accuracy : 93.16\n",
            "iteration : 350, loss : 0.2388, accuracy : 93.10\n",
            "Epoch :  19, training loss : 0.2399, training accuracy : 93.09, test loss : 0.2855, test accuracy : 91.69\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.2177, accuracy : 93.64\n",
            "iteration : 100, loss : 0.2272, accuracy : 93.32\n",
            "iteration : 150, loss : 0.2283, accuracy : 93.15\n",
            "iteration : 200, loss : 0.2254, accuracy : 93.23\n",
            "iteration : 250, loss : 0.2272, accuracy : 93.20\n",
            "iteration : 300, loss : 0.2289, accuracy : 93.19\n",
            "iteration : 350, loss : 0.2313, accuracy : 93.13\n",
            "Epoch :  20, training loss : 0.2323, training accuracy : 93.12, test loss : 0.2775, test accuracy : 91.82\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.2273, accuracy : 93.27\n",
            "iteration : 100, loss : 0.2287, accuracy : 93.32\n",
            "iteration : 150, loss : 0.2259, accuracy : 93.44\n",
            "iteration : 200, loss : 0.2262, accuracy : 93.31\n",
            "iteration : 250, loss : 0.2247, accuracy : 93.38\n",
            "iteration : 300, loss : 0.2270, accuracy : 93.30\n",
            "iteration : 350, loss : 0.2270, accuracy : 93.28\n",
            "Epoch :  21, training loss : 0.2279, training accuracy : 93.28, test loss : 0.2592, test accuracy : 92.55\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.2140, accuracy : 93.73\n",
            "iteration : 100, loss : 0.2195, accuracy : 93.52\n",
            "iteration : 150, loss : 0.2236, accuracy : 93.42\n",
            "iteration : 200, loss : 0.2273, accuracy : 93.34\n",
            "iteration : 250, loss : 0.2234, accuracy : 93.44\n",
            "iteration : 300, loss : 0.2244, accuracy : 93.41\n",
            "iteration : 350, loss : 0.2252, accuracy : 93.40\n",
            "Epoch :  22, training loss : 0.2257, training accuracy : 93.38, test loss : 0.2489, test accuracy : 92.83\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.2187, accuracy : 93.91\n",
            "iteration : 100, loss : 0.2228, accuracy : 93.63\n",
            "iteration : 150, loss : 0.2237, accuracy : 93.57\n",
            "iteration : 200, loss : 0.2236, accuracy : 93.61\n",
            "iteration : 250, loss : 0.2211, accuracy : 93.63\n",
            "iteration : 300, loss : 0.2240, accuracy : 93.53\n",
            "iteration : 350, loss : 0.2247, accuracy : 93.49\n",
            "Epoch :  23, training loss : 0.2250, training accuracy : 93.47, test loss : 0.2645, test accuracy : 92.39\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.2055, accuracy : 94.34\n",
            "iteration : 100, loss : 0.2172, accuracy : 93.82\n",
            "iteration : 150, loss : 0.2147, accuracy : 93.88\n",
            "iteration : 200, loss : 0.2153, accuracy : 93.78\n",
            "iteration : 250, loss : 0.2149, accuracy : 93.85\n",
            "iteration : 300, loss : 0.2145, accuracy : 93.82\n",
            "iteration : 350, loss : 0.2166, accuracy : 93.73\n",
            "Epoch :  24, training loss : 0.2194, training accuracy : 93.68, test loss : 0.2969, test accuracy : 91.19\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.1989, accuracy : 93.86\n",
            "iteration : 100, loss : 0.2044, accuracy : 93.86\n",
            "iteration : 150, loss : 0.2090, accuracy : 93.72\n",
            "iteration : 200, loss : 0.2122, accuracy : 93.71\n",
            "iteration : 250, loss : 0.2140, accuracy : 93.65\n",
            "iteration : 300, loss : 0.2174, accuracy : 93.60\n",
            "iteration : 350, loss : 0.2171, accuracy : 93.63\n",
            "Epoch :  25, training loss : 0.2184, training accuracy : 93.60, test loss : 0.2676, test accuracy : 92.22\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.2169, accuracy : 93.94\n",
            "iteration : 100, loss : 0.2100, accuracy : 93.84\n",
            "iteration : 150, loss : 0.2107, accuracy : 93.76\n",
            "iteration : 200, loss : 0.2103, accuracy : 93.82\n",
            "iteration : 250, loss : 0.2097, accuracy : 93.88\n",
            "iteration : 300, loss : 0.2117, accuracy : 93.78\n",
            "iteration : 350, loss : 0.2150, accuracy : 93.69\n",
            "Epoch :  26, training loss : 0.2156, training accuracy : 93.65, test loss : 0.2462, test accuracy : 92.82\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.1829, accuracy : 94.72\n",
            "iteration : 100, loss : 0.2084, accuracy : 94.09\n",
            "iteration : 150, loss : 0.2123, accuracy : 94.04\n",
            "iteration : 200, loss : 0.2156, accuracy : 93.96\n",
            "iteration : 250, loss : 0.2153, accuracy : 93.89\n",
            "iteration : 300, loss : 0.2135, accuracy : 93.97\n",
            "iteration : 350, loss : 0.2141, accuracy : 93.91\n",
            "Epoch :  27, training loss : 0.2141, training accuracy : 93.93, test loss : 0.2657, test accuracy : 92.24\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.2074, accuracy : 93.86\n",
            "iteration : 100, loss : 0.2057, accuracy : 94.02\n",
            "iteration : 150, loss : 0.2074, accuracy : 94.03\n",
            "iteration : 200, loss : 0.2111, accuracy : 93.87\n",
            "iteration : 250, loss : 0.2117, accuracy : 93.90\n",
            "iteration : 300, loss : 0.2126, accuracy : 93.85\n",
            "iteration : 350, loss : 0.2110, accuracy : 93.88\n",
            "Epoch :  28, training loss : 0.2112, training accuracy : 93.88, test loss : 0.2502, test accuracy : 92.82\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.1859, accuracy : 94.36\n",
            "iteration : 100, loss : 0.2048, accuracy : 93.96\n",
            "iteration : 150, loss : 0.2121, accuracy : 93.87\n",
            "iteration : 200, loss : 0.2167, accuracy : 93.76\n",
            "iteration : 250, loss : 0.2145, accuracy : 93.75\n",
            "iteration : 300, loss : 0.2144, accuracy : 93.76\n",
            "iteration : 350, loss : 0.2135, accuracy : 93.80\n",
            "Epoch :  29, training loss : 0.2124, training accuracy : 93.83, test loss : 0.2519, test accuracy : 92.71\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.1913, accuracy : 94.56\n",
            "iteration : 100, loss : 0.1957, accuracy : 94.55\n",
            "iteration : 150, loss : 0.1939, accuracy : 94.52\n",
            "iteration : 200, loss : 0.1986, accuracy : 94.34\n",
            "iteration : 250, loss : 0.1991, accuracy : 94.32\n",
            "iteration : 300, loss : 0.2036, accuracy : 94.18\n",
            "iteration : 350, loss : 0.2079, accuracy : 94.04\n",
            "Epoch :  30, training loss : 0.2060, training accuracy : 94.10, test loss : 0.2460, test accuracy : 92.94\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.2008, accuracy : 94.41\n",
            "iteration : 100, loss : 0.1947, accuracy : 94.42\n",
            "iteration : 150, loss : 0.1944, accuracy : 94.48\n",
            "iteration : 200, loss : 0.1949, accuracy : 94.46\n",
            "iteration : 250, loss : 0.2008, accuracy : 94.28\n",
            "iteration : 300, loss : 0.2035, accuracy : 94.18\n",
            "iteration : 350, loss : 0.2058, accuracy : 94.14\n",
            "Epoch :  31, training loss : 0.2047, training accuracy : 94.15, test loss : 0.2657, test accuracy : 92.41\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.1985, accuracy : 94.38\n",
            "iteration : 100, loss : 0.2023, accuracy : 94.30\n",
            "iteration : 150, loss : 0.2001, accuracy : 94.25\n",
            "iteration : 200, loss : 0.1971, accuracy : 94.26\n",
            "iteration : 250, loss : 0.2038, accuracy : 94.07\n",
            "iteration : 300, loss : 0.2017, accuracy : 94.11\n",
            "iteration : 350, loss : 0.2034, accuracy : 94.04\n",
            "Epoch :  32, training loss : 0.2037, training accuracy : 94.03, test loss : 0.2461, test accuracy : 92.89\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.1897, accuracy : 94.59\n",
            "iteration : 100, loss : 0.1922, accuracy : 94.41\n",
            "iteration : 150, loss : 0.1938, accuracy : 94.43\n",
            "iteration : 200, loss : 0.1968, accuracy : 94.37\n",
            "iteration : 250, loss : 0.1998, accuracy : 94.32\n",
            "iteration : 300, loss : 0.2024, accuracy : 94.21\n",
            "iteration : 350, loss : 0.2021, accuracy : 94.16\n",
            "Epoch :  33, training loss : 0.2035, training accuracy : 94.13, test loss : 0.2656, test accuracy : 92.42\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.1932, accuracy : 94.34\n",
            "iteration : 100, loss : 0.1966, accuracy : 94.36\n",
            "iteration : 150, loss : 0.2005, accuracy : 94.37\n",
            "iteration : 200, loss : 0.2010, accuracy : 94.34\n",
            "iteration : 250, loss : 0.1983, accuracy : 94.35\n",
            "iteration : 300, loss : 0.2013, accuracy : 94.23\n",
            "iteration : 350, loss : 0.2021, accuracy : 94.18\n",
            "Epoch :  34, training loss : 0.2025, training accuracy : 94.16, test loss : 0.2666, test accuracy : 92.20\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.2012, accuracy : 94.34\n",
            "iteration : 100, loss : 0.1938, accuracy : 94.34\n",
            "iteration : 150, loss : 0.1950, accuracy : 94.37\n",
            "iteration : 200, loss : 0.1945, accuracy : 94.36\n",
            "iteration : 250, loss : 0.1964, accuracy : 94.30\n",
            "iteration : 300, loss : 0.1946, accuracy : 94.36\n",
            "iteration : 350, loss : 0.1971, accuracy : 94.31\n",
            "Epoch :  35, training loss : 0.1971, training accuracy : 94.31, test loss : 0.2784, test accuracy : 92.03\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.1974, accuracy : 94.30\n",
            "iteration : 100, loss : 0.1902, accuracy : 94.39\n",
            "iteration : 150, loss : 0.1948, accuracy : 94.33\n",
            "iteration : 200, loss : 0.1967, accuracy : 94.29\n",
            "iteration : 250, loss : 0.1989, accuracy : 94.20\n",
            "iteration : 300, loss : 0.2005, accuracy : 94.20\n",
            "iteration : 350, loss : 0.2006, accuracy : 94.18\n",
            "Epoch :  36, training loss : 0.1986, training accuracy : 94.21, test loss : 0.2499, test accuracy : 92.72\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.2095, accuracy : 94.08\n",
            "iteration : 100, loss : 0.2079, accuracy : 94.11\n",
            "iteration : 150, loss : 0.2023, accuracy : 94.18\n",
            "iteration : 200, loss : 0.1986, accuracy : 94.24\n",
            "iteration : 250, loss : 0.1990, accuracy : 94.23\n",
            "iteration : 300, loss : 0.1960, accuracy : 94.28\n",
            "iteration : 350, loss : 0.1974, accuracy : 94.19\n",
            "Epoch :  37, training loss : 0.1974, training accuracy : 94.20, test loss : 0.2481, test accuracy : 92.87\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.1979, accuracy : 94.44\n",
            "iteration : 100, loss : 0.1955, accuracy : 94.33\n",
            "iteration : 150, loss : 0.1928, accuracy : 94.46\n",
            "iteration : 200, loss : 0.1962, accuracy : 94.40\n",
            "iteration : 250, loss : 0.1970, accuracy : 94.31\n",
            "iteration : 300, loss : 0.1971, accuracy : 94.28\n",
            "iteration : 350, loss : 0.1988, accuracy : 94.25\n",
            "Epoch :  38, training loss : 0.1982, training accuracy : 94.26, test loss : 0.2449, test accuracy : 92.96\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.1837, accuracy : 94.95\n",
            "iteration : 100, loss : 0.1851, accuracy : 94.69\n",
            "iteration : 150, loss : 0.1872, accuracy : 94.61\n",
            "iteration : 200, loss : 0.1887, accuracy : 94.61\n",
            "iteration : 250, loss : 0.1901, accuracy : 94.55\n",
            "iteration : 300, loss : 0.1911, accuracy : 94.51\n",
            "iteration : 350, loss : 0.1912, accuracy : 94.49\n",
            "Epoch :  39, training loss : 0.1905, training accuracy : 94.53, test loss : 0.2487, test accuracy : 92.84\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.1739, accuracy : 95.09\n",
            "iteration : 100, loss : 0.1794, accuracy : 94.86\n",
            "iteration : 150, loss : 0.1825, accuracy : 94.74\n",
            "iteration : 200, loss : 0.1885, accuracy : 94.59\n",
            "iteration : 250, loss : 0.1918, accuracy : 94.54\n",
            "iteration : 300, loss : 0.1894, accuracy : 94.61\n",
            "iteration : 350, loss : 0.1957, accuracy : 94.42\n",
            "Epoch :  40, training loss : 0.1952, training accuracy : 94.46, test loss : 0.2409, test accuracy : 93.14\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.1725, accuracy : 94.94\n",
            "iteration : 100, loss : 0.1777, accuracy : 94.95\n",
            "iteration : 150, loss : 0.1768, accuracy : 94.89\n",
            "iteration : 200, loss : 0.1844, accuracy : 94.70\n",
            "iteration : 250, loss : 0.1877, accuracy : 94.61\n",
            "iteration : 300, loss : 0.1906, accuracy : 94.51\n",
            "iteration : 350, loss : 0.1915, accuracy : 94.50\n",
            "Epoch :  41, training loss : 0.1918, training accuracy : 94.50, test loss : 0.2354, test accuracy : 93.33\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.1809, accuracy : 94.95\n",
            "iteration : 100, loss : 0.1825, accuracy : 94.80\n",
            "iteration : 150, loss : 0.1784, accuracy : 94.87\n",
            "iteration : 200, loss : 0.1836, accuracy : 94.68\n",
            "iteration : 250, loss : 0.1847, accuracy : 94.68\n",
            "iteration : 300, loss : 0.1877, accuracy : 94.63\n",
            "iteration : 350, loss : 0.1883, accuracy : 94.59\n",
            "Epoch :  42, training loss : 0.1883, training accuracy : 94.57, test loss : 0.2476, test accuracy : 92.99\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.1771, accuracy : 95.05\n",
            "iteration : 100, loss : 0.1871, accuracy : 94.56\n",
            "iteration : 150, loss : 0.1866, accuracy : 94.47\n",
            "iteration : 200, loss : 0.1882, accuracy : 94.44\n",
            "iteration : 250, loss : 0.1870, accuracy : 94.45\n",
            "iteration : 300, loss : 0.1886, accuracy : 94.42\n",
            "iteration : 350, loss : 0.1900, accuracy : 94.42\n",
            "Epoch :  43, training loss : 0.1907, training accuracy : 94.41, test loss : 0.2477, test accuracy : 93.01\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.1724, accuracy : 94.73\n",
            "iteration : 100, loss : 0.1741, accuracy : 94.89\n",
            "iteration : 150, loss : 0.1732, accuracy : 94.95\n",
            "iteration : 200, loss : 0.1813, accuracy : 94.79\n",
            "iteration : 250, loss : 0.1866, accuracy : 94.67\n",
            "iteration : 300, loss : 0.1875, accuracy : 94.64\n",
            "iteration : 350, loss : 0.1884, accuracy : 94.58\n",
            "Epoch :  44, training loss : 0.1881, training accuracy : 94.58, test loss : 0.2472, test accuracy : 93.03\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.1746, accuracy : 94.84\n",
            "iteration : 100, loss : 0.1873, accuracy : 94.45\n",
            "iteration : 150, loss : 0.1854, accuracy : 94.52\n",
            "iteration : 200, loss : 0.1853, accuracy : 94.57\n",
            "iteration : 250, loss : 0.1854, accuracy : 94.58\n",
            "iteration : 300, loss : 0.1859, accuracy : 94.58\n",
            "iteration : 350, loss : 0.1868, accuracy : 94.58\n",
            "Epoch :  45, training loss : 0.1875, training accuracy : 94.53, test loss : 0.2396, test accuracy : 93.22\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.1827, accuracy : 94.86\n",
            "iteration : 100, loss : 0.1774, accuracy : 94.91\n",
            "iteration : 150, loss : 0.1784, accuracy : 94.88\n",
            "iteration : 200, loss : 0.1817, accuracy : 94.81\n",
            "iteration : 250, loss : 0.1864, accuracy : 94.68\n",
            "iteration : 300, loss : 0.1884, accuracy : 94.63\n",
            "iteration : 350, loss : 0.1876, accuracy : 94.66\n",
            "Epoch :  46, training loss : 0.1876, training accuracy : 94.62, test loss : 0.2466, test accuracy : 92.97\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.1774, accuracy : 95.00\n",
            "iteration : 100, loss : 0.1757, accuracy : 94.91\n",
            "iteration : 150, loss : 0.1775, accuracy : 94.94\n",
            "iteration : 200, loss : 0.1794, accuracy : 94.94\n",
            "iteration : 250, loss : 0.1829, accuracy : 94.83\n",
            "iteration : 300, loss : 0.1834, accuracy : 94.79\n",
            "iteration : 350, loss : 0.1854, accuracy : 94.76\n",
            "Epoch :  47, training loss : 0.1849, training accuracy : 94.77, test loss : 0.2384, test accuracy : 93.29\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.1774, accuracy : 95.00\n",
            "iteration : 100, loss : 0.1690, accuracy : 95.27\n",
            "iteration : 150, loss : 0.1791, accuracy : 94.95\n",
            "iteration : 200, loss : 0.1831, accuracy : 94.78\n",
            "iteration : 250, loss : 0.1859, accuracy : 94.67\n",
            "iteration : 300, loss : 0.1874, accuracy : 94.62\n",
            "iteration : 350, loss : 0.1872, accuracy : 94.65\n",
            "Epoch :  48, training loss : 0.1866, training accuracy : 94.67, test loss : 0.2449, test accuracy : 93.04\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.1741, accuracy : 94.92\n",
            "iteration : 100, loss : 0.1778, accuracy : 94.71\n",
            "iteration : 150, loss : 0.1777, accuracy : 94.77\n",
            "iteration : 200, loss : 0.1779, accuracy : 94.83\n",
            "iteration : 250, loss : 0.1814, accuracy : 94.77\n",
            "iteration : 300, loss : 0.1870, accuracy : 94.65\n",
            "iteration : 350, loss : 0.1874, accuracy : 94.64\n",
            "Epoch :  49, training loss : 0.1872, training accuracy : 94.65, test loss : 0.2343, test accuracy : 93.26\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.1818, accuracy : 94.83\n",
            "iteration : 100, loss : 0.1791, accuracy : 94.86\n",
            "iteration : 150, loss : 0.1790, accuracy : 94.83\n",
            "iteration : 200, loss : 0.1814, accuracy : 94.71\n",
            "iteration : 250, loss : 0.1839, accuracy : 94.68\n",
            "iteration : 300, loss : 0.1850, accuracy : 94.67\n",
            "iteration : 350, loss : 0.1841, accuracy : 94.67\n",
            "Epoch :  50, training loss : 0.1836, training accuracy : 94.68, test loss : 0.2599, test accuracy : 92.66\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.1668, accuracy : 95.41\n",
            "iteration : 100, loss : 0.1727, accuracy : 95.21\n",
            "iteration : 150, loss : 0.1763, accuracy : 95.04\n",
            "iteration : 200, loss : 0.1804, accuracy : 94.94\n",
            "iteration : 250, loss : 0.1830, accuracy : 94.82\n",
            "iteration : 300, loss : 0.1820, accuracy : 94.79\n",
            "iteration : 350, loss : 0.1814, accuracy : 94.77\n",
            "Epoch :  51, training loss : 0.1822, training accuracy : 94.74, test loss : 0.2622, test accuracy : 92.59\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.1768, accuracy : 94.84\n",
            "iteration : 100, loss : 0.1800, accuracy : 94.89\n",
            "iteration : 150, loss : 0.1800, accuracy : 94.92\n",
            "iteration : 200, loss : 0.1827, accuracy : 94.73\n",
            "iteration : 250, loss : 0.1819, accuracy : 94.75\n",
            "iteration : 300, loss : 0.1817, accuracy : 94.75\n",
            "iteration : 350, loss : 0.1828, accuracy : 94.70\n",
            "Epoch :  52, training loss : 0.1832, training accuracy : 94.71, test loss : 0.2333, test accuracy : 93.41\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.1649, accuracy : 95.36\n",
            "iteration : 100, loss : 0.1685, accuracy : 95.27\n",
            "iteration : 150, loss : 0.1696, accuracy : 95.24\n",
            "iteration : 200, loss : 0.1744, accuracy : 95.14\n",
            "iteration : 250, loss : 0.1793, accuracy : 94.93\n",
            "iteration : 300, loss : 0.1763, accuracy : 94.99\n",
            "iteration : 350, loss : 0.1789, accuracy : 94.93\n",
            "Epoch :  53, training loss : 0.1783, training accuracy : 94.96, test loss : 0.2362, test accuracy : 93.12\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.1693, accuracy : 95.27\n",
            "iteration : 100, loss : 0.1812, accuracy : 94.95\n",
            "iteration : 150, loss : 0.1848, accuracy : 94.77\n",
            "iteration : 200, loss : 0.1837, accuracy : 94.75\n",
            "iteration : 250, loss : 0.1848, accuracy : 94.72\n",
            "iteration : 300, loss : 0.1845, accuracy : 94.72\n",
            "iteration : 350, loss : 0.1842, accuracy : 94.69\n",
            "Epoch :  54, training loss : 0.1837, training accuracy : 94.68, test loss : 0.2403, test accuracy : 93.28\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.1654, accuracy : 95.27\n",
            "iteration : 100, loss : 0.1746, accuracy : 95.11\n",
            "iteration : 150, loss : 0.1708, accuracy : 95.09\n",
            "iteration : 200, loss : 0.1737, accuracy : 95.00\n",
            "iteration : 250, loss : 0.1779, accuracy : 94.88\n",
            "iteration : 300, loss : 0.1780, accuracy : 94.88\n",
            "iteration : 350, loss : 0.1789, accuracy : 94.88\n",
            "Epoch :  55, training loss : 0.1781, training accuracy : 94.88, test loss : 0.2526, test accuracy : 92.85\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.1678, accuracy : 95.09\n",
            "iteration : 100, loss : 0.1688, accuracy : 95.08\n",
            "iteration : 150, loss : 0.1696, accuracy : 95.09\n",
            "iteration : 200, loss : 0.1734, accuracy : 94.94\n",
            "iteration : 250, loss : 0.1776, accuracy : 94.86\n",
            "iteration : 300, loss : 0.1801, accuracy : 94.79\n",
            "iteration : 350, loss : 0.1830, accuracy : 94.71\n",
            "Epoch :  56, training loss : 0.1832, training accuracy : 94.71, test loss : 0.2374, test accuracy : 93.31\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.1630, accuracy : 95.20\n",
            "iteration : 100, loss : 0.1679, accuracy : 95.06\n",
            "iteration : 150, loss : 0.1706, accuracy : 95.03\n",
            "iteration : 200, loss : 0.1735, accuracy : 94.89\n",
            "iteration : 250, loss : 0.1785, accuracy : 94.76\n",
            "iteration : 300, loss : 0.1796, accuracy : 94.77\n",
            "iteration : 350, loss : 0.1777, accuracy : 94.83\n",
            "Epoch :  57, training loss : 0.1778, training accuracy : 94.83, test loss : 0.2335, test accuracy : 93.42\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.1764, accuracy : 94.77\n",
            "iteration : 100, loss : 0.1679, accuracy : 95.03\n",
            "iteration : 150, loss : 0.1723, accuracy : 94.98\n",
            "iteration : 200, loss : 0.1739, accuracy : 94.91\n",
            "iteration : 250, loss : 0.1759, accuracy : 94.91\n",
            "iteration : 300, loss : 0.1742, accuracy : 95.00\n",
            "iteration : 350, loss : 0.1750, accuracy : 94.95\n",
            "Epoch :  58, training loss : 0.1764, training accuracy : 94.91, test loss : 0.2509, test accuracy : 93.00\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.1687, accuracy : 94.84\n",
            "iteration : 100, loss : 0.1611, accuracy : 95.20\n",
            "iteration : 150, loss : 0.1595, accuracy : 95.39\n",
            "iteration : 200, loss : 0.1679, accuracy : 95.15\n",
            "iteration : 250, loss : 0.1708, accuracy : 95.06\n",
            "iteration : 300, loss : 0.1714, accuracy : 95.02\n",
            "iteration : 350, loss : 0.1719, accuracy : 95.01\n",
            "Epoch :  59, training loss : 0.1740, training accuracy : 94.92, test loss : 0.2439, test accuracy : 93.09\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.1787, accuracy : 94.84\n",
            "iteration : 100, loss : 0.1821, accuracy : 94.74\n",
            "iteration : 150, loss : 0.1734, accuracy : 94.97\n",
            "iteration : 200, loss : 0.1771, accuracy : 94.87\n",
            "iteration : 250, loss : 0.1798, accuracy : 94.80\n",
            "iteration : 300, loss : 0.1789, accuracy : 94.88\n",
            "iteration : 350, loss : 0.1769, accuracy : 94.96\n",
            "Epoch :  60, training loss : 0.1764, training accuracy : 94.99, test loss : 0.2328, test accuracy : 93.52\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.1672, accuracy : 95.41\n",
            "iteration : 100, loss : 0.1648, accuracy : 95.35\n",
            "iteration : 150, loss : 0.1712, accuracy : 95.06\n",
            "iteration : 200, loss : 0.1728, accuracy : 95.03\n",
            "iteration : 250, loss : 0.1749, accuracy : 94.95\n",
            "iteration : 300, loss : 0.1747, accuracy : 94.94\n",
            "iteration : 350, loss : 0.1773, accuracy : 94.90\n",
            "Epoch :  61, training loss : 0.1755, training accuracy : 94.95, test loss : 0.2481, test accuracy : 93.01\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.1622, accuracy : 95.11\n",
            "iteration : 100, loss : 0.1595, accuracy : 95.26\n",
            "iteration : 150, loss : 0.1667, accuracy : 95.09\n",
            "iteration : 200, loss : 0.1751, accuracy : 94.89\n",
            "iteration : 250, loss : 0.1764, accuracy : 94.84\n",
            "iteration : 300, loss : 0.1780, accuracy : 94.84\n",
            "iteration : 350, loss : 0.1778, accuracy : 94.86\n",
            "Epoch :  62, training loss : 0.1768, training accuracy : 94.89, test loss : 0.2351, test accuracy : 93.37\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.1669, accuracy : 95.39\n",
            "iteration : 100, loss : 0.1674, accuracy : 95.20\n",
            "iteration : 150, loss : 0.1699, accuracy : 95.24\n",
            "iteration : 200, loss : 0.1718, accuracy : 95.22\n",
            "iteration : 250, loss : 0.1715, accuracy : 95.22\n",
            "iteration : 300, loss : 0.1728, accuracy : 95.18\n",
            "iteration : 350, loss : 0.1734, accuracy : 95.13\n",
            "Epoch :  63, training loss : 0.1729, training accuracy : 95.14, test loss : 0.2409, test accuracy : 93.21\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.1621, accuracy : 95.25\n",
            "iteration : 100, loss : 0.1638, accuracy : 95.10\n",
            "iteration : 150, loss : 0.1636, accuracy : 95.18\n",
            "iteration : 200, loss : 0.1650, accuracy : 95.17\n",
            "iteration : 250, loss : 0.1684, accuracy : 95.16\n",
            "iteration : 300, loss : 0.1676, accuracy : 95.16\n",
            "iteration : 350, loss : 0.1683, accuracy : 95.11\n",
            "Epoch :  64, training loss : 0.1693, training accuracy : 95.11, test loss : 0.2442, test accuracy : 92.99\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.1508, accuracy : 95.53\n",
            "iteration : 100, loss : 0.1574, accuracy : 95.45\n",
            "iteration : 150, loss : 0.1655, accuracy : 95.21\n",
            "iteration : 200, loss : 0.1683, accuracy : 95.17\n",
            "iteration : 250, loss : 0.1699, accuracy : 95.12\n",
            "iteration : 300, loss : 0.1703, accuracy : 95.14\n",
            "iteration : 350, loss : 0.1685, accuracy : 95.18\n",
            "Epoch :  65, training loss : 0.1699, training accuracy : 95.14, test loss : 0.2373, test accuracy : 93.39\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.1667, accuracy : 95.22\n",
            "iteration : 100, loss : 0.1751, accuracy : 94.96\n",
            "iteration : 150, loss : 0.1724, accuracy : 95.03\n",
            "iteration : 200, loss : 0.1743, accuracy : 94.98\n",
            "iteration : 250, loss : 0.1741, accuracy : 94.96\n",
            "iteration : 300, loss : 0.1741, accuracy : 94.97\n",
            "iteration : 350, loss : 0.1737, accuracy : 94.98\n",
            "Epoch :  66, training loss : 0.1739, training accuracy : 94.98, test loss : 0.2592, test accuracy : 92.60\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.1604, accuracy : 95.39\n",
            "iteration : 100, loss : 0.1621, accuracy : 95.38\n",
            "iteration : 150, loss : 0.1641, accuracy : 95.29\n",
            "iteration : 200, loss : 0.1680, accuracy : 95.23\n",
            "iteration : 250, loss : 0.1681, accuracy : 95.20\n",
            "iteration : 300, loss : 0.1684, accuracy : 95.18\n",
            "iteration : 350, loss : 0.1702, accuracy : 95.12\n",
            "Epoch :  67, training loss : 0.1708, training accuracy : 95.09, test loss : 0.2407, test accuracy : 93.18\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.1571, accuracy : 95.55\n",
            "iteration : 100, loss : 0.1601, accuracy : 95.55\n",
            "iteration : 150, loss : 0.1649, accuracy : 95.33\n",
            "iteration : 200, loss : 0.1655, accuracy : 95.33\n",
            "iteration : 250, loss : 0.1642, accuracy : 95.38\n",
            "iteration : 300, loss : 0.1651, accuracy : 95.36\n",
            "iteration : 350, loss : 0.1673, accuracy : 95.28\n",
            "Epoch :  68, training loss : 0.1682, training accuracy : 95.23, test loss : 0.2356, test accuracy : 93.35\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.1557, accuracy : 95.58\n",
            "iteration : 100, loss : 0.1633, accuracy : 95.20\n",
            "iteration : 150, loss : 0.1676, accuracy : 95.20\n",
            "iteration : 200, loss : 0.1685, accuracy : 95.18\n",
            "iteration : 250, loss : 0.1682, accuracy : 95.20\n",
            "iteration : 300, loss : 0.1681, accuracy : 95.19\n",
            "iteration : 350, loss : 0.1687, accuracy : 95.20\n",
            "Epoch :  69, training loss : 0.1690, training accuracy : 95.18, test loss : 0.2247, test accuracy : 93.68\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.1603, accuracy : 95.34\n",
            "iteration : 100, loss : 0.1637, accuracy : 95.19\n",
            "iteration : 150, loss : 0.1721, accuracy : 94.95\n",
            "iteration : 200, loss : 0.1691, accuracy : 94.99\n",
            "iteration : 250, loss : 0.1686, accuracy : 95.05\n",
            "iteration : 300, loss : 0.1699, accuracy : 95.03\n",
            "iteration : 350, loss : 0.1701, accuracy : 95.02\n",
            "Epoch :  70, training loss : 0.1705, training accuracy : 95.00, test loss : 0.2398, test accuracy : 93.28\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.1462, accuracy : 95.83\n",
            "iteration : 100, loss : 0.1534, accuracy : 95.68\n",
            "iteration : 150, loss : 0.1587, accuracy : 95.52\n",
            "iteration : 200, loss : 0.1601, accuracy : 95.44\n",
            "iteration : 250, loss : 0.1608, accuracy : 95.38\n",
            "iteration : 300, loss : 0.1651, accuracy : 95.22\n",
            "iteration : 350, loss : 0.1688, accuracy : 95.14\n",
            "Epoch :  71, training loss : 0.1687, training accuracy : 95.13, test loss : 0.2293, test accuracy : 93.52\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.1606, accuracy : 95.33\n",
            "iteration : 100, loss : 0.1618, accuracy : 95.35\n",
            "iteration : 150, loss : 0.1580, accuracy : 95.33\n",
            "iteration : 200, loss : 0.1624, accuracy : 95.19\n",
            "iteration : 250, loss : 0.1637, accuracy : 95.20\n",
            "iteration : 300, loss : 0.1658, accuracy : 95.13\n",
            "iteration : 350, loss : 0.1676, accuracy : 95.07\n",
            "Epoch :  72, training loss : 0.1672, training accuracy : 95.07, test loss : 0.2265, test accuracy : 93.66\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.1615, accuracy : 95.00\n",
            "iteration : 100, loss : 0.1617, accuracy : 95.12\n",
            "iteration : 150, loss : 0.1662, accuracy : 95.15\n",
            "iteration : 200, loss : 0.1618, accuracy : 95.24\n",
            "iteration : 250, loss : 0.1635, accuracy : 95.24\n",
            "iteration : 300, loss : 0.1644, accuracy : 95.26\n",
            "iteration : 350, loss : 0.1662, accuracy : 95.19\n",
            "Epoch :  73, training loss : 0.1659, training accuracy : 95.18, test loss : 0.2350, test accuracy : 93.52\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.1683, accuracy : 95.22\n",
            "iteration : 100, loss : 0.1612, accuracy : 95.34\n",
            "iteration : 150, loss : 0.1588, accuracy : 95.31\n",
            "iteration : 200, loss : 0.1556, accuracy : 95.47\n",
            "iteration : 250, loss : 0.1594, accuracy : 95.39\n",
            "iteration : 300, loss : 0.1609, accuracy : 95.37\n",
            "iteration : 350, loss : 0.1625, accuracy : 95.32\n",
            "Epoch :  74, training loss : 0.1628, training accuracy : 95.33, test loss : 0.2445, test accuracy : 93.12\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.1544, accuracy : 95.56\n",
            "iteration : 100, loss : 0.1587, accuracy : 95.45\n",
            "iteration : 150, loss : 0.1593, accuracy : 95.46\n",
            "iteration : 200, loss : 0.1598, accuracy : 95.47\n",
            "iteration : 250, loss : 0.1601, accuracy : 95.44\n",
            "iteration : 300, loss : 0.1635, accuracy : 95.30\n",
            "iteration : 350, loss : 0.1667, accuracy : 95.25\n",
            "Epoch :  75, training loss : 0.1659, training accuracy : 95.27, test loss : 0.2456, test accuracy : 92.99\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.1544, accuracy : 95.58\n",
            "iteration : 100, loss : 0.1570, accuracy : 95.47\n",
            "iteration : 150, loss : 0.1616, accuracy : 95.28\n",
            "iteration : 200, loss : 0.1611, accuracy : 95.26\n",
            "iteration : 250, loss : 0.1606, accuracy : 95.25\n",
            "iteration : 300, loss : 0.1609, accuracy : 95.26\n",
            "iteration : 350, loss : 0.1639, accuracy : 95.21\n",
            "Epoch :  76, training loss : 0.1636, training accuracy : 95.22, test loss : 0.2386, test accuracy : 93.27\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.1458, accuracy : 95.81\n",
            "iteration : 100, loss : 0.1525, accuracy : 95.54\n",
            "iteration : 150, loss : 0.1561, accuracy : 95.47\n",
            "iteration : 200, loss : 0.1541, accuracy : 95.53\n",
            "iteration : 250, loss : 0.1577, accuracy : 95.47\n",
            "iteration : 300, loss : 0.1600, accuracy : 95.41\n",
            "iteration : 350, loss : 0.1603, accuracy : 95.40\n",
            "Epoch :  77, training loss : 0.1614, training accuracy : 95.36, test loss : 0.2473, test accuracy : 93.15\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.1575, accuracy : 95.73\n",
            "iteration : 100, loss : 0.1610, accuracy : 95.56\n",
            "iteration : 150, loss : 0.1607, accuracy : 95.54\n",
            "iteration : 200, loss : 0.1635, accuracy : 95.41\n",
            "iteration : 250, loss : 0.1631, accuracy : 95.34\n",
            "iteration : 300, loss : 0.1622, accuracy : 95.33\n",
            "iteration : 350, loss : 0.1625, accuracy : 95.31\n",
            "Epoch :  78, training loss : 0.1624, training accuracy : 95.29, test loss : 0.2432, test accuracy : 93.39\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.1449, accuracy : 95.81\n",
            "iteration : 100, loss : 0.1591, accuracy : 95.55\n",
            "iteration : 150, loss : 0.1537, accuracy : 95.72\n",
            "iteration : 200, loss : 0.1575, accuracy : 95.55\n",
            "iteration : 250, loss : 0.1596, accuracy : 95.51\n",
            "iteration : 300, loss : 0.1598, accuracy : 95.50\n",
            "iteration : 350, loss : 0.1629, accuracy : 95.41\n",
            "Epoch :  79, training loss : 0.1630, training accuracy : 95.40, test loss : 0.2358, test accuracy : 93.45\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.1495, accuracy : 95.72\n",
            "iteration : 100, loss : 0.1545, accuracy : 95.68\n",
            "iteration : 150, loss : 0.1568, accuracy : 95.52\n",
            "iteration : 200, loss : 0.1574, accuracy : 95.47\n",
            "iteration : 250, loss : 0.1575, accuracy : 95.46\n",
            "iteration : 300, loss : 0.1593, accuracy : 95.39\n",
            "iteration : 350, loss : 0.1617, accuracy : 95.34\n",
            "Epoch :  80, training loss : 0.1627, training accuracy : 95.32, test loss : 0.2369, test accuracy : 93.32\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.1644, accuracy : 95.36\n",
            "iteration : 100, loss : 0.1553, accuracy : 95.64\n",
            "iteration : 150, loss : 0.1557, accuracy : 95.59\n",
            "iteration : 200, loss : 0.1569, accuracy : 95.50\n",
            "iteration : 250, loss : 0.1598, accuracy : 95.41\n",
            "iteration : 300, loss : 0.1590, accuracy : 95.45\n",
            "iteration : 350, loss : 0.1594, accuracy : 95.46\n",
            "Epoch :  81, training loss : 0.1592, training accuracy : 95.48, test loss : 0.2317, test accuracy : 93.44\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.1550, accuracy : 95.44\n",
            "iteration : 100, loss : 0.1572, accuracy : 95.54\n",
            "iteration : 150, loss : 0.1585, accuracy : 95.46\n",
            "iteration : 200, loss : 0.1557, accuracy : 95.53\n",
            "iteration : 250, loss : 0.1553, accuracy : 95.48\n",
            "iteration : 300, loss : 0.1555, accuracy : 95.49\n",
            "iteration : 350, loss : 0.1570, accuracy : 95.40\n",
            "Epoch :  82, training loss : 0.1587, training accuracy : 95.36, test loss : 0.2389, test accuracy : 93.38\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.1534, accuracy : 95.48\n",
            "iteration : 100, loss : 0.1534, accuracy : 95.66\n",
            "iteration : 150, loss : 0.1538, accuracy : 95.62\n",
            "iteration : 200, loss : 0.1541, accuracy : 95.61\n",
            "iteration : 250, loss : 0.1563, accuracy : 95.53\n",
            "iteration : 300, loss : 0.1585, accuracy : 95.47\n",
            "iteration : 350, loss : 0.1603, accuracy : 95.39\n",
            "Epoch :  83, training loss : 0.1624, training accuracy : 95.35, test loss : 0.2416, test accuracy : 93.46\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.1490, accuracy : 95.48\n",
            "iteration : 100, loss : 0.1471, accuracy : 95.55\n",
            "iteration : 150, loss : 0.1481, accuracy : 95.57\n",
            "iteration : 200, loss : 0.1542, accuracy : 95.49\n",
            "iteration : 250, loss : 0.1577, accuracy : 95.42\n",
            "iteration : 300, loss : 0.1570, accuracy : 95.47\n",
            "iteration : 350, loss : 0.1569, accuracy : 95.45\n",
            "Epoch :  84, training loss : 0.1576, training accuracy : 95.43, test loss : 0.2268, test accuracy : 93.60\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.1593, accuracy : 95.48\n",
            "iteration : 100, loss : 0.1543, accuracy : 95.60\n",
            "iteration : 150, loss : 0.1542, accuracy : 95.58\n",
            "iteration : 200, loss : 0.1585, accuracy : 95.41\n",
            "iteration : 250, loss : 0.1582, accuracy : 95.46\n",
            "iteration : 300, loss : 0.1584, accuracy : 95.46\n",
            "iteration : 350, loss : 0.1582, accuracy : 95.45\n",
            "Epoch :  85, training loss : 0.1586, training accuracy : 95.47, test loss : 0.2431, test accuracy : 93.42\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.1508, accuracy : 95.70\n",
            "iteration : 100, loss : 0.1374, accuracy : 96.09\n",
            "iteration : 150, loss : 0.1398, accuracy : 96.01\n",
            "iteration : 200, loss : 0.1468, accuracy : 95.81\n",
            "iteration : 250, loss : 0.1487, accuracy : 95.70\n",
            "iteration : 300, loss : 0.1543, accuracy : 95.53\n",
            "iteration : 350, loss : 0.1568, accuracy : 95.45\n",
            "Epoch :  86, training loss : 0.1579, training accuracy : 95.43, test loss : 0.2399, test accuracy : 93.35\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.1496, accuracy : 95.84\n",
            "iteration : 100, loss : 0.1428, accuracy : 95.94\n",
            "iteration : 150, loss : 0.1444, accuracy : 95.94\n",
            "iteration : 200, loss : 0.1458, accuracy : 95.89\n",
            "iteration : 250, loss : 0.1522, accuracy : 95.72\n",
            "iteration : 300, loss : 0.1537, accuracy : 95.63\n",
            "iteration : 350, loss : 0.1534, accuracy : 95.59\n",
            "Epoch :  87, training loss : 0.1542, training accuracy : 95.57, test loss : 0.2406, test accuracy : 93.32\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.1480, accuracy : 95.70\n",
            "iteration : 100, loss : 0.1516, accuracy : 95.61\n",
            "iteration : 150, loss : 0.1546, accuracy : 95.51\n",
            "iteration : 200, loss : 0.1520, accuracy : 95.55\n",
            "iteration : 250, loss : 0.1521, accuracy : 95.54\n",
            "iteration : 300, loss : 0.1535, accuracy : 95.47\n",
            "iteration : 350, loss : 0.1555, accuracy : 95.41\n",
            "Epoch :  88, training loss : 0.1552, training accuracy : 95.42, test loss : 0.2310, test accuracy : 93.71\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.1359, accuracy : 96.03\n",
            "iteration : 100, loss : 0.1408, accuracy : 95.91\n",
            "iteration : 150, loss : 0.1459, accuracy : 95.72\n",
            "iteration : 200, loss : 0.1497, accuracy : 95.66\n",
            "iteration : 250, loss : 0.1512, accuracy : 95.61\n",
            "iteration : 300, loss : 0.1519, accuracy : 95.55\n",
            "iteration : 350, loss : 0.1554, accuracy : 95.47\n",
            "Epoch :  89, training loss : 0.1563, training accuracy : 95.46, test loss : 0.2320, test accuracy : 93.42\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.1399, accuracy : 96.20\n",
            "iteration : 100, loss : 0.1494, accuracy : 95.78\n",
            "iteration : 150, loss : 0.1533, accuracy : 95.71\n",
            "iteration : 200, loss : 0.1539, accuracy : 95.72\n",
            "iteration : 250, loss : 0.1559, accuracy : 95.64\n",
            "iteration : 300, loss : 0.1549, accuracy : 95.66\n",
            "iteration : 350, loss : 0.1555, accuracy : 95.63\n",
            "Epoch :  90, training loss : 0.1546, training accuracy : 95.63, test loss : 0.2402, test accuracy : 93.35\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.1465, accuracy : 95.80\n",
            "iteration : 100, loss : 0.1494, accuracy : 95.49\n",
            "iteration : 150, loss : 0.1537, accuracy : 95.54\n",
            "iteration : 200, loss : 0.1535, accuracy : 95.49\n",
            "iteration : 250, loss : 0.1538, accuracy : 95.50\n",
            "iteration : 300, loss : 0.1541, accuracy : 95.55\n",
            "iteration : 350, loss : 0.1538, accuracy : 95.57\n",
            "Epoch :  91, training loss : 0.1544, training accuracy : 95.56, test loss : 0.2377, test accuracy : 93.50\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.1475, accuracy : 95.61\n",
            "iteration : 100, loss : 0.1526, accuracy : 95.71\n",
            "iteration : 150, loss : 0.1548, accuracy : 95.62\n",
            "iteration : 200, loss : 0.1536, accuracy : 95.68\n",
            "iteration : 250, loss : 0.1551, accuracy : 95.63\n",
            "iteration : 300, loss : 0.1566, accuracy : 95.57\n",
            "iteration : 350, loss : 0.1567, accuracy : 95.53\n",
            "Epoch :  92, training loss : 0.1572, training accuracy : 95.52, test loss : 0.2340, test accuracy : 93.48\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.1572, accuracy : 95.81\n",
            "iteration : 100, loss : 0.1520, accuracy : 95.85\n",
            "iteration : 150, loss : 0.1472, accuracy : 95.88\n",
            "iteration : 200, loss : 0.1481, accuracy : 95.86\n",
            "iteration : 250, loss : 0.1500, accuracy : 95.77\n",
            "iteration : 300, loss : 0.1499, accuracy : 95.74\n",
            "iteration : 350, loss : 0.1501, accuracy : 95.75\n",
            "Epoch :  93, training loss : 0.1505, training accuracy : 95.74, test loss : 0.2280, test accuracy : 93.66\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.1332, accuracy : 96.03\n",
            "iteration : 100, loss : 0.1371, accuracy : 96.07\n",
            "iteration : 150, loss : 0.1408, accuracy : 95.90\n",
            "iteration : 200, loss : 0.1453, accuracy : 95.82\n",
            "iteration : 250, loss : 0.1456, accuracy : 95.85\n",
            "iteration : 300, loss : 0.1471, accuracy : 95.82\n",
            "iteration : 350, loss : 0.1488, accuracy : 95.76\n",
            "Epoch :  94, training loss : 0.1495, training accuracy : 95.72, test loss : 0.2437, test accuracy : 93.37\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.1380, accuracy : 96.02\n",
            "iteration : 100, loss : 0.1388, accuracy : 96.11\n",
            "iteration : 150, loss : 0.1450, accuracy : 95.90\n",
            "iteration : 200, loss : 0.1474, accuracy : 95.85\n",
            "iteration : 250, loss : 0.1484, accuracy : 95.82\n",
            "iteration : 300, loss : 0.1493, accuracy : 95.72\n",
            "iteration : 350, loss : 0.1510, accuracy : 95.65\n",
            "Epoch :  95, training loss : 0.1505, training accuracy : 95.67, test loss : 0.2520, test accuracy : 93.12\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.1482, accuracy : 95.64\n",
            "iteration : 100, loss : 0.1400, accuracy : 95.83\n",
            "iteration : 150, loss : 0.1409, accuracy : 95.89\n",
            "iteration : 200, loss : 0.1468, accuracy : 95.69\n",
            "iteration : 250, loss : 0.1466, accuracy : 95.69\n",
            "iteration : 300, loss : 0.1507, accuracy : 95.61\n",
            "iteration : 350, loss : 0.1519, accuracy : 95.60\n",
            "Epoch :  96, training loss : 0.1531, training accuracy : 95.58, test loss : 0.2446, test accuracy : 93.17\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.1305, accuracy : 96.03\n",
            "iteration : 100, loss : 0.1369, accuracy : 95.95\n",
            "iteration : 150, loss : 0.1377, accuracy : 95.94\n",
            "iteration : 200, loss : 0.1422, accuracy : 95.91\n",
            "iteration : 250, loss : 0.1446, accuracy : 95.85\n",
            "iteration : 300, loss : 0.1479, accuracy : 95.75\n",
            "iteration : 350, loss : 0.1496, accuracy : 95.67\n",
            "Epoch :  97, training loss : 0.1489, training accuracy : 95.67, test loss : 0.2420, test accuracy : 93.25\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.1330, accuracy : 96.17\n",
            "iteration : 100, loss : 0.1388, accuracy : 96.08\n",
            "iteration : 150, loss : 0.1382, accuracy : 96.13\n",
            "iteration : 200, loss : 0.1404, accuracy : 96.07\n",
            "iteration : 250, loss : 0.1409, accuracy : 96.03\n",
            "iteration : 300, loss : 0.1429, accuracy : 95.96\n",
            "iteration : 350, loss : 0.1470, accuracy : 95.80\n",
            "Epoch :  98, training loss : 0.1479, training accuracy : 95.76, test loss : 0.2364, test accuracy : 93.45\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.1510, accuracy : 95.70\n",
            "iteration : 100, loss : 0.1495, accuracy : 95.70\n",
            "iteration : 150, loss : 0.1520, accuracy : 95.73\n",
            "iteration : 200, loss : 0.1508, accuracy : 95.71\n",
            "iteration : 250, loss : 0.1525, accuracy : 95.65\n",
            "iteration : 300, loss : 0.1515, accuracy : 95.67\n",
            "iteration : 350, loss : 0.1516, accuracy : 95.65\n",
            "Epoch :  99, training loss : 0.1511, training accuracy : 95.66, test loss : 0.2285, test accuracy : 93.72\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.1373, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1420, accuracy : 95.97\n",
            "iteration : 150, loss : 0.1416, accuracy : 95.97\n",
            "iteration : 200, loss : 0.1458, accuracy : 95.88\n",
            "iteration : 250, loss : 0.1451, accuracy : 95.88\n",
            "iteration : 300, loss : 0.1460, accuracy : 95.83\n",
            "iteration : 350, loss : 0.1450, accuracy : 95.83\n",
            "Epoch : 100, training loss : 0.1461, training accuracy : 95.81, test loss : 0.2370, test accuracy : 93.40\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.1309, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1381, accuracy : 96.03\n",
            "iteration : 150, loss : 0.1439, accuracy : 95.86\n",
            "iteration : 200, loss : 0.1449, accuracy : 95.83\n",
            "iteration : 250, loss : 0.1455, accuracy : 95.83\n",
            "iteration : 300, loss : 0.1483, accuracy : 95.73\n",
            "iteration : 350, loss : 0.1504, accuracy : 95.68\n",
            "Epoch : 101, training loss : 0.1509, training accuracy : 95.69, test loss : 0.2356, test accuracy : 93.50\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.1392, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1388, accuracy : 96.05\n",
            "iteration : 150, loss : 0.1446, accuracy : 95.80\n",
            "iteration : 200, loss : 0.1461, accuracy : 95.80\n",
            "iteration : 250, loss : 0.1464, accuracy : 95.77\n",
            "iteration : 300, loss : 0.1472, accuracy : 95.74\n",
            "iteration : 350, loss : 0.1481, accuracy : 95.70\n",
            "Epoch : 102, training loss : 0.1485, training accuracy : 95.72, test loss : 0.2294, test accuracy : 93.58\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.1458, accuracy : 95.84\n",
            "iteration : 100, loss : 0.1449, accuracy : 95.85\n",
            "iteration : 150, loss : 0.1409, accuracy : 95.92\n",
            "iteration : 200, loss : 0.1415, accuracy : 95.95\n",
            "iteration : 250, loss : 0.1426, accuracy : 95.95\n",
            "iteration : 300, loss : 0.1443, accuracy : 95.88\n",
            "iteration : 350, loss : 0.1446, accuracy : 95.89\n",
            "Epoch : 103, training loss : 0.1450, training accuracy : 95.85, test loss : 0.2248, test accuracy : 93.99\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.1415, accuracy : 95.78\n",
            "iteration : 100, loss : 0.1384, accuracy : 95.86\n",
            "iteration : 150, loss : 0.1404, accuracy : 95.93\n",
            "iteration : 200, loss : 0.1429, accuracy : 95.90\n",
            "iteration : 250, loss : 0.1451, accuracy : 95.87\n",
            "iteration : 300, loss : 0.1451, accuracy : 95.85\n",
            "iteration : 350, loss : 0.1451, accuracy : 95.88\n",
            "Epoch : 104, training loss : 0.1458, training accuracy : 95.86, test loss : 0.2420, test accuracy : 93.27\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.1184, accuracy : 96.62\n",
            "iteration : 100, loss : 0.1256, accuracy : 96.33\n",
            "iteration : 150, loss : 0.1328, accuracy : 96.23\n",
            "iteration : 200, loss : 0.1364, accuracy : 96.07\n",
            "iteration : 250, loss : 0.1399, accuracy : 95.99\n",
            "iteration : 300, loss : 0.1404, accuracy : 95.97\n",
            "iteration : 350, loss : 0.1406, accuracy : 95.97\n",
            "Epoch : 105, training loss : 0.1408, training accuracy : 95.96, test loss : 0.2488, test accuracy : 93.02\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.1375, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1392, accuracy : 95.98\n",
            "iteration : 150, loss : 0.1361, accuracy : 96.06\n",
            "iteration : 200, loss : 0.1375, accuracy : 96.03\n",
            "iteration : 250, loss : 0.1390, accuracy : 96.00\n",
            "iteration : 300, loss : 0.1415, accuracy : 95.95\n",
            "iteration : 350, loss : 0.1423, accuracy : 95.95\n",
            "Epoch : 106, training loss : 0.1430, training accuracy : 95.92, test loss : 0.2350, test accuracy : 93.62\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.1371, accuracy : 95.98\n",
            "iteration : 100, loss : 0.1391, accuracy : 95.96\n",
            "iteration : 150, loss : 0.1383, accuracy : 96.09\n",
            "iteration : 200, loss : 0.1392, accuracy : 96.04\n",
            "iteration : 250, loss : 0.1419, accuracy : 95.93\n",
            "iteration : 300, loss : 0.1428, accuracy : 95.89\n",
            "iteration : 350, loss : 0.1442, accuracy : 95.87\n",
            "Epoch : 107, training loss : 0.1451, training accuracy : 95.85, test loss : 0.2345, test accuracy : 93.52\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.1377, accuracy : 96.02\n",
            "iteration : 100, loss : 0.1393, accuracy : 96.01\n",
            "iteration : 150, loss : 0.1414, accuracy : 95.91\n",
            "iteration : 200, loss : 0.1420, accuracy : 95.83\n",
            "iteration : 250, loss : 0.1448, accuracy : 95.78\n",
            "iteration : 300, loss : 0.1450, accuracy : 95.79\n",
            "iteration : 350, loss : 0.1442, accuracy : 95.80\n",
            "Epoch : 108, training loss : 0.1448, training accuracy : 95.79, test loss : 0.2393, test accuracy : 93.39\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.1371, accuracy : 96.23\n",
            "iteration : 100, loss : 0.1382, accuracy : 96.10\n",
            "iteration : 150, loss : 0.1346, accuracy : 96.23\n",
            "iteration : 200, loss : 0.1313, accuracy : 96.28\n",
            "iteration : 250, loss : 0.1316, accuracy : 96.20\n",
            "iteration : 300, loss : 0.1359, accuracy : 96.12\n",
            "iteration : 350, loss : 0.1392, accuracy : 96.04\n",
            "Epoch : 109, training loss : 0.1404, training accuracy : 96.02, test loss : 0.2230, test accuracy : 93.92\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.1348, accuracy : 96.27\n",
            "iteration : 100, loss : 0.1388, accuracy : 96.07\n",
            "iteration : 150, loss : 0.1375, accuracy : 96.01\n",
            "iteration : 200, loss : 0.1371, accuracy : 96.00\n",
            "iteration : 250, loss : 0.1390, accuracy : 96.02\n",
            "iteration : 300, loss : 0.1381, accuracy : 96.03\n",
            "iteration : 350, loss : 0.1382, accuracy : 96.03\n",
            "Epoch : 110, training loss : 0.1393, training accuracy : 95.99, test loss : 0.2327, test accuracy : 93.72\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.1463, accuracy : 95.80\n",
            "iteration : 100, loss : 0.1432, accuracy : 95.88\n",
            "iteration : 150, loss : 0.1417, accuracy : 95.83\n",
            "iteration : 200, loss : 0.1409, accuracy : 95.91\n",
            "iteration : 250, loss : 0.1384, accuracy : 95.98\n",
            "iteration : 300, loss : 0.1379, accuracy : 95.99\n",
            "iteration : 350, loss : 0.1387, accuracy : 95.98\n",
            "Epoch : 111, training loss : 0.1372, training accuracy : 96.02, test loss : 0.2301, test accuracy : 93.86\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.1402, accuracy : 96.11\n",
            "iteration : 100, loss : 0.1377, accuracy : 96.12\n",
            "iteration : 150, loss : 0.1429, accuracy : 95.94\n",
            "iteration : 200, loss : 0.1402, accuracy : 96.00\n",
            "iteration : 250, loss : 0.1409, accuracy : 96.00\n",
            "iteration : 300, loss : 0.1410, accuracy : 96.02\n",
            "iteration : 350, loss : 0.1417, accuracy : 95.99\n",
            "Epoch : 112, training loss : 0.1421, training accuracy : 95.98, test loss : 0.2274, test accuracy : 93.78\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.1331, accuracy : 96.05\n",
            "iteration : 100, loss : 0.1351, accuracy : 96.08\n",
            "iteration : 150, loss : 0.1361, accuracy : 96.15\n",
            "iteration : 200, loss : 0.1375, accuracy : 96.06\n",
            "iteration : 250, loss : 0.1379, accuracy : 96.03\n",
            "iteration : 300, loss : 0.1396, accuracy : 95.96\n",
            "iteration : 350, loss : 0.1410, accuracy : 95.94\n",
            "Epoch : 113, training loss : 0.1410, training accuracy : 95.94, test loss : 0.2403, test accuracy : 93.37\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.1356, accuracy : 96.33\n",
            "iteration : 100, loss : 0.1369, accuracy : 96.14\n",
            "iteration : 150, loss : 0.1364, accuracy : 96.20\n",
            "iteration : 200, loss : 0.1355, accuracy : 96.25\n",
            "iteration : 250, loss : 0.1375, accuracy : 96.15\n",
            "iteration : 300, loss : 0.1388, accuracy : 96.13\n",
            "iteration : 350, loss : 0.1385, accuracy : 96.12\n",
            "Epoch : 114, training loss : 0.1387, training accuracy : 96.09, test loss : 0.2303, test accuracy : 93.69\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.1221, accuracy : 96.50\n",
            "iteration : 100, loss : 0.1318, accuracy : 96.15\n",
            "iteration : 150, loss : 0.1348, accuracy : 96.09\n",
            "iteration : 200, loss : 0.1379, accuracy : 96.02\n",
            "iteration : 250, loss : 0.1363, accuracy : 96.05\n",
            "iteration : 300, loss : 0.1364, accuracy : 96.01\n",
            "iteration : 350, loss : 0.1379, accuracy : 96.01\n",
            "Epoch : 115, training loss : 0.1381, training accuracy : 96.00, test loss : 0.2323, test accuracy : 93.66\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.1311, accuracy : 96.36\n",
            "iteration : 100, loss : 0.1282, accuracy : 96.36\n",
            "iteration : 150, loss : 0.1272, accuracy : 96.44\n",
            "iteration : 200, loss : 0.1311, accuracy : 96.36\n",
            "iteration : 250, loss : 0.1336, accuracy : 96.22\n",
            "iteration : 300, loss : 0.1346, accuracy : 96.15\n",
            "iteration : 350, loss : 0.1348, accuracy : 96.12\n",
            "Epoch : 116, training loss : 0.1344, training accuracy : 96.12, test loss : 0.2310, test accuracy : 93.81\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.1263, accuracy : 96.48\n",
            "iteration : 100, loss : 0.1314, accuracy : 96.32\n",
            "iteration : 150, loss : 0.1290, accuracy : 96.36\n",
            "iteration : 200, loss : 0.1311, accuracy : 96.30\n",
            "iteration : 250, loss : 0.1343, accuracy : 96.22\n",
            "iteration : 300, loss : 0.1338, accuracy : 96.21\n",
            "iteration : 350, loss : 0.1335, accuracy : 96.16\n",
            "Epoch : 117, training loss : 0.1341, training accuracy : 96.15, test loss : 0.2350, test accuracy : 93.49\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.1302, accuracy : 95.98\n",
            "iteration : 100, loss : 0.1321, accuracy : 96.12\n",
            "iteration : 150, loss : 0.1257, accuracy : 96.40\n",
            "iteration : 200, loss : 0.1311, accuracy : 96.29\n",
            "iteration : 250, loss : 0.1301, accuracy : 96.29\n",
            "iteration : 300, loss : 0.1335, accuracy : 96.17\n",
            "iteration : 350, loss : 0.1347, accuracy : 96.14\n",
            "Epoch : 118, training loss : 0.1343, training accuracy : 96.16, test loss : 0.2322, test accuracy : 93.67\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.1204, accuracy : 96.47\n",
            "iteration : 100, loss : 0.1264, accuracy : 96.49\n",
            "iteration : 150, loss : 0.1303, accuracy : 96.28\n",
            "iteration : 200, loss : 0.1314, accuracy : 96.24\n",
            "iteration : 250, loss : 0.1314, accuracy : 96.23\n",
            "iteration : 300, loss : 0.1314, accuracy : 96.23\n",
            "iteration : 350, loss : 0.1313, accuracy : 96.25\n",
            "Epoch : 119, training loss : 0.1322, training accuracy : 96.22, test loss : 0.2435, test accuracy : 93.40\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.1122, accuracy : 96.70\n",
            "iteration : 100, loss : 0.1149, accuracy : 96.70\n",
            "iteration : 150, loss : 0.1226, accuracy : 96.49\n",
            "iteration : 200, loss : 0.1273, accuracy : 96.33\n",
            "iteration : 250, loss : 0.1290, accuracy : 96.29\n",
            "iteration : 300, loss : 0.1310, accuracy : 96.21\n",
            "iteration : 350, loss : 0.1315, accuracy : 96.20\n",
            "Epoch : 120, training loss : 0.1316, training accuracy : 96.19, test loss : 0.2342, test accuracy : 93.66\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.1249, accuracy : 96.67\n",
            "iteration : 100, loss : 0.1226, accuracy : 96.49\n",
            "iteration : 150, loss : 0.1267, accuracy : 96.41\n",
            "iteration : 200, loss : 0.1295, accuracy : 96.35\n",
            "iteration : 250, loss : 0.1297, accuracy : 96.30\n",
            "iteration : 300, loss : 0.1321, accuracy : 96.21\n",
            "iteration : 350, loss : 0.1321, accuracy : 96.24\n",
            "Epoch : 121, training loss : 0.1327, training accuracy : 96.21, test loss : 0.2342, test accuracy : 93.68\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.1013, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1151, accuracy : 96.52\n",
            "iteration : 150, loss : 0.1208, accuracy : 96.42\n",
            "iteration : 200, loss : 0.1265, accuracy : 96.27\n",
            "iteration : 250, loss : 0.1262, accuracy : 96.29\n",
            "iteration : 300, loss : 0.1269, accuracy : 96.28\n",
            "iteration : 350, loss : 0.1307, accuracy : 96.19\n",
            "Epoch : 122, training loss : 0.1318, training accuracy : 96.17, test loss : 0.2290, test accuracy : 93.67\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.1182, accuracy : 96.48\n",
            "iteration : 100, loss : 0.1196, accuracy : 96.58\n",
            "iteration : 150, loss : 0.1227, accuracy : 96.53\n",
            "iteration : 200, loss : 0.1238, accuracy : 96.48\n",
            "iteration : 250, loss : 0.1275, accuracy : 96.41\n",
            "iteration : 300, loss : 0.1273, accuracy : 96.38\n",
            "iteration : 350, loss : 0.1268, accuracy : 96.37\n",
            "Epoch : 123, training loss : 0.1274, training accuracy : 96.36, test loss : 0.2354, test accuracy : 93.62\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.1279, accuracy : 96.42\n",
            "iteration : 100, loss : 0.1250, accuracy : 96.48\n",
            "iteration : 150, loss : 0.1239, accuracy : 96.45\n",
            "iteration : 200, loss : 0.1225, accuracy : 96.50\n",
            "iteration : 250, loss : 0.1244, accuracy : 96.44\n",
            "iteration : 300, loss : 0.1266, accuracy : 96.36\n",
            "iteration : 350, loss : 0.1276, accuracy : 96.36\n",
            "Epoch : 124, training loss : 0.1268, training accuracy : 96.39, test loss : 0.2273, test accuracy : 94.08\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.1181, accuracy : 96.73\n",
            "iteration : 100, loss : 0.1228, accuracy : 96.65\n",
            "iteration : 150, loss : 0.1228, accuracy : 96.47\n",
            "iteration : 200, loss : 0.1248, accuracy : 96.39\n",
            "iteration : 250, loss : 0.1266, accuracy : 96.37\n",
            "iteration : 300, loss : 0.1285, accuracy : 96.30\n",
            "iteration : 350, loss : 0.1277, accuracy : 96.31\n",
            "Epoch : 125, training loss : 0.1294, training accuracy : 96.26, test loss : 0.2473, test accuracy : 93.60\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.1141, accuracy : 96.94\n",
            "iteration : 100, loss : 0.1188, accuracy : 96.66\n",
            "iteration : 150, loss : 0.1232, accuracy : 96.52\n",
            "iteration : 200, loss : 0.1244, accuracy : 96.42\n",
            "iteration : 250, loss : 0.1281, accuracy : 96.30\n",
            "iteration : 300, loss : 0.1292, accuracy : 96.28\n",
            "iteration : 350, loss : 0.1319, accuracy : 96.21\n",
            "Epoch : 126, training loss : 0.1312, training accuracy : 96.24, test loss : 0.2319, test accuracy : 93.65\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.1172, accuracy : 96.47\n",
            "iteration : 100, loss : 0.1187, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1203, accuracy : 96.52\n",
            "iteration : 200, loss : 0.1229, accuracy : 96.46\n",
            "iteration : 250, loss : 0.1242, accuracy : 96.43\n",
            "iteration : 300, loss : 0.1252, accuracy : 96.42\n",
            "iteration : 350, loss : 0.1271, accuracy : 96.36\n",
            "Epoch : 127, training loss : 0.1272, training accuracy : 96.36, test loss : 0.2373, test accuracy : 93.65\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.1131, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1205, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1166, accuracy : 96.68\n",
            "iteration : 200, loss : 0.1179, accuracy : 96.66\n",
            "iteration : 250, loss : 0.1189, accuracy : 96.62\n",
            "iteration : 300, loss : 0.1199, accuracy : 96.59\n",
            "iteration : 350, loss : 0.1233, accuracy : 96.54\n",
            "Epoch : 128, training loss : 0.1227, training accuracy : 96.57, test loss : 0.2368, test accuracy : 93.62\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.1086, accuracy : 96.69\n",
            "iteration : 100, loss : 0.1184, accuracy : 96.56\n",
            "iteration : 150, loss : 0.1210, accuracy : 96.45\n",
            "iteration : 200, loss : 0.1237, accuracy : 96.38\n",
            "iteration : 250, loss : 0.1262, accuracy : 96.32\n",
            "iteration : 300, loss : 0.1262, accuracy : 96.35\n",
            "iteration : 350, loss : 0.1264, accuracy : 96.35\n",
            "Epoch : 129, training loss : 0.1257, training accuracy : 96.38, test loss : 0.2382, test accuracy : 93.61\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.1183, accuracy : 96.52\n",
            "iteration : 100, loss : 0.1245, accuracy : 96.48\n",
            "iteration : 150, loss : 0.1224, accuracy : 96.52\n",
            "iteration : 200, loss : 0.1202, accuracy : 96.57\n",
            "iteration : 250, loss : 0.1235, accuracy : 96.52\n",
            "iteration : 300, loss : 0.1229, accuracy : 96.56\n",
            "iteration : 350, loss : 0.1233, accuracy : 96.54\n",
            "Epoch : 130, training loss : 0.1229, training accuracy : 96.55, test loss : 0.2423, test accuracy : 93.49\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.1056, accuracy : 96.88\n",
            "iteration : 100, loss : 0.1167, accuracy : 96.58\n",
            "iteration : 150, loss : 0.1200, accuracy : 96.47\n",
            "iteration : 200, loss : 0.1188, accuracy : 96.53\n",
            "iteration : 250, loss : 0.1214, accuracy : 96.46\n",
            "iteration : 300, loss : 0.1227, accuracy : 96.41\n",
            "iteration : 350, loss : 0.1235, accuracy : 96.40\n",
            "Epoch : 131, training loss : 0.1243, training accuracy : 96.39, test loss : 0.2405, test accuracy : 93.58\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.1206, accuracy : 96.22\n",
            "iteration : 100, loss : 0.1296, accuracy : 96.22\n",
            "iteration : 150, loss : 0.1286, accuracy : 96.27\n",
            "iteration : 200, loss : 0.1292, accuracy : 96.29\n",
            "iteration : 250, loss : 0.1279, accuracy : 96.32\n",
            "iteration : 300, loss : 0.1269, accuracy : 96.34\n",
            "iteration : 350, loss : 0.1279, accuracy : 96.30\n",
            "Epoch : 132, training loss : 0.1271, training accuracy : 96.34, test loss : 0.2307, test accuracy : 93.74\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.1042, accuracy : 97.00\n",
            "iteration : 100, loss : 0.1109, accuracy : 96.86\n",
            "iteration : 150, loss : 0.1133, accuracy : 96.86\n",
            "iteration : 200, loss : 0.1176, accuracy : 96.67\n",
            "iteration : 250, loss : 0.1194, accuracy : 96.59\n",
            "iteration : 300, loss : 0.1191, accuracy : 96.61\n",
            "iteration : 350, loss : 0.1215, accuracy : 96.58\n",
            "Epoch : 133, training loss : 0.1218, training accuracy : 96.57, test loss : 0.2310, test accuracy : 93.73\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.1016, accuracy : 97.17\n",
            "iteration : 100, loss : 0.1121, accuracy : 96.87\n",
            "iteration : 150, loss : 0.1131, accuracy : 96.77\n",
            "iteration : 200, loss : 0.1139, accuracy : 96.75\n",
            "iteration : 250, loss : 0.1161, accuracy : 96.73\n",
            "iteration : 300, loss : 0.1175, accuracy : 96.66\n",
            "iteration : 350, loss : 0.1178, accuracy : 96.67\n",
            "Epoch : 134, training loss : 0.1183, training accuracy : 96.65, test loss : 0.2365, test accuracy : 93.66\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.1187, accuracy : 96.81\n",
            "iteration : 100, loss : 0.1216, accuracy : 96.70\n",
            "iteration : 150, loss : 0.1183, accuracy : 96.83\n",
            "iteration : 200, loss : 0.1177, accuracy : 96.74\n",
            "iteration : 250, loss : 0.1177, accuracy : 96.71\n",
            "iteration : 300, loss : 0.1194, accuracy : 96.68\n",
            "iteration : 350, loss : 0.1205, accuracy : 96.64\n",
            "Epoch : 135, training loss : 0.1200, training accuracy : 96.65, test loss : 0.2498, test accuracy : 93.41\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.1197, accuracy : 96.59\n",
            "iteration : 100, loss : 0.1137, accuracy : 96.81\n",
            "iteration : 150, loss : 0.1148, accuracy : 96.78\n",
            "iteration : 200, loss : 0.1154, accuracy : 96.75\n",
            "iteration : 250, loss : 0.1186, accuracy : 96.70\n",
            "iteration : 300, loss : 0.1194, accuracy : 96.63\n",
            "iteration : 350, loss : 0.1213, accuracy : 96.58\n",
            "Epoch : 136, training loss : 0.1206, training accuracy : 96.61, test loss : 0.2329, test accuracy : 93.75\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.1095, accuracy : 96.80\n",
            "iteration : 100, loss : 0.1135, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1139, accuracy : 96.65\n",
            "iteration : 200, loss : 0.1131, accuracy : 96.68\n",
            "iteration : 250, loss : 0.1136, accuracy : 96.69\n",
            "iteration : 300, loss : 0.1141, accuracy : 96.68\n",
            "iteration : 350, loss : 0.1146, accuracy : 96.66\n",
            "Epoch : 137, training loss : 0.1143, training accuracy : 96.66, test loss : 0.2476, test accuracy : 93.48\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.1257, accuracy : 96.48\n",
            "iteration : 100, loss : 0.1217, accuracy : 96.57\n",
            "iteration : 150, loss : 0.1202, accuracy : 96.55\n",
            "iteration : 200, loss : 0.1217, accuracy : 96.53\n",
            "iteration : 250, loss : 0.1201, accuracy : 96.58\n",
            "iteration : 300, loss : 0.1190, accuracy : 96.57\n",
            "iteration : 350, loss : 0.1192, accuracy : 96.53\n",
            "Epoch : 138, training loss : 0.1198, training accuracy : 96.52, test loss : 0.2493, test accuracy : 93.58\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.1073, accuracy : 97.05\n",
            "iteration : 100, loss : 0.1043, accuracy : 97.07\n",
            "iteration : 150, loss : 0.1111, accuracy : 96.82\n",
            "iteration : 200, loss : 0.1160, accuracy : 96.66\n",
            "iteration : 250, loss : 0.1156, accuracy : 96.73\n",
            "iteration : 300, loss : 0.1149, accuracy : 96.75\n",
            "iteration : 350, loss : 0.1145, accuracy : 96.75\n",
            "Epoch : 139, training loss : 0.1142, training accuracy : 96.77, test loss : 0.2296, test accuracy : 93.87\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.1022, accuracy : 97.03\n",
            "iteration : 100, loss : 0.1030, accuracy : 97.02\n",
            "iteration : 150, loss : 0.1070, accuracy : 96.90\n",
            "iteration : 200, loss : 0.1059, accuracy : 96.93\n",
            "iteration : 250, loss : 0.1103, accuracy : 96.83\n",
            "iteration : 300, loss : 0.1130, accuracy : 96.72\n",
            "iteration : 350, loss : 0.1144, accuracy : 96.71\n",
            "Epoch : 140, training loss : 0.1151, training accuracy : 96.68, test loss : 0.2354, test accuracy : 93.73\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.1156, accuracy : 96.45\n",
            "iteration : 100, loss : 0.1129, accuracy : 96.62\n",
            "iteration : 150, loss : 0.1132, accuracy : 96.66\n",
            "iteration : 200, loss : 0.1130, accuracy : 96.68\n",
            "iteration : 250, loss : 0.1140, accuracy : 96.66\n",
            "iteration : 300, loss : 0.1139, accuracy : 96.67\n",
            "iteration : 350, loss : 0.1144, accuracy : 96.69\n",
            "Epoch : 141, training loss : 0.1142, training accuracy : 96.73, test loss : 0.2426, test accuracy : 93.48\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0996, accuracy : 97.19\n",
            "iteration : 100, loss : 0.1036, accuracy : 97.12\n",
            "iteration : 150, loss : 0.1064, accuracy : 96.95\n",
            "iteration : 200, loss : 0.1098, accuracy : 96.80\n",
            "iteration : 250, loss : 0.1100, accuracy : 96.80\n",
            "iteration : 300, loss : 0.1127, accuracy : 96.69\n",
            "iteration : 350, loss : 0.1136, accuracy : 96.67\n",
            "Epoch : 142, training loss : 0.1137, training accuracy : 96.67, test loss : 0.2354, test accuracy : 93.91\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0958, accuracy : 97.30\n",
            "iteration : 100, loss : 0.1032, accuracy : 97.12\n",
            "iteration : 150, loss : 0.1059, accuracy : 96.98\n",
            "iteration : 200, loss : 0.1099, accuracy : 96.86\n",
            "iteration : 250, loss : 0.1112, accuracy : 96.84\n",
            "iteration : 300, loss : 0.1127, accuracy : 96.78\n",
            "iteration : 350, loss : 0.1117, accuracy : 96.80\n",
            "Epoch : 143, training loss : 0.1115, training accuracy : 96.80, test loss : 0.2259, test accuracy : 94.20\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.1070, accuracy : 96.86\n",
            "iteration : 100, loss : 0.1055, accuracy : 96.97\n",
            "iteration : 150, loss : 0.1099, accuracy : 96.89\n",
            "iteration : 200, loss : 0.1067, accuracy : 97.00\n",
            "iteration : 250, loss : 0.1085, accuracy : 96.97\n",
            "iteration : 300, loss : 0.1090, accuracy : 96.94\n",
            "iteration : 350, loss : 0.1089, accuracy : 96.95\n",
            "Epoch : 144, training loss : 0.1096, training accuracy : 96.93, test loss : 0.2393, test accuracy : 93.67\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.1056, accuracy : 97.16\n",
            "iteration : 100, loss : 0.1037, accuracy : 97.08\n",
            "iteration : 150, loss : 0.1074, accuracy : 96.98\n",
            "iteration : 200, loss : 0.1051, accuracy : 97.03\n",
            "iteration : 250, loss : 0.1038, accuracy : 97.06\n",
            "iteration : 300, loss : 0.1053, accuracy : 97.01\n",
            "iteration : 350, loss : 0.1080, accuracy : 96.94\n",
            "Epoch : 145, training loss : 0.1085, training accuracy : 96.93, test loss : 0.2362, test accuracy : 93.67\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.1110, accuracy : 96.86\n",
            "iteration : 100, loss : 0.1055, accuracy : 96.95\n",
            "iteration : 150, loss : 0.1037, accuracy : 97.04\n",
            "iteration : 200, loss : 0.1046, accuracy : 97.01\n",
            "iteration : 250, loss : 0.1062, accuracy : 96.98\n",
            "iteration : 300, loss : 0.1078, accuracy : 96.89\n",
            "iteration : 350, loss : 0.1087, accuracy : 96.83\n",
            "Epoch : 146, training loss : 0.1079, training accuracy : 96.84, test loss : 0.2324, test accuracy : 93.80\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0971, accuracy : 97.30\n",
            "iteration : 100, loss : 0.0971, accuracy : 97.17\n",
            "iteration : 150, loss : 0.1049, accuracy : 97.05\n",
            "iteration : 200, loss : 0.1069, accuracy : 96.99\n",
            "iteration : 250, loss : 0.1076, accuracy : 96.96\n",
            "iteration : 300, loss : 0.1092, accuracy : 96.91\n",
            "iteration : 350, loss : 0.1116, accuracy : 96.80\n",
            "Epoch : 147, training loss : 0.1119, training accuracy : 96.79, test loss : 0.2396, test accuracy : 93.70\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.1063, accuracy : 96.98\n",
            "iteration : 100, loss : 0.1046, accuracy : 97.10\n",
            "iteration : 150, loss : 0.1063, accuracy : 96.96\n",
            "iteration : 200, loss : 0.1080, accuracy : 96.93\n",
            "iteration : 250, loss : 0.1080, accuracy : 96.99\n",
            "iteration : 300, loss : 0.1070, accuracy : 96.97\n",
            "iteration : 350, loss : 0.1081, accuracy : 96.91\n",
            "Epoch : 148, training loss : 0.1077, training accuracy : 96.90, test loss : 0.2333, test accuracy : 93.88\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.1032, accuracy : 97.11\n",
            "iteration : 100, loss : 0.1030, accuracy : 97.17\n",
            "iteration : 150, loss : 0.1028, accuracy : 97.14\n",
            "iteration : 200, loss : 0.1059, accuracy : 97.07\n",
            "iteration : 250, loss : 0.1058, accuracy : 97.01\n",
            "iteration : 300, loss : 0.1068, accuracy : 96.98\n",
            "iteration : 350, loss : 0.1086, accuracy : 96.93\n",
            "Epoch : 149, training loss : 0.1083, training accuracy : 96.93, test loss : 0.2421, test accuracy : 93.81\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.1112, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1122, accuracy : 96.75\n",
            "iteration : 150, loss : 0.1058, accuracy : 96.88\n",
            "iteration : 200, loss : 0.1020, accuracy : 97.00\n",
            "iteration : 250, loss : 0.1040, accuracy : 96.93\n",
            "iteration : 300, loss : 0.1038, accuracy : 96.96\n",
            "iteration : 350, loss : 0.1065, accuracy : 96.89\n",
            "Epoch : 150, training loss : 0.1074, training accuracy : 96.86, test loss : 0.2411, test accuracy : 93.77\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0988, accuracy : 97.36\n",
            "iteration : 100, loss : 0.1036, accuracy : 97.12\n",
            "iteration : 150, loss : 0.1040, accuracy : 97.08\n",
            "iteration : 200, loss : 0.1054, accuracy : 97.06\n",
            "iteration : 250, loss : 0.1061, accuracy : 96.99\n",
            "iteration : 300, loss : 0.1054, accuracy : 97.00\n",
            "iteration : 350, loss : 0.1045, accuracy : 97.01\n",
            "Epoch : 151, training loss : 0.1046, training accuracy : 97.01, test loss : 0.2358, test accuracy : 93.75\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0897, accuracy : 97.47\n",
            "iteration : 100, loss : 0.0963, accuracy : 97.30\n",
            "iteration : 150, loss : 0.0977, accuracy : 97.18\n",
            "iteration : 200, loss : 0.1018, accuracy : 97.07\n",
            "iteration : 250, loss : 0.1019, accuracy : 97.12\n",
            "iteration : 300, loss : 0.1010, accuracy : 97.15\n",
            "iteration : 350, loss : 0.1019, accuracy : 97.09\n",
            "Epoch : 152, training loss : 0.1023, training accuracy : 97.07, test loss : 0.2317, test accuracy : 93.87\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.1093, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1062, accuracy : 97.13\n",
            "iteration : 150, loss : 0.1017, accuracy : 97.19\n",
            "iteration : 200, loss : 0.1021, accuracy : 97.12\n",
            "iteration : 250, loss : 0.1028, accuracy : 97.06\n",
            "iteration : 300, loss : 0.1035, accuracy : 97.04\n",
            "iteration : 350, loss : 0.1029, accuracy : 97.05\n",
            "Epoch : 153, training loss : 0.1031, training accuracy : 97.05, test loss : 0.2339, test accuracy : 93.89\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0996, accuracy : 97.11\n",
            "iteration : 100, loss : 0.1008, accuracy : 97.09\n",
            "iteration : 150, loss : 0.1002, accuracy : 97.11\n",
            "iteration : 200, loss : 0.1012, accuracy : 97.09\n",
            "iteration : 250, loss : 0.1030, accuracy : 97.03\n",
            "iteration : 300, loss : 0.1032, accuracy : 97.02\n",
            "iteration : 350, loss : 0.1032, accuracy : 97.00\n",
            "Epoch : 154, training loss : 0.1039, training accuracy : 96.98, test loss : 0.2515, test accuracy : 93.61\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.1008, accuracy : 97.11\n",
            "iteration : 100, loss : 0.1030, accuracy : 97.11\n",
            "iteration : 150, loss : 0.1015, accuracy : 97.11\n",
            "iteration : 200, loss : 0.1025, accuracy : 97.10\n",
            "iteration : 250, loss : 0.1046, accuracy : 97.05\n",
            "iteration : 300, loss : 0.1040, accuracy : 97.03\n",
            "iteration : 350, loss : 0.1037, accuracy : 97.05\n",
            "Epoch : 155, training loss : 0.1036, training accuracy : 97.05, test loss : 0.2433, test accuracy : 93.76\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.1026, accuracy : 97.16\n",
            "iteration : 100, loss : 0.0980, accuracy : 97.22\n",
            "iteration : 150, loss : 0.0976, accuracy : 97.31\n",
            "iteration : 200, loss : 0.0982, accuracy : 97.29\n",
            "iteration : 250, loss : 0.0980, accuracy : 97.26\n",
            "iteration : 300, loss : 0.0992, accuracy : 97.22\n",
            "iteration : 350, loss : 0.1001, accuracy : 97.20\n",
            "Epoch : 156, training loss : 0.1005, training accuracy : 97.19, test loss : 0.2378, test accuracy : 93.76\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0861, accuracy : 97.44\n",
            "iteration : 100, loss : 0.0921, accuracy : 97.31\n",
            "iteration : 150, loss : 0.0994, accuracy : 97.12\n",
            "iteration : 200, loss : 0.0984, accuracy : 97.09\n",
            "iteration : 250, loss : 0.0991, accuracy : 97.11\n",
            "iteration : 300, loss : 0.0995, accuracy : 97.14\n",
            "iteration : 350, loss : 0.1002, accuracy : 97.12\n",
            "Epoch : 157, training loss : 0.1007, training accuracy : 97.12, test loss : 0.2299, test accuracy : 93.87\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.1051, accuracy : 96.91\n",
            "iteration : 100, loss : 0.0954, accuracy : 97.16\n",
            "iteration : 150, loss : 0.1025, accuracy : 96.96\n",
            "iteration : 200, loss : 0.1036, accuracy : 96.95\n",
            "iteration : 250, loss : 0.1044, accuracy : 96.97\n",
            "iteration : 300, loss : 0.1048, accuracy : 96.99\n",
            "iteration : 350, loss : 0.1037, accuracy : 97.03\n",
            "Epoch : 158, training loss : 0.1025, training accuracy : 97.08, test loss : 0.2310, test accuracy : 94.04\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0881, accuracy : 97.44\n",
            "iteration : 100, loss : 0.0948, accuracy : 97.28\n",
            "iteration : 150, loss : 0.0953, accuracy : 97.32\n",
            "iteration : 200, loss : 0.0969, accuracy : 97.25\n",
            "iteration : 250, loss : 0.0965, accuracy : 97.32\n",
            "iteration : 300, loss : 0.0954, accuracy : 97.37\n",
            "iteration : 350, loss : 0.0949, accuracy : 97.37\n",
            "Epoch : 159, training loss : 0.0956, training accuracy : 97.33, test loss : 0.2373, test accuracy : 93.93\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0942, accuracy : 97.33\n",
            "iteration : 100, loss : 0.0935, accuracy : 97.45\n",
            "iteration : 150, loss : 0.0943, accuracy : 97.41\n",
            "iteration : 200, loss : 0.0967, accuracy : 97.27\n",
            "iteration : 250, loss : 0.0986, accuracy : 97.20\n",
            "iteration : 300, loss : 0.0967, accuracy : 97.26\n",
            "iteration : 350, loss : 0.0963, accuracy : 97.29\n",
            "Epoch : 160, training loss : 0.0963, training accuracy : 97.30, test loss : 0.2443, test accuracy : 93.85\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0984, accuracy : 97.06\n",
            "iteration : 100, loss : 0.0957, accuracy : 97.10\n",
            "iteration : 150, loss : 0.0951, accuracy : 97.23\n",
            "iteration : 200, loss : 0.0956, accuracy : 97.27\n",
            "iteration : 250, loss : 0.0980, accuracy : 97.22\n",
            "iteration : 300, loss : 0.0991, accuracy : 97.18\n",
            "iteration : 350, loss : 0.1005, accuracy : 97.17\n",
            "Epoch : 161, training loss : 0.0998, training accuracy : 97.19, test loss : 0.2394, test accuracy : 93.92\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0850, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0890, accuracy : 97.47\n",
            "iteration : 150, loss : 0.0911, accuracy : 97.33\n",
            "iteration : 200, loss : 0.0915, accuracy : 97.34\n",
            "iteration : 250, loss : 0.0934, accuracy : 97.31\n",
            "iteration : 300, loss : 0.0958, accuracy : 97.27\n",
            "iteration : 350, loss : 0.0974, accuracy : 97.25\n",
            "Epoch : 162, training loss : 0.0971, training accuracy : 97.25, test loss : 0.2494, test accuracy : 93.65\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.1015, accuracy : 97.11\n",
            "iteration : 100, loss : 0.0959, accuracy : 97.29\n",
            "iteration : 150, loss : 0.0952, accuracy : 97.24\n",
            "iteration : 200, loss : 0.0955, accuracy : 97.22\n",
            "iteration : 250, loss : 0.0947, accuracy : 97.28\n",
            "iteration : 300, loss : 0.0938, accuracy : 97.26\n",
            "iteration : 350, loss : 0.0935, accuracy : 97.29\n",
            "Epoch : 163, training loss : 0.0938, training accuracy : 97.29, test loss : 0.2466, test accuracy : 93.99\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0835, accuracy : 97.56\n",
            "iteration : 100, loss : 0.0792, accuracy : 97.73\n",
            "iteration : 150, loss : 0.0844, accuracy : 97.53\n",
            "iteration : 200, loss : 0.0867, accuracy : 97.45\n",
            "iteration : 250, loss : 0.0884, accuracy : 97.44\n",
            "iteration : 300, loss : 0.0899, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0910, accuracy : 97.36\n",
            "Epoch : 164, training loss : 0.0915, training accuracy : 97.34, test loss : 0.2478, test accuracy : 93.67\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0813, accuracy : 97.42\n",
            "iteration : 100, loss : 0.0860, accuracy : 97.48\n",
            "iteration : 150, loss : 0.0861, accuracy : 97.49\n",
            "iteration : 200, loss : 0.0859, accuracy : 97.54\n",
            "iteration : 250, loss : 0.0866, accuracy : 97.55\n",
            "iteration : 300, loss : 0.0884, accuracy : 97.49\n",
            "iteration : 350, loss : 0.0891, accuracy : 97.47\n",
            "Epoch : 165, training loss : 0.0896, training accuracy : 97.44, test loss : 0.2557, test accuracy : 93.54\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0839, accuracy : 97.55\n",
            "iteration : 100, loss : 0.0837, accuracy : 97.59\n",
            "iteration : 150, loss : 0.0845, accuracy : 97.58\n",
            "iteration : 200, loss : 0.0868, accuracy : 97.59\n",
            "iteration : 250, loss : 0.0892, accuracy : 97.52\n",
            "iteration : 300, loss : 0.0900, accuracy : 97.47\n",
            "iteration : 350, loss : 0.0913, accuracy : 97.44\n",
            "Epoch : 166, training loss : 0.0917, training accuracy : 97.44, test loss : 0.2409, test accuracy : 93.90\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0880, accuracy : 97.44\n",
            "iteration : 100, loss : 0.0887, accuracy : 97.44\n",
            "iteration : 150, loss : 0.0884, accuracy : 97.48\n",
            "iteration : 200, loss : 0.0876, accuracy : 97.50\n",
            "iteration : 250, loss : 0.0876, accuracy : 97.48\n",
            "iteration : 300, loss : 0.0904, accuracy : 97.41\n",
            "iteration : 350, loss : 0.0900, accuracy : 97.42\n",
            "Epoch : 167, training loss : 0.0903, training accuracy : 97.41, test loss : 0.2434, test accuracy : 94.02\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0910, accuracy : 97.36\n",
            "iteration : 100, loss : 0.0881, accuracy : 97.43\n",
            "iteration : 150, loss : 0.0889, accuracy : 97.41\n",
            "iteration : 200, loss : 0.0885, accuracy : 97.49\n",
            "iteration : 250, loss : 0.0868, accuracy : 97.53\n",
            "iteration : 300, loss : 0.0876, accuracy : 97.51\n",
            "iteration : 350, loss : 0.0896, accuracy : 97.44\n",
            "Epoch : 168, training loss : 0.0906, training accuracy : 97.43, test loss : 0.2449, test accuracy : 94.06\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0882, accuracy : 97.55\n",
            "iteration : 100, loss : 0.0893, accuracy : 97.45\n",
            "iteration : 150, loss : 0.0887, accuracy : 97.51\n",
            "iteration : 200, loss : 0.0886, accuracy : 97.52\n",
            "iteration : 250, loss : 0.0905, accuracy : 97.43\n",
            "iteration : 300, loss : 0.0902, accuracy : 97.44\n",
            "iteration : 350, loss : 0.0890, accuracy : 97.48\n",
            "Epoch : 169, training loss : 0.0890, training accuracy : 97.47, test loss : 0.2545, test accuracy : 93.71\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0792, accuracy : 97.73\n",
            "iteration : 100, loss : 0.0805, accuracy : 97.67\n",
            "iteration : 150, loss : 0.0851, accuracy : 97.53\n",
            "iteration : 200, loss : 0.0855, accuracy : 97.48\n",
            "iteration : 250, loss : 0.0871, accuracy : 97.43\n",
            "iteration : 300, loss : 0.0862, accuracy : 97.46\n",
            "iteration : 350, loss : 0.0858, accuracy : 97.47\n",
            "Epoch : 170, training loss : 0.0871, training accuracy : 97.45, test loss : 0.2469, test accuracy : 93.80\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0820, accuracy : 97.80\n",
            "iteration : 100, loss : 0.0799, accuracy : 97.77\n",
            "iteration : 150, loss : 0.0808, accuracy : 97.68\n",
            "iteration : 200, loss : 0.0808, accuracy : 97.62\n",
            "iteration : 250, loss : 0.0823, accuracy : 97.62\n",
            "iteration : 300, loss : 0.0825, accuracy : 97.62\n",
            "iteration : 350, loss : 0.0852, accuracy : 97.58\n",
            "Epoch : 171, training loss : 0.0855, training accuracy : 97.56, test loss : 0.2548, test accuracy : 93.59\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0941, accuracy : 97.34\n",
            "iteration : 100, loss : 0.0918, accuracy : 97.39\n",
            "iteration : 150, loss : 0.0871, accuracy : 97.51\n",
            "iteration : 200, loss : 0.0838, accuracy : 97.62\n",
            "iteration : 250, loss : 0.0833, accuracy : 97.61\n",
            "iteration : 300, loss : 0.0846, accuracy : 97.59\n",
            "iteration : 350, loss : 0.0844, accuracy : 97.58\n",
            "Epoch : 172, training loss : 0.0846, training accuracy : 97.58, test loss : 0.2426, test accuracy : 93.92\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0775, accuracy : 97.81\n",
            "iteration : 100, loss : 0.0768, accuracy : 97.83\n",
            "iteration : 150, loss : 0.0775, accuracy : 97.74\n",
            "iteration : 200, loss : 0.0771, accuracy : 97.77\n",
            "iteration : 250, loss : 0.0779, accuracy : 97.73\n",
            "iteration : 300, loss : 0.0802, accuracy : 97.68\n",
            "iteration : 350, loss : 0.0821, accuracy : 97.63\n",
            "Epoch : 173, training loss : 0.0828, training accuracy : 97.61, test loss : 0.2453, test accuracy : 94.03\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0729, accuracy : 97.88\n",
            "iteration : 100, loss : 0.0815, accuracy : 97.70\n",
            "iteration : 150, loss : 0.0799, accuracy : 97.78\n",
            "iteration : 200, loss : 0.0807, accuracy : 97.72\n",
            "iteration : 250, loss : 0.0808, accuracy : 97.75\n",
            "iteration : 300, loss : 0.0829, accuracy : 97.69\n",
            "iteration : 350, loss : 0.0832, accuracy : 97.68\n",
            "Epoch : 174, training loss : 0.0832, training accuracy : 97.66, test loss : 0.2514, test accuracy : 93.82\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0745, accuracy : 97.77\n",
            "iteration : 100, loss : 0.0749, accuracy : 97.71\n",
            "iteration : 150, loss : 0.0784, accuracy : 97.66\n",
            "iteration : 200, loss : 0.0788, accuracy : 97.67\n",
            "iteration : 250, loss : 0.0825, accuracy : 97.58\n",
            "iteration : 300, loss : 0.0836, accuracy : 97.58\n",
            "iteration : 350, loss : 0.0840, accuracy : 97.54\n",
            "Epoch : 175, training loss : 0.0844, training accuracy : 97.54, test loss : 0.2454, test accuracy : 94.02\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0844, accuracy : 97.56\n",
            "iteration : 100, loss : 0.0776, accuracy : 97.80\n",
            "iteration : 150, loss : 0.0759, accuracy : 97.86\n",
            "iteration : 200, loss : 0.0780, accuracy : 97.80\n",
            "iteration : 250, loss : 0.0790, accuracy : 97.76\n",
            "iteration : 300, loss : 0.0789, accuracy : 97.73\n",
            "iteration : 350, loss : 0.0788, accuracy : 97.71\n",
            "Epoch : 176, training loss : 0.0790, training accuracy : 97.71, test loss : 0.2496, test accuracy : 93.91\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0757, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0784, accuracy : 97.63\n",
            "iteration : 150, loss : 0.0807, accuracy : 97.61\n",
            "iteration : 200, loss : 0.0817, accuracy : 97.62\n",
            "iteration : 250, loss : 0.0800, accuracy : 97.68\n",
            "iteration : 300, loss : 0.0806, accuracy : 97.68\n",
            "iteration : 350, loss : 0.0807, accuracy : 97.66\n",
            "Epoch : 177, training loss : 0.0804, training accuracy : 97.66, test loss : 0.2490, test accuracy : 93.97\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0782, accuracy : 97.84\n",
            "iteration : 100, loss : 0.0744, accuracy : 97.94\n",
            "iteration : 150, loss : 0.0733, accuracy : 97.92\n",
            "iteration : 200, loss : 0.0763, accuracy : 97.79\n",
            "iteration : 250, loss : 0.0804, accuracy : 97.67\n",
            "iteration : 300, loss : 0.0806, accuracy : 97.66\n",
            "iteration : 350, loss : 0.0807, accuracy : 97.68\n",
            "Epoch : 178, training loss : 0.0809, training accuracy : 97.65, test loss : 0.2479, test accuracy : 93.91\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0663, accuracy : 98.02\n",
            "iteration : 100, loss : 0.0701, accuracy : 98.00\n",
            "iteration : 150, loss : 0.0744, accuracy : 97.89\n",
            "iteration : 200, loss : 0.0760, accuracy : 97.84\n",
            "iteration : 250, loss : 0.0762, accuracy : 97.84\n",
            "iteration : 300, loss : 0.0757, accuracy : 97.87\n",
            "iteration : 350, loss : 0.0765, accuracy : 97.81\n",
            "Epoch : 179, training loss : 0.0781, training accuracy : 97.77, test loss : 0.2526, test accuracy : 93.77\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0733, accuracy : 97.72\n",
            "iteration : 100, loss : 0.0723, accuracy : 97.84\n",
            "iteration : 150, loss : 0.0697, accuracy : 97.92\n",
            "iteration : 200, loss : 0.0724, accuracy : 97.89\n",
            "iteration : 250, loss : 0.0744, accuracy : 97.85\n",
            "iteration : 300, loss : 0.0737, accuracy : 97.90\n",
            "iteration : 350, loss : 0.0745, accuracy : 97.88\n",
            "Epoch : 180, training loss : 0.0745, training accuracy : 97.88, test loss : 0.2464, test accuracy : 94.09\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0684, accuracy : 98.00\n",
            "iteration : 100, loss : 0.0683, accuracy : 98.03\n",
            "iteration : 150, loss : 0.0709, accuracy : 97.97\n",
            "iteration : 200, loss : 0.0714, accuracy : 97.97\n",
            "iteration : 250, loss : 0.0711, accuracy : 97.97\n",
            "iteration : 300, loss : 0.0737, accuracy : 97.91\n",
            "iteration : 350, loss : 0.0754, accuracy : 97.86\n",
            "Epoch : 181, training loss : 0.0757, training accuracy : 97.85, test loss : 0.2529, test accuracy : 93.79\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0855, accuracy : 97.77\n",
            "iteration : 100, loss : 0.0795, accuracy : 97.77\n",
            "iteration : 150, loss : 0.0807, accuracy : 97.72\n",
            "iteration : 200, loss : 0.0794, accuracy : 97.73\n",
            "iteration : 250, loss : 0.0784, accuracy : 97.78\n",
            "iteration : 300, loss : 0.0780, accuracy : 97.80\n",
            "iteration : 350, loss : 0.0779, accuracy : 97.80\n",
            "Epoch : 182, training loss : 0.0778, training accuracy : 97.81, test loss : 0.2446, test accuracy : 93.97\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0673, accuracy : 98.16\n",
            "iteration : 100, loss : 0.0690, accuracy : 98.00\n",
            "iteration : 150, loss : 0.0728, accuracy : 97.91\n",
            "iteration : 200, loss : 0.0736, accuracy : 97.91\n",
            "iteration : 250, loss : 0.0740, accuracy : 97.91\n",
            "iteration : 300, loss : 0.0733, accuracy : 97.92\n",
            "iteration : 350, loss : 0.0731, accuracy : 97.90\n",
            "Epoch : 183, training loss : 0.0732, training accuracy : 97.89, test loss : 0.2410, test accuracy : 94.10\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0718, accuracy : 97.92\n",
            "iteration : 100, loss : 0.0717, accuracy : 97.95\n",
            "iteration : 150, loss : 0.0713, accuracy : 98.01\n",
            "iteration : 200, loss : 0.0710, accuracy : 98.03\n",
            "iteration : 250, loss : 0.0719, accuracy : 98.01\n",
            "iteration : 300, loss : 0.0728, accuracy : 97.92\n",
            "iteration : 350, loss : 0.0743, accuracy : 97.85\n",
            "Epoch : 184, training loss : 0.0746, training accuracy : 97.84, test loss : 0.2478, test accuracy : 94.15\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0620, accuracy : 98.39\n",
            "iteration : 100, loss : 0.0689, accuracy : 98.13\n",
            "iteration : 150, loss : 0.0708, accuracy : 98.03\n",
            "iteration : 200, loss : 0.0721, accuracy : 97.97\n",
            "iteration : 250, loss : 0.0722, accuracy : 97.98\n",
            "iteration : 300, loss : 0.0735, accuracy : 97.91\n",
            "iteration : 350, loss : 0.0742, accuracy : 97.88\n",
            "Epoch : 185, training loss : 0.0743, training accuracy : 97.87, test loss : 0.2516, test accuracy : 93.89\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0688, accuracy : 98.06\n",
            "iteration : 100, loss : 0.0676, accuracy : 98.04\n",
            "iteration : 150, loss : 0.0685, accuracy : 98.01\n",
            "iteration : 200, loss : 0.0694, accuracy : 97.98\n",
            "iteration : 250, loss : 0.0710, accuracy : 97.92\n",
            "iteration : 300, loss : 0.0716, accuracy : 97.94\n",
            "iteration : 350, loss : 0.0716, accuracy : 97.93\n",
            "Epoch : 186, training loss : 0.0720, training accuracy : 97.92, test loss : 0.2530, test accuracy : 93.92\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0580, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0651, accuracy : 98.18\n",
            "iteration : 150, loss : 0.0651, accuracy : 98.15\n",
            "iteration : 200, loss : 0.0671, accuracy : 98.10\n",
            "iteration : 250, loss : 0.0681, accuracy : 98.07\n",
            "iteration : 300, loss : 0.0697, accuracy : 98.02\n",
            "iteration : 350, loss : 0.0688, accuracy : 98.03\n",
            "Epoch : 187, training loss : 0.0695, training accuracy : 98.01, test loss : 0.2538, test accuracy : 93.89\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0633, accuracy : 97.89\n",
            "iteration : 100, loss : 0.0676, accuracy : 97.91\n",
            "iteration : 150, loss : 0.0689, accuracy : 97.96\n",
            "iteration : 200, loss : 0.0674, accuracy : 97.98\n",
            "iteration : 250, loss : 0.0684, accuracy : 97.95\n",
            "iteration : 300, loss : 0.0690, accuracy : 97.93\n",
            "iteration : 350, loss : 0.0694, accuracy : 97.93\n",
            "Epoch : 188, training loss : 0.0700, training accuracy : 97.93, test loss : 0.2470, test accuracy : 94.26\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0638, accuracy : 98.16\n",
            "iteration : 100, loss : 0.0662, accuracy : 98.12\n",
            "iteration : 150, loss : 0.0639, accuracy : 98.16\n",
            "iteration : 200, loss : 0.0664, accuracy : 98.04\n",
            "iteration : 250, loss : 0.0677, accuracy : 97.99\n",
            "iteration : 300, loss : 0.0683, accuracy : 98.01\n",
            "iteration : 350, loss : 0.0695, accuracy : 97.97\n",
            "Epoch : 189, training loss : 0.0697, training accuracy : 97.97, test loss : 0.2586, test accuracy : 93.81\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0587, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0583, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0589, accuracy : 98.34\n",
            "iteration : 200, loss : 0.0611, accuracy : 98.28\n",
            "iteration : 250, loss : 0.0629, accuracy : 98.23\n",
            "iteration : 300, loss : 0.0658, accuracy : 98.12\n",
            "iteration : 350, loss : 0.0683, accuracy : 98.07\n",
            "Epoch : 190, training loss : 0.0680, training accuracy : 98.08, test loss : 0.2538, test accuracy : 93.88\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0593, accuracy : 98.25\n",
            "iteration : 100, loss : 0.0618, accuracy : 98.18\n",
            "iteration : 150, loss : 0.0630, accuracy : 98.20\n",
            "iteration : 200, loss : 0.0646, accuracy : 98.11\n",
            "iteration : 250, loss : 0.0649, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0653, accuracy : 98.11\n",
            "iteration : 350, loss : 0.0665, accuracy : 98.05\n",
            "Epoch : 191, training loss : 0.0665, training accuracy : 98.05, test loss : 0.2551, test accuracy : 94.02\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0629, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0622, accuracy : 98.26\n",
            "iteration : 150, loss : 0.0614, accuracy : 98.26\n",
            "iteration : 200, loss : 0.0642, accuracy : 98.16\n",
            "iteration : 250, loss : 0.0658, accuracy : 98.08\n",
            "iteration : 300, loss : 0.0643, accuracy : 98.14\n",
            "iteration : 350, loss : 0.0662, accuracy : 98.08\n",
            "Epoch : 192, training loss : 0.0663, training accuracy : 98.08, test loss : 0.2525, test accuracy : 94.08\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0549, accuracy : 98.30\n",
            "iteration : 100, loss : 0.0566, accuracy : 98.26\n",
            "iteration : 150, loss : 0.0616, accuracy : 98.16\n",
            "iteration : 200, loss : 0.0614, accuracy : 98.17\n",
            "iteration : 250, loss : 0.0628, accuracy : 98.13\n",
            "iteration : 300, loss : 0.0640, accuracy : 98.09\n",
            "iteration : 350, loss : 0.0636, accuracy : 98.14\n",
            "Epoch : 193, training loss : 0.0635, training accuracy : 98.15, test loss : 0.2513, test accuracy : 94.15\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0553, accuracy : 98.33\n",
            "iteration : 100, loss : 0.0571, accuracy : 98.33\n",
            "iteration : 150, loss : 0.0602, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0604, accuracy : 98.26\n",
            "iteration : 250, loss : 0.0626, accuracy : 98.21\n",
            "iteration : 300, loss : 0.0631, accuracy : 98.18\n",
            "iteration : 350, loss : 0.0629, accuracy : 98.19\n",
            "Epoch : 194, training loss : 0.0632, training accuracy : 98.19, test loss : 0.2571, test accuracy : 94.10\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0655, accuracy : 98.14\n",
            "iteration : 100, loss : 0.0590, accuracy : 98.26\n",
            "iteration : 150, loss : 0.0583, accuracy : 98.33\n",
            "iteration : 200, loss : 0.0601, accuracy : 98.28\n",
            "iteration : 250, loss : 0.0608, accuracy : 98.25\n",
            "iteration : 300, loss : 0.0604, accuracy : 98.23\n",
            "iteration : 350, loss : 0.0623, accuracy : 98.19\n",
            "Epoch : 195, training loss : 0.0625, training accuracy : 98.19, test loss : 0.2602, test accuracy : 93.85\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0563, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0572, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0604, accuracy : 98.34\n",
            "iteration : 200, loss : 0.0600, accuracy : 98.34\n",
            "iteration : 250, loss : 0.0621, accuracy : 98.24\n",
            "iteration : 300, loss : 0.0632, accuracy : 98.22\n",
            "iteration : 350, loss : 0.0619, accuracy : 98.24\n",
            "Epoch : 196, training loss : 0.0616, training accuracy : 98.24, test loss : 0.2536, test accuracy : 94.03\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0547, accuracy : 98.42\n",
            "iteration : 100, loss : 0.0569, accuracy : 98.37\n",
            "iteration : 150, loss : 0.0611, accuracy : 98.20\n",
            "iteration : 200, loss : 0.0607, accuracy : 98.20\n",
            "iteration : 250, loss : 0.0604, accuracy : 98.19\n",
            "iteration : 300, loss : 0.0604, accuracy : 98.19\n",
            "iteration : 350, loss : 0.0612, accuracy : 98.18\n",
            "Epoch : 197, training loss : 0.0612, training accuracy : 98.18, test loss : 0.2597, test accuracy : 93.92\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0586, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0590, accuracy : 98.35\n",
            "iteration : 150, loss : 0.0590, accuracy : 98.38\n",
            "iteration : 200, loss : 0.0565, accuracy : 98.43\n",
            "iteration : 250, loss : 0.0592, accuracy : 98.33\n",
            "iteration : 300, loss : 0.0607, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0601, accuracy : 98.29\n",
            "Epoch : 198, training loss : 0.0604, training accuracy : 98.28, test loss : 0.2626, test accuracy : 93.87\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0527, accuracy : 98.58\n",
            "iteration : 100, loss : 0.0514, accuracy : 98.52\n",
            "iteration : 150, loss : 0.0539, accuracy : 98.39\n",
            "iteration : 200, loss : 0.0561, accuracy : 98.31\n",
            "iteration : 250, loss : 0.0565, accuracy : 98.35\n",
            "iteration : 300, loss : 0.0575, accuracy : 98.31\n",
            "iteration : 350, loss : 0.0588, accuracy : 98.27\n",
            "Epoch : 199, training loss : 0.0594, training accuracy : 98.27, test loss : 0.2576, test accuracy : 94.13\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0565, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0533, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0565, accuracy : 98.30\n",
            "iteration : 200, loss : 0.0597, accuracy : 98.21\n",
            "iteration : 250, loss : 0.0606, accuracy : 98.19\n",
            "iteration : 300, loss : 0.0595, accuracy : 98.23\n",
            "iteration : 350, loss : 0.0589, accuracy : 98.24\n",
            "Epoch : 200, training loss : 0.0588, training accuracy : 98.25, test loss : 0.2537, test accuracy : 94.14\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0508, accuracy : 98.59\n",
            "iteration : 100, loss : 0.0498, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0515, accuracy : 98.54\n",
            "iteration : 200, loss : 0.0515, accuracy : 98.54\n",
            "iteration : 250, loss : 0.0529, accuracy : 98.48\n",
            "iteration : 300, loss : 0.0533, accuracy : 98.47\n",
            "iteration : 350, loss : 0.0541, accuracy : 98.47\n",
            "Epoch : 201, training loss : 0.0547, training accuracy : 98.44, test loss : 0.2684, test accuracy : 93.94\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0560, accuracy : 98.52\n",
            "iteration : 100, loss : 0.0543, accuracy : 98.53\n",
            "iteration : 150, loss : 0.0527, accuracy : 98.55\n",
            "iteration : 200, loss : 0.0533, accuracy : 98.52\n",
            "iteration : 250, loss : 0.0547, accuracy : 98.50\n",
            "iteration : 300, loss : 0.0558, accuracy : 98.46\n",
            "iteration : 350, loss : 0.0564, accuracy : 98.42\n",
            "Epoch : 202, training loss : 0.0561, training accuracy : 98.42, test loss : 0.2604, test accuracy : 94.08\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0535, accuracy : 98.30\n",
            "iteration : 100, loss : 0.0518, accuracy : 98.46\n",
            "iteration : 150, loss : 0.0547, accuracy : 98.37\n",
            "iteration : 200, loss : 0.0552, accuracy : 98.36\n",
            "iteration : 250, loss : 0.0557, accuracy : 98.34\n",
            "iteration : 300, loss : 0.0556, accuracy : 98.32\n",
            "iteration : 350, loss : 0.0569, accuracy : 98.29\n",
            "Epoch : 203, training loss : 0.0565, training accuracy : 98.31, test loss : 0.2634, test accuracy : 94.08\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0582, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0544, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0552, accuracy : 98.38\n",
            "iteration : 200, loss : 0.0542, accuracy : 98.43\n",
            "iteration : 250, loss : 0.0545, accuracy : 98.44\n",
            "iteration : 300, loss : 0.0548, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0552, accuracy : 98.40\n",
            "Epoch : 204, training loss : 0.0556, training accuracy : 98.37, test loss : 0.2612, test accuracy : 94.04\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0456, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0512, accuracy : 98.52\n",
            "iteration : 150, loss : 0.0500, accuracy : 98.55\n",
            "iteration : 200, loss : 0.0496, accuracy : 98.59\n",
            "iteration : 250, loss : 0.0500, accuracy : 98.57\n",
            "iteration : 300, loss : 0.0498, accuracy : 98.60\n",
            "iteration : 350, loss : 0.0509, accuracy : 98.56\n",
            "Epoch : 205, training loss : 0.0513, training accuracy : 98.54, test loss : 0.2759, test accuracy : 93.86\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0527, accuracy : 98.53\n",
            "iteration : 100, loss : 0.0481, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0498, accuracy : 98.62\n",
            "iteration : 200, loss : 0.0513, accuracy : 98.57\n",
            "iteration : 250, loss : 0.0500, accuracy : 98.62\n",
            "iteration : 300, loss : 0.0508, accuracy : 98.61\n",
            "iteration : 350, loss : 0.0518, accuracy : 98.54\n",
            "Epoch : 206, training loss : 0.0522, training accuracy : 98.52, test loss : 0.2699, test accuracy : 94.03\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0545, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0482, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0515, accuracy : 98.57\n",
            "iteration : 200, loss : 0.0522, accuracy : 98.52\n",
            "iteration : 250, loss : 0.0517, accuracy : 98.53\n",
            "iteration : 300, loss : 0.0520, accuracy : 98.53\n",
            "iteration : 350, loss : 0.0513, accuracy : 98.52\n",
            "Epoch : 207, training loss : 0.0516, training accuracy : 98.52, test loss : 0.2690, test accuracy : 94.00\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0482, accuracy : 98.55\n",
            "iteration : 100, loss : 0.0455, accuracy : 98.70\n",
            "iteration : 150, loss : 0.0458, accuracy : 98.70\n",
            "iteration : 200, loss : 0.0493, accuracy : 98.59\n",
            "iteration : 250, loss : 0.0500, accuracy : 98.57\n",
            "iteration : 300, loss : 0.0496, accuracy : 98.59\n",
            "iteration : 350, loss : 0.0493, accuracy : 98.62\n",
            "Epoch : 208, training loss : 0.0495, training accuracy : 98.62, test loss : 0.2612, test accuracy : 94.19\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0481, accuracy : 98.55\n",
            "iteration : 100, loss : 0.0446, accuracy : 98.72\n",
            "iteration : 150, loss : 0.0466, accuracy : 98.71\n",
            "iteration : 200, loss : 0.0469, accuracy : 98.73\n",
            "iteration : 250, loss : 0.0497, accuracy : 98.64\n",
            "iteration : 300, loss : 0.0509, accuracy : 98.57\n",
            "iteration : 350, loss : 0.0511, accuracy : 98.56\n",
            "Epoch : 209, training loss : 0.0512, training accuracy : 98.54, test loss : 0.2696, test accuracy : 94.02\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0479, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0449, accuracy : 98.71\n",
            "iteration : 150, loss : 0.0459, accuracy : 98.69\n",
            "iteration : 200, loss : 0.0459, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0461, accuracy : 98.73\n",
            "iteration : 300, loss : 0.0450, accuracy : 98.76\n",
            "iteration : 350, loss : 0.0473, accuracy : 98.69\n",
            "Epoch : 210, training loss : 0.0482, training accuracy : 98.67, test loss : 0.2699, test accuracy : 94.09\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0422, accuracy : 98.61\n",
            "iteration : 100, loss : 0.0447, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0469, accuracy : 98.52\n",
            "iteration : 200, loss : 0.0475, accuracy : 98.58\n",
            "iteration : 250, loss : 0.0472, accuracy : 98.59\n",
            "iteration : 300, loss : 0.0468, accuracy : 98.58\n",
            "iteration : 350, loss : 0.0473, accuracy : 98.58\n",
            "Epoch : 211, training loss : 0.0471, training accuracy : 98.60, test loss : 0.2700, test accuracy : 94.22\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0463, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0455, accuracy : 98.62\n",
            "iteration : 150, loss : 0.0473, accuracy : 98.56\n",
            "iteration : 200, loss : 0.0455, accuracy : 98.61\n",
            "iteration : 250, loss : 0.0452, accuracy : 98.66\n",
            "iteration : 300, loss : 0.0458, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0460, accuracy : 98.67\n",
            "Epoch : 212, training loss : 0.0464, training accuracy : 98.65, test loss : 0.2781, test accuracy : 93.99\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0424, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0433, accuracy : 98.73\n",
            "iteration : 150, loss : 0.0418, accuracy : 98.79\n",
            "iteration : 200, loss : 0.0437, accuracy : 98.77\n",
            "iteration : 250, loss : 0.0437, accuracy : 98.75\n",
            "iteration : 300, loss : 0.0438, accuracy : 98.76\n",
            "iteration : 350, loss : 0.0439, accuracy : 98.75\n",
            "Epoch : 213, training loss : 0.0442, training accuracy : 98.75, test loss : 0.2692, test accuracy : 94.06\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0467, accuracy : 98.77\n",
            "iteration : 100, loss : 0.0412, accuracy : 98.90\n",
            "iteration : 150, loss : 0.0398, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0415, accuracy : 98.81\n",
            "iteration : 250, loss : 0.0437, accuracy : 98.74\n",
            "iteration : 300, loss : 0.0452, accuracy : 98.71\n",
            "iteration : 350, loss : 0.0456, accuracy : 98.69\n",
            "Epoch : 214, training loss : 0.0452, training accuracy : 98.69, test loss : 0.2656, test accuracy : 94.28\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0389, accuracy : 98.95\n",
            "iteration : 100, loss : 0.0392, accuracy : 98.87\n",
            "iteration : 150, loss : 0.0403, accuracy : 98.89\n",
            "iteration : 200, loss : 0.0408, accuracy : 98.86\n",
            "iteration : 250, loss : 0.0423, accuracy : 98.78\n",
            "iteration : 300, loss : 0.0427, accuracy : 98.76\n",
            "iteration : 350, loss : 0.0433, accuracy : 98.73\n",
            "Epoch : 215, training loss : 0.0440, training accuracy : 98.70, test loss : 0.2779, test accuracy : 94.00\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0417, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0423, accuracy : 98.72\n",
            "iteration : 150, loss : 0.0423, accuracy : 98.74\n",
            "iteration : 200, loss : 0.0423, accuracy : 98.78\n",
            "iteration : 250, loss : 0.0429, accuracy : 98.74\n",
            "iteration : 300, loss : 0.0427, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0425, accuracy : 98.75\n",
            "Epoch : 216, training loss : 0.0427, training accuracy : 98.75, test loss : 0.2727, test accuracy : 93.97\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0451, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0412, accuracy : 98.85\n",
            "iteration : 150, loss : 0.0424, accuracy : 98.78\n",
            "iteration : 200, loss : 0.0403, accuracy : 98.88\n",
            "iteration : 250, loss : 0.0417, accuracy : 98.80\n",
            "iteration : 300, loss : 0.0407, accuracy : 98.83\n",
            "iteration : 350, loss : 0.0421, accuracy : 98.78\n",
            "Epoch : 217, training loss : 0.0426, training accuracy : 98.77, test loss : 0.2676, test accuracy : 94.28\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0415, accuracy : 98.75\n",
            "iteration : 100, loss : 0.0435, accuracy : 98.72\n",
            "iteration : 150, loss : 0.0429, accuracy : 98.73\n",
            "iteration : 200, loss : 0.0451, accuracy : 98.68\n",
            "iteration : 250, loss : 0.0442, accuracy : 98.68\n",
            "iteration : 300, loss : 0.0429, accuracy : 98.72\n",
            "iteration : 350, loss : 0.0434, accuracy : 98.67\n",
            "Epoch : 218, training loss : 0.0438, training accuracy : 98.67, test loss : 0.2811, test accuracy : 93.98\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0393, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0385, accuracy : 98.91\n",
            "iteration : 150, loss : 0.0383, accuracy : 98.91\n",
            "iteration : 200, loss : 0.0385, accuracy : 98.92\n",
            "iteration : 250, loss : 0.0370, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0383, accuracy : 98.92\n",
            "iteration : 350, loss : 0.0392, accuracy : 98.91\n",
            "Epoch : 219, training loss : 0.0394, training accuracy : 98.90, test loss : 0.2740, test accuracy : 94.23\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0361, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0347, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0362, accuracy : 98.93\n",
            "iteration : 200, loss : 0.0353, accuracy : 98.95\n",
            "iteration : 250, loss : 0.0381, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0388, accuracy : 98.84\n",
            "iteration : 350, loss : 0.0400, accuracy : 98.80\n",
            "Epoch : 220, training loss : 0.0397, training accuracy : 98.80, test loss : 0.2791, test accuracy : 94.24\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0340, accuracy : 98.88\n",
            "iteration : 100, loss : 0.0345, accuracy : 98.89\n",
            "iteration : 150, loss : 0.0371, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0373, accuracy : 98.84\n",
            "iteration : 250, loss : 0.0409, accuracy : 98.76\n",
            "iteration : 300, loss : 0.0412, accuracy : 98.78\n",
            "iteration : 350, loss : 0.0415, accuracy : 98.75\n",
            "Epoch : 221, training loss : 0.0409, training accuracy : 98.78, test loss : 0.2764, test accuracy : 94.06\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0369, accuracy : 98.88\n",
            "iteration : 100, loss : 0.0375, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0381, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0371, accuracy : 98.90\n",
            "iteration : 250, loss : 0.0373, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0378, accuracy : 98.86\n",
            "iteration : 350, loss : 0.0384, accuracy : 98.85\n",
            "Epoch : 222, training loss : 0.0384, training accuracy : 98.85, test loss : 0.2755, test accuracy : 94.13\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0384, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0363, accuracy : 98.95\n",
            "iteration : 150, loss : 0.0371, accuracy : 98.91\n",
            "iteration : 200, loss : 0.0370, accuracy : 98.92\n",
            "iteration : 250, loss : 0.0382, accuracy : 98.90\n",
            "iteration : 300, loss : 0.0382, accuracy : 98.90\n",
            "iteration : 350, loss : 0.0379, accuracy : 98.88\n",
            "Epoch : 223, training loss : 0.0380, training accuracy : 98.87, test loss : 0.2721, test accuracy : 94.19\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0374, accuracy : 98.92\n",
            "iteration : 100, loss : 0.0368, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0349, accuracy : 98.93\n",
            "iteration : 200, loss : 0.0352, accuracy : 98.96\n",
            "iteration : 250, loss : 0.0353, accuracy : 98.94\n",
            "iteration : 300, loss : 0.0353, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0359, accuracy : 98.95\n",
            "Epoch : 224, training loss : 0.0360, training accuracy : 98.94, test loss : 0.2717, test accuracy : 94.49\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0322, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0336, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0369, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0362, accuracy : 98.97\n",
            "iteration : 250, loss : 0.0359, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0361, accuracy : 98.96\n",
            "iteration : 350, loss : 0.0352, accuracy : 98.97\n",
            "Epoch : 225, training loss : 0.0348, training accuracy : 98.98, test loss : 0.2938, test accuracy : 93.99\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0380, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0368, accuracy : 99.00\n",
            "iteration : 150, loss : 0.0360, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0357, accuracy : 99.03\n",
            "iteration : 250, loss : 0.0354, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0359, accuracy : 98.98\n",
            "iteration : 350, loss : 0.0356, accuracy : 98.96\n",
            "Epoch : 226, training loss : 0.0352, training accuracy : 98.98, test loss : 0.2742, test accuracy : 94.29\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0371, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0340, accuracy : 99.08\n",
            "iteration : 150, loss : 0.0312, accuracy : 99.15\n",
            "iteration : 200, loss : 0.0323, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0319, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0327, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0330, accuracy : 99.08\n",
            "Epoch : 227, training loss : 0.0328, training accuracy : 99.07, test loss : 0.2835, test accuracy : 94.03\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0328, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0334, accuracy : 98.99\n",
            "iteration : 150, loss : 0.0320, accuracy : 99.07\n",
            "iteration : 200, loss : 0.0321, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0317, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0325, accuracy : 99.08\n",
            "iteration : 350, loss : 0.0326, accuracy : 99.06\n",
            "Epoch : 228, training loss : 0.0327, training accuracy : 99.07, test loss : 0.2800, test accuracy : 94.39\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0311, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0305, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0327, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0332, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0340, accuracy : 98.99\n",
            "iteration : 300, loss : 0.0358, accuracy : 98.96\n",
            "iteration : 350, loss : 0.0354, accuracy : 98.96\n",
            "Epoch : 229, training loss : 0.0357, training accuracy : 98.96, test loss : 0.2747, test accuracy : 94.20\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0288, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0294, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0302, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0311, accuracy : 99.12\n",
            "iteration : 250, loss : 0.0309, accuracy : 99.10\n",
            "iteration : 300, loss : 0.0329, accuracy : 99.05\n",
            "iteration : 350, loss : 0.0331, accuracy : 99.05\n",
            "Epoch : 230, training loss : 0.0334, training accuracy : 99.05, test loss : 0.2820, test accuracy : 94.18\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0314, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0316, accuracy : 99.00\n",
            "iteration : 150, loss : 0.0311, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0319, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0314, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0307, accuracy : 99.11\n",
            "iteration : 350, loss : 0.0309, accuracy : 99.09\n",
            "Epoch : 231, training loss : 0.0312, training accuracy : 99.09, test loss : 0.2871, test accuracy : 94.11\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0317, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0317, accuracy : 99.03\n",
            "iteration : 150, loss : 0.0325, accuracy : 99.02\n",
            "iteration : 200, loss : 0.0314, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0307, accuracy : 99.07\n",
            "iteration : 300, loss : 0.0299, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0303, accuracy : 99.08\n",
            "Epoch : 232, training loss : 0.0304, training accuracy : 99.09, test loss : 0.2875, test accuracy : 94.07\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0366, accuracy : 98.89\n",
            "iteration : 100, loss : 0.0353, accuracy : 98.97\n",
            "iteration : 150, loss : 0.0330, accuracy : 99.01\n",
            "iteration : 200, loss : 0.0333, accuracy : 98.99\n",
            "iteration : 250, loss : 0.0333, accuracy : 98.99\n",
            "iteration : 300, loss : 0.0330, accuracy : 99.02\n",
            "iteration : 350, loss : 0.0328, accuracy : 99.01\n",
            "Epoch : 233, training loss : 0.0324, training accuracy : 99.03, test loss : 0.2798, test accuracy : 94.23\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0268, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0302, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0286, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0284, accuracy : 99.16\n",
            "iteration : 250, loss : 0.0282, accuracy : 99.18\n",
            "iteration : 300, loss : 0.0284, accuracy : 99.18\n",
            "iteration : 350, loss : 0.0284, accuracy : 99.19\n",
            "Epoch : 234, training loss : 0.0290, training accuracy : 99.17, test loss : 0.2877, test accuracy : 94.38\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0262, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0277, accuracy : 99.18\n",
            "iteration : 150, loss : 0.0277, accuracy : 99.20\n",
            "iteration : 200, loss : 0.0274, accuracy : 99.20\n",
            "iteration : 250, loss : 0.0271, accuracy : 99.22\n",
            "iteration : 300, loss : 0.0266, accuracy : 99.24\n",
            "iteration : 350, loss : 0.0265, accuracy : 99.25\n",
            "Epoch : 235, training loss : 0.0264, training accuracy : 99.26, test loss : 0.2826, test accuracy : 94.32\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0290, accuracy : 99.08\n",
            "iteration : 100, loss : 0.0294, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0280, accuracy : 99.20\n",
            "iteration : 200, loss : 0.0280, accuracy : 99.22\n",
            "iteration : 250, loss : 0.0283, accuracy : 99.20\n",
            "iteration : 300, loss : 0.0281, accuracy : 99.21\n",
            "iteration : 350, loss : 0.0271, accuracy : 99.22\n",
            "Epoch : 236, training loss : 0.0270, training accuracy : 99.21, test loss : 0.2889, test accuracy : 94.21\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0242, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0252, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0249, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0258, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0263, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0261, accuracy : 99.23\n",
            "iteration : 350, loss : 0.0259, accuracy : 99.27\n",
            "Epoch : 237, training loss : 0.0264, training accuracy : 99.24, test loss : 0.2812, test accuracy : 94.22\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0258, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0262, accuracy : 99.30\n",
            "iteration : 150, loss : 0.0280, accuracy : 99.28\n",
            "iteration : 200, loss : 0.0273, accuracy : 99.24\n",
            "iteration : 250, loss : 0.0262, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0253, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0263, accuracy : 99.25\n",
            "Epoch : 238, training loss : 0.0264, training accuracy : 99.25, test loss : 0.2927, test accuracy : 94.25\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0202, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0241, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0229, accuracy : 99.37\n",
            "iteration : 200, loss : 0.0238, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0236, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0238, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0242, accuracy : 99.34\n",
            "Epoch : 239, training loss : 0.0246, training accuracy : 99.32, test loss : 0.2832, test accuracy : 94.32\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0252, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0259, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0246, accuracy : 99.26\n",
            "iteration : 200, loss : 0.0256, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0253, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0249, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0256, accuracy : 99.23\n",
            "Epoch : 240, training loss : 0.0258, training accuracy : 99.22, test loss : 0.2852, test accuracy : 94.35\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0208, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0239, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0251, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0248, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0240, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0246, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0249, accuracy : 99.31\n",
            "Epoch : 241, training loss : 0.0249, training accuracy : 99.30, test loss : 0.2897, test accuracy : 94.22\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0248, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0228, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0231, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0239, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0237, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0240, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0240, accuracy : 99.29\n",
            "Epoch : 242, training loss : 0.0239, training accuracy : 99.29, test loss : 0.2853, test accuracy : 94.30\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0270, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0257, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0257, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0252, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0239, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0236, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0235, accuracy : 99.29\n",
            "Epoch : 243, training loss : 0.0236, training accuracy : 99.28, test loss : 0.2913, test accuracy : 94.25\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0178, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0214, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0232, accuracy : 99.30\n",
            "iteration : 200, loss : 0.0231, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0221, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0226, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0228, accuracy : 99.33\n",
            "Epoch : 244, training loss : 0.0231, training accuracy : 99.33, test loss : 0.2903, test accuracy : 94.26\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0175, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0185, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0198, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0198, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0205, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0208, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0214, accuracy : 99.38\n",
            "Epoch : 245, training loss : 0.0217, training accuracy : 99.37, test loss : 0.3014, test accuracy : 94.30\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0232, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0212, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0210, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0211, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0205, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0200, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0198, accuracy : 99.45\n",
            "Epoch : 246, training loss : 0.0198, training accuracy : 99.45, test loss : 0.2923, test accuracy : 94.42\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0252, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0248, accuracy : 99.27\n",
            "iteration : 150, loss : 0.0229, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0224, accuracy : 99.33\n",
            "iteration : 250, loss : 0.0215, accuracy : 99.37\n",
            "iteration : 300, loss : 0.0216, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0221, accuracy : 99.35\n",
            "Epoch : 247, training loss : 0.0224, training accuracy : 99.35, test loss : 0.3025, test accuracy : 94.18\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0171, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0169, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0179, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0191, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0197, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0191, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0187, accuracy : 99.47\n",
            "Epoch : 248, training loss : 0.0191, training accuracy : 99.47, test loss : 0.2924, test accuracy : 94.36\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0156, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0160, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0192, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0184, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0195, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0200, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0206, accuracy : 99.40\n",
            "Epoch : 249, training loss : 0.0204, training accuracy : 99.41, test loss : 0.2922, test accuracy : 94.37\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0175, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0184, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0196, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0200, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0199, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0193, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0196, accuracy : 99.44\n",
            "Epoch : 250, training loss : 0.0195, training accuracy : 99.44, test loss : 0.2971, test accuracy : 94.13\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0188, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0162, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0160, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0158, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0161, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0175, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0181, accuracy : 99.47\n",
            "Epoch : 251, training loss : 0.0181, training accuracy : 99.47, test loss : 0.3012, test accuracy : 94.20\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0178, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0190, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0189, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0182, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0179, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0189, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0188, accuracy : 99.46\n",
            "Epoch : 252, training loss : 0.0190, training accuracy : 99.45, test loss : 0.2938, test accuracy : 94.34\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0201, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0196, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0210, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0195, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0195, accuracy : 99.45\n",
            "iteration : 300, loss : 0.0193, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0189, accuracy : 99.47\n",
            "Epoch : 253, training loss : 0.0193, training accuracy : 99.46, test loss : 0.2977, test accuracy : 94.34\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0180, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0168, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0177, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0172, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0177, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0171, accuracy : 99.51\n",
            "Epoch : 254, training loss : 0.0172, training accuracy : 99.51, test loss : 0.2941, test accuracy : 94.37\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0186, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0176, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0171, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0163, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0159, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0164, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0169, accuracy : 99.52\n",
            "Epoch : 255, training loss : 0.0169, training accuracy : 99.51, test loss : 0.2996, test accuracy : 94.35\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0149, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0145, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0151, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0153, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0153, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0147, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0154, accuracy : 99.57\n",
            "Epoch : 256, training loss : 0.0156, training accuracy : 99.56, test loss : 0.2989, test accuracy : 94.24\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0153, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0141, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0150, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0155, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0158, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0158, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0157, accuracy : 99.58\n",
            "Epoch : 257, training loss : 0.0155, training accuracy : 99.59, test loss : 0.3002, test accuracy : 94.37\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0183, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0175, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0158, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0158, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0155, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0157, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0163, accuracy : 99.52\n",
            "Epoch : 258, training loss : 0.0162, training accuracy : 99.53, test loss : 0.2969, test accuracy : 94.36\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0180, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0171, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0158, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0163, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0152, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0154, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0155, accuracy : 99.58\n",
            "Epoch : 259, training loss : 0.0157, training accuracy : 99.56, test loss : 0.2963, test accuracy : 94.40\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0146, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0151, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0152, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0150, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0153, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0157, accuracy : 99.56\n",
            "Epoch : 260, training loss : 0.0159, training accuracy : 99.57, test loss : 0.3068, test accuracy : 94.26\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0143, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0148, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0150, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0155, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0149, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0146, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0148, accuracy : 99.60\n",
            "Epoch : 261, training loss : 0.0146, training accuracy : 99.60, test loss : 0.2958, test accuracy : 94.44\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0125, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0123, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0129, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0131, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0137, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0140, accuracy : 99.61\n",
            "Epoch : 262, training loss : 0.0139, training accuracy : 99.61, test loss : 0.2997, test accuracy : 94.51\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0126, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0134, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0137, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0134, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0133, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0135, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0136, accuracy : 99.64\n",
            "Epoch : 263, training loss : 0.0134, training accuracy : 99.65, test loss : 0.2924, test accuracy : 94.54\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0163, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0130, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0139, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0137, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0136, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0137, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0137, accuracy : 99.62\n",
            "Epoch : 264, training loss : 0.0134, training accuracy : 99.63, test loss : 0.2960, test accuracy : 94.40\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0100, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0105, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0112, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0110, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0113, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0118, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.73\n",
            "Epoch : 265, training loss : 0.0119, training accuracy : 99.72, test loss : 0.2993, test accuracy : 94.41\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0106, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0129, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0134, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0134, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0131, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0132, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0133, accuracy : 99.63\n",
            "Epoch : 266, training loss : 0.0132, training accuracy : 99.64, test loss : 0.3057, test accuracy : 94.25\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0113, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0113, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0118, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0121, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0124, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0127, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0125, accuracy : 99.67\n",
            "Epoch : 267, training loss : 0.0124, training accuracy : 99.68, test loss : 0.3159, test accuracy : 94.41\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0136, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0130, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0133, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0135, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0129, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0130, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0137, accuracy : 99.65\n",
            "Epoch : 268, training loss : 0.0138, training accuracy : 99.64, test loss : 0.3084, test accuracy : 94.35\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0127, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0120, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0115, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0116, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0115, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0114, accuracy : 99.71\n",
            "Epoch : 269, training loss : 0.0117, training accuracy : 99.70, test loss : 0.3033, test accuracy : 94.45\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0131, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0136, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0126, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0120, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0117, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0121, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0119, accuracy : 99.69\n",
            "Epoch : 270, training loss : 0.0118, training accuracy : 99.70, test loss : 0.2999, test accuracy : 94.41\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0126, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0103, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0108, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.72\n",
            "Epoch : 271, training loss : 0.0111, training accuracy : 99.72, test loss : 0.3051, test accuracy : 94.37\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0117, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0118, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0125, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0126, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0122, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0119, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0116, accuracy : 99.69\n",
            "Epoch : 272, training loss : 0.0116, training accuracy : 99.68, test loss : 0.2995, test accuracy : 94.35\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0116, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0106, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0110, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0114, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0114, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0113, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.70\n",
            "Epoch : 273, training loss : 0.0111, training accuracy : 99.71, test loss : 0.3001, test accuracy : 94.35\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0111, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0110, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0108, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0105, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0108, accuracy : 99.72\n",
            "Epoch : 274, training loss : 0.0108, training accuracy : 99.72, test loss : 0.3020, test accuracy : 94.49\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0099, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0098, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0099, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0099, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0100, accuracy : 99.76\n",
            "Epoch : 275, training loss : 0.0100, training accuracy : 99.76, test loss : 0.3047, test accuracy : 94.30\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0117, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0117, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0107, accuracy : 99.71\n",
            "Epoch : 276, training loss : 0.0107, training accuracy : 99.72, test loss : 0.3001, test accuracy : 94.53\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0101, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0096, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0104, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0104, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0103, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0103, accuracy : 99.75\n",
            "Epoch : 277, training loss : 0.0101, training accuracy : 99.76, test loss : 0.2962, test accuracy : 94.54\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0097, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0087, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0102, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0101, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0103, accuracy : 99.73\n",
            "Epoch : 278, training loss : 0.0104, training accuracy : 99.73, test loss : 0.3030, test accuracy : 94.37\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0114, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0112, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0112, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0102, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0100, accuracy : 99.74\n",
            "Epoch : 279, training loss : 0.0100, training accuracy : 99.74, test loss : 0.2991, test accuracy : 94.43\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0118, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0118, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0109, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0102, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0097, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0093, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0093, accuracy : 99.77\n",
            "Epoch : 280, training loss : 0.0095, training accuracy : 99.77, test loss : 0.3035, test accuracy : 94.43\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0093, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0089, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.79\n",
            "Epoch : 281, training loss : 0.0090, training accuracy : 99.79, test loss : 0.3054, test accuracy : 94.34\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0079, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0092, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0092, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0090, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0092, accuracy : 99.78\n",
            "Epoch : 282, training loss : 0.0093, training accuracy : 99.78, test loss : 0.2971, test accuracy : 94.46\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0093, accuracy : 99.75\n",
            "Epoch : 283, training loss : 0.0091, training accuracy : 99.76, test loss : 0.3043, test accuracy : 94.36\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0093, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0090, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0093, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0089, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0097, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0096, accuracy : 99.75\n",
            "Epoch : 284, training loss : 0.0098, training accuracy : 99.75, test loss : 0.3032, test accuracy : 94.53\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0122, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0105, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0099, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0089, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0093, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0096, accuracy : 99.75\n",
            "Epoch : 285, training loss : 0.0096, training accuracy : 99.75, test loss : 0.3068, test accuracy : 94.38\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0094, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0104, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0095, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0097, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0094, accuracy : 99.75\n",
            "Epoch : 286, training loss : 0.0094, training accuracy : 99.75, test loss : 0.3057, test accuracy : 94.35\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0084, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0096, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0089, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0087, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0088, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0091, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0093, accuracy : 99.76\n",
            "Epoch : 287, training loss : 0.0091, training accuracy : 99.76, test loss : 0.3028, test accuracy : 94.44\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0080, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0083, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0089, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0087, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0087, accuracy : 99.79\n",
            "Epoch : 288, training loss : 0.0087, training accuracy : 99.79, test loss : 0.3041, test accuracy : 94.36\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0079, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0084, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0085, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0086, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0086, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.79\n",
            "Epoch : 289, training loss : 0.0087, training accuracy : 99.79, test loss : 0.3118, test accuracy : 94.34\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0074, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0085, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0082, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0079, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0081, accuracy : 99.79\n",
            "Epoch : 290, training loss : 0.0081, training accuracy : 99.79, test loss : 0.3035, test accuracy : 94.35\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0088, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0091, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0087, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0087, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0082, accuracy : 99.81\n",
            "Epoch : 291, training loss : 0.0082, training accuracy : 99.81, test loss : 0.3063, test accuracy : 94.40\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0092, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0087, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0083, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0084, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0084, accuracy : 99.82\n",
            "Epoch : 292, training loss : 0.0083, training accuracy : 99.82, test loss : 0.3057, test accuracy : 94.38\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0071, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0081, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0083, accuracy : 99.79\n",
            "Epoch : 293, training loss : 0.0082, training accuracy : 99.79, test loss : 0.3013, test accuracy : 94.45\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0077, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0079, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0080, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0080, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0082, accuracy : 99.81\n",
            "Epoch : 294, training loss : 0.0082, training accuracy : 99.82, test loss : 0.3063, test accuracy : 94.40\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0090, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0085, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0085, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0087, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0085, accuracy : 99.81\n",
            "Epoch : 295, training loss : 0.0085, training accuracy : 99.80, test loss : 0.3133, test accuracy : 94.26\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0072, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0078, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0086, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0086, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0086, accuracy : 99.76\n",
            "Epoch : 296, training loss : 0.0084, training accuracy : 99.76, test loss : 0.2977, test accuracy : 94.41\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0073, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0093, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0090, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.74\n",
            "Epoch : 297, training loss : 0.0091, training accuracy : 99.74, test loss : 0.3059, test accuracy : 94.40\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0080, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0092, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0096, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0094, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0089, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0090, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0089, accuracy : 99.77\n",
            "Epoch : 298, training loss : 0.0089, training accuracy : 99.78, test loss : 0.2989, test accuracy : 94.54\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0091, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0090, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0089, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0086, accuracy : 99.78\n",
            "Epoch : 299, training loss : 0.0086, training accuracy : 99.79, test loss : 0.2985, test accuracy : 94.49\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0081, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0080, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0083, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0080, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0078, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0081, accuracy : 99.82\n",
            "Epoch : 300, training loss : 0.0082, training accuracy : 99.81, test loss : 0.3086, test accuracy : 94.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "id": "iUQVIKR-X3v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a5483b-8dcc-480a-f9e7-4b0e09e149f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.26001455211171914, 95.02919483712354)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : Weight decay coefficient = 5e-4\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7CNz1iabSB21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "65e28c26-7db9-4f2b-fecf-540d8a0f5ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : Exponential\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3436792e-aec7-4517-8384-471e58f94206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_exp = {train_loss_list}\") \n",
        "print(f\"train_acc_list_exp = {train_acc_list}\")\n",
        "print(f\"test_loss_list_exp = {test_loss_list}\")\n",
        "print(f\"test_acc_list_exp = {test_acc_list}\")"
      ],
      "metadata": {
        "id": "3eiY3bTlWipW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01644761-084c-46ca-a0b1-cae868783581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_exp = [1.4802386524877573, 0.5145602391825782, 0.4270873037860969, 0.3911595640143728, 0.3666531684275888, 0.3449077741482717, 0.3271806535278232, 0.31609291052269095, 0.30408265799041684, 0.29365021432560634, 0.28160075124524797, 0.2770334360439603, 0.26884961707724464, 0.2641961875202533, 0.25375436896554177, 0.2502837141112583, 0.2470766868048567, 0.23852058467745457, 0.23988390942091542, 0.23230066057546997, 0.2278682370051782, 0.22565480602304464, 0.22504844034953816, 0.2194394195467476, 0.2184035734028674, 0.21561946675464067, 0.21413457837854297, 0.21124843120332656, 0.21243113762923696, 0.20603898459374098, 0.20471800083350036, 0.20373490892936222, 0.20350655851769578, 0.20246598469774899, 0.1970768437334677, 0.1986252810896897, 0.19743034299109685, 0.19816612322034874, 0.19047524803254987, 0.19524715533909112, 0.19178811626301873, 0.1882726750904467, 0.19070912812783467, 0.18814858636720394, 0.18752294930258417, 0.1876332449012494, 0.18488309697972405, 0.1866223762634647, 0.1871858422712582, 0.18362822461378606, 0.18218793275798886, 0.18321568313935585, 0.17826895162382422, 0.18373284461658176, 0.17807404935198426, 0.1832454036466959, 0.17776847138072094, 0.17643137354918612, 0.17395331942776676, 0.17643668946777255, 0.17554401157065458, 0.17678272510124093, 0.17290620969199552, 0.16925092947575943, 0.16994382533923721, 0.17394802385189023, 0.17083977395037647, 0.16816641190430012, 0.16901177336489606, 0.17049305643734894, 0.1686860634806518, 0.16724471368279237, 0.16594552272945884, 0.16281739412365245, 0.16591493490588696, 0.16363012161721704, 0.1613770334055107, 0.1623856468898494, 0.16301845652139607, 0.162652157650892, 0.1592392718626393, 0.15867647268180926, 0.16240244100329676, 0.15764614089636944, 0.15857719487654484, 0.1579075544810069, 0.15415261916028775, 0.15517796557850955, 0.15625821325554434, 0.15458142586799495, 0.15438464393985626, 0.1572210744003132, 0.15048929238262862, 0.14945686451060985, 0.1505291250440971, 0.15305787657556658, 0.14894246368550348, 0.147898023902643, 0.15114493573180382, 0.14610998743897693, 0.15086569631002783, 0.1484981526729214, 0.1449999701039901, 0.14578966223975506, 0.14083756948631954, 0.14295084347448697, 0.14506198940319098, 0.14481029806905968, 0.14044388525470647, 0.13931026286508655, 0.13716323383980328, 0.14206144404484006, 0.14102018442822667, 0.13867486305635796, 0.13811369409385896, 0.1344010781062651, 0.13408899530364568, 0.1343176202560828, 0.13219609678417524, 0.13159260965459715, 0.13269671327295665, 0.13183512560377114, 0.12735726753487012, 0.12682472022002952, 0.12942703321656884, 0.1311747801154448, 0.12721967680990534, 0.1226911582961315, 0.12565258846474373, 0.12294719365695467, 0.12425805079601807, 0.12707497591410227, 0.12184429030895717, 0.11833101225799823, 0.11996741157762081, 0.12062076490806532, 0.11433951715184099, 0.11977466401267989, 0.11421744397217988, 0.11513779362345614, 0.11420854406688756, 0.11369123289659418, 0.11147084164869818, 0.10956912912014458, 0.10847692516680332, 0.10791541966369886, 0.11187728162006877, 0.1076874076956656, 0.10829882281340235, 0.10742072247190851, 0.10462035163530328, 0.10231590462308428, 0.10307718535789306, 0.10392249831399782, 0.10356237866277214, 0.10049116989057562, 0.10070552909620571, 0.10252807007188881, 0.0956033191373597, 0.09626625927523545, 0.09980903398548442, 0.09713503368814705, 0.09384269924546645, 0.0914715800489877, 0.08955621895276838, 0.09167020563995289, 0.090305259370505, 0.09060482793616328, 0.08901907593203673, 0.08711527488610851, 0.08550666729467833, 0.08462664417589341, 0.08277387312313082, 0.0831775262416782, 0.08437745708992407, 0.07903462006309168, 0.08039322875561268, 0.08088428793136827, 0.07809663470834494, 0.07452205053169714, 0.07565905422547727, 0.07784699146360724, 0.07322079027046356, 0.07456694824272782, 0.07434099188786213, 0.07195496950349026, 0.06945457162340563, 0.06995407763495032, 0.06968635793139295, 0.06799854681420779, 0.0665388950500082, 0.06632828803324117, 0.06345813088421899, 0.06316864148266917, 0.06253555392074149, 0.06164193874588463, 0.0611763400048381, 0.06035886153573711, 0.05943589109351965, 0.05881024452638093, 0.05468251183250449, 0.0560565908366264, 0.05653399054899933, 0.05562049048248588, 0.051311384115448895, 0.052225344276767435, 0.05155897112739926, 0.049526256553293645, 0.0512430043146519, 0.04822691060415896, 0.047050642753196606, 0.04643308828764047, 0.04417288107519873, 0.04520953228056108, 0.04404221094140233, 0.04265529490744841, 0.042559341516164015, 0.04379070952893071, 0.03938076019963517, 0.03966092142758532, 0.04086068194664934, 0.03843482981150753, 0.03796145588356426, 0.03604420552757091, 0.03480926173332786, 0.035225326502645204, 0.032808877683565184, 0.03266389738152505, 0.03565281066162698, 0.03340759336365372, 0.031172389340609674, 0.030425866454685747, 0.03243158847853279, 0.028973121385324777, 0.026384176789886422, 0.026978798047197744, 0.026420692340937774, 0.0263953241601657, 0.024609150329029293, 0.025792538209220138, 0.02489618113110483, 0.02391088762519967, 0.023590878200172107, 0.023072245570535704, 0.021719697244505735, 0.01978167379306513, 0.0224422716837235, 0.01906526322807198, 0.02043990451882929, 0.0194835271951326, 0.01808247663810102, 0.018989686450673657, 0.019306088054456765, 0.017153792940761622, 0.016933396825223636, 0.015568477072599054, 0.015502154842576865, 0.016233396913596307, 0.015708776872332502, 0.01585502114947027, 0.014598894385724433, 0.013940555258366332, 0.013421826192990531, 0.013425068274836957, 0.011932213828257893, 0.013165008134731069, 0.012398852705501201, 0.013773291333588622, 0.011650380652604831, 0.011782062857671765, 0.011110670982890316, 0.011588883696175698, 0.011087752127802874, 0.010806514879342845, 0.009987936503137777, 0.01071135072052721, 0.010115013357193833, 0.010417424021213035, 0.010015223428940142, 0.009483683681138224, 0.008984847573833514, 0.009285728387196414, 0.009138723183864965, 0.009764598564845498, 0.00961481691192997, 0.009357751404228027, 0.009121658366620237, 0.00870354351181244, 0.008720945385802124, 0.008100192143321845, 0.008175653403910094, 0.008284241586373387, 0.008199483759103085, 0.008166708315275913, 0.008518998447406865, 0.008448232254922026, 0.009054402698987343, 0.008894852471574484, 0.008632234484777561, 0.008205045137029434]\n",
            "train_acc_list_exp = [48.277395447326626, 83.64002117522499, 86.79724722075171, 88.12493382742191, 88.89147697194282, 89.55214399152992, 89.9947061937533, 90.69772366331392, 90.96029645314981, 91.36050820539968, 91.55955532027528, 91.68660667019587, 92.08046585494971, 92.15457914240339, 92.65643197458974, 92.62466913710958, 92.80254102699841, 93.1011116993118, 93.08840656431974, 93.11593435680254, 93.28110111169931, 93.37850714663843, 93.46956061408153, 93.68131286394917, 93.59661196400212, 93.65166754896771, 93.93118051879301, 93.87612493382743, 93.82953943885654, 94.09846479618847, 94.14716781365802, 94.02858655373214, 94.13022763366861, 94.16410799364743, 94.31445209105347, 94.212811011117, 94.20434092112228, 94.25939650608788, 94.53255690841715, 94.46056114346214, 94.500794070937, 94.56855479089465, 94.40974060349392, 94.58125992588671, 94.53255690841715, 94.62361037586024, 94.77183695076761, 94.66807834833246, 94.64690312334568, 94.67866596082584, 94.7443091582848, 94.71042879830598, 94.96029645314981, 94.68290100582318, 94.88406564319746, 94.70619375330863, 94.8268925357332, 94.91371095817892, 94.92429857067232, 94.99417681312865, 94.94759131815776, 94.89253573319216, 95.142403388036, 95.1148755955532, 95.13816834303864, 94.97511911064055, 95.08946532556908, 95.23133933298041, 95.18475383800953, 95.00264690312335, 95.13393329804128, 95.06617257808364, 95.18475383800953, 95.32874536791954, 95.26521969295923, 95.21651667548967, 95.358390682901, 95.29274748544204, 95.39862361037586, 95.3223928004235, 95.47908946532557, 95.358390682901, 95.3499205929063, 95.42826892535733, 95.46638433033351, 95.43462149285337, 95.57014293276866, 95.4219163578613, 95.45579671784013, 95.63366860772896, 95.55532027527792, 95.5214399152991, 95.74166225516146, 95.71836950767602, 95.66543144520911, 95.5849655902594, 95.67178401270513, 95.75860243515088, 95.6569613552144, 95.80518793012176, 95.6929592376919, 95.71836950767602, 95.84542085759661, 95.8644785600847, 95.95764955002647, 95.92165166754897, 95.84965590259397, 95.78613022763366, 96.01694017998942, 95.98941238750662, 96.02117522498676, 95.97670725251456, 95.93859184753838, 96.09317098994177, 96.00211752249868, 96.11858125992589, 96.14822657490735, 96.15669666490207, 96.22022233986236, 96.18845950238222, 96.2075172048703, 96.16516675489677, 96.35997882477501, 96.38538909475913, 96.26257278983589, 96.2350449973531, 96.36421386977237, 96.56961355214399, 96.3790365272631, 96.54843832715723, 96.38750661725781, 96.3430386447856, 96.56749602964531, 96.65431445209106, 96.64584436209634, 96.6056114346215, 96.66490206458444, 96.52302805717311, 96.76654314452091, 96.68395976707252, 96.72631021704606, 96.67337215457914, 96.80254102699841, 96.92535733192165, 96.93170989941768, 96.84277395447327, 96.79195341450503, 96.9020645844362, 96.929592376919, 96.86394917946004, 97.01429327686607, 97.07146638433034, 97.05452620434092, 96.98041291688725, 97.05029115934357, 97.19428268925357, 97.12228692429856, 97.07993647432504, 97.32556908417152, 97.30227633668608, 97.18581259925887, 97.25357331921651, 97.28533615669666, 97.3446267866596, 97.43991529910005, 97.44415034409741, 97.41450502911593, 97.43144520910535, 97.46744309158285, 97.45262043409211, 97.56484912652196, 97.57755426151402, 97.60508205399682, 97.66225516146109, 97.53732133403918, 97.7067231339333, 97.66013763896241, 97.65378507146639, 97.76601376389624, 97.87612493382743, 97.8507146638433, 97.80836421386977, 97.89306511381683, 97.83800952885125, 97.86553732133405, 97.91847538380095, 98.0052938062467, 97.92906299629433, 97.97141344626786, 98.07517204870302, 98.04552673372154, 98.08152461619905, 98.14505029115935, 98.18951826363156, 98.1852832186342, 98.24033880359978, 98.18316569613552, 98.27633668607729, 98.26786659608258, 98.24669137109582, 98.44150344097406, 98.41821069348862, 98.30598200105877, 98.37374272101641, 98.54314452091053, 98.52196929592377, 98.51561672842774, 98.61725780836422, 98.53890947591319, 98.66807834833246, 98.59820010587613, 98.64690312334568, 98.74854420328217, 98.68925357331922, 98.70407623080995, 98.74642668078349, 98.76548438327157, 98.67019587083112, 98.89677077818952, 98.80359978824775, 98.77818951826363, 98.854420328216, 98.86924298570672, 98.94123875066173, 98.97511911064055, 98.97511911064055, 99.07252514557968, 99.06617257808364, 98.96029645314981, 99.04711487559555, 99.09158284806776, 99.08523028057174, 99.02593965060879, 99.16569613552144, 99.26098464796189, 99.214399152991, 99.2419269454738, 99.2503970354685, 99.31604023292748, 99.22498676548439, 99.30333509793542, 99.29062996294336, 99.28427739544733, 99.32874536791954, 99.36897829539438, 99.44944415034409, 99.34568554790894, 99.46638433033351, 99.40709370037057, 99.43673901535203, 99.47273689782953, 99.44944415034409, 99.46003176283747, 99.50661725780836, 99.51085230280572, 99.56167284277396, 99.58708311275808, 99.5299100052938, 99.56379036527264, 99.56802541026998, 99.59978824775013, 99.61461090524087, 99.65272631021705, 99.62519851773425, 99.7204870301747, 99.63578613022763, 99.67813658020117, 99.64002117522499, 99.69931180518793, 99.69719428268925, 99.72472207517205, 99.68448914769719, 99.70566437268396, 99.7204870301747, 99.76283748014822, 99.71625198517734, 99.75860243515088, 99.72683959767072, 99.74377977766014, 99.76707252514558, 99.78613022763366, 99.78189518263632, 99.7564849126522, 99.75224986765484, 99.75224986765484, 99.75436739015352, 99.7649550026469, 99.79460031762838, 99.7924827951297, 99.7924827951297, 99.81154049761778, 99.82424563260984, 99.79460031762838, 99.82001058761249, 99.80307040762308, 99.7649550026469, 99.73954473266278, 99.77766013763896, 99.78613022763366, 99.81365802011646]\n",
            "test_loss_list_exp = [0.8332259766027039, 0.6243491267748907, 0.4589659714815663, 0.4804437569543427, 0.427488262965983, 0.4090569673800001, 0.3808782717906961, 0.35566168862814995, 0.33532728992548644, 0.3060293437949583, 0.3283534451734786, 0.2954925663184886, 0.28179204358043625, 0.30891463029034, 0.3015652174750964, 0.28739879868340257, 0.30414057471880723, 0.2780561326254232, 0.2854650225125107, 0.2774708117632305, 0.2591885347649747, 0.2489084253343297, 0.26453685285706147, 0.2969295278775926, 0.2676425111644408, 0.24623633818883522, 0.2657030882569505, 0.2502379405367024, 0.2519026789814234, 0.24595809209288336, 0.2657459572907172, 0.24606278589835354, 0.26561722005991373, 0.26659162207415293, 0.27839325434144807, 0.24987868000479305, 0.24812665541528486, 0.24487176610558642, 0.2487423630321727, 0.2408910162162547, 0.23539556405853992, 0.24755567443721435, 0.24770869559371003, 0.24718565263730638, 0.239624624816226, 0.24662797083603402, 0.2383566615445649, 0.2449228444374075, 0.23434106121752776, 0.2598630274627723, 0.2621555033998162, 0.23332552688525005, 0.23622890707908892, 0.24026584826117636, 0.2526375870494282, 0.23738020169092158, 0.2334751014177706, 0.2508628443796553, 0.2439220617068749, 0.23279762205978236, 0.24812749104903026, 0.23510865852528928, 0.24088096268036785, 0.2441782963772615, 0.2372785997120481, 0.2591664406160514, 0.24067971285651713, 0.2356143380497016, 0.22471256245512003, 0.23980964548593642, 0.22926432496922858, 0.22651415298163308, 0.23500783566166372, 0.24451460265645794, 0.24563370374780075, 0.238621749385607, 0.24734771609598516, 0.24317575092701352, 0.2357527948002897, 0.23691468079592667, 0.2317277501318969, 0.2389378704167172, 0.2415773031100923, 0.22682394105575832, 0.24305828332024462, 0.23989992117618814, 0.24055047181672326, 0.2310189416851191, 0.2320305799663651, 0.24022304533305122, 0.23773051294333794, 0.23404084142370551, 0.22795141963105575, 0.24373077162924936, 0.2519711878004612, 0.24464675623412227, 0.24204467719092088, 0.23641339631057254, 0.22852404189168238, 0.23696348417148577, 0.23560348832431963, 0.2293583228636314, 0.22483952014761813, 0.2419880603750547, 0.24878904088308998, 0.23500451945937148, 0.23449834377742282, 0.23925709554596858, 0.22304683715543328, 0.23272172509528258, 0.23005880462918796, 0.2274422672230239, 0.24031657513742352, 0.23025121483221359, 0.23231310611042907, 0.23103023649138563, 0.23504440533910312, 0.23217533609154178, 0.24353400623316274, 0.23417713490369566, 0.23423893881194732, 0.228971739591775, 0.23540909750861863, 0.22729575031382196, 0.24729683045663087, 0.23187032206824013, 0.23728142879611136, 0.2368104361983783, 0.23822332604550847, 0.24225895598019456, 0.24053165952072425, 0.23070352232339336, 0.23100480022748895, 0.23646043049281135, 0.2497626494835405, 0.2328693575280554, 0.2476191526169286, 0.24926081999186792, 0.22955395642887144, 0.23540200713072337, 0.24256359033432662, 0.23535256659356402, 0.22585722259884955, 0.23929224061030968, 0.2362090051356776, 0.2323842137759807, 0.23957762155024445, 0.23328206293723164, 0.24211033893858686, 0.24111823025433457, 0.2357599771234627, 0.23172222019410602, 0.23386546501926347, 0.2514914059828894, 0.2433166304037121, 0.2378106768261276, 0.22987670018611586, 0.230959424767278, 0.23733513216104576, 0.24432600466717108, 0.23943918337132417, 0.24938077077853912, 0.24655740848724164, 0.2478333150456641, 0.25572608141045944, 0.24093728490612087, 0.24338949404145574, 0.24488756612089335, 0.2545141223452839, 0.24692704464655882, 0.2547990579669382, 0.2425614733468084, 0.24526120959689804, 0.2513910967254025, 0.24536202011593417, 0.24960096059914896, 0.24901917171390617, 0.24787598632860416, 0.25261163421194344, 0.24638482389569866, 0.2529066298522201, 0.24463685903259935, 0.2410413659926431, 0.2477684386403245, 0.2515937814708142, 0.25298880261606443, 0.2537755832423036, 0.24700629240011468, 0.25862112336372045, 0.2538327400974345, 0.25513705633142414, 0.25248319325128604, 0.25129741044970705, 0.25710856879823935, 0.2602397995252235, 0.2535753774401896, 0.25966272229219184, 0.26258349593947916, 0.2575982420467863, 0.25366368794850275, 0.26840075843182265, 0.2604089367623423, 0.26335741690926107, 0.26115994502370266, 0.27586783806556, 0.2699058622340946, 0.2690262606038767, 0.26124058914023873, 0.2695645693068703, 0.269948998539179, 0.2699922961274199, 0.2781037180410588, 0.26918452950742316, 0.2656496547746892, 0.27793413524826366, 0.2727473574854872, 0.26761945906807394, 0.28111935063612226, 0.2739635939691581, 0.2791407521741063, 0.27640176702327296, 0.27548765678269166, 0.2720885994460653, 0.2716641928087555, 0.29375703590830754, 0.2741913403402649, 0.2834579655352761, 0.2800045997799173, 0.2746915383556602, 0.28199352176093, 0.2871078792502921, 0.2875168366430729, 0.2797919814200962, 0.2876746460211043, 0.2826278824541791, 0.2888666771662732, 0.28122600651912244, 0.2926978058565189, 0.2832188007111351, 0.2851658707082856, 0.2897379945543613, 0.2852640647404626, 0.29130700086334754, 0.29031389254127066, 0.301364694575907, 0.29234155917576715, 0.30245087251943703, 0.29240411739138994, 0.2921989212068273, 0.29711635103997064, 0.30123716481395213, 0.29383554357085745, 0.2976747955944316, 0.2940556710385078, 0.29956718912238584, 0.2989022258906534, 0.3001505616760137, 0.29694770219023614, 0.296321282619793, 0.30678381798241067, 0.2958103663415885, 0.29974740378412545, 0.29243658106847137, 0.29601444117724895, 0.2992933266522253, 0.3056794012914978, 0.3159245145963688, 0.3084262056714472, 0.3033023014774217, 0.2999487395635715, 0.305078714501624, 0.29950604752144394, 0.3001469790424202, 0.3019553049303153, 0.3047376123220459, 0.30005093909087865, 0.2961901391984201, 0.3029952965421127, 0.2991081877867235, 0.3034919787508746, 0.30540474100659293, 0.29712304374312654, 0.304328731279455, 0.30321371049492385, 0.3067686218019648, 0.30573491976760764, 0.30279137281810536, 0.304144932066693, 0.3118132336941712, 0.3035398056803673, 0.3062666428193231, 0.3056575841106036, 0.30131836995190264, 0.3063342159285265, 0.31325177424678613, 0.2976547159327596, 0.3059330361475255, 0.2988617525655119, 0.29846939126796584, 0.3085705796804498]\n",
            "test_acc_list_exp = [73.40580823601721, 79.92086662569146, 86.0479409956976, 85.25276582667486, 87.02750460971113, 87.60755992624462, 88.40273509526736, 89.31699446834665, 89.8740012292563, 90.96496619545175, 90.20052243392747, 91.4259373079287, 91.59496004917025, 91.09557467732022, 91.30301167793485, 91.72172710510141, 91.09557467732022, 91.82928703134604, 91.69483712354025, 91.82160417947142, 92.55147510755992, 92.82805777504609, 92.3939766441303, 91.19161032575292, 92.22111247695145, 92.82037492317149, 92.24031960663798, 92.8242163491088, 92.70513214505225, 92.94330055316533, 92.41318377381684, 92.88952059004302, 92.42086662569146, 92.1980639213276, 92.02519975414874, 92.71665642286416, 92.86647203441917, 92.96250768285188, 92.83958205285802, 93.14305470190534, 93.32744314689613, 92.98939766441303, 93.01244622003688, 93.0278119237861, 93.2160417947142, 92.97019053472648, 93.29287031346036, 93.04317762753534, 93.25829748002458, 92.65903503380454, 92.59373079287032, 93.4119545175169, 93.1238475722188, 93.28134603564843, 92.85494775660726, 93.31207744314689, 93.42347879532882, 93.00476336816226, 93.08927473878303, 93.51951444376152, 93.00860479409957, 93.36585740626921, 93.20835894283958, 92.98555623847572, 93.39274738783037, 92.60141364474492, 93.18146896127843, 93.35049170251997, 93.68469575906576, 93.27750460971113, 93.51951444376152, 93.6578057775046, 93.51951444376152, 93.1238475722188, 92.98555623847572, 93.27366318377382, 93.15457897971727, 93.38890596189306, 93.45421020282728, 93.32360172095882, 93.44268592501537, 93.38122311001844, 93.4618930547019, 93.59634296250768, 93.42347879532882, 93.34665027658266, 93.31976029502151, 93.7077443146896, 93.42347879532882, 93.34665027658266, 93.49646588813768, 93.48110018438844, 93.6578057775046, 93.37354025814382, 93.1238475722188, 93.1737861094038, 93.24677320221267, 93.45421020282728, 93.71542716656423, 93.39658881376766, 93.5041487400123, 93.58097725875845, 93.9881684081131, 93.26982175783651, 93.0201290719115, 93.61555009219423, 93.52335586969883, 93.39274738783037, 93.91518131530424, 93.71926859250154, 93.86140135218193, 93.77688998156115, 93.37354025814382, 93.69237861094038, 93.6578057775046, 93.80762138905962, 93.49262446220037, 93.66548862937923, 93.40427166564228, 93.6578057775046, 93.68469575906576, 93.66548862937923, 93.61939151813154, 94.08420405654579, 93.59634296250768, 93.65012292563, 93.6539643515673, 93.61939151813154, 93.61170866625692, 93.48878303626306, 93.58097725875845, 93.73847572218807, 93.73079287031346, 93.66164720344192, 93.4119545175169, 93.75, 93.48110018438844, 93.58097725875845, 93.86524277811924, 93.73079287031346, 93.47725875845114, 93.90749846342962, 94.19944683466503, 93.66933005531654, 93.66933005531654, 93.79609711124769, 93.700061462815, 93.88060848186846, 93.80762138905962, 93.76920712968654, 93.75, 93.86524277811924, 93.88829133374308, 93.6078672403196, 93.75768285187462, 93.76152427781193, 93.87292562999386, 94.0419483712354, 93.93054701905348, 93.85371850030731, 93.91902274124155, 93.6539643515673, 93.9881684081131, 93.66548862937923, 93.53872157344806, 93.89981561155501, 94.01505838967425, 94.06115550092194, 93.7077443146896, 93.799938537185, 93.59250153657038, 93.91902274124155, 94.02658266748617, 93.82298709280884, 94.01505838967425, 93.90749846342962, 93.96511985248924, 93.91133988936693, 93.76536570374923, 94.0918869084204, 93.78841425937308, 93.96896127842655, 94.09956976029503, 94.14566687154272, 93.8921327596804, 93.91518131530424, 93.8921327596804, 94.26090964966195, 93.80762138905962, 93.88060848186846, 94.01889981561156, 94.07652120467118, 94.14566687154272, 94.10341118623234, 93.8460356484327, 94.02658266748617, 93.91902274124155, 93.86908420405655, 94.12645974185618, 94.1379840196681, 93.93822987092808, 94.07652120467118, 94.08420405654579, 94.0381069452981, 93.85755992624462, 94.02658266748617, 94.00353411186232, 94.18792255685311, 94.02274124154886, 94.0918869084204, 94.21865396435157, 93.99200983405039, 94.06499692685925, 94.28011677934849, 93.99969268592501, 93.96896127842655, 94.28011677934849, 93.97664413030117, 94.22633681622618, 94.24170251997542, 94.05731407498463, 94.13414259373079, 94.1917639827904, 94.49139520590043, 93.99200983405039, 94.28779963122311, 94.03042409342348, 94.38767670559312, 94.19944683466503, 94.1840811309158, 94.11109403810694, 94.07267977873387, 94.23017824216349, 94.38383527965581, 94.32237246465888, 94.21481253841426, 94.22249539028887, 94.24938537185002, 94.31853103872157, 94.34926244622004, 94.21865396435157, 94.29932390903504, 94.25322679778733, 94.26090964966195, 94.29548248309773, 94.41840811309157, 94.17639827904118, 94.36078672403197, 94.37231100184388, 94.13414259373079, 94.19944683466503, 94.34157959434542, 94.34157959434542, 94.36846957590657, 94.35310387215735, 94.24170251997542, 94.37231100184388, 94.36462814996926, 94.39920098340504, 94.25706822372464, 94.43761524277812, 94.50676090964966, 94.53749231714812, 94.40304240934235, 94.40688383527966, 94.24554394591273, 94.41072526121697, 94.35310387215735, 94.45298094652735, 94.41072526121697, 94.36846957590657, 94.35310387215735, 94.34926244622004, 94.48755377996312, 94.30316533497235, 94.5259680393362, 94.54133374308543, 94.36846957590657, 94.4299323909035, 94.4299323909035, 94.34157959434542, 94.45682237246466, 94.36462814996926, 94.5259680393362, 94.38383527965581, 94.34542102028273, 94.43761524277812, 94.35694529809466, 94.34157959434542, 94.35310387215735, 94.39920098340504, 94.3761524277812, 94.44913952059004, 94.39920098340504, 94.25706822372464, 94.40688383527966, 94.40304240934235, 94.53749231714812, 94.49139520590043, 94.39920098340504]\n"
          ]
        }
      ]
    }
  ]
}