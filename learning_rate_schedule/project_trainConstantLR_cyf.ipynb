{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgAiImV0uURP",
        "outputId": "c4adf409-aeaa-4cca-a28f-d00a815c2f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./data\\train_32x32.mat\n",
            "Using downloaded and verified file: ./data\\test_32x32.mat\n"
          ]
        }
      ],
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "d6a0bf3a-2d2f-4f76-f11c-5ec665a8b45d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "73257 26032\n"
          ]
        }
      ],
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    #scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "outputs": [],
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "outputs": [],
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "e9d8cddb-ef09-4a87-8154-e80569d015c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "74b13d96-61d4-43b8-c27c-b968190ddc62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(class_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArgupDVRwB8i",
        "outputId": "9f1c3564-7355-4f9e-fc00-6b5dc5df2566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3605, accuracy : 15.77\n",
            "iteration : 100, loss : 2.3059, accuracy : 17.33\n",
            "iteration : 150, loss : 2.2518, accuracy : 19.51\n",
            "iteration : 200, loss : 2.1372, accuracy : 23.86\n",
            "iteration : 250, loss : 1.9687, accuracy : 29.94\n",
            "iteration : 300, loss : 1.7984, accuracy : 36.38\n",
            "iteration : 350, loss : 1.6467, accuracy : 42.15\n",
            "Epoch :   1, training loss : 1.5945, training accuracy : 44.03, test loss : 0.9403, test accuracy : 71.61\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.5972, accuracy : 80.56\n",
            "iteration : 100, loss : 0.5706, accuracy : 81.42\n",
            "iteration : 150, loss : 0.5461, accuracy : 82.35\n",
            "iteration : 200, loss : 0.5306, accuracy : 82.92\n",
            "iteration : 250, loss : 0.5108, accuracy : 83.58\n",
            "iteration : 300, loss : 0.4969, accuracy : 84.13\n",
            "iteration : 350, loss : 0.4903, accuracy : 84.38\n",
            "Epoch :   2, training loss : 0.4869, training accuracy : 84.50, test loss : 0.4697, test accuracy : 85.50\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.3953, accuracy : 87.31\n",
            "iteration : 100, loss : 0.3975, accuracy : 87.67\n",
            "iteration : 150, loss : 0.3835, accuracy : 88.08\n",
            "iteration : 200, loss : 0.3782, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3758, accuracy : 88.34\n",
            "iteration : 300, loss : 0.3749, accuracy : 88.46\n",
            "iteration : 350, loss : 0.3721, accuracy : 88.54\n",
            "Epoch :   3, training loss : 0.3711, training accuracy : 88.61, test loss : 0.3431, test accuracy : 89.67\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3366, accuracy : 89.33\n",
            "iteration : 100, loss : 0.3389, accuracy : 89.39\n",
            "iteration : 150, loss : 0.3315, accuracy : 89.68\n",
            "iteration : 200, loss : 0.3350, accuracy : 89.64\n",
            "iteration : 250, loss : 0.3341, accuracy : 89.72\n",
            "iteration : 300, loss : 0.3323, accuracy : 89.75\n",
            "iteration : 350, loss : 0.3293, accuracy : 89.88\n",
            "Epoch :   4, training loss : 0.3297, training accuracy : 89.87, test loss : 0.3384, test accuracy : 89.59\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.2872, accuracy : 90.92\n",
            "iteration : 100, loss : 0.2937, accuracy : 90.89\n",
            "iteration : 150, loss : 0.2974, accuracy : 90.83\n",
            "iteration : 200, loss : 0.3020, accuracy : 90.80\n",
            "iteration : 250, loss : 0.3041, accuracy : 90.80\n",
            "iteration : 300, loss : 0.3001, accuracy : 90.93\n",
            "iteration : 350, loss : 0.2992, accuracy : 90.99\n",
            "Epoch :   5, training loss : 0.2988, training accuracy : 91.02, test loss : 0.3165, test accuracy : 90.46\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.2760, accuracy : 91.98\n",
            "iteration : 100, loss : 0.2761, accuracy : 91.68\n",
            "iteration : 150, loss : 0.2795, accuracy : 91.60\n",
            "iteration : 200, loss : 0.2770, accuracy : 91.67\n",
            "iteration : 250, loss : 0.2779, accuracy : 91.64\n",
            "iteration : 300, loss : 0.2758, accuracy : 91.67\n",
            "iteration : 350, loss : 0.2732, accuracy : 91.74\n",
            "Epoch :   6, training loss : 0.2715, training accuracy : 91.79, test loss : 0.3176, test accuracy : 90.56\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.2533, accuracy : 92.33\n",
            "iteration : 100, loss : 0.2629, accuracy : 92.09\n",
            "iteration : 150, loss : 0.2644, accuracy : 92.04\n",
            "iteration : 200, loss : 0.2641, accuracy : 92.10\n",
            "iteration : 250, loss : 0.2606, accuracy : 92.19\n",
            "iteration : 300, loss : 0.2603, accuracy : 92.20\n",
            "iteration : 350, loss : 0.2602, accuracy : 92.24\n",
            "Epoch :   7, training loss : 0.2604, training accuracy : 92.24, test loss : 0.2828, test accuracy : 91.50\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.2463, accuracy : 92.77\n",
            "iteration : 100, loss : 0.2460, accuracy : 92.75\n",
            "iteration : 150, loss : 0.2452, accuracy : 92.74\n",
            "iteration : 200, loss : 0.2440, accuracy : 92.75\n",
            "iteration : 250, loss : 0.2446, accuracy : 92.80\n",
            "iteration : 300, loss : 0.2484, accuracy : 92.71\n",
            "iteration : 350, loss : 0.2454, accuracy : 92.78\n",
            "Epoch :   8, training loss : 0.2442, training accuracy : 92.81, test loss : 0.2724, test accuracy : 92.05\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.2384, accuracy : 92.97\n",
            "iteration : 100, loss : 0.2326, accuracy : 93.12\n",
            "iteration : 150, loss : 0.2341, accuracy : 92.96\n",
            "iteration : 200, loss : 0.2364, accuracy : 92.90\n",
            "iteration : 250, loss : 0.2346, accuracy : 93.00\n",
            "iteration : 300, loss : 0.2350, accuracy : 93.05\n",
            "iteration : 350, loss : 0.2346, accuracy : 93.03\n",
            "Epoch :   9, training loss : 0.2342, training accuracy : 93.04, test loss : 0.2788, test accuracy : 91.97\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2280, accuracy : 93.31\n",
            "iteration : 100, loss : 0.2262, accuracy : 93.34\n",
            "iteration : 150, loss : 0.2240, accuracy : 93.42\n",
            "iteration : 200, loss : 0.2241, accuracy : 93.37\n",
            "iteration : 250, loss : 0.2240, accuracy : 93.34\n",
            "iteration : 300, loss : 0.2226, accuracy : 93.31\n",
            "iteration : 350, loss : 0.2255, accuracy : 93.34\n",
            "Epoch :  10, training loss : 0.2246, training accuracy : 93.37, test loss : 0.2606, test accuracy : 92.47\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.2041, accuracy : 94.06\n",
            "iteration : 100, loss : 0.2161, accuracy : 93.73\n",
            "iteration : 150, loss : 0.2113, accuracy : 93.85\n",
            "iteration : 200, loss : 0.2109, accuracy : 93.91\n",
            "iteration : 250, loss : 0.2131, accuracy : 93.81\n",
            "iteration : 300, loss : 0.2119, accuracy : 93.77\n",
            "iteration : 350, loss : 0.2118, accuracy : 93.79\n",
            "Epoch :  11, training loss : 0.2117, training accuracy : 93.80, test loss : 0.2570, test accuracy : 92.68\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.1908, accuracy : 94.39\n",
            "iteration : 100, loss : 0.1960, accuracy : 94.30\n",
            "iteration : 150, loss : 0.2065, accuracy : 93.90\n",
            "iteration : 200, loss : 0.2080, accuracy : 93.92\n",
            "iteration : 250, loss : 0.2061, accuracy : 93.94\n",
            "iteration : 300, loss : 0.2038, accuracy : 94.00\n",
            "iteration : 350, loss : 0.2007, accuracy : 94.04\n",
            "Epoch :  12, training loss : 0.2020, training accuracy : 94.01, test loss : 0.2515, test accuracy : 92.89\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.2000, accuracy : 94.28\n",
            "iteration : 100, loss : 0.2019, accuracy : 94.19\n",
            "iteration : 150, loss : 0.2036, accuracy : 94.19\n",
            "iteration : 200, loss : 0.1990, accuracy : 94.29\n",
            "iteration : 250, loss : 0.1960, accuracy : 94.34\n",
            "iteration : 300, loss : 0.1964, accuracy : 94.33\n",
            "iteration : 350, loss : 0.1957, accuracy : 94.32\n",
            "Epoch :  13, training loss : 0.1943, training accuracy : 94.38, test loss : 0.2304, test accuracy : 93.56\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.1833, accuracy : 94.56\n",
            "iteration : 100, loss : 0.1841, accuracy : 94.60\n",
            "iteration : 150, loss : 0.1841, accuracy : 94.74\n",
            "iteration : 200, loss : 0.1843, accuracy : 94.70\n",
            "iteration : 250, loss : 0.1854, accuracy : 94.71\n",
            "iteration : 300, loss : 0.1864, accuracy : 94.65\n",
            "iteration : 350, loss : 0.1869, accuracy : 94.65\n",
            "Epoch :  14, training loss : 0.1875, training accuracy : 94.63, test loss : 0.2339, test accuracy : 93.46\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.1617, accuracy : 95.31\n",
            "iteration : 100, loss : 0.1693, accuracy : 95.16\n",
            "iteration : 150, loss : 0.1686, accuracy : 95.16\n",
            "iteration : 200, loss : 0.1751, accuracy : 95.00\n",
            "iteration : 250, loss : 0.1745, accuracy : 95.02\n",
            "iteration : 300, loss : 0.1768, accuracy : 94.90\n",
            "iteration : 350, loss : 0.1767, accuracy : 94.92\n",
            "Epoch :  15, training loss : 0.1775, training accuracy : 94.89, test loss : 0.2332, test accuracy : 93.67\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.1801, accuracy : 94.59\n",
            "iteration : 100, loss : 0.1718, accuracy : 94.93\n",
            "iteration : 150, loss : 0.1690, accuracy : 94.98\n",
            "iteration : 200, loss : 0.1696, accuracy : 95.01\n",
            "iteration : 250, loss : 0.1718, accuracy : 94.97\n",
            "iteration : 300, loss : 0.1706, accuracy : 95.03\n",
            "iteration : 350, loss : 0.1725, accuracy : 95.01\n",
            "Epoch :  16, training loss : 0.1721, training accuracy : 95.01, test loss : 0.2243, test accuracy : 93.80\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.1763, accuracy : 94.88\n",
            "iteration : 100, loss : 0.1670, accuracy : 95.16\n",
            "iteration : 150, loss : 0.1626, accuracy : 95.26\n",
            "iteration : 200, loss : 0.1632, accuracy : 95.25\n",
            "iteration : 250, loss : 0.1639, accuracy : 95.25\n",
            "iteration : 300, loss : 0.1646, accuracy : 95.22\n",
            "iteration : 350, loss : 0.1653, accuracy : 95.20\n",
            "Epoch :  17, training loss : 0.1655, training accuracy : 95.19, test loss : 0.2389, test accuracy : 93.35\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.1500, accuracy : 95.91\n",
            "iteration : 100, loss : 0.1605, accuracy : 95.73\n",
            "iteration : 150, loss : 0.1543, accuracy : 95.81\n",
            "iteration : 200, loss : 0.1514, accuracy : 95.88\n",
            "iteration : 250, loss : 0.1540, accuracy : 95.70\n",
            "iteration : 300, loss : 0.1546, accuracy : 95.66\n",
            "iteration : 350, loss : 0.1542, accuracy : 95.68\n",
            "Epoch :  18, training loss : 0.1547, training accuracy : 95.67, test loss : 0.2297, test accuracy : 93.71\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.1422, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1436, accuracy : 95.90\n",
            "iteration : 150, loss : 0.1464, accuracy : 95.81\n",
            "iteration : 200, loss : 0.1458, accuracy : 95.79\n",
            "iteration : 250, loss : 0.1470, accuracy : 95.73\n",
            "iteration : 300, loss : 0.1498, accuracy : 95.71\n",
            "iteration : 350, loss : 0.1531, accuracy : 95.60\n",
            "Epoch :  19, training loss : 0.1540, training accuracy : 95.57, test loss : 0.2351, test accuracy : 93.50\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.1310, accuracy : 96.36\n",
            "iteration : 100, loss : 0.1368, accuracy : 96.11\n",
            "iteration : 150, loss : 0.1368, accuracy : 96.08\n",
            "iteration : 200, loss : 0.1364, accuracy : 96.11\n",
            "iteration : 250, loss : 0.1383, accuracy : 96.03\n",
            "iteration : 300, loss : 0.1387, accuracy : 95.98\n",
            "iteration : 350, loss : 0.1415, accuracy : 95.92\n",
            "Epoch :  20, training loss : 0.1421, training accuracy : 95.92, test loss : 0.2337, test accuracy : 93.57\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.1380, accuracy : 96.12\n",
            "iteration : 100, loss : 0.1375, accuracy : 96.00\n",
            "iteration : 150, loss : 0.1350, accuracy : 96.11\n",
            "iteration : 200, loss : 0.1355, accuracy : 96.05\n",
            "iteration : 250, loss : 0.1345, accuracy : 96.09\n",
            "iteration : 300, loss : 0.1356, accuracy : 96.06\n",
            "iteration : 350, loss : 0.1350, accuracy : 96.04\n",
            "Epoch :  21, training loss : 0.1369, training accuracy : 96.02, test loss : 0.2218, test accuracy : 94.06\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.1325, accuracy : 96.16\n",
            "iteration : 100, loss : 0.1328, accuracy : 96.25\n",
            "iteration : 150, loss : 0.1347, accuracy : 96.12\n",
            "iteration : 200, loss : 0.1348, accuracy : 96.00\n",
            "iteration : 250, loss : 0.1327, accuracy : 96.10\n",
            "iteration : 300, loss : 0.1317, accuracy : 96.15\n",
            "iteration : 350, loss : 0.1318, accuracy : 96.17\n",
            "Epoch :  22, training loss : 0.1327, training accuracy : 96.16, test loss : 0.2491, test accuracy : 93.34\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.1316, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1252, accuracy : 96.47\n",
            "iteration : 150, loss : 0.1272, accuracy : 96.29\n",
            "iteration : 200, loss : 0.1255, accuracy : 96.36\n",
            "iteration : 250, loss : 0.1252, accuracy : 96.37\n",
            "iteration : 300, loss : 0.1268, accuracy : 96.34\n",
            "iteration : 350, loss : 0.1265, accuracy : 96.35\n",
            "Epoch :  23, training loss : 0.1264, training accuracy : 96.35, test loss : 0.2340, test accuracy : 93.72\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.1185, accuracy : 96.78\n",
            "iteration : 100, loss : 0.1223, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1199, accuracy : 96.65\n",
            "iteration : 200, loss : 0.1199, accuracy : 96.62\n",
            "iteration : 250, loss : 0.1198, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1183, accuracy : 96.67\n",
            "iteration : 350, loss : 0.1195, accuracy : 96.59\n",
            "Epoch :  24, training loss : 0.1224, training accuracy : 96.55, test loss : 0.2536, test accuracy : 92.94\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.0973, accuracy : 97.22\n",
            "iteration : 100, loss : 0.1011, accuracy : 96.99\n",
            "iteration : 150, loss : 0.1052, accuracy : 96.92\n",
            "iteration : 200, loss : 0.1065, accuracy : 96.91\n",
            "iteration : 250, loss : 0.1100, accuracy : 96.82\n",
            "iteration : 300, loss : 0.1144, accuracy : 96.73\n",
            "iteration : 350, loss : 0.1150, accuracy : 96.76\n",
            "Epoch :  25, training loss : 0.1157, training accuracy : 96.74, test loss : 0.2350, test accuracy : 93.69\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.1001, accuracy : 97.23\n",
            "iteration : 100, loss : 0.0965, accuracy : 97.24\n",
            "iteration : 150, loss : 0.1003, accuracy : 97.07\n",
            "iteration : 200, loss : 0.1045, accuracy : 97.00\n",
            "iteration : 250, loss : 0.1050, accuracy : 96.95\n",
            "iteration : 300, loss : 0.1062, accuracy : 96.93\n",
            "iteration : 350, loss : 0.1090, accuracy : 96.87\n",
            "Epoch :  26, training loss : 0.1087, training accuracy : 96.86, test loss : 0.2248, test accuracy : 94.19\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.0854, accuracy : 97.42\n",
            "iteration : 100, loss : 0.0992, accuracy : 97.02\n",
            "iteration : 150, loss : 0.1069, accuracy : 96.89\n",
            "iteration : 200, loss : 0.1075, accuracy : 96.92\n",
            "iteration : 250, loss : 0.1067, accuracy : 96.93\n",
            "iteration : 300, loss : 0.1060, accuracy : 96.97\n",
            "iteration : 350, loss : 0.1059, accuracy : 96.95\n",
            "Epoch :  27, training loss : 0.1061, training accuracy : 96.96, test loss : 0.2325, test accuracy : 93.96\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.0860, accuracy : 97.53\n",
            "iteration : 100, loss : 0.0885, accuracy : 97.46\n",
            "iteration : 150, loss : 0.0939, accuracy : 97.41\n",
            "iteration : 200, loss : 0.0943, accuracy : 97.33\n",
            "iteration : 250, loss : 0.0981, accuracy : 97.25\n",
            "iteration : 300, loss : 0.0997, accuracy : 97.18\n",
            "iteration : 350, loss : 0.1008, accuracy : 97.12\n",
            "Epoch :  28, training loss : 0.1014, training accuracy : 97.11, test loss : 0.2375, test accuracy : 93.92\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.0837, accuracy : 97.69\n",
            "iteration : 100, loss : 0.0915, accuracy : 97.45\n",
            "iteration : 150, loss : 0.0970, accuracy : 97.41\n",
            "iteration : 200, loss : 0.0991, accuracy : 97.29\n",
            "iteration : 250, loss : 0.0982, accuracy : 97.25\n",
            "iteration : 300, loss : 0.0988, accuracy : 97.21\n",
            "iteration : 350, loss : 0.0988, accuracy : 97.20\n",
            "Epoch :  29, training loss : 0.0985, training accuracy : 97.22, test loss : 0.2436, test accuracy : 93.83\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.0878, accuracy : 97.58\n",
            "iteration : 100, loss : 0.0857, accuracy : 97.60\n",
            "iteration : 150, loss : 0.0853, accuracy : 97.51\n",
            "iteration : 200, loss : 0.0870, accuracy : 97.47\n",
            "iteration : 250, loss : 0.0882, accuracy : 97.44\n",
            "iteration : 300, loss : 0.0894, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0919, accuracy : 97.30\n",
            "Epoch :  30, training loss : 0.0916, training accuracy : 97.32, test loss : 0.2495, test accuracy : 94.03\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.0836, accuracy : 97.50\n",
            "iteration : 100, loss : 0.0819, accuracy : 97.66\n",
            "iteration : 150, loss : 0.0854, accuracy : 97.54\n",
            "iteration : 200, loss : 0.0839, accuracy : 97.54\n",
            "iteration : 250, loss : 0.0884, accuracy : 97.46\n",
            "iteration : 300, loss : 0.0911, accuracy : 97.38\n",
            "iteration : 350, loss : 0.0924, accuracy : 97.33\n",
            "Epoch :  31, training loss : 0.0918, training accuracy : 97.33, test loss : 0.2393, test accuracy : 94.19\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.0758, accuracy : 98.05\n",
            "iteration : 100, loss : 0.0764, accuracy : 97.88\n",
            "iteration : 150, loss : 0.0755, accuracy : 97.84\n",
            "iteration : 200, loss : 0.0777, accuracy : 97.77\n",
            "iteration : 250, loss : 0.0809, accuracy : 97.72\n",
            "iteration : 300, loss : 0.0808, accuracy : 97.73\n",
            "iteration : 350, loss : 0.0827, accuracy : 97.66\n",
            "Epoch :  32, training loss : 0.0832, training accuracy : 97.64, test loss : 0.2544, test accuracy : 93.77\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.0667, accuracy : 98.20\n",
            "iteration : 100, loss : 0.0682, accuracy : 98.15\n",
            "iteration : 150, loss : 0.0672, accuracy : 98.09\n",
            "iteration : 200, loss : 0.0708, accuracy : 97.98\n",
            "iteration : 250, loss : 0.0768, accuracy : 97.80\n",
            "iteration : 300, loss : 0.0787, accuracy : 97.75\n",
            "iteration : 350, loss : 0.0786, accuracy : 97.76\n",
            "Epoch :  33, training loss : 0.0797, training accuracy : 97.72, test loss : 0.2537, test accuracy : 93.94\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.0786, accuracy : 97.81\n",
            "iteration : 100, loss : 0.0786, accuracy : 97.80\n",
            "iteration : 150, loss : 0.0812, accuracy : 97.71\n",
            "iteration : 200, loss : 0.0801, accuracy : 97.74\n",
            "iteration : 250, loss : 0.0772, accuracy : 97.84\n",
            "iteration : 300, loss : 0.0786, accuracy : 97.78\n",
            "iteration : 350, loss : 0.0794, accuracy : 97.73\n",
            "Epoch :  34, training loss : 0.0793, training accuracy : 97.73, test loss : 0.2661, test accuracy : 93.69\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.0664, accuracy : 97.98\n",
            "iteration : 100, loss : 0.0619, accuracy : 98.08\n",
            "iteration : 150, loss : 0.0642, accuracy : 98.09\n",
            "iteration : 200, loss : 0.0648, accuracy : 98.04\n",
            "iteration : 250, loss : 0.0669, accuracy : 97.97\n",
            "iteration : 300, loss : 0.0677, accuracy : 97.94\n",
            "iteration : 350, loss : 0.0694, accuracy : 97.91\n",
            "Epoch :  35, training loss : 0.0698, training accuracy : 97.89, test loss : 0.2672, test accuracy : 94.08\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.0660, accuracy : 98.03\n",
            "iteration : 100, loss : 0.0644, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0668, accuracy : 98.00\n",
            "iteration : 200, loss : 0.0680, accuracy : 97.96\n",
            "iteration : 250, loss : 0.0719, accuracy : 97.82\n",
            "iteration : 300, loss : 0.0708, accuracy : 97.87\n",
            "iteration : 350, loss : 0.0717, accuracy : 97.85\n",
            "Epoch :  36, training loss : 0.0709, training accuracy : 97.87, test loss : 0.2747, test accuracy : 93.77\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.0678, accuracy : 98.11\n",
            "iteration : 100, loss : 0.0659, accuracy : 98.14\n",
            "iteration : 150, loss : 0.0650, accuracy : 98.14\n",
            "iteration : 200, loss : 0.0641, accuracy : 98.12\n",
            "iteration : 250, loss : 0.0655, accuracy : 98.03\n",
            "iteration : 300, loss : 0.0657, accuracy : 98.01\n",
            "iteration : 350, loss : 0.0665, accuracy : 97.97\n",
            "Epoch :  37, training loss : 0.0672, training accuracy : 97.97, test loss : 0.2582, test accuracy : 93.92\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.0576, accuracy : 98.45\n",
            "iteration : 100, loss : 0.0577, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0568, accuracy : 98.35\n",
            "iteration : 200, loss : 0.0617, accuracy : 98.18\n",
            "iteration : 250, loss : 0.0617, accuracy : 98.17\n",
            "iteration : 300, loss : 0.0641, accuracy : 98.12\n",
            "iteration : 350, loss : 0.0655, accuracy : 98.08\n",
            "Epoch :  38, training loss : 0.0647, training accuracy : 98.09, test loss : 0.2603, test accuracy : 94.15\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.0540, accuracy : 98.44\n",
            "iteration : 100, loss : 0.0563, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0600, accuracy : 98.30\n",
            "iteration : 200, loss : 0.0613, accuracy : 98.31\n",
            "iteration : 250, loss : 0.0618, accuracy : 98.28\n",
            "iteration : 300, loss : 0.0614, accuracy : 98.28\n",
            "iteration : 350, loss : 0.0610, accuracy : 98.26\n",
            "Epoch :  39, training loss : 0.0616, training accuracy : 98.24, test loss : 0.2791, test accuracy : 93.94\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.0471, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0578, accuracy : 98.27\n",
            "iteration : 150, loss : 0.0562, accuracy : 98.30\n",
            "iteration : 200, loss : 0.0587, accuracy : 98.28\n",
            "iteration : 250, loss : 0.0604, accuracy : 98.22\n",
            "iteration : 300, loss : 0.0602, accuracy : 98.24\n",
            "iteration : 350, loss : 0.0613, accuracy : 98.20\n",
            "Epoch :  40, training loss : 0.0615, training accuracy : 98.17, test loss : 0.2692, test accuracy : 94.15\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.0503, accuracy : 98.48\n",
            "iteration : 100, loss : 0.0540, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0560, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0565, accuracy : 98.31\n",
            "iteration : 250, loss : 0.0567, accuracy : 98.32\n",
            "iteration : 300, loss : 0.0576, accuracy : 98.30\n",
            "iteration : 350, loss : 0.0572, accuracy : 98.29\n",
            "Epoch :  41, training loss : 0.0571, training accuracy : 98.29, test loss : 0.2764, test accuracy : 94.06\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.0526, accuracy : 98.39\n",
            "iteration : 100, loss : 0.0539, accuracy : 98.39\n",
            "iteration : 150, loss : 0.0499, accuracy : 98.47\n",
            "iteration : 200, loss : 0.0499, accuracy : 98.48\n",
            "iteration : 250, loss : 0.0519, accuracy : 98.42\n",
            "iteration : 300, loss : 0.0531, accuracy : 98.38\n",
            "iteration : 350, loss : 0.0526, accuracy : 98.37\n",
            "Epoch :  42, training loss : 0.0534, training accuracy : 98.35, test loss : 0.2997, test accuracy : 93.73\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.0471, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0459, accuracy : 98.65\n",
            "iteration : 150, loss : 0.0456, accuracy : 98.58\n",
            "iteration : 200, loss : 0.0439, accuracy : 98.61\n",
            "iteration : 250, loss : 0.0442, accuracy : 98.58\n",
            "iteration : 300, loss : 0.0470, accuracy : 98.50\n",
            "iteration : 350, loss : 0.0495, accuracy : 98.45\n",
            "Epoch :  43, training loss : 0.0499, training accuracy : 98.45, test loss : 0.2916, test accuracy : 93.89\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.0409, accuracy : 98.83\n",
            "iteration : 100, loss : 0.0488, accuracy : 98.55\n",
            "iteration : 150, loss : 0.0468, accuracy : 98.61\n",
            "iteration : 200, loss : 0.0491, accuracy : 98.57\n",
            "iteration : 250, loss : 0.0509, accuracy : 98.50\n",
            "iteration : 300, loss : 0.0511, accuracy : 98.49\n",
            "iteration : 350, loss : 0.0510, accuracy : 98.48\n",
            "Epoch :  44, training loss : 0.0507, training accuracy : 98.49, test loss : 0.3004, test accuracy : 94.07\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.0440, accuracy : 98.78\n",
            "iteration : 100, loss : 0.0469, accuracy : 98.61\n",
            "iteration : 150, loss : 0.0463, accuracy : 98.59\n",
            "iteration : 200, loss : 0.0465, accuracy : 98.60\n",
            "iteration : 250, loss : 0.0463, accuracy : 98.62\n",
            "iteration : 300, loss : 0.0474, accuracy : 98.59\n",
            "iteration : 350, loss : 0.0495, accuracy : 98.52\n",
            "Epoch :  45, training loss : 0.0499, training accuracy : 98.50, test loss : 0.2806, test accuracy : 94.02\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.0520, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0440, accuracy : 98.62\n",
            "iteration : 150, loss : 0.0430, accuracy : 98.66\n",
            "iteration : 200, loss : 0.0427, accuracy : 98.67\n",
            "iteration : 250, loss : 0.0441, accuracy : 98.59\n",
            "iteration : 300, loss : 0.0461, accuracy : 98.54\n",
            "iteration : 350, loss : 0.0466, accuracy : 98.54\n",
            "Epoch :  46, training loss : 0.0468, training accuracy : 98.54, test loss : 0.2931, test accuracy : 94.23\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.0474, accuracy : 98.48\n",
            "iteration : 100, loss : 0.0443, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0455, accuracy : 98.61\n",
            "iteration : 200, loss : 0.0453, accuracy : 98.59\n",
            "iteration : 250, loss : 0.0465, accuracy : 98.54\n",
            "iteration : 300, loss : 0.0460, accuracy : 98.56\n",
            "iteration : 350, loss : 0.0471, accuracy : 98.55\n",
            "Epoch :  47, training loss : 0.0471, training accuracy : 98.56, test loss : 0.2961, test accuracy : 93.86\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.0411, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0360, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0379, accuracy : 98.77\n",
            "iteration : 200, loss : 0.0403, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0403, accuracy : 98.73\n",
            "iteration : 300, loss : 0.0408, accuracy : 98.74\n",
            "iteration : 350, loss : 0.0421, accuracy : 98.70\n",
            "Epoch :  48, training loss : 0.0426, training accuracy : 98.67, test loss : 0.3087, test accuracy : 93.91\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.0414, accuracy : 98.80\n",
            "iteration : 100, loss : 0.0422, accuracy : 98.66\n",
            "iteration : 150, loss : 0.0423, accuracy : 98.66\n",
            "iteration : 200, loss : 0.0419, accuracy : 98.69\n",
            "iteration : 250, loss : 0.0421, accuracy : 98.69\n",
            "iteration : 300, loss : 0.0432, accuracy : 98.62\n",
            "iteration : 350, loss : 0.0426, accuracy : 98.64\n",
            "Epoch :  49, training loss : 0.0430, training accuracy : 98.63, test loss : 0.3280, test accuracy : 93.98\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.0523, accuracy : 98.34\n",
            "iteration : 100, loss : 0.0479, accuracy : 98.54\n",
            "iteration : 150, loss : 0.0438, accuracy : 98.59\n",
            "iteration : 200, loss : 0.0422, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0420, accuracy : 98.70\n",
            "iteration : 300, loss : 0.0415, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0413, accuracy : 98.73\n",
            "Epoch :  50, training loss : 0.0413, training accuracy : 98.74, test loss : 0.3095, test accuracy : 94.08\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.0312, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0311, accuracy : 99.06\n",
            "iteration : 150, loss : 0.0319, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0336, accuracy : 98.96\n",
            "iteration : 250, loss : 0.0354, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0358, accuracy : 98.87\n",
            "iteration : 350, loss : 0.0360, accuracy : 98.88\n",
            "Epoch :  51, training loss : 0.0363, training accuracy : 98.88, test loss : 0.3121, test accuracy : 94.03\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.0357, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0361, accuracy : 98.86\n",
            "iteration : 150, loss : 0.0369, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0371, accuracy : 98.88\n",
            "iteration : 250, loss : 0.0373, accuracy : 98.82\n",
            "iteration : 300, loss : 0.0372, accuracy : 98.81\n",
            "iteration : 350, loss : 0.0378, accuracy : 98.77\n",
            "Epoch :  52, training loss : 0.0378, training accuracy : 98.78, test loss : 0.3265, test accuracy : 93.97\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.0330, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0286, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0332, accuracy : 98.91\n",
            "iteration : 200, loss : 0.0366, accuracy : 98.82\n",
            "iteration : 250, loss : 0.0394, accuracy : 98.75\n",
            "iteration : 300, loss : 0.0385, accuracy : 98.77\n",
            "iteration : 350, loss : 0.0384, accuracy : 98.76\n",
            "Epoch :  53, training loss : 0.0380, training accuracy : 98.78, test loss : 0.3161, test accuracy : 94.22\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.0267, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0287, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0287, accuracy : 99.16\n",
            "iteration : 200, loss : 0.0295, accuracy : 99.12\n",
            "iteration : 250, loss : 0.0319, accuracy : 99.05\n",
            "iteration : 300, loss : 0.0339, accuracy : 99.00\n",
            "iteration : 350, loss : 0.0340, accuracy : 98.99\n",
            "Epoch :  54, training loss : 0.0342, training accuracy : 98.98, test loss : 0.3249, test accuracy : 94.07\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.0344, accuracy : 98.98\n",
            "iteration : 100, loss : 0.0327, accuracy : 98.95\n",
            "iteration : 150, loss : 0.0328, accuracy : 98.92\n",
            "iteration : 200, loss : 0.0335, accuracy : 98.90\n",
            "iteration : 250, loss : 0.0336, accuracy : 98.91\n",
            "iteration : 300, loss : 0.0339, accuracy : 98.93\n",
            "iteration : 350, loss : 0.0338, accuracy : 98.95\n",
            "Epoch :  55, training loss : 0.0338, training accuracy : 98.95, test loss : 0.3219, test accuracy : 94.10\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.0296, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0317, accuracy : 99.00\n",
            "iteration : 150, loss : 0.0308, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0310, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0326, accuracy : 98.99\n",
            "iteration : 300, loss : 0.0324, accuracy : 98.97\n",
            "iteration : 350, loss : 0.0334, accuracy : 98.95\n",
            "Epoch :  56, training loss : 0.0337, training accuracy : 98.94, test loss : 0.3129, test accuracy : 94.22\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.0291, accuracy : 99.06\n",
            "iteration : 100, loss : 0.0282, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0284, accuracy : 99.10\n",
            "iteration : 200, loss : 0.0291, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0315, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0325, accuracy : 98.96\n",
            "iteration : 350, loss : 0.0336, accuracy : 98.92\n",
            "Epoch :  57, training loss : 0.0340, training accuracy : 98.91, test loss : 0.3158, test accuracy : 94.31\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.0312, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0287, accuracy : 99.04\n",
            "iteration : 150, loss : 0.0297, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0301, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0311, accuracy : 98.99\n",
            "iteration : 300, loss : 0.0311, accuracy : 99.01\n",
            "iteration : 350, loss : 0.0316, accuracy : 98.99\n",
            "Epoch :  58, training loss : 0.0317, training accuracy : 98.99, test loss : 0.3338, test accuracy : 93.88\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.0303, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0272, accuracy : 99.03\n",
            "iteration : 150, loss : 0.0274, accuracy : 99.01\n",
            "iteration : 200, loss : 0.0303, accuracy : 98.96\n",
            "iteration : 250, loss : 0.0303, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0299, accuracy : 98.99\n",
            "iteration : 350, loss : 0.0298, accuracy : 98.99\n",
            "Epoch :  59, training loss : 0.0304, training accuracy : 98.96, test loss : 0.3453, test accuracy : 94.07\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.0282, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0261, accuracy : 99.18\n",
            "iteration : 150, loss : 0.0254, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0287, accuracy : 99.10\n",
            "iteration : 250, loss : 0.0306, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0300, accuracy : 99.03\n",
            "iteration : 350, loss : 0.0291, accuracy : 99.06\n",
            "Epoch :  60, training loss : 0.0284, training accuracy : 99.08, test loss : 0.3241, test accuracy : 94.30\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.0245, accuracy : 99.06\n",
            "iteration : 100, loss : 0.0260, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0263, accuracy : 99.13\n",
            "iteration : 200, loss : 0.0254, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0274, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0297, accuracy : 99.04\n",
            "iteration : 350, loss : 0.0297, accuracy : 99.05\n",
            "Epoch :  61, training loss : 0.0294, training accuracy : 99.06, test loss : 0.3322, test accuracy : 94.23\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.0259, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0225, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0232, accuracy : 99.25\n",
            "iteration : 200, loss : 0.0243, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0267, accuracy : 99.15\n",
            "iteration : 300, loss : 0.0288, accuracy : 99.07\n",
            "iteration : 350, loss : 0.0289, accuracy : 99.06\n",
            "Epoch :  62, training loss : 0.0289, training accuracy : 99.05, test loss : 0.3325, test accuracy : 94.20\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.0274, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0265, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0279, accuracy : 99.07\n",
            "iteration : 200, loss : 0.0291, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0288, accuracy : 99.07\n",
            "iteration : 300, loss : 0.0285, accuracy : 99.05\n",
            "iteration : 350, loss : 0.0287, accuracy : 99.04\n",
            "Epoch :  63, training loss : 0.0289, training accuracy : 99.04, test loss : 0.3341, test accuracy : 94.06\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.0235, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0242, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0238, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0244, accuracy : 99.21\n",
            "iteration : 250, loss : 0.0248, accuracy : 99.20\n",
            "iteration : 300, loss : 0.0251, accuracy : 99.20\n",
            "iteration : 350, loss : 0.0251, accuracy : 99.19\n",
            "Epoch :  64, training loss : 0.0257, training accuracy : 99.18, test loss : 0.3346, test accuracy : 94.37\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.0197, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0214, accuracy : 99.36\n",
            "iteration : 150, loss : 0.0240, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0266, accuracy : 99.17\n",
            "iteration : 250, loss : 0.0276, accuracy : 99.13\n",
            "iteration : 300, loss : 0.0278, accuracy : 99.15\n",
            "iteration : 350, loss : 0.0278, accuracy : 99.14\n",
            "Epoch :  65, training loss : 0.0277, training accuracy : 99.13, test loss : 0.3316, test accuracy : 94.26\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.0267, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.16\n",
            "iteration : 150, loss : 0.0247, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0241, accuracy : 99.22\n",
            "iteration : 250, loss : 0.0252, accuracy : 99.17\n",
            "iteration : 300, loss : 0.0260, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0263, accuracy : 99.12\n",
            "Epoch :  66, training loss : 0.0263, training accuracy : 99.11, test loss : 0.3544, test accuracy : 94.05\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.0191, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0199, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0190, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0207, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0213, accuracy : 99.26\n",
            "iteration : 300, loss : 0.0232, accuracy : 99.21\n",
            "iteration : 350, loss : 0.0236, accuracy : 99.20\n",
            "Epoch :  67, training loss : 0.0234, training accuracy : 99.20, test loss : 0.3273, test accuracy : 94.31\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.0243, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0253, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0249, accuracy : 99.21\n",
            "iteration : 200, loss : 0.0243, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0240, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0244, accuracy : 99.19\n",
            "iteration : 350, loss : 0.0244, accuracy : 99.21\n",
            "Epoch :  68, training loss : 0.0249, training accuracy : 99.18, test loss : 0.3605, test accuracy : 94.13\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.0198, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0202, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0192, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0217, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0228, accuracy : 99.29\n",
            "iteration : 300, loss : 0.0229, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0230, accuracy : 99.28\n",
            "Epoch :  69, training loss : 0.0230, training accuracy : 99.28, test loss : 0.3315, test accuracy : 94.33\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.0175, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0180, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0197, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0200, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0207, accuracy : 99.29\n",
            "iteration : 300, loss : 0.0206, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0215, accuracy : 99.25\n",
            "Epoch :  70, training loss : 0.0223, training accuracy : 99.24, test loss : 0.3645, test accuracy : 94.06\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.0225, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0209, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0210, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0228, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0241, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0249, accuracy : 99.21\n",
            "iteration : 350, loss : 0.0249, accuracy : 99.19\n",
            "Epoch :  71, training loss : 0.0251, training accuracy : 99.18, test loss : 0.3752, test accuracy : 93.98\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.0199, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0208, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0196, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0204, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0215, accuracy : 99.30\n",
            "iteration : 300, loss : 0.0212, accuracy : 99.31\n",
            "iteration : 350, loss : 0.0221, accuracy : 99.26\n",
            "Epoch :  72, training loss : 0.0223, training accuracy : 99.26, test loss : 0.3792, test accuracy : 94.35\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.0215, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0192, accuracy : 99.40\n",
            "iteration : 150, loss : 0.0187, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0183, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0185, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0185, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0194, accuracy : 99.35\n",
            "Epoch :  73, training loss : 0.0196, training accuracy : 99.35, test loss : 0.3675, test accuracy : 94.33\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.0215, accuracy : 99.20\n",
            "iteration : 100, loss : 0.0224, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0216, accuracy : 99.29\n",
            "iteration : 200, loss : 0.0206, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0218, accuracy : 99.30\n",
            "iteration : 300, loss : 0.0222, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0219, accuracy : 99.27\n",
            "Epoch :  74, training loss : 0.0222, training accuracy : 99.26, test loss : 0.3611, test accuracy : 94.10\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.0191, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0196, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0227, accuracy : 99.29\n",
            "iteration : 200, loss : 0.0225, accuracy : 99.28\n",
            "iteration : 250, loss : 0.0232, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0236, accuracy : 99.24\n",
            "iteration : 350, loss : 0.0230, accuracy : 99.25\n",
            "Epoch :  75, training loss : 0.0228, training accuracy : 99.25, test loss : 0.3513, test accuracy : 94.36\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.0162, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0188, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0191, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0198, accuracy : 99.35\n",
            "iteration : 250, loss : 0.0204, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0201, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0199, accuracy : 99.34\n",
            "Epoch :  76, training loss : 0.0204, training accuracy : 99.33, test loss : 0.3665, test accuracy : 94.00\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.0197, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0173, accuracy : 99.39\n",
            "iteration : 150, loss : 0.0171, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0170, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0183, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0190, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0206, accuracy : 99.27\n",
            "Epoch :  77, training loss : 0.0206, training accuracy : 99.28, test loss : 0.3580, test accuracy : 94.21\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.0211, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0213, accuracy : 99.31\n",
            "iteration : 150, loss : 0.0222, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0222, accuracy : 99.25\n",
            "iteration : 250, loss : 0.0216, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0210, accuracy : 99.27\n",
            "iteration : 350, loss : 0.0211, accuracy : 99.27\n",
            "Epoch :  78, training loss : 0.0212, training accuracy : 99.27, test loss : 0.3582, test accuracy : 94.42\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.0229, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0235, accuracy : 99.19\n",
            "iteration : 150, loss : 0.0216, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0201, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0200, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0200, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0200, accuracy : 99.29\n",
            "Epoch :  79, training loss : 0.0198, training accuracy : 99.29, test loss : 0.3750, test accuracy : 94.13\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.0198, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0186, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0201, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0213, accuracy : 99.29\n",
            "iteration : 250, loss : 0.0214, accuracy : 99.29\n",
            "iteration : 300, loss : 0.0218, accuracy : 99.27\n",
            "iteration : 350, loss : 0.0226, accuracy : 99.25\n",
            "Epoch :  80, training loss : 0.0228, training accuracy : 99.24, test loss : 0.3517, test accuracy : 94.23\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.0217, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0178, accuracy : 99.36\n",
            "iteration : 150, loss : 0.0169, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0169, accuracy : 99.40\n",
            "iteration : 250, loss : 0.0167, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0171, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0171, accuracy : 99.43\n",
            "Epoch :  81, training loss : 0.0172, training accuracy : 99.43, test loss : 0.3685, test accuracy : 94.40\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.0251, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0234, accuracy : 99.26\n",
            "iteration : 150, loss : 0.0215, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0211, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0209, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0208, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0206, accuracy : 99.28\n",
            "Epoch :  82, training loss : 0.0208, training accuracy : 99.28, test loss : 0.3710, test accuracy : 94.10\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.0154, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0193, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0188, accuracy : 99.34\n",
            "iteration : 200, loss : 0.0184, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0186, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0196, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0197, accuracy : 99.32\n",
            "Epoch :  83, training loss : 0.0201, training accuracy : 99.31, test loss : 0.3717, test accuracy : 94.04\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.0172, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0147, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0143, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0149, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0162, accuracy : 99.45\n",
            "iteration : 300, loss : 0.0163, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0156, accuracy : 99.48\n",
            "Epoch :  84, training loss : 0.0159, training accuracy : 99.46, test loss : 0.3887, test accuracy : 94.37\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0161, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0163, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0165, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0173, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0174, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.41\n",
            "Epoch :  85, training loss : 0.0177, training accuracy : 99.41, test loss : 0.3685, test accuracy : 94.45\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.0141, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0153, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0184, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0197, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0204, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0204, accuracy : 99.36\n",
            "Epoch :  86, training loss : 0.0204, training accuracy : 99.36, test loss : 0.3531, test accuracy : 94.30\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.0141, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0144, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0142, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0137, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0138, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0148, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0150, accuracy : 99.51\n",
            "Epoch :  87, training loss : 0.0153, training accuracy : 99.50, test loss : 0.3660, test accuracy : 94.40\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.0163, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0158, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0143, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0134, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0142, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0155, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0164, accuracy : 99.44\n",
            "Epoch :  88, training loss : 0.0164, training accuracy : 99.44, test loss : 0.3742, test accuracy : 94.28\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.0128, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0129, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0149, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0168, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0172, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0170, accuracy : 99.43\n",
            "Epoch :  89, training loss : 0.0171, training accuracy : 99.43, test loss : 0.3748, test accuracy : 94.35\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.0173, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0163, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0166, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0167, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0183, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0182, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0175, accuracy : 99.42\n",
            "Epoch :  90, training loss : 0.0177, training accuracy : 99.41, test loss : 0.3667, test accuracy : 94.51\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0147, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0171, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0170, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0171, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0171, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0182, accuracy : 99.41\n",
            "Epoch :  91, training loss : 0.0180, training accuracy : 99.41, test loss : 0.3643, test accuracy : 94.35\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.0118, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0115, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0119, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0134, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0145, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0155, accuracy : 99.48\n",
            "Epoch :  92, training loss : 0.0156, training accuracy : 99.48, test loss : 0.3745, test accuracy : 94.14\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.0162, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0158, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0148, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0153, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0150, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0144, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0152, accuracy : 99.49\n",
            "Epoch :  93, training loss : 0.0153, training accuracy : 99.50, test loss : 0.3744, test accuracy : 94.46\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.0092, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0118, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0124, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0125, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0138, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0148, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0153, accuracy : 99.50\n",
            "Epoch :  94, training loss : 0.0153, training accuracy : 99.50, test loss : 0.3670, test accuracy : 94.52\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.0100, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0131, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0148, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0146, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0145, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0154, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0156, accuracy : 99.47\n",
            "Epoch :  95, training loss : 0.0154, training accuracy : 99.47, test loss : 0.3704, test accuracy : 94.41\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0125, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0140, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0140, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0145, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0145, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0139, accuracy : 99.51\n",
            "Epoch :  96, training loss : 0.0142, training accuracy : 99.50, test loss : 0.3895, test accuracy : 94.45\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.0232, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0178, accuracy : 99.39\n",
            "iteration : 150, loss : 0.0166, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0176, accuracy : 99.40\n",
            "iteration : 250, loss : 0.0177, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0178, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0184, accuracy : 99.38\n",
            "Epoch :  97, training loss : 0.0181, training accuracy : 99.39, test loss : 0.3927, test accuracy : 94.00\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.0101, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0131, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0138, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0141, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0137, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0144, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0162, accuracy : 99.45\n",
            "Epoch :  98, training loss : 0.0166, training accuracy : 99.45, test loss : 0.3687, test accuracy : 94.25\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.0154, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0146, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0132, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0127, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0125, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0131, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0132, accuracy : 99.57\n",
            "Epoch :  99, training loss : 0.0131, training accuracy : 99.58, test loss : 0.3656, test accuracy : 94.58\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0114, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0117, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0116, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0130, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0135, accuracy : 99.56\n",
            "Epoch : 100, training loss : 0.0139, training accuracy : 99.55, test loss : 0.3954, test accuracy : 94.14\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.0136, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0151, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0140, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0147, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0147, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0150, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0145, accuracy : 99.55\n",
            "Epoch : 101, training loss : 0.0147, training accuracy : 99.54, test loss : 0.3782, test accuracy : 94.19\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.0108, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0090, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0128, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0134, accuracy : 99.57\n",
            "Epoch : 102, training loss : 0.0135, training accuracy : 99.57, test loss : 0.3859, test accuracy : 94.35\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0137, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0131, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0142, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0142, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0145, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0142, accuracy : 99.53\n",
            "Epoch : 103, training loss : 0.0143, training accuracy : 99.53, test loss : 0.3739, test accuracy : 94.48\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0105, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0122, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0129, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0135, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0138, accuracy : 99.55\n",
            "Epoch : 104, training loss : 0.0139, training accuracy : 99.55, test loss : 0.3743, test accuracy : 94.40\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.0111, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0108, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0114, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0113, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0123, accuracy : 99.58\n",
            "Epoch : 105, training loss : 0.0125, training accuracy : 99.58, test loss : 0.3893, test accuracy : 94.39\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0115, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0111, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0130, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0131, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0126, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0124, accuracy : 99.59\n",
            "Epoch : 106, training loss : 0.0124, training accuracy : 99.58, test loss : 0.3787, test accuracy : 94.40\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0116, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0110, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0112, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0117, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0129, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0137, accuracy : 99.50\n",
            "Epoch : 107, training loss : 0.0139, training accuracy : 99.49, test loss : 0.3784, test accuracy : 94.41\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0147, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0133, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0123, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0119, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0122, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0124, accuracy : 99.58\n",
            "Epoch : 108, training loss : 0.0122, training accuracy : 99.59, test loss : 0.4001, test accuracy : 94.14\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.0175, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0150, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0155, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0154, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0156, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0153, accuracy : 99.51\n",
            "Epoch : 109, training loss : 0.0151, training accuracy : 99.52, test loss : 0.3838, test accuracy : 94.25\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.0133, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0098, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.67\n",
            "Epoch : 110, training loss : 0.0104, training accuracy : 99.67, test loss : 0.3888, test accuracy : 94.40\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.0123, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0118, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0111, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0107, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0122, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0132, accuracy : 99.56\n",
            "Epoch : 111, training loss : 0.0132, training accuracy : 99.56, test loss : 0.3812, test accuracy : 94.50\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.0179, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0153, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0156, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0150, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0143, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0143, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0139, accuracy : 99.57\n",
            "Epoch : 112, training loss : 0.0146, training accuracy : 99.56, test loss : 0.3989, test accuracy : 94.33\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.0137, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0150, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0139, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0136, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0137, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0132, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0125, accuracy : 99.57\n",
            "Epoch : 113, training loss : 0.0127, training accuracy : 99.57, test loss : 0.4040, test accuracy : 94.23\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.0093, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0106, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0101, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0099, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.61\n",
            "Epoch : 114, training loss : 0.0114, training accuracy : 99.60, test loss : 0.4085, test accuracy : 94.35\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0121, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0118, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0119, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0116, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0117, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0118, accuracy : 99.62\n",
            "Epoch : 115, training loss : 0.0117, training accuracy : 99.63, test loss : 0.4040, test accuracy : 94.42\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0099, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0100, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0111, accuracy : 99.60\n",
            "Epoch : 116, training loss : 0.0112, training accuracy : 99.60, test loss : 0.4059, test accuracy : 94.28\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0131, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0139, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0134, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0131, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0128, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0126, accuracy : 99.59\n",
            "Epoch : 117, training loss : 0.0126, training accuracy : 99.59, test loss : 0.3862, test accuracy : 94.46\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.0092, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0088, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0097, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.64\n",
            "Epoch : 118, training loss : 0.0117, training accuracy : 99.63, test loss : 0.3869, test accuracy : 94.18\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0126, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0111, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0116, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0119, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0125, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.60\n",
            "Epoch : 119, training loss : 0.0125, training accuracy : 99.60, test loss : 0.4114, test accuracy : 94.19\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.0101, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0119, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0116, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0103, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.63\n",
            "Epoch : 120, training loss : 0.0118, training accuracy : 99.61, test loss : 0.3980, test accuracy : 94.50\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.0082, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0103, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0097, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0114, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.62\n",
            "Epoch : 121, training loss : 0.0115, training accuracy : 99.61, test loss : 0.4007, test accuracy : 94.10\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.0095, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0103, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0100, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0110, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.65\n",
            "Epoch : 122, training loss : 0.0114, training accuracy : 99.64, test loss : 0.4037, test accuracy : 94.27\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.0141, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0146, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0138, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0135, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0133, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0130, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.59\n",
            "Epoch : 123, training loss : 0.0125, training accuracy : 99.60, test loss : 0.3852, test accuracy : 94.38\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0077, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0078, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0082, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0086, accuracy : 99.71\n",
            "Epoch : 124, training loss : 0.0086, training accuracy : 99.71, test loss : 0.4210, test accuracy : 94.40\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.0132, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0111, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0105, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0106, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0103, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.67\n",
            "Epoch : 125, training loss : 0.0107, training accuracy : 99.66, test loss : 0.4246, test accuracy : 94.11\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.0103, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0100, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0122, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0116, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0108, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.63\n",
            "Epoch : 126, training loss : 0.0112, training accuracy : 99.64, test loss : 0.4092, test accuracy : 94.36\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0072, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0086, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0091, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0093, accuracy : 99.69\n",
            "Epoch : 127, training loss : 0.0093, training accuracy : 99.70, test loss : 0.4053, test accuracy : 94.53\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0090, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0098, accuracy : 99.68\n",
            "Epoch : 128, training loss : 0.0100, training accuracy : 99.68, test loss : 0.4231, test accuracy : 94.36\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0095, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0120, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0123, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0128, accuracy : 99.58\n",
            "Epoch : 129, training loss : 0.0127, training accuracy : 99.59, test loss : 0.3874, test accuracy : 94.51\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.61\n",
            "iteration : 150, loss : 0.0109, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0101, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0105, accuracy : 99.65\n",
            "Epoch : 130, training loss : 0.0103, training accuracy : 99.66, test loss : 0.4195, test accuracy : 94.19\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.0087, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0103, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0104, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0100, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.63\n",
            "Epoch : 131, training loss : 0.0106, training accuracy : 99.63, test loss : 0.4175, test accuracy : 94.26\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.0094, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0113, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0106, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.65\n",
            "Epoch : 132, training loss : 0.0107, training accuracy : 99.66, test loss : 0.4164, test accuracy : 94.31\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0082, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0103, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0105, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0101, accuracy : 99.67\n",
            "Epoch : 133, training loss : 0.0101, training accuracy : 99.67, test loss : 0.3940, test accuracy : 94.44\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0080, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0117, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0118, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0121, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0122, accuracy : 99.55\n",
            "Epoch : 134, training loss : 0.0127, training accuracy : 99.54, test loss : 0.3938, test accuracy : 94.36\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.0101, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0097, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0097, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0102, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0098, accuracy : 99.69\n",
            "Epoch : 135, training loss : 0.0096, training accuracy : 99.70, test loss : 0.4173, test accuracy : 94.42\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0104, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.64\n",
            "Epoch : 136, training loss : 0.0103, training accuracy : 99.63, test loss : 0.4208, test accuracy : 94.21\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0101, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0104, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0102, accuracy : 99.68\n",
            "Epoch : 137, training loss : 0.0102, training accuracy : 99.67, test loss : 0.4082, test accuracy : 94.38\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.0079, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0110, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0120, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0113, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0107, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.61\n",
            "Epoch : 138, training loss : 0.0111, training accuracy : 99.61, test loss : 0.4369, test accuracy : 94.18\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0140, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.61\n",
            "iteration : 150, loss : 0.0115, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0120, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0117, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0122, accuracy : 99.60\n",
            "Epoch : 139, training loss : 0.0125, training accuracy : 99.60, test loss : 0.3842, test accuracy : 94.22\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0065, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0070, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.76\n",
            "Epoch : 140, training loss : 0.0075, training accuracy : 99.75, test loss : 0.4022, test accuracy : 94.53\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0068, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0068, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0067, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0069, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0070, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0072, accuracy : 99.75\n",
            "Epoch : 141, training loss : 0.0074, training accuracy : 99.75, test loss : 0.4153, test accuracy : 94.53\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0077, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0099, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0098, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0098, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0110, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0111, accuracy : 99.59\n",
            "Epoch : 142, training loss : 0.0111, training accuracy : 99.60, test loss : 0.4123, test accuracy : 94.22\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0110, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0106, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0112, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0115, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0114, accuracy : 99.62\n",
            "Epoch : 143, training loss : 0.0114, training accuracy : 99.63, test loss : 0.3947, test accuracy : 94.57\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0081, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0101, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0090, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0089, accuracy : 99.70\n",
            "Epoch : 144, training loss : 0.0089, training accuracy : 99.69, test loss : 0.4345, test accuracy : 94.22\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0080, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0086, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0105, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0102, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0096, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0096, accuracy : 99.66\n",
            "Epoch : 145, training loss : 0.0099, training accuracy : 99.65, test loss : 0.4225, test accuracy : 94.31\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0104, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0103, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0100, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0103, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0102, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0100, accuracy : 99.66\n",
            "Epoch : 146, training loss : 0.0098, training accuracy : 99.67, test loss : 0.4120, test accuracy : 94.41\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0084, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0081, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0082, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0081, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0081, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0076, accuracy : 99.73\n",
            "Epoch : 147, training loss : 0.0076, training accuracy : 99.73, test loss : 0.4085, test accuracy : 94.50\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0076, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0105, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0110, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0093, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0094, accuracy : 99.72\n",
            "Epoch : 148, training loss : 0.0093, training accuracy : 99.72, test loss : 0.4208, test accuracy : 94.35\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0107, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0089, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0084, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0085, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0083, accuracy : 99.70\n",
            "Epoch : 149, training loss : 0.0084, training accuracy : 99.69, test loss : 0.4342, test accuracy : 94.51\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0101, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0087, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0092, accuracy : 99.67\n",
            "Epoch : 150, training loss : 0.0091, training accuracy : 99.68, test loss : 0.4342, test accuracy : 94.26\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0115, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0117, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0110, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0103, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.66\n",
            "Epoch : 151, training loss : 0.0104, training accuracy : 99.66, test loss : 0.4145, test accuracy : 94.33\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0077, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0068, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0061, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0057, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0059, accuracy : 99.80\n",
            "Epoch : 152, training loss : 0.0061, training accuracy : 99.80, test loss : 0.4202, test accuracy : 94.43\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0063, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.79\n",
            "Epoch : 153, training loss : 0.0065, training accuracy : 99.79, test loss : 0.4367, test accuracy : 94.41\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0077, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0077, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0083, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0098, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0104, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0112, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0114, accuracy : 99.61\n",
            "Epoch : 154, training loss : 0.0111, training accuracy : 99.63, test loss : 0.3971, test accuracy : 94.66\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0093, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0090, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0089, accuracy : 99.72\n",
            "Epoch : 155, training loss : 0.0087, training accuracy : 99.72, test loss : 0.4121, test accuracy : 94.41\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0070, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0080, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0084, accuracy : 99.73\n",
            "Epoch : 156, training loss : 0.0088, training accuracy : 99.73, test loss : 0.4072, test accuracy : 94.40\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0080, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0088, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0095, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0098, accuracy : 99.69\n",
            "Epoch : 157, training loss : 0.0098, training accuracy : 99.69, test loss : 0.4362, test accuracy : 94.24\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0082, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0104, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0097, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0098, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0087, accuracy : 99.73\n",
            "Epoch : 158, training loss : 0.0084, training accuracy : 99.73, test loss : 0.4073, test accuracy : 94.44\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0056, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0074, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0075, accuracy : 99.73\n",
            "Epoch : 159, training loss : 0.0074, training accuracy : 99.73, test loss : 0.4083, test accuracy : 94.30\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0100, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0089, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0092, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0092, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0091, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0097, accuracy : 99.68\n",
            "Epoch : 160, training loss : 0.0099, training accuracy : 99.68, test loss : 0.4300, test accuracy : 94.38\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0079, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0090, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0092, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.71\n",
            "Epoch : 161, training loss : 0.0088, training accuracy : 99.71, test loss : 0.4489, test accuracy : 94.05\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0072, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0075, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.73\n",
            "Epoch : 162, training loss : 0.0077, training accuracy : 99.73, test loss : 0.4490, test accuracy : 94.37\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0087, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0096, accuracy : 99.68\n",
            "Epoch : 163, training loss : 0.0094, training accuracy : 99.68, test loss : 0.4271, test accuracy : 94.33\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0092, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0087, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0085, accuracy : 99.72\n",
            "Epoch : 164, training loss : 0.0083, training accuracy : 99.72, test loss : 0.4120, test accuracy : 94.44\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0065, accuracy : 99.79\n",
            "Epoch : 165, training loss : 0.0066, training accuracy : 99.79, test loss : 0.4172, test accuracy : 94.65\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0031, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.82\n",
            "Epoch : 166, training loss : 0.0061, training accuracy : 99.81, test loss : 0.4613, test accuracy : 93.99\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0161, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0124, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0126, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0121, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0115, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.64\n",
            "Epoch : 167, training loss : 0.0109, training accuracy : 99.65, test loss : 0.4246, test accuracy : 94.53\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0083, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0084, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0078, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0082, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0076, accuracy : 99.73\n",
            "Epoch : 168, training loss : 0.0077, training accuracy : 99.73, test loss : 0.4266, test accuracy : 94.53\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0082, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0077, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0068, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.80\n",
            "Epoch : 169, training loss : 0.0064, training accuracy : 99.80, test loss : 0.4437, test accuracy : 94.38\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0062, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0075, accuracy : 99.75\n",
            "Epoch : 170, training loss : 0.0077, training accuracy : 99.74, test loss : 0.4234, test accuracy : 94.39\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0140, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0100, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0099, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0093, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.69\n",
            "Epoch : 171, training loss : 0.0089, training accuracy : 99.70, test loss : 0.4164, test accuracy : 94.54\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0061, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0075, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0082, accuracy : 99.71\n",
            "Epoch : 172, training loss : 0.0086, training accuracy : 99.70, test loss : 0.4196, test accuracy : 94.36\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0109, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0069, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0071, accuracy : 99.77\n",
            "Epoch : 173, training loss : 0.0071, training accuracy : 99.77, test loss : 0.4373, test accuracy : 94.45\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0068, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0064, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0070, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0080, accuracy : 99.73\n",
            "Epoch : 174, training loss : 0.0085, training accuracy : 99.72, test loss : 0.4136, test accuracy : 94.29\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0060, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0062, accuracy : 99.80\n",
            "Epoch : 175, training loss : 0.0062, training accuracy : 99.79, test loss : 0.4485, test accuracy : 94.49\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0073, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0064, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.79\n",
            "Epoch : 176, training loss : 0.0066, training accuracy : 99.78, test loss : 0.4382, test accuracy : 94.52\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0066, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0081, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0086, accuracy : 99.71\n",
            "Epoch : 177, training loss : 0.0086, training accuracy : 99.70, test loss : 0.4255, test accuracy : 94.47\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0075, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0086, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0080, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0075, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0077, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.77\n",
            "Epoch : 178, training loss : 0.0079, training accuracy : 99.76, test loss : 0.4139, test accuracy : 94.46\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.74\n",
            "Epoch : 179, training loss : 0.0075, training accuracy : 99.73, test loss : 0.4226, test accuracy : 94.40\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0087, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0080, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0081, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0074, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0070, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0067, accuracy : 99.79\n",
            "Epoch : 180, training loss : 0.0066, training accuracy : 99.79, test loss : 0.4426, test accuracy : 94.51\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0076, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0078, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0069, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.78\n",
            "Epoch : 181, training loss : 0.0062, training accuracy : 99.78, test loss : 0.5021, test accuracy : 94.10\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0070, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0071, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0062, accuracy : 99.79\n",
            "Epoch : 182, training loss : 0.0061, training accuracy : 99.79, test loss : 0.4218, test accuracy : 94.61\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0065, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.74\n",
            "Epoch : 183, training loss : 0.0079, training accuracy : 99.74, test loss : 0.4514, test accuracy : 94.42\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0076, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0072, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0071, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0073, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0080, accuracy : 99.71\n",
            "Epoch : 184, training loss : 0.0080, training accuracy : 99.70, test loss : 0.4259, test accuracy : 94.22\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0126, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0085, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0076, accuracy : 99.75\n",
            "Epoch : 185, training loss : 0.0079, training accuracy : 99.75, test loss : 0.4617, test accuracy : 94.46\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0105, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0086, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0079, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0075, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0077, accuracy : 99.72\n",
            "Epoch : 186, training loss : 0.0076, training accuracy : 99.72, test loss : 0.4279, test accuracy : 94.45\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0066, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0074, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.77\n",
            "Epoch : 187, training loss : 0.0074, training accuracy : 99.76, test loss : 0.4181, test accuracy : 94.48\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.84\n",
            "Epoch : 188, training loss : 0.0046, training accuracy : 99.84, test loss : 0.4346, test accuracy : 94.61\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0065, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0065, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0063, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.79\n",
            "Epoch : 189, training loss : 0.0069, training accuracy : 99.78, test loss : 0.4565, test accuracy : 94.23\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0095, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0086, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0073, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0081, accuracy : 99.74\n",
            "Epoch : 190, training loss : 0.0081, training accuracy : 99.74, test loss : 0.4420, test accuracy : 94.63\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0076, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0077, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0085, accuracy : 99.68\n",
            "Epoch : 191, training loss : 0.0084, training accuracy : 99.69, test loss : 0.4399, test accuracy : 94.51\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0072, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0061, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.82\n",
            "Epoch : 192, training loss : 0.0055, training accuracy : 99.82, test loss : 0.4580, test accuracy : 94.44\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0076, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0073, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0084, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0076, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0071, accuracy : 99.76\n",
            "Epoch : 193, training loss : 0.0069, training accuracy : 99.77, test loss : 0.4536, test accuracy : 94.52\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0097, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0089, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0084, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0084, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0086, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0083, accuracy : 99.73\n",
            "Epoch : 194, training loss : 0.0082, training accuracy : 99.73, test loss : 0.4266, test accuracy : 94.50\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.86\n",
            "Epoch : 195, training loss : 0.0042, training accuracy : 99.86, test loss : 0.4774, test accuracy : 93.93\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0091, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.70\n",
            "Epoch : 196, training loss : 0.0086, training accuracy : 99.71, test loss : 0.4315, test accuracy : 94.47\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0083, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0082, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.70\n",
            "Epoch : 197, training loss : 0.0087, training accuracy : 99.70, test loss : 0.4298, test accuracy : 94.48\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0078, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0082, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0082, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0077, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0068, accuracy : 99.76\n",
            "Epoch : 198, training loss : 0.0068, training accuracy : 99.76, test loss : 0.4258, test accuracy : 94.64\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0063, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0067, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0069, accuracy : 99.78\n",
            "Epoch : 199, training loss : 0.0067, training accuracy : 99.79, test loss : 0.4135, test accuracy : 94.58\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.84\n",
            "Epoch : 200, training loss : 0.0055, training accuracy : 99.83, test loss : 0.4423, test accuracy : 94.61\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0070, accuracy : 99.80\n",
            "Epoch : 201, training loss : 0.0069, training accuracy : 99.79, test loss : 0.4451, test accuracy : 94.40\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0083, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0087, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0085, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0081, accuracy : 99.72\n",
            "Epoch : 202, training loss : 0.0084, training accuracy : 99.72, test loss : 0.4236, test accuracy : 94.57\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0033, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.80\n",
            "Epoch : 203, training loss : 0.0057, training accuracy : 99.80, test loss : 0.4562, test accuracy : 94.21\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0100, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0082, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0072, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0065, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0062, accuracy : 99.81\n",
            "Epoch : 204, training loss : 0.0061, training accuracy : 99.82, test loss : 0.4532, test accuracy : 94.60\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0073, accuracy : 99.75\n",
            "Epoch : 205, training loss : 0.0072, training accuracy : 99.75, test loss : 0.4544, test accuracy : 94.32\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0064, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0063, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0064, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.79\n",
            "Epoch : 206, training loss : 0.0062, training accuracy : 99.79, test loss : 0.4289, test accuracy : 94.59\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0058, accuracy : 99.81\n",
            "Epoch : 207, training loss : 0.0058, training accuracy : 99.81, test loss : 0.4577, test accuracy : 94.25\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0082, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0079, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0064, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.80\n",
            "Epoch : 208, training loss : 0.0055, training accuracy : 99.80, test loss : 0.4652, test accuracy : 94.43\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0074, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0077, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0071, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0074, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0070, accuracy : 99.77\n",
            "Epoch : 209, training loss : 0.0069, training accuracy : 99.78, test loss : 0.4482, test accuracy : 94.55\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0076, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0069, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.77\n",
            "Epoch : 210, training loss : 0.0064, training accuracy : 99.78, test loss : 0.4504, test accuracy : 94.33\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0041, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.86\n",
            "Epoch : 211, training loss : 0.0045, training accuracy : 99.85, test loss : 0.4581, test accuracy : 94.41\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0084, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0088, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.69\n",
            "Epoch : 212, training loss : 0.0094, training accuracy : 99.68, test loss : 0.4833, test accuracy : 94.08\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0087, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0069, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0062, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0062, accuracy : 99.81\n",
            "Epoch : 213, training loss : 0.0060, training accuracy : 99.82, test loss : 0.4330, test accuracy : 94.53\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 214, training loss : 0.0044, training accuracy : 99.87, test loss : 0.4501, test accuracy : 94.60\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0056, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0073, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0071, accuracy : 99.74\n",
            "Epoch : 215, training loss : 0.0072, training accuracy : 99.74, test loss : 0.4441, test accuracy : 94.67\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0060, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0061, accuracy : 99.78\n",
            "Epoch : 216, training loss : 0.0061, training accuracy : 99.78, test loss : 0.4246, test accuracy : 94.61\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0041, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.85\n",
            "Epoch : 217, training loss : 0.0043, training accuracy : 99.85, test loss : 0.4792, test accuracy : 94.45\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0070, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0072, accuracy : 99.77\n",
            "Epoch : 218, training loss : 0.0075, training accuracy : 99.77, test loss : 0.4441, test accuracy : 94.48\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0068, accuracy : 99.79\n",
            "Epoch : 219, training loss : 0.0068, training accuracy : 99.79, test loss : 0.4233, test accuracy : 94.49\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0064, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.82\n",
            "Epoch : 220, training loss : 0.0062, training accuracy : 99.82, test loss : 0.4208, test accuracy : 94.68\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0032, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.82\n",
            "Epoch : 221, training loss : 0.0049, training accuracy : 99.81, test loss : 0.4612, test accuracy : 94.37\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0066, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0064, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0062, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0059, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0064, accuracy : 99.77\n",
            "Epoch : 222, training loss : 0.0065, training accuracy : 99.77, test loss : 0.4447, test accuracy : 94.10\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0057, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0060, accuracy : 99.78\n",
            "Epoch : 223, training loss : 0.0061, training accuracy : 99.78, test loss : 0.4583, test accuracy : 94.36\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0062, accuracy : 99.80\n",
            "Epoch : 224, training loss : 0.0061, training accuracy : 99.80, test loss : 0.4615, test accuracy : 94.43\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0062, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0060, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0059, accuracy : 99.81\n",
            "Epoch : 225, training loss : 0.0061, training accuracy : 99.81, test loss : 0.4621, test accuracy : 94.56\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0061, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0062, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0059, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.83\n",
            "Epoch : 226, training loss : 0.0057, training accuracy : 99.83, test loss : 0.4440, test accuracy : 94.61\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0040, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.80\n",
            "Epoch : 227, training loss : 0.0055, training accuracy : 99.79, test loss : 0.4464, test accuracy : 94.36\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.82\n",
            "Epoch : 228, training loss : 0.0048, training accuracy : 99.83, test loss : 0.4640, test accuracy : 94.62\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.82\n",
            "Epoch : 229, training loss : 0.0052, training accuracy : 99.81, test loss : 0.4664, test accuracy : 94.69\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0068, accuracy : 99.76\n",
            "Epoch : 230, training loss : 0.0069, training accuracy : 99.76, test loss : 0.4645, test accuracy : 94.40\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0030, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0033, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0035, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.80\n",
            "Epoch : 231, training loss : 0.0054, training accuracy : 99.81, test loss : 0.4498, test accuracy : 94.48\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0057, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.83\n",
            "Epoch : 232, training loss : 0.0052, training accuracy : 99.83, test loss : 0.4492, test accuracy : 94.44\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0059, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0064, accuracy : 99.81\n",
            "Epoch : 233, training loss : 0.0062, training accuracy : 99.81, test loss : 0.4654, test accuracy : 94.33\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0037, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0034, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0040, accuracy : 99.88\n",
            "Epoch : 234, training loss : 0.0040, training accuracy : 99.88, test loss : 0.4704, test accuracy : 94.69\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0074, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0082, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0076, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0067, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0067, accuracy : 99.76\n",
            "Epoch : 235, training loss : 0.0067, training accuracy : 99.75, test loss : 0.4714, test accuracy : 94.57\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0069, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0065, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0061, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0058, accuracy : 99.79\n",
            "Epoch : 236, training loss : 0.0057, training accuracy : 99.80, test loss : 0.4597, test accuracy : 94.45\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.86\n",
            "Epoch : 237, training loss : 0.0045, training accuracy : 99.85, test loss : 0.4512, test accuracy : 94.62\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0028, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0029, accuracy : 99.92\n",
            "iteration : 300, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0036, accuracy : 99.89\n",
            "Epoch : 238, training loss : 0.0038, training accuracy : 99.89, test loss : 0.4842, test accuracy : 94.40\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.83\n",
            "Epoch : 239, training loss : 0.0049, training accuracy : 99.83, test loss : 0.4735, test accuracy : 94.48\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.81\n",
            "Epoch : 240, training loss : 0.0053, training accuracy : 99.81, test loss : 0.4673, test accuracy : 94.63\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.84\n",
            "Epoch : 241, training loss : 0.0045, training accuracy : 99.84, test loss : 0.4606, test accuracy : 94.55\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0068, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0069, accuracy : 99.76\n",
            "Epoch : 242, training loss : 0.0067, training accuracy : 99.77, test loss : 0.4460, test accuracy : 94.52\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0068, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.81\n",
            "Epoch : 243, training loss : 0.0065, training accuracy : 99.81, test loss : 0.4506, test accuracy : 94.61\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0026, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0029, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0028, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0028, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0034, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0034, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0041, accuracy : 99.85\n",
            "Epoch : 244, training loss : 0.0042, training accuracy : 99.85, test loss : 0.4551, test accuracy : 94.56\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.83\n",
            "Epoch : 245, training loss : 0.0052, training accuracy : 99.83, test loss : 0.4554, test accuracy : 94.51\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.82\n",
            "Epoch : 246, training loss : 0.0057, training accuracy : 99.81, test loss : 0.4848, test accuracy : 94.53\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.83\n",
            "Epoch : 247, training loss : 0.0046, training accuracy : 99.83, test loss : 0.4727, test accuracy : 94.60\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 248, training loss : 0.0047, training accuracy : 99.86, test loss : 0.4594, test accuracy : 94.46\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.83\n",
            "Epoch : 249, training loss : 0.0049, training accuracy : 99.84, test loss : 0.4590, test accuracy : 94.40\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.82\n",
            "Epoch : 250, training loss : 0.0048, training accuracy : 99.82, test loss : 0.4751, test accuracy : 94.49\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0059, accuracy : 99.81\n",
            "Epoch : 251, training loss : 0.0060, training accuracy : 99.80, test loss : 0.4753, test accuracy : 94.43\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0092, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0085, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0081, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0076, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.78\n",
            "Epoch : 252, training loss : 0.0073, training accuracy : 99.79, test loss : 0.4564, test accuracy : 94.34\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0017, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0030, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0035, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0039, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0038, accuracy : 99.85\n",
            "Epoch : 253, training loss : 0.0039, training accuracy : 99.84, test loss : 0.4544, test accuracy : 94.67\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.82\n",
            "Epoch : 254, training loss : 0.0050, training accuracy : 99.83, test loss : 0.4464, test accuracy : 94.58\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.82\n",
            "Epoch : 255, training loss : 0.0050, training accuracy : 99.83, test loss : 0.4552, test accuracy : 94.61\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0061, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.79\n",
            "Epoch : 256, training loss : 0.0064, training accuracy : 99.78, test loss : 0.4642, test accuracy : 94.35\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0035, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 257, training loss : 0.0046, training accuracy : 99.86, test loss : 0.4458, test accuracy : 94.58\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0022, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0028, accuracy : 99.93\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 258, training loss : 0.0047, training accuracy : 99.85, test loss : 0.4676, test accuracy : 94.51\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0037, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0036, accuracy : 99.90\n",
            "Epoch : 259, training loss : 0.0036, training accuracy : 99.90, test loss : 0.4686, test accuracy : 94.71\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0017, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0019, accuracy : 99.92\n",
            "iteration : 150, loss : 0.0025, accuracy : 99.93\n",
            "iteration : 200, loss : 0.0027, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0029, accuracy : 99.91\n",
            "iteration : 300, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0032, accuracy : 99.90\n",
            "Epoch : 260, training loss : 0.0031, training accuracy : 99.90, test loss : 0.4767, test accuracy : 94.71\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0076, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0076, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0074, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0069, accuracy : 99.77\n",
            "Epoch : 261, training loss : 0.0072, training accuracy : 99.77, test loss : 0.4614, test accuracy : 94.52\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0068, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0057, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.84\n",
            "Epoch : 262, training loss : 0.0050, training accuracy : 99.84, test loss : 0.4683, test accuracy : 94.38\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0072, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0057, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.81\n",
            "Epoch : 263, training loss : 0.0052, training accuracy : 99.82, test loss : 0.4728, test accuracy : 94.50\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.83\n",
            "Epoch : 264, training loss : 0.0053, training accuracy : 99.84, test loss : 0.4412, test accuracy : 94.59\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0025, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0034, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0032, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0040, accuracy : 99.85\n",
            "Epoch : 265, training loss : 0.0041, training accuracy : 99.86, test loss : 0.4607, test accuracy : 94.53\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0028, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0028, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0026, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0026, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0029, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0031, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0034, accuracy : 99.87\n",
            "Epoch : 266, training loss : 0.0036, training accuracy : 99.87, test loss : 0.5259, test accuracy : 94.41\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0112, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0106, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0085, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0074, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.81\n",
            "Epoch : 267, training loss : 0.0062, training accuracy : 99.81, test loss : 0.4835, test accuracy : 94.54\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0060, accuracy : 99.79\n",
            "Epoch : 268, training loss : 0.0059, training accuracy : 99.80, test loss : 0.4697, test accuracy : 94.30\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0078, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0039, accuracy : 99.88\n",
            "Epoch : 269, training loss : 0.0039, training accuracy : 99.88, test loss : 0.4692, test accuracy : 94.57\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0069, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0068, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0067, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0065, accuracy : 99.80\n",
            "Epoch : 270, training loss : 0.0066, training accuracy : 99.80, test loss : 0.4540, test accuracy : 94.69\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0037, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0037, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0036, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0037, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0037, accuracy : 99.86\n",
            "Epoch : 271, training loss : 0.0037, training accuracy : 99.86, test loss : 0.4712, test accuracy : 94.62\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0032, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0034, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0033, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.86\n",
            "Epoch : 272, training loss : 0.0044, training accuracy : 99.85, test loss : 0.4692, test accuracy : 94.55\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0057, accuracy : 99.83\n",
            "Epoch : 273, training loss : 0.0054, training accuracy : 99.84, test loss : 0.4425, test accuracy : 94.67\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0036, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0038, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0040, accuracy : 99.89\n",
            "Epoch : 274, training loss : 0.0040, training accuracy : 99.88, test loss : 0.4557, test accuracy : 94.71\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0015, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0020, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 275, training loss : 0.0047, training accuracy : 99.85, test loss : 0.4648, test accuracy : 94.59\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.84\n",
            "Epoch : 276, training loss : 0.0055, training accuracy : 99.84, test loss : 0.4752, test accuracy : 94.40\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.83\n",
            "Epoch : 277, training loss : 0.0053, training accuracy : 99.84, test loss : 0.4409, test accuracy : 94.69\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0037, accuracy : 99.89\n",
            "Epoch : 278, training loss : 0.0037, training accuracy : 99.89, test loss : 0.4787, test accuracy : 94.69\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0028, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0030, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0035, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0035, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0035, accuracy : 99.88\n",
            "Epoch : 279, training loss : 0.0036, training accuracy : 99.88, test loss : 0.4882, test accuracy : 94.52\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0068, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.83\n",
            "Epoch : 280, training loss : 0.0049, training accuracy : 99.83, test loss : 0.4832, test accuracy : 94.46\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0027, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 281, training loss : 0.0048, training accuracy : 99.84, test loss : 0.4533, test accuracy : 94.52\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0041, accuracy : 99.85\n",
            "Epoch : 282, training loss : 0.0041, training accuracy : 99.86, test loss : 0.4667, test accuracy : 94.58\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.86\n",
            "Epoch : 283, training loss : 0.0049, training accuracy : 99.87, test loss : 0.4660, test accuracy : 94.61\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0023, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0027, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0027, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0034, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.84\n",
            "Epoch : 284, training loss : 0.0047, training accuracy : 99.84, test loss : 0.4592, test accuracy : 94.62\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0028, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0024, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0038, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0037, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0037, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.83\n",
            "Epoch : 285, training loss : 0.0044, training accuracy : 99.83, test loss : 0.4795, test accuracy : 94.35\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.84\n",
            "Epoch : 286, training loss : 0.0042, training accuracy : 99.85, test loss : 0.4735, test accuracy : 94.54\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0037, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0035, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0040, accuracy : 99.85\n",
            "Epoch : 287, training loss : 0.0042, training accuracy : 99.85, test loss : 0.4946, test accuracy : 94.41\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0057, accuracy : 99.79\n",
            "Epoch : 288, training loss : 0.0059, training accuracy : 99.79, test loss : 0.4771, test accuracy : 94.50\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.79\n",
            "Epoch : 289, training loss : 0.0056, training accuracy : 99.80, test loss : 0.4517, test accuracy : 94.43\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0041, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0039, accuracy : 99.88\n",
            "Epoch : 290, training loss : 0.0037, training accuracy : 99.88, test loss : 0.4416, test accuracy : 94.68\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0029, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0023, accuracy : 99.93\n",
            "iteration : 200, loss : 0.0023, accuracy : 99.93\n",
            "iteration : 250, loss : 0.0021, accuracy : 99.93\n",
            "iteration : 300, loss : 0.0021, accuracy : 99.93\n",
            "iteration : 350, loss : 0.0023, accuracy : 99.93\n",
            "Epoch : 291, training loss : 0.0025, training accuracy : 99.92, test loss : 0.4855, test accuracy : 94.42\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0019, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0015, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0020, accuracy : 99.94\n",
            "iteration : 200, loss : 0.0024, accuracy : 99.93\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 292, training loss : 0.0049, training accuracy : 99.86, test loss : 0.4604, test accuracy : 94.43\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0073, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0065, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 293, training loss : 0.0047, training accuracy : 99.85, test loss : 0.4560, test accuracy : 94.56\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0036, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0036, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0033, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0030, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0035, accuracy : 99.86\n",
            "Epoch : 294, training loss : 0.0038, training accuracy : 99.86, test loss : 0.4806, test accuracy : 94.65\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0033, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0025, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0030, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0030, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0034, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0035, accuracy : 99.88\n",
            "Epoch : 295, training loss : 0.0035, training accuracy : 99.88, test loss : 0.4871, test accuracy : 94.49\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0024, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0033, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0038, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0037, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0038, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0037, accuracy : 99.88\n",
            "Epoch : 296, training loss : 0.0037, training accuracy : 99.88, test loss : 0.5040, test accuracy : 94.47\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0067, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0068, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.80\n",
            "Epoch : 297, training loss : 0.0067, training accuracy : 99.80, test loss : 0.4785, test accuracy : 94.33\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.86\n",
            "Epoch : 298, training loss : 0.0046, training accuracy : 99.86, test loss : 0.4454, test accuracy : 94.56\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0037, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 299, training loss : 0.0046, training accuracy : 99.85, test loss : 0.4540, test accuracy : 94.71\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0027, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0033, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0040, accuracy : 99.87\n",
            "Epoch : 300, training loss : 0.0040, training accuracy : 99.87, test loss : 0.4752, test accuracy : 94.46\n"
          ]
        }
      ],
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUQVIKR-X3v6",
        "outputId": "3cf839f8-c49c-4b7a-edfd-e5389a12bc66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.4086923391065177, 95.4940073755378)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CNz1iabSB21",
        "outputId": "b60b0ed2-9325-491d-d705-5955d2ccfcf2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : constant LR\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "outputId": "5b6e1598-cd7c-4b9c-8518-f204fa801fd7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : constant LR\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eiY3bTlWipW",
        "outputId": "63a55e5d-2ed0-443d-f720-e3a15251efd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_loss_list_const = [1.5944857293674293, 0.4869092239677745, 0.3710624326454592, 0.3296906750215101, 0.29884549410039496, 0.27146996971633697, 0.2604337949053382, 0.24417246496532022, 0.23419785655045575, 0.22457996957751147, 0.21173857996457315, 0.20202396321425917, 0.1943116241051414, 0.18745123495052501, 0.1774632519257424, 0.17214223601650902, 0.16552220173886797, 0.15473239550866733, 0.15404348591276948, 0.14213740165398372, 0.13690419029192066, 0.13265049481779578, 0.12643728150322348, 0.1223953123176647, 0.11572830584960256, 0.10872174589453028, 0.10607895330301306, 0.10143619665729645, 0.09846852844464908, 0.09155762590831373, 0.09176684167932689, 0.08320119946962853, 0.07973214024854547, 0.07931727142424037, 0.06983993944351063, 0.07091535234995448, 0.0671700671211713, 0.06470882359558974, 0.061619590856841586, 0.061459206172131346, 0.057121708876198427, 0.053374216585170206, 0.0498615506466044, 0.05073856983490308, 0.04987424655961312, 0.046790412364042994, 0.04711244383266544, 0.04263703334889801, 0.043034394194061555, 0.04126133985860339, 0.03625233271121373, 0.03783284285014904, 0.0379838087573284, 0.03416189216178284, 0.03383261713717981, 0.03370409071676403, 0.033968668481206325, 0.03165312097212274, 0.03038872497441464, 0.02841417055683044, 0.029356409022635036, 0.02889881345480024, 0.028939039615515447, 0.02568263415692672, 0.027656220918051838, 0.026301205131101617, 0.023402816848356156, 0.024891200900357974, 0.022950206206843942, 0.022271349775869285, 0.0250707899006071, 0.022344395426495713, 0.019562920480307198, 0.022191016989943715, 0.02281398802152065, 0.0203801996838415, 0.020550807893653537, 0.02117716848076041, 0.019771566791103416, 0.022764958889409142, 0.017198871523580392, 0.02078044665365311, 0.020109122048629333, 0.01588759385191313, 0.01774437841210012, 0.020397339600326397, 0.01534856222032823, 0.016435430918308794, 0.01711455919842304, 0.01770936636729633, 0.018040290616609177, 0.01564757464108749, 0.015251886521385311, 0.01534154344099917, 0.015427147975282334, 0.01419120683843651, 0.01811075882562171, 0.016642653122089984, 0.01305790530750528, 0.013949767823940267, 0.014656124457392462, 0.013457846312966957, 0.014343252129156173, 0.013850259383991146, 0.012460459422866397, 0.012413865320129272, 0.013889214684304953, 0.01221698466477806, 0.015144580592092777, 0.010421825547786688, 0.01315658220278792, 0.014559990600058128, 0.012665790851758906, 0.01143317199561781, 0.011717887349175301, 0.011158559258571676, 0.012634062059679534, 0.011741106513152792, 0.012515452659958159, 0.011751638104695717, 0.011457235534427129, 0.011438692509360596, 0.012512708038015468, 0.008602326916451698, 0.010682771495822786, 0.011196336384384224, 0.009268063090769713, 0.009998142540840234, 0.012729091897495166, 0.010267185127698763, 0.010566324319321028, 0.010713477439735606, 0.010119398856430022, 0.012654226244056406, 0.009628543746825273, 0.0102562899387879, 0.010169452542220153, 0.011113815507671713, 0.01246779066452764, 0.007518461719419533, 0.007389902458939484, 0.011104748547043264, 0.01135712010794771, 0.008941290669380526, 0.009924649224834431, 0.009838336234147059, 0.007612355080635734, 0.009282812758648675, 0.008423602285464519, 0.009065308554876006, 0.010365516194014629, 0.006115070432431249, 0.006490228478146479, 0.011122480412586441, 0.00872685812082587, 0.008791860558152852, 0.009787340952780856, 0.008418811355189983, 0.007423892029066022, 0.009898100130695444, 0.008828991230531168, 0.007722080036888794, 0.009378696144602659, 0.00829515335842403, 0.006554383687879633, 0.006126663065066873, 0.010910632581322311, 0.007716069375264186, 0.006358072765228986, 0.007697185452836498, 0.008877029283653514, 0.008559577807589543, 0.00707696825589556, 0.00850764569837624, 0.0062317561151193695, 0.0066259507287960455, 0.008618629247509826, 0.007878521594295732, 0.007498281048782551, 0.006598806819186967, 0.006230976371933824, 0.006141664580167378, 0.007923200858393221, 0.008045455975570557, 0.00792612456981016, 0.007601726206049413, 0.007419711852838272, 0.004584900550783639, 0.00685006011318293, 0.008050930358640324, 0.008370204849807444, 0.005507358266772813, 0.006857401374734981, 0.008172955556791895, 0.004244687181667273, 0.008633549227632837, 0.008705449396538356, 0.0067560189635043925, 0.006710546794999935, 0.00550614756518684, 0.0069180619830691474, 0.00835907400430254, 0.005712247417545953, 0.006098340349643973, 0.007154704712266156, 0.0061693518397019, 0.005750073719051822, 0.005536363746094307, 0.006909997050809464, 0.006392437509654407, 0.004530645426771153, 0.00935107147158372, 0.005991038718238898, 0.004413133404240297, 0.007152540924743997, 0.006123794079590131, 0.004265024197029019, 0.007488072243164834, 0.006849384789138319, 0.006199823599835201, 0.004939858833603685, 0.006542191565922548, 0.006134184821023406, 0.006127372573951168, 0.006120713498504846, 0.005705225819374859, 0.0054501006014126396, 0.00476737067947397, 0.005204442173390228, 0.006888031104115721, 0.005438639611901732, 0.005198591940753804, 0.006226397409455738, 0.00398913007948585, 0.006725981141281091, 0.0057350191533619786, 0.004547678196042624, 0.003760278201966155, 0.004862335098237806, 0.0053206211415940214, 0.004511028374038128, 0.006714381331292853, 0.006507630908786355, 0.004150821972775074, 0.005154698122662017, 0.005662354828146746, 0.004606740834664833, 0.004731273036621596, 0.004916682220897782, 0.0048261530498232945, 0.006001339212270287, 0.007263619651073496, 0.0038986454153652154, 0.005036929771996826, 0.005049693082181207, 0.00644224065052205, 0.004578751418381182, 0.004686958738437559, 0.003585059260631736, 0.003101582141557616, 0.007221638428928184, 0.005000114876925785, 0.0051700827908760325, 0.005254589704386773, 0.00406367993565528, 0.0036232792127412505, 0.006171727926178867, 0.005875237415135579, 0.00392443083374783, 0.006561909474143628, 0.0037487958143322068, 0.004443046452993911, 0.005447318903908573, 0.004017829913798502, 0.00472008602112813, 0.0055074588290326305, 0.005279580161203713, 0.0036886348879267314, 0.0035813810519778975, 0.004883374933941502, 0.004846986711028166, 0.004054715485154408, 0.004949977601104352, 0.0046826732979178415, 0.004384392334953691, 0.004187341836835158, 0.004243570542040143, 0.005868256121943751, 0.00557879406272601, 0.0037290168971954394, 0.0024883511193469366, 0.004939023949055762, 0.004748034648033541, 0.0038421762496379727, 0.003486280636854575, 0.0036575751641310756, 0.00670746078328728, 0.004645855993902198, 0.004636629221823842, 0.004005611049817734]\n",
            "train_acc_list_const = [44.03176283748015, 84.49761778718899, 88.61196400211752, 89.87400741132875, 91.01958708311275, 91.79460031762838, 92.2350449973531, 92.80889359449444, 93.03546850185283, 93.37427210164108, 93.80412916887242, 94.0052938062467, 94.38009528851244, 94.6278454208576, 94.88618316569614, 95.01323451561673, 95.1868713605082, 95.66543144520911, 95.57226045526734, 95.91953414505029, 96.01905770248808, 96.16093170989942, 96.35150873478031, 96.54632080465855, 96.74113287453679, 96.86183165696136, 96.96347273689783, 97.10746426680784, 97.21757543673901, 97.32133403917416, 97.3276866066702, 97.63896241397565, 97.72154579142403, 97.7342509264161, 97.88883006881949, 97.87400741132875, 97.96506087877184, 98.09211222869243, 98.24245632609846, 98.17257808364214, 98.28692429857067, 98.34833245103229, 98.44997353096876, 98.49444150344097, 98.49655902593965, 98.54102699841185, 98.55796717840127, 98.67443091582848, 98.63419798835362, 98.74219163578613, 98.87559555320276, 98.77607199576495, 98.78030704076231, 98.9793541556379, 98.94970884065643, 98.9433562731604, 98.90947591318158, 98.98782424563261, 98.96453149814717, 99.08099523557438, 99.05770248808894, 99.05134992059291, 99.0428798305982, 99.1784012705135, 99.13393329804128, 99.1148755955532, 99.20381154049761, 99.18051879301217, 99.27580730545262, 99.23980942297511, 99.1784012705135, 99.25674960296453, 99.3499205929063, 99.26310217046056, 99.2503970354685, 99.32874536791954, 99.28215987294865, 99.27157226045527, 99.29486500794071, 99.24404446797247, 99.42615140285865, 99.2779248279513, 99.31180518793012, 99.46214928533615, 99.40921122286925, 99.36262572789836, 99.49602964531498, 99.43673901535203, 99.42615140285865, 99.40921122286925, 99.41132874536792, 99.47697194282689, 99.49602964531498, 99.49814716781366, 99.47273689782953, 99.5044997353097, 99.39015352038115, 99.45156167284277, 99.57649550026468, 99.55320275277924, 99.5404976177872, 99.5659078877713, 99.53414505029116, 99.54685018528322, 99.58073054526204, 99.5849655902594, 99.49391212281631, 99.58920063525674, 99.51720487030175, 99.66543144520911, 99.5574377977766, 99.56167284277396, 99.56802541026998, 99.60402329274748, 99.62519851773425, 99.6019057702488, 99.5934356802541, 99.62519851773425, 99.60402329274748, 99.61461090524087, 99.61461090524087, 99.63790365272631, 99.6019057702488, 99.70566437268396, 99.66119640021175, 99.64002117522499, 99.69507676019057, 99.68025410269983, 99.58920063525674, 99.6569613552144, 99.62519851773425, 99.6569613552144, 99.66754896770779, 99.54261514028586, 99.69719428268925, 99.63366860772896, 99.67178401270513, 99.60825833774484, 99.6019057702488, 99.75436739015352, 99.75436739015352, 99.59555320275278, 99.62731604023293, 99.6929592376919, 99.65484383271573, 99.66966649020645, 99.7289571201694, 99.71836950767602, 99.69084171519323, 99.67601905770249, 99.66119640021175, 99.79671784012704, 99.78824775013234, 99.62519851773425, 99.71836950767602, 99.72683959767072, 99.68872419269455, 99.73107464266808, 99.73107464266808, 99.67601905770249, 99.70989941768131, 99.73319216516676, 99.68025410269983, 99.7204870301747, 99.78824775013234, 99.80518793012176, 99.64849126521969, 99.73107464266808, 99.79883536262572, 99.74377977766014, 99.69719428268925, 99.69719428268925, 99.77130757014294, 99.71625198517734, 99.7924827951297, 99.784012705135, 99.70354685018528, 99.75860243515088, 99.73319216516676, 99.79036527263102, 99.78189518263632, 99.79036527263102, 99.73530968766543, 99.69719428268925, 99.75013234515616, 99.72472207517205, 99.76071995764956, 99.84330333509793, 99.784012705135, 99.73530968766543, 99.69084171519323, 99.82212811011117, 99.76919004764426, 99.73319216516676, 99.85812599258867, 99.70989941768131, 99.70142932768661, 99.76071995764956, 99.78824775013234, 99.8284806776072, 99.7924827951297, 99.7204870301747, 99.8009528851244, 99.81577554261514, 99.75436739015352, 99.7924827951297, 99.81365802011646, 99.8009528851244, 99.77554261514028, 99.77977766013764, 99.85389094759132, 99.67601905770249, 99.81789306511382, 99.86871360508205, 99.74377977766014, 99.78189518263632, 99.84965590259397, 99.77130757014294, 99.78824775013234, 99.82001058761249, 99.81365802011646, 99.77130757014294, 99.78189518263632, 99.79883536262572, 99.80730545262044, 99.82636315510852, 99.79036527263102, 99.83059820010588, 99.81154049761778, 99.7564849126522, 99.8094229751191, 99.8284806776072, 99.81154049761778, 99.88353626257279, 99.75224986765484, 99.79671784012704, 99.85389094759132, 99.88565378507147, 99.83059820010588, 99.80730545262044, 99.84118581259926, 99.77130757014294, 99.81154049761778, 99.85177342509265, 99.83271572260455, 99.8094229751191, 99.8284806776072, 99.86236103758603, 99.8369507676019, 99.82424563260984, 99.8009528851244, 99.78824775013234, 99.84330333509793, 99.82636315510852, 99.8284806776072, 99.78189518263632, 99.85812599258867, 99.85177342509265, 99.89624139756485, 99.89835892006353, 99.77130757014294, 99.84118581259926, 99.81577554261514, 99.8369507676019, 99.85600847008999, 99.86659608258337, 99.81154049761778, 99.79671784012704, 99.87718369507677, 99.79883536262572, 99.85812599258867, 99.85389094759132, 99.83906829010058, 99.88353626257279, 99.84753838009529, 99.8369507676019, 99.83906829010058, 99.88565378507147, 99.87506617257809, 99.83271572260455, 99.84330333509793, 99.85812599258867, 99.86659608258337, 99.8369507676019, 99.8284806776072, 99.84753838009529, 99.85177342509265, 99.7924827951297, 99.79671784012704, 99.88141874007411, 99.92376919004765, 99.86024351508735, 99.84753838009529, 99.85600847008999, 99.87506617257809, 99.87718369507677, 99.79671784012704, 99.86024351508735, 99.85177342509265, 99.87083112758073]\n",
            "test_loss_list_const = [0.9403148495099124, 0.46971940738605517, 0.34306909633325594, 0.3384028169892582, 0.3164549315823059, 0.31764499748162195, 0.282772644879479, 0.27239831069520876, 0.2788072020618939, 0.2605946859454407, 0.2569786167758353, 0.25151387514436946, 0.23040011815507622, 0.23389688687508597, 0.23323347488893012, 0.22432381940969065, 0.23885389055837603, 0.22965874502837075, 0.23512107754747072, 0.23369459731175618, 0.22183155821745887, 0.2491116562639089, 0.23399925456546686, 0.25359821337841304, 0.23499618255186314, 0.22481991891183106, 0.2324665375966944, 0.23745987874766192, 0.24356824685545528, 0.24952035153503804, 0.23930090873082185, 0.25436064657554325, 0.25367995643732594, 0.26609678615761156, 0.2672480665622096, 0.2746706861893044, 0.25824960023529975, 0.26025373894063863, 0.2791316908072023, 0.26918739248432366, 0.2763673086997633, 0.2997446244436444, 0.2916372886438872, 0.3003798500981693, 0.2806282534501424, 0.2931029381979184, 0.29610180335265457, 0.3087221756942716, 0.32797835285172744, 0.30948590080929445, 0.31205729929292025, 0.3264701991711798, 0.31612680869761345, 0.3249107254029927, 0.3218845636081681, 0.3128845189679779, 0.3158182442261308, 0.3337518404621412, 0.34533743259003935, 0.32409434777447116, 0.3321887091642209, 0.3324712138604738, 0.33405348602864965, 0.3346349073199592, 0.3316200801226146, 0.35435040097446274, 0.3272651647738017, 0.3605334343612377, 0.3315225986252521, 0.36446072681642633, 0.37520757036320135, 0.3792235794586732, 0.3674935271379118, 0.3611337201596767, 0.35125305392213313, 0.3664919300844856, 0.3579704279956572, 0.358177753384499, 0.37499817195074525, 0.3516562903372973, 0.36848902222061275, 0.37098927200570997, 0.37174441270968495, 0.3886868230103716, 0.36849470644751015, 0.3531273581982389, 0.3660184144471571, 0.37421249364520986, 0.3748011891273599, 0.3667490633350669, 0.3643053767691348, 0.37450089440772344, 0.37444961906465535, 0.3669995936729452, 0.37039543743080955, 0.38952702733085437, 0.39265704210208474, 0.3686957350274658, 0.3655954713695774, 0.39540933398529887, 0.37815322328870205, 0.3858970995974161, 0.37387897242682383, 0.37425000457020074, 0.38926156392941874, 0.3786647673787586, 0.37843504400156874, 0.4000659274078869, 0.38383779517721894, 0.38883355182513374, 0.38116612786189746, 0.39886108679952575, 0.4039541971048011, 0.4085461224575399, 0.4040325807553588, 0.40590017215878355, 0.38623136059180196, 0.38693335685221586, 0.41144327609343273, 0.3980357802459313, 0.4007313655798926, 0.40366758222636934, 0.38523547610669745, 0.42097171660804866, 0.4246416387294291, 0.40921079142786126, 0.4052840040850581, 0.4231068941344525, 0.38739459854824576, 0.4194799537094785, 0.41747657055327414, 0.4164134435739149, 0.3939945743985328, 0.393845445202554, 0.4172966992665155, 0.4208380446276244, 0.408174136686641, 0.43686796033608855, 0.3842315211042981, 0.4022488394277353, 0.41533097460427704, 0.4123304054533661, 0.3947430606748836, 0.4344836208585869, 0.42245670334509045, 0.4120090134880122, 0.4084896664774301, 0.4208405370477076, 0.4342218170389898, 0.4342225538966173, 0.414541769489719, 0.42016579197290554, 0.43672865715023934, 0.3970657474470927, 0.4120631425404081, 0.4072342333657777, 0.4361969401840778, 0.40728934426043256, 0.4082725720443562, 0.43000999789721533, 0.4488575828864294, 0.4490402939628956, 0.42708765610358584, 0.4119907574152903, 0.4171651719850214, 0.46127884272559017, 0.424621216852364, 0.42663686249551236, 0.443727373143238, 0.4234333980655042, 0.4163527505554478, 0.41962003758446514, 0.43725314623146666, 0.4135830312939909, 0.4485339165719993, 0.438206387026345, 0.42548432996423513, 0.4139385414218493, 0.422614497990877, 0.44258191624619797, 0.5020544356643679, 0.42181872726217207, 0.45136614169414135, 0.42586187040889817, 0.4617275644605066, 0.42785005136758236, 0.4181436715811929, 0.43456211805745376, 0.456493051905258, 0.4419752404130265, 0.439949865836431, 0.458016514120733, 0.45355927485370023, 0.4266304138993077, 0.4774111132959233, 0.43147677636942733, 0.42984621565533326, 0.4257584124226888, 0.41347907219703, 0.44227188734301165, 0.44509480125270784, 0.42359101505694435, 0.45620652116542937, 0.4532003666402078, 0.4544208156419735, 0.4288948582868804, 0.45772276510365817, 0.4652134206776014, 0.4481767811538542, 0.45044724102926387, 0.45808432965228957, 0.4832801581086481, 0.4330355538119215, 0.450099578350965, 0.44405912482818843, 0.4245691280629413, 0.47918898603130206, 0.4441465862088508, 0.4232571859679678, 0.42079631317699073, 0.46119900758140814, 0.4447093260310152, 0.45832808096619215, 0.4615354106998911, 0.46208279893970955, 0.44396297247651234, 0.44644354116719437, 0.4640259245666219, 0.466397661642701, 0.46452038604811785, 0.4498248620908342, 0.4491576017337103, 0.465446551038208, 0.47035092626716574, 0.4713521593012938, 0.4597488452506927, 0.4512224776633814, 0.48416280633240355, 0.4734927043455708, 0.46729338517887337, 0.46055449181357305, 0.44595291037751617, 0.45059747884606977, 0.4550898942740305, 0.4553583972247354, 0.484807043427638, 0.47273430763972085, 0.45941312947109636, 0.4590342458538419, 0.4751177701332113, 0.47533442642466694, 0.45637929342760175, 0.4544383191900766, 0.4463540438526109, 0.45517611408414427, 0.4642113188975582, 0.44581558580930325, 0.4676268981386195, 0.46861990691874833, 0.47670730448090565, 0.4613685188541079, 0.46828112538502203, 0.47280027833310706, 0.44121663249097764, 0.4606796685479554, 0.5259090004415781, 0.48354477134040175, 0.469667333146265, 0.4692415113266393, 0.45400641179944884, 0.47124824484846756, 0.4692198993460111, 0.44249545135900525, 0.4556816854969571, 0.4647824307538423, 0.47515893393360514, 0.44086019356972445, 0.47874322776481804, 0.48824521492911027, 0.4832109012732319, 0.45331234720138397, 0.46669590011166007, 0.4659509803990231, 0.4592359331253843, 0.4794528412395248, 0.4734935767528619, 0.4946094546649678, 0.47707713397183255, 0.4516702479244593, 0.44164761501893984, 0.48553112554637823, 0.4603754159190929, 0.4560155915005096, 0.48063205852739366, 0.4870615708185177, 0.503982417563926, 0.4785489882890354, 0.44539653152848285, 0.4539821001874539, 0.47518135870204253]\n",
            "test_acc_list_const = [71.6080208973571, 85.50245851259987, 89.67040565457899, 89.58589428395821, 90.46173939766442, 90.5577750460971, 91.50276582667486, 92.0520897357099, 91.96757836508912, 92.47464658881377, 92.6820835894284, 92.88952059004302, 93.56177012907192, 93.4580516287646, 93.66548862937923, 93.80377996312231, 93.34665027658266, 93.71158574062692, 93.5041487400123, 93.57329440688383, 94.06499692685925, 93.33896742470804, 93.72311001843885, 92.94330055316533, 93.68853718500307, 94.18792255685311, 93.96127842655194, 93.91518131530424, 93.82682851874615, 94.03042409342348, 94.1917639827904, 93.77304855562384, 93.9420712968654, 93.68853718500307, 94.08420405654579, 93.77304855562384, 93.91518131530424, 94.14566687154272, 93.93822987092808, 94.14566687154272, 94.06499692685925, 93.73079287031346, 93.8921327596804, 94.07267977873387, 94.01889981561156, 94.22633681622618, 93.86140135218193, 93.90749846342962, 93.97664413030117, 94.07652120467118, 94.03042409342348, 93.97280270436386, 94.21865396435157, 94.07267977873387, 94.09956976029503, 94.21865396435157, 94.30700676090964, 93.88444990780577, 94.06883835279656, 94.30316533497235, 94.23017824216349, 94.19944683466503, 94.05731407498463, 94.36846957590657, 94.26475107559926, 94.04578979717272, 94.31468961278426, 94.13030116779349, 94.3262138905962, 94.06499692685925, 93.98432698217579, 94.34926244622004, 94.3262138905962, 94.10341118623234, 94.35694529809466, 93.99969268592501, 94.21481253841426, 94.41840811309157, 94.13414259373079, 94.23017824216349, 94.39920098340504, 94.10341118623234, 94.0381069452981, 94.36846957590657, 94.44529809465274, 94.29548248309773, 94.39920098340504, 94.2839582052858, 94.34542102028273, 94.51444376152428, 94.34926244622004, 94.1418254456054, 94.45682237246466, 94.5221266133989, 94.41072526121697, 94.45298094652735, 93.9958512599877, 94.24938537185002, 94.57974800245852, 94.1418254456054, 94.18792255685311, 94.34926244622004, 94.48371235402581, 94.39535955746773, 94.39151813153042, 94.39535955746773, 94.41072526121697, 94.1418254456054, 94.24554394591273, 94.39535955746773, 94.49907805777505, 94.3262138905962, 94.22633681622618, 94.34542102028273, 94.42224953902888, 94.27627535341118, 94.46450522433928, 94.1840811309158, 94.1917639827904, 94.50291948371235, 94.09956976029503, 94.27243392747388, 94.38383527965581, 94.40304240934235, 94.10725261216963, 94.36462814996926, 94.52980946527352, 94.36078672403197, 94.50676090964966, 94.1917639827904, 94.25706822372464, 94.31084818684695, 94.43761524277812, 94.36462814996926, 94.42224953902888, 94.21097111247695, 94.38383527965581, 94.18023970497849, 94.21865396435157, 94.5259680393362, 94.53365089121081, 94.21865396435157, 94.56822372464659, 94.21865396435157, 94.31468961278426, 94.41072526121697, 94.50291948371235, 94.35310387215735, 94.51060233558697, 94.26090964966195, 94.3338967424708, 94.4299323909035, 94.40688383527966, 94.66425937307929, 94.41072526121697, 94.39535955746773, 94.23786109403811, 94.44145666871543, 94.30316533497235, 94.3761524277812, 94.04578979717272, 94.36846957590657, 94.3300553165335, 94.44145666871543, 94.64505224339274, 93.99200983405039, 94.53365089121081, 94.52980946527352, 94.3799938537185, 94.38767670559312, 94.54133374308543, 94.36078672403197, 94.44913952059004, 94.28779963122311, 94.48755377996312, 94.51828518746159, 94.4721880762139, 94.45682237246466, 94.39920098340504, 94.51444376152428, 94.10341118623234, 94.61432083589429, 94.41840811309157, 94.21865396435157, 94.46450522433928, 94.44913952059004, 94.4798709280885, 94.61432083589429, 94.22633681622618, 94.63352796558083, 94.51060233558697, 94.44145666871543, 94.5221266133989, 94.50291948371235, 93.93438844499079, 94.4721880762139, 94.48371235402581, 94.63736939151813, 94.5759065765212, 94.61047940995698, 94.39920098340504, 94.56822372464659, 94.21481253841426, 94.59895513214505, 94.32237246465888, 94.59127228027043, 94.25322679778733, 94.4299323909035, 94.54517516902274, 94.3338967424708, 94.40688383527966, 94.08420405654579, 94.5259680393362, 94.59895513214505, 94.6681007990166, 94.61047940995698, 94.44529809465274, 94.48371235402581, 94.48755377996312, 94.67962507682851, 94.36846957590657, 94.09572833435772, 94.35694529809466, 94.43377381684081, 94.55669944683467, 94.61047940995698, 94.36462814996926, 94.6220036877689, 94.69499078057775, 94.39535955746773, 94.47602950215119, 94.43761524277812, 94.3300553165335, 94.68730792870313, 94.56822372464659, 94.45298094652735, 94.61816226183159, 94.40304240934235, 94.48371235402581, 94.62968653964352, 94.54517516902274, 94.51828518746159, 94.60663798401967, 94.55669944683467, 94.50676090964966, 94.52980946527352, 94.59511370620774, 94.46066379840197, 94.40304240934235, 94.49139520590043, 94.4299323909035, 94.34157959434542, 94.6681007990166, 94.58358942839583, 94.61047940995698, 94.34926244622004, 94.57974800245852, 94.50676090964966, 94.71035648432698, 94.70651505838967, 94.5221266133989, 94.3761524277812, 94.50291948371235, 94.59127228027043, 94.52980946527352, 94.41456668715428, 94.53749231714812, 94.29932390903504, 94.5720651505839, 94.68730792870313, 94.61816226183159, 94.55285802089736, 94.6681007990166, 94.70651505838967, 94.59127228027043, 94.40304240934235, 94.69114935464044, 94.68730792870313, 94.5221266133989, 94.45682237246466, 94.51828518746159, 94.57974800245852, 94.61432083589429, 94.6220036877689, 94.34542102028273, 94.53749231714812, 94.40688383527966, 94.49523663183774, 94.42609096496619, 94.6757836508912, 94.42224953902888, 94.43377381684081, 94.56438229870928, 94.65273509526736, 94.49139520590043, 94.4721880762139, 94.3338967424708, 94.56054087277197, 94.70651505838967, 94.45682237246466]\n"
          ]
        }
      ],
      "source": [
        "print(f\"train_loss_list_const = {train_loss_list}\") \n",
        "print(f\"train_acc_list_const = {train_acc_list}\")\n",
        "print(f\"test_loss_list_const = {test_loss_list}\")\n",
        "print(f\"test_acc_list_const = {test_acc_list}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "2d35a857fdeedecb30594b1c7eb95a8c0480700735195f416faf3d51f501baa5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}