{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4964385645a24b1faff977c5b9d5adac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d881792c1564227ac6d2ea305bd5b13",
              "IPY_MODEL_8cb7468137364219917ebaaf5ef85e3b",
              "IPY_MODEL_d1b1ac2f95e94046a308c5fd89baf137"
            ],
            "layout": "IPY_MODEL_1db72ad0f4734e2eb4b31c6f72148c1d"
          }
        },
        "2d881792c1564227ac6d2ea305bd5b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8825f9630bf34fe5b01ba7f4ef9c300b",
            "placeholder": "​",
            "style": "IPY_MODEL_176902798b4d485db4e5ddd403ab455d",
            "value": "100%"
          }
        },
        "8cb7468137364219917ebaaf5ef85e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28aeec1fe3b547bd958f522067cd7541",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9361b5f4c3d844fbb00603f4243ceb63",
            "value": 182040794
          }
        },
        "d1b1ac2f95e94046a308c5fd89baf137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa0724452e964f99a86ba29fcb574b1c",
            "placeholder": "​",
            "style": "IPY_MODEL_b922a3d4528749d28d024d97529f616d",
            "value": " 182040794/182040794 [00:12&lt;00:00, 18968202.92it/s]"
          }
        },
        "1db72ad0f4734e2eb4b31c6f72148c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8825f9630bf34fe5b01ba7f4ef9c300b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "176902798b4d485db4e5ddd403ab455d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28aeec1fe3b547bd958f522067cd7541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9361b5f4c3d844fbb00603f4243ceb63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa0724452e964f99a86ba29fcb574b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b922a3d4528749d28d024d97529f616d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43c89db4bd18434e98c69149b8aecc4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50e118520b884313a5dc9666ae3ba9b3",
              "IPY_MODEL_eef5cb5938b44439b894f0fad2441c1c",
              "IPY_MODEL_bc0d013e77fd4d4994f54d4c1968e268"
            ],
            "layout": "IPY_MODEL_4e6bc8b3b6814c759486734bc42789b3"
          }
        },
        "50e118520b884313a5dc9666ae3ba9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ab950774fd94851b09a1664145f9876",
            "placeholder": "​",
            "style": "IPY_MODEL_8d237a8ae1004b18adefaf7d25985be5",
            "value": "100%"
          }
        },
        "eef5cb5938b44439b894f0fad2441c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8b03617e35346f59e7c3a01a1856e4e",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_766c5aec612044e9bde15b85023bfaf9",
            "value": 64275384
          }
        },
        "bc0d013e77fd4d4994f54d4c1968e268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bbae6f096414ca5aece57e0929792a6",
            "placeholder": "​",
            "style": "IPY_MODEL_c0413fdca50048bca532a9ebbe5cd1bf",
            "value": " 64275384/64275384 [00:07&lt;00:00, 18666255.78it/s]"
          }
        },
        "4e6bc8b3b6814c759486734bc42789b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ab950774fd94851b09a1664145f9876": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d237a8ae1004b18adefaf7d25985be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8b03617e35346f59e7c3a01a1856e4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "766c5aec612044e9bde15b85023bfaf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3bbae6f096414ca5aece57e0929792a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0413fdca50048bca532a9ebbe5cd1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "4964385645a24b1faff977c5b9d5adac",
            "2d881792c1564227ac6d2ea305bd5b13",
            "8cb7468137364219917ebaaf5ef85e3b",
            "d1b1ac2f95e94046a308c5fd89baf137",
            "1db72ad0f4734e2eb4b31c6f72148c1d",
            "8825f9630bf34fe5b01ba7f4ef9c300b",
            "176902798b4d485db4e5ddd403ab455d",
            "28aeec1fe3b547bd958f522067cd7541",
            "9361b5f4c3d844fbb00603f4243ceb63",
            "aa0724452e964f99a86ba29fcb574b1c",
            "b922a3d4528749d28d024d97529f616d",
            "43c89db4bd18434e98c69149b8aecc4e",
            "50e118520b884313a5dc9666ae3ba9b3",
            "eef5cb5938b44439b894f0fad2441c1c",
            "bc0d013e77fd4d4994f54d4c1968e268",
            "4e6bc8b3b6814c759486734bc42789b3",
            "4ab950774fd94851b09a1664145f9876",
            "8d237a8ae1004b18adefaf7d25985be5",
            "d8b03617e35346f59e7c3a01a1856e4e",
            "766c5aec612044e9bde15b85023bfaf9",
            "3bbae6f096414ca5aece57e0929792a6",
            "c0413fdca50048bca532a9ebbe5cd1bf"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "c399e72f-3088-4427-81a7-dfa22b0c029a"
      },
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4964385645a24b1faff977c5b9d5adac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43c89db4bd18434e98c69149b8aecc4e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "69527763-6469-404a-96cf-6ff9bec1d2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "03ced685-fe5a-4ce2-d2f5-7505086fe4dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "08a28cf5-7b8e-445b-d4a6-b9bdac604ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArgupDVRwB8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750c31a0-d085-4458-ffa7-a41ef7ea8a26"
      },
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3450, accuracy : 16.69\n",
            "iteration : 100, loss : 2.2695, accuracy : 19.49\n",
            "iteration : 150, loss : 2.1872, accuracy : 22.55\n",
            "iteration : 200, loss : 2.0258, accuracy : 28.01\n",
            "iteration : 250, loss : 1.8504, accuracy : 34.45\n",
            "iteration : 300, loss : 1.6890, accuracy : 40.47\n",
            "iteration : 350, loss : 1.5511, accuracy : 45.71\n",
            "Epoch :   1, training loss : 1.5044, training accuracy : 47.39, test loss : 0.7522, test accuracy : 75.81\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.5928, accuracy : 80.27\n",
            "iteration : 100, loss : 0.5669, accuracy : 81.55\n",
            "iteration : 150, loss : 0.5442, accuracy : 82.55\n",
            "iteration : 200, loss : 0.5317, accuracy : 83.05\n",
            "iteration : 250, loss : 0.5165, accuracy : 83.46\n",
            "iteration : 300, loss : 0.5028, accuracy : 84.00\n",
            "iteration : 350, loss : 0.4965, accuracy : 84.21\n",
            "Epoch :   2, training loss : 0.4921, training accuracy : 84.39, test loss : 0.4192, test accuracy : 86.97\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.3989, accuracy : 87.34\n",
            "iteration : 100, loss : 0.4083, accuracy : 87.34\n",
            "iteration : 150, loss : 0.3962, accuracy : 87.71\n",
            "iteration : 200, loss : 0.3895, accuracy : 87.89\n",
            "iteration : 250, loss : 0.3847, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3831, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3794, accuracy : 88.28\n",
            "Epoch :   3, training loss : 0.3783, training accuracy : 88.33, test loss : 0.3968, test accuracy : 87.58\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3446, accuracy : 89.33\n",
            "iteration : 100, loss : 0.3432, accuracy : 89.54\n",
            "iteration : 150, loss : 0.3376, accuracy : 89.58\n",
            "iteration : 200, loss : 0.3417, accuracy : 89.41\n",
            "iteration : 250, loss : 0.3389, accuracy : 89.53\n",
            "iteration : 300, loss : 0.3369, accuracy : 89.55\n",
            "iteration : 350, loss : 0.3342, accuracy : 89.65\n",
            "Epoch :   4, training loss : 0.3336, training accuracy : 89.66, test loss : 0.3568, test accuracy : 89.28\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.2865, accuracy : 91.12\n",
            "iteration : 100, loss : 0.2914, accuracy : 91.11\n",
            "iteration : 150, loss : 0.2981, accuracy : 90.82\n",
            "iteration : 200, loss : 0.3041, accuracy : 90.71\n",
            "iteration : 250, loss : 0.3051, accuracy : 90.74\n",
            "iteration : 300, loss : 0.3018, accuracy : 90.83\n",
            "iteration : 350, loss : 0.3010, accuracy : 90.89\n",
            "Epoch :   5, training loss : 0.3005, training accuracy : 90.94, test loss : 0.3251, test accuracy : 90.12\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.2799, accuracy : 92.05\n",
            "iteration : 100, loss : 0.2811, accuracy : 91.74\n",
            "iteration : 150, loss : 0.2823, accuracy : 91.59\n",
            "iteration : 200, loss : 0.2800, accuracy : 91.58\n",
            "iteration : 250, loss : 0.2800, accuracy : 91.55\n",
            "iteration : 300, loss : 0.2775, accuracy : 91.62\n",
            "iteration : 350, loss : 0.2746, accuracy : 91.73\n",
            "Epoch :   6, training loss : 0.2731, training accuracy : 91.76, test loss : 0.3064, test accuracy : 91.05\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.2495, accuracy : 92.53\n",
            "iteration : 100, loss : 0.2633, accuracy : 91.99\n",
            "iteration : 150, loss : 0.2638, accuracy : 92.07\n",
            "iteration : 200, loss : 0.2628, accuracy : 92.17\n",
            "iteration : 250, loss : 0.2600, accuracy : 92.26\n",
            "iteration : 300, loss : 0.2600, accuracy : 92.22\n",
            "iteration : 350, loss : 0.2597, accuracy : 92.29\n",
            "Epoch :   7, training loss : 0.2593, training accuracy : 92.30, test loss : 0.2780, test accuracy : 91.73\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.2404, accuracy : 93.11\n",
            "iteration : 100, loss : 0.2487, accuracy : 92.79\n",
            "iteration : 150, loss : 0.2486, accuracy : 92.82\n",
            "iteration : 200, loss : 0.2468, accuracy : 92.81\n",
            "iteration : 250, loss : 0.2482, accuracy : 92.80\n",
            "iteration : 300, loss : 0.2526, accuracy : 92.66\n",
            "iteration : 350, loss : 0.2491, accuracy : 92.68\n",
            "Epoch :   8, training loss : 0.2475, training accuracy : 92.70, test loss : 0.2646, test accuracy : 92.37\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.2335, accuracy : 93.30\n",
            "iteration : 100, loss : 0.2277, accuracy : 93.45\n",
            "iteration : 150, loss : 0.2329, accuracy : 93.16\n",
            "iteration : 200, loss : 0.2362, accuracy : 93.09\n",
            "iteration : 250, loss : 0.2344, accuracy : 93.11\n",
            "iteration : 300, loss : 0.2352, accuracy : 93.12\n",
            "iteration : 350, loss : 0.2342, accuracy : 93.10\n",
            "Epoch :   9, training loss : 0.2334, training accuracy : 93.14, test loss : 0.2640, test accuracy : 92.39\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2226, accuracy : 93.48\n",
            "iteration : 100, loss : 0.2208, accuracy : 93.37\n",
            "iteration : 150, loss : 0.2205, accuracy : 93.43\n",
            "iteration : 200, loss : 0.2222, accuracy : 93.33\n",
            "iteration : 250, loss : 0.2228, accuracy : 93.27\n",
            "iteration : 300, loss : 0.2225, accuracy : 93.27\n",
            "iteration : 350, loss : 0.2259, accuracy : 93.26\n",
            "Epoch :  10, training loss : 0.2255, training accuracy : 93.28, test loss : 0.2669, test accuracy : 92.24\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.1981, accuracy : 94.23\n",
            "iteration : 100, loss : 0.2104, accuracy : 93.98\n",
            "iteration : 150, loss : 0.2091, accuracy : 93.92\n",
            "iteration : 200, loss : 0.2097, accuracy : 93.92\n",
            "iteration : 250, loss : 0.2141, accuracy : 93.82\n",
            "iteration : 300, loss : 0.2132, accuracy : 93.82\n",
            "iteration : 350, loss : 0.2127, accuracy : 93.84\n",
            "Epoch :  11, training loss : 0.2125, training accuracy : 93.85, test loss : 0.2610, test accuracy : 92.37\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.1958, accuracy : 94.09\n",
            "iteration : 100, loss : 0.2004, accuracy : 93.98\n",
            "iteration : 150, loss : 0.2103, accuracy : 93.72\n",
            "iteration : 200, loss : 0.2090, accuracy : 93.89\n",
            "iteration : 250, loss : 0.2064, accuracy : 93.95\n",
            "iteration : 300, loss : 0.2051, accuracy : 93.98\n",
            "iteration : 350, loss : 0.2031, accuracy : 94.03\n",
            "Epoch :  12, training loss : 0.2046, training accuracy : 93.98, test loss : 0.2513, test accuracy : 92.98\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.1977, accuracy : 94.33\n",
            "iteration : 100, loss : 0.2034, accuracy : 94.10\n",
            "iteration : 150, loss : 0.2036, accuracy : 94.17\n",
            "iteration : 200, loss : 0.2012, accuracy : 94.22\n",
            "iteration : 250, loss : 0.1981, accuracy : 94.33\n",
            "iteration : 300, loss : 0.1970, accuracy : 94.35\n",
            "iteration : 350, loss : 0.1950, accuracy : 94.38\n",
            "Epoch :  13, training loss : 0.1935, training accuracy : 94.42, test loss : 0.2485, test accuracy : 93.17\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.1900, accuracy : 94.45\n",
            "iteration : 100, loss : 0.1880, accuracy : 94.65\n",
            "iteration : 150, loss : 0.1860, accuracy : 94.70\n",
            "iteration : 200, loss : 0.1861, accuracy : 94.66\n",
            "iteration : 250, loss : 0.1871, accuracy : 94.62\n",
            "iteration : 300, loss : 0.1882, accuracy : 94.57\n",
            "iteration : 350, loss : 0.1883, accuracy : 94.57\n",
            "Epoch :  14, training loss : 0.1889, training accuracy : 94.57, test loss : 0.2454, test accuracy : 93.05\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.1651, accuracy : 95.25\n",
            "iteration : 100, loss : 0.1688, accuracy : 95.09\n",
            "iteration : 150, loss : 0.1684, accuracy : 95.13\n",
            "iteration : 200, loss : 0.1760, accuracy : 94.89\n",
            "iteration : 250, loss : 0.1762, accuracy : 94.83\n",
            "iteration : 300, loss : 0.1780, accuracy : 94.83\n",
            "iteration : 350, loss : 0.1785, accuracy : 94.86\n",
            "Epoch :  15, training loss : 0.1790, training accuracy : 94.83, test loss : 0.2352, test accuracy : 93.42\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.1742, accuracy : 95.05\n",
            "iteration : 100, loss : 0.1745, accuracy : 95.08\n",
            "iteration : 150, loss : 0.1727, accuracy : 95.06\n",
            "iteration : 200, loss : 0.1712, accuracy : 95.08\n",
            "iteration : 250, loss : 0.1717, accuracy : 95.09\n",
            "iteration : 300, loss : 0.1716, accuracy : 95.10\n",
            "iteration : 350, loss : 0.1731, accuracy : 95.07\n",
            "Epoch :  16, training loss : 0.1731, training accuracy : 95.06, test loss : 0.2371, test accuracy : 93.38\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.1758, accuracy : 94.78\n",
            "iteration : 100, loss : 0.1689, accuracy : 95.11\n",
            "iteration : 150, loss : 0.1638, accuracy : 95.20\n",
            "iteration : 200, loss : 0.1644, accuracy : 95.19\n",
            "iteration : 250, loss : 0.1640, accuracy : 95.19\n",
            "iteration : 300, loss : 0.1647, accuracy : 95.16\n",
            "iteration : 350, loss : 0.1655, accuracy : 95.15\n",
            "Epoch :  17, training loss : 0.1656, training accuracy : 95.14, test loss : 0.2406, test accuracy : 93.32\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.1523, accuracy : 95.89\n",
            "iteration : 100, loss : 0.1612, accuracy : 95.63\n",
            "iteration : 150, loss : 0.1560, accuracy : 95.66\n",
            "iteration : 200, loss : 0.1548, accuracy : 95.63\n",
            "iteration : 250, loss : 0.1580, accuracy : 95.47\n",
            "iteration : 300, loss : 0.1587, accuracy : 95.42\n",
            "iteration : 350, loss : 0.1587, accuracy : 95.41\n",
            "Epoch :  18, training loss : 0.1594, training accuracy : 95.38, test loss : 0.2463, test accuracy : 93.03\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.1493, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1458, accuracy : 96.01\n",
            "iteration : 150, loss : 0.1473, accuracy : 95.94\n",
            "iteration : 200, loss : 0.1472, accuracy : 95.88\n",
            "iteration : 250, loss : 0.1479, accuracy : 95.84\n",
            "iteration : 300, loss : 0.1505, accuracy : 95.73\n",
            "iteration : 350, loss : 0.1539, accuracy : 95.63\n",
            "Epoch :  19, training loss : 0.1541, training accuracy : 95.61, test loss : 0.2287, test accuracy : 93.71\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.1258, accuracy : 96.44\n",
            "iteration : 100, loss : 0.1357, accuracy : 96.13\n",
            "iteration : 150, loss : 0.1369, accuracy : 96.08\n",
            "iteration : 200, loss : 0.1358, accuracy : 96.09\n",
            "iteration : 250, loss : 0.1382, accuracy : 95.98\n",
            "iteration : 300, loss : 0.1411, accuracy : 95.88\n",
            "iteration : 350, loss : 0.1434, accuracy : 95.88\n",
            "Epoch :  20, training loss : 0.1441, training accuracy : 95.87, test loss : 0.2308, test accuracy : 93.73\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.1387, accuracy : 96.17\n",
            "iteration : 100, loss : 0.1377, accuracy : 96.09\n",
            "iteration : 150, loss : 0.1353, accuracy : 96.23\n",
            "iteration : 200, loss : 0.1371, accuracy : 96.07\n",
            "iteration : 250, loss : 0.1362, accuracy : 96.08\n",
            "iteration : 300, loss : 0.1386, accuracy : 96.01\n",
            "iteration : 350, loss : 0.1385, accuracy : 95.96\n",
            "Epoch :  21, training loss : 0.1398, training accuracy : 95.94, test loss : 0.2321, test accuracy : 93.69\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.1249, accuracy : 96.19\n",
            "iteration : 100, loss : 0.1293, accuracy : 96.19\n",
            "iteration : 150, loss : 0.1315, accuracy : 96.14\n",
            "iteration : 200, loss : 0.1328, accuracy : 96.07\n",
            "iteration : 250, loss : 0.1312, accuracy : 96.15\n",
            "iteration : 300, loss : 0.1325, accuracy : 96.16\n",
            "iteration : 350, loss : 0.1319, accuracy : 96.24\n",
            "Epoch :  22, training loss : 0.1325, training accuracy : 96.24, test loss : 0.2338, test accuracy : 93.78\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.1333, accuracy : 96.16\n",
            "iteration : 100, loss : 0.1238, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1231, accuracy : 96.46\n",
            "iteration : 200, loss : 0.1237, accuracy : 96.43\n",
            "iteration : 250, loss : 0.1237, accuracy : 96.42\n",
            "iteration : 300, loss : 0.1265, accuracy : 96.35\n",
            "iteration : 350, loss : 0.1267, accuracy : 96.34\n",
            "Epoch :  23, training loss : 0.1264, training accuracy : 96.33, test loss : 0.2326, test accuracy : 93.74\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.1231, accuracy : 96.86\n",
            "iteration : 100, loss : 0.1211, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1171, accuracy : 96.68\n",
            "iteration : 200, loss : 0.1165, accuracy : 96.66\n",
            "iteration : 250, loss : 0.1177, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1158, accuracy : 96.66\n",
            "iteration : 350, loss : 0.1184, accuracy : 96.58\n",
            "Epoch :  24, training loss : 0.1213, training accuracy : 96.52, test loss : 0.2492, test accuracy : 93.26\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.0996, accuracy : 96.95\n",
            "iteration : 100, loss : 0.1057, accuracy : 96.87\n",
            "iteration : 150, loss : 0.1097, accuracy : 96.75\n",
            "iteration : 200, loss : 0.1135, accuracy : 96.65\n",
            "iteration : 250, loss : 0.1128, accuracy : 96.65\n",
            "iteration : 300, loss : 0.1167, accuracy : 96.57\n",
            "iteration : 350, loss : 0.1170, accuracy : 96.61\n",
            "Epoch :  25, training loss : 0.1179, training accuracy : 96.58, test loss : 0.2467, test accuracy : 93.30\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.1028, accuracy : 97.14\n",
            "iteration : 100, loss : 0.1014, accuracy : 97.01\n",
            "iteration : 150, loss : 0.1055, accuracy : 96.86\n",
            "iteration : 200, loss : 0.1068, accuracy : 96.89\n",
            "iteration : 250, loss : 0.1080, accuracy : 96.86\n",
            "iteration : 300, loss : 0.1098, accuracy : 96.85\n",
            "iteration : 350, loss : 0.1123, accuracy : 96.79\n",
            "Epoch :  26, training loss : 0.1115, training accuracy : 96.80, test loss : 0.2332, test accuracy : 93.93\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.0842, accuracy : 97.56\n",
            "iteration : 100, loss : 0.0989, accuracy : 97.06\n",
            "iteration : 150, loss : 0.1015, accuracy : 97.05\n",
            "iteration : 200, loss : 0.1054, accuracy : 96.96\n",
            "iteration : 250, loss : 0.1056, accuracy : 96.98\n",
            "iteration : 300, loss : 0.1046, accuracy : 97.04\n",
            "iteration : 350, loss : 0.1046, accuracy : 96.99\n",
            "Epoch :  27, training loss : 0.1049, training accuracy : 96.98, test loss : 0.2333, test accuracy : 94.07\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.0942, accuracy : 97.11\n",
            "iteration : 100, loss : 0.0935, accuracy : 97.27\n",
            "iteration : 150, loss : 0.0975, accuracy : 97.27\n",
            "iteration : 200, loss : 0.0970, accuracy : 97.20\n",
            "iteration : 250, loss : 0.0996, accuracy : 97.17\n",
            "iteration : 300, loss : 0.1006, accuracy : 97.15\n",
            "iteration : 350, loss : 0.1012, accuracy : 97.12\n",
            "Epoch :  28, training loss : 0.1011, training accuracy : 97.12, test loss : 0.2326, test accuracy : 94.12\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.0853, accuracy : 97.73\n",
            "iteration : 100, loss : 0.0902, accuracy : 97.48\n",
            "iteration : 150, loss : 0.0979, accuracy : 97.27\n",
            "iteration : 200, loss : 0.0998, accuracy : 97.22\n",
            "iteration : 250, loss : 0.0997, accuracy : 97.19\n",
            "iteration : 300, loss : 0.0997, accuracy : 97.17\n",
            "iteration : 350, loss : 0.0991, accuracy : 97.17\n",
            "Epoch :  29, training loss : 0.0986, training accuracy : 97.20, test loss : 0.2575, test accuracy : 93.60\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.0840, accuracy : 97.42\n",
            "iteration : 100, loss : 0.0861, accuracy : 97.48\n",
            "iteration : 150, loss : 0.0847, accuracy : 97.51\n",
            "iteration : 200, loss : 0.0882, accuracy : 97.42\n",
            "iteration : 250, loss : 0.0904, accuracy : 97.32\n",
            "iteration : 300, loss : 0.0917, accuracy : 97.28\n",
            "iteration : 350, loss : 0.0942, accuracy : 97.21\n",
            "Epoch :  30, training loss : 0.0933, training accuracy : 97.23, test loss : 0.2394, test accuracy : 94.04\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.0767, accuracy : 97.81\n",
            "iteration : 100, loss : 0.0716, accuracy : 98.03\n",
            "iteration : 150, loss : 0.0690, accuracy : 98.08\n",
            "iteration : 200, loss : 0.0639, accuracy : 98.23\n",
            "iteration : 250, loss : 0.0639, accuracy : 98.24\n",
            "iteration : 300, loss : 0.0632, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0621, accuracy : 98.33\n",
            "Epoch :  31, training loss : 0.0616, training accuracy : 98.33, test loss : 0.2186, test accuracy : 94.88\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.0562, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0527, accuracy : 98.61\n",
            "iteration : 150, loss : 0.0493, accuracy : 98.71\n",
            "iteration : 200, loss : 0.0481, accuracy : 98.74\n",
            "iteration : 250, loss : 0.0509, accuracy : 98.69\n",
            "iteration : 300, loss : 0.0493, accuracy : 98.71\n",
            "iteration : 350, loss : 0.0501, accuracy : 98.71\n",
            "Epoch :  32, training loss : 0.0501, training accuracy : 98.71, test loss : 0.2249, test accuracy : 94.84\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.0467, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0429, accuracy : 98.95\n",
            "iteration : 150, loss : 0.0418, accuracy : 98.92\n",
            "iteration : 200, loss : 0.0439, accuracy : 98.89\n",
            "iteration : 250, loss : 0.0452, accuracy : 98.83\n",
            "iteration : 300, loss : 0.0456, accuracy : 98.82\n",
            "iteration : 350, loss : 0.0455, accuracy : 98.82\n",
            "Epoch :  33, training loss : 0.0457, training accuracy : 98.82, test loss : 0.2260, test accuracy : 94.81\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.0430, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0437, accuracy : 98.83\n",
            "iteration : 150, loss : 0.0432, accuracy : 98.89\n",
            "iteration : 200, loss : 0.0432, accuracy : 98.92\n",
            "iteration : 250, loss : 0.0413, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0418, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0421, accuracy : 98.94\n",
            "Epoch :  34, training loss : 0.0421, training accuracy : 98.95, test loss : 0.2331, test accuracy : 94.87\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.0370, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0370, accuracy : 98.98\n",
            "iteration : 150, loss : 0.0367, accuracy : 99.06\n",
            "iteration : 200, loss : 0.0364, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0372, accuracy : 99.04\n",
            "iteration : 300, loss : 0.0371, accuracy : 99.05\n",
            "iteration : 350, loss : 0.0379, accuracy : 99.03\n",
            "Epoch :  35, training loss : 0.0376, training accuracy : 99.04, test loss : 0.2381, test accuracy : 94.92\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.0359, accuracy : 98.98\n",
            "iteration : 100, loss : 0.0329, accuracy : 99.10\n",
            "iteration : 150, loss : 0.0346, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0344, accuracy : 99.11\n",
            "iteration : 250, loss : 0.0350, accuracy : 99.09\n",
            "iteration : 300, loss : 0.0355, accuracy : 99.07\n",
            "iteration : 350, loss : 0.0357, accuracy : 99.05\n",
            "Epoch :  36, training loss : 0.0353, training accuracy : 99.06, test loss : 0.2410, test accuracy : 94.89\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.0412, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0417, accuracy : 98.98\n",
            "iteration : 150, loss : 0.0394, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0368, accuracy : 99.10\n",
            "iteration : 250, loss : 0.0353, accuracy : 99.11\n",
            "iteration : 300, loss : 0.0341, accuracy : 99.11\n",
            "iteration : 350, loss : 0.0336, accuracy : 99.14\n",
            "Epoch :  37, training loss : 0.0340, training accuracy : 99.13, test loss : 0.2527, test accuracy : 94.87\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.0296, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0287, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0284, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0306, accuracy : 99.22\n",
            "iteration : 250, loss : 0.0297, accuracy : 99.24\n",
            "iteration : 300, loss : 0.0306, accuracy : 99.22\n",
            "iteration : 350, loss : 0.0314, accuracy : 99.21\n",
            "Epoch :  38, training loss : 0.0312, training accuracy : 99.22, test loss : 0.2539, test accuracy : 94.84\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.0309, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0294, accuracy : 99.22\n",
            "iteration : 150, loss : 0.0305, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0323, accuracy : 99.15\n",
            "iteration : 250, loss : 0.0323, accuracy : 99.15\n",
            "iteration : 300, loss : 0.0325, accuracy : 99.15\n",
            "iteration : 350, loss : 0.0312, accuracy : 99.17\n",
            "Epoch :  39, training loss : 0.0309, training accuracy : 99.17, test loss : 0.2590, test accuracy : 94.71\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.0318, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0288, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0265, accuracy : 99.28\n",
            "iteration : 200, loss : 0.0280, accuracy : 99.26\n",
            "iteration : 250, loss : 0.0291, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0287, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0292, accuracy : 99.25\n",
            "Epoch :  40, training loss : 0.0291, training accuracy : 99.25, test loss : 0.2654, test accuracy : 94.56\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.0263, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0286, accuracy : 99.32\n",
            "iteration : 150, loss : 0.0265, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0268, accuracy : 99.31\n",
            "iteration : 250, loss : 0.0268, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0274, accuracy : 99.31\n",
            "iteration : 350, loss : 0.0270, accuracy : 99.33\n",
            "Epoch :  41, training loss : 0.0269, training accuracy : 99.32, test loss : 0.2675, test accuracy : 94.76\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.0239, accuracy : 99.28\n",
            "iteration : 100, loss : 0.0259, accuracy : 99.24\n",
            "iteration : 150, loss : 0.0249, accuracy : 99.28\n",
            "iteration : 200, loss : 0.0254, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0248, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0252, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0246, accuracy : 99.35\n",
            "Epoch :  42, training loss : 0.0248, training accuracy : 99.34, test loss : 0.2758, test accuracy : 94.74\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.0228, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0241, accuracy : 99.40\n",
            "iteration : 150, loss : 0.0239, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0220, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0219, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0217, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0228, accuracy : 99.40\n",
            "Epoch :  43, training loss : 0.0233, training accuracy : 99.38, test loss : 0.2835, test accuracy : 94.65\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.0177, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0201, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0196, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0210, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0220, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0223, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0223, accuracy : 99.40\n",
            "Epoch :  44, training loss : 0.0220, training accuracy : 99.40, test loss : 0.2891, test accuracy : 94.65\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.0210, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0232, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0219, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0206, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0202, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0201, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0210, accuracy : 99.45\n",
            "Epoch :  45, training loss : 0.0211, training accuracy : 99.45, test loss : 0.2938, test accuracy : 94.69\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.0247, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0201, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0197, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0199, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0199, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0201, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0202, accuracy : 99.46\n",
            "Epoch :  46, training loss : 0.0202, training accuracy : 99.46, test loss : 0.3016, test accuracy : 94.73\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.0194, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0190, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0193, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0201, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0207, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0204, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0205, accuracy : 99.44\n",
            "Epoch :  47, training loss : 0.0204, training accuracy : 99.44, test loss : 0.2968, test accuracy : 94.68\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.0151, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0159, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0175, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0175, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0174, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0178, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0180, accuracy : 99.50\n",
            "Epoch :  48, training loss : 0.0181, training accuracy : 99.50, test loss : 0.3060, test accuracy : 94.74\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.0142, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0149, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0161, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0166, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0170, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0173, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0170, accuracy : 99.52\n",
            "Epoch :  49, training loss : 0.0169, training accuracy : 99.52, test loss : 0.3124, test accuracy : 94.62\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.0170, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0183, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0169, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0174, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0174, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0168, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0165, accuracy : 99.55\n",
            "Epoch :  50, training loss : 0.0163, training accuracy : 99.55, test loss : 0.3139, test accuracy : 94.65\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.0180, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0168, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0159, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0162, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0166, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0160, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0156, accuracy : 99.58\n",
            "Epoch :  51, training loss : 0.0157, training accuracy : 99.58, test loss : 0.3220, test accuracy : 94.55\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.0178, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0170, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0167, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0162, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0160, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0163, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0158, accuracy : 99.57\n",
            "Epoch :  52, training loss : 0.0158, training accuracy : 99.56, test loss : 0.3318, test accuracy : 94.54\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0129, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0128, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0136, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0134, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0144, accuracy : 99.57\n",
            "Epoch :  53, training loss : 0.0150, training accuracy : 99.56, test loss : 0.3355, test accuracy : 94.63\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.0136, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0157, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0147, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0141, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0142, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0141, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0140, accuracy : 99.62\n",
            "Epoch :  54, training loss : 0.0139, training accuracy : 99.62, test loss : 0.3384, test accuracy : 94.62\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.0148, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0145, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0135, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0130, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0125, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0128, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0125, accuracy : 99.66\n",
            "Epoch :  55, training loss : 0.0125, training accuracy : 99.66, test loss : 0.3436, test accuracy : 94.61\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.0133, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0120, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0103, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0097, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0110, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0111, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0116, accuracy : 99.68\n",
            "Epoch :  56, training loss : 0.0115, training accuracy : 99.68, test loss : 0.3455, test accuracy : 94.52\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0111, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0120, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0121, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0121, accuracy : 99.62\n",
            "Epoch :  57, training loss : 0.0121, training accuracy : 99.62, test loss : 0.3456, test accuracy : 94.64\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0118, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0113, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0111, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0115, accuracy : 99.64\n",
            "Epoch :  58, training loss : 0.0117, training accuracy : 99.63, test loss : 0.3532, test accuracy : 94.56\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.0098, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0105, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0108, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0103, accuracy : 99.71\n",
            "Epoch :  59, training loss : 0.0106, training accuracy : 99.70, test loss : 0.3543, test accuracy : 94.74\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.0109, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0104, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0091, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0097, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0104, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0105, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0105, accuracy : 99.73\n",
            "Epoch :  60, training loss : 0.0106, training accuracy : 99.72, test loss : 0.3555, test accuracy : 94.61\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.0111, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0092, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0093, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0093, accuracy : 99.73\n",
            "Epoch :  61, training loss : 0.0091, training accuracy : 99.74, test loss : 0.3601, test accuracy : 94.59\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0068, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0081, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0078, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0080, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0084, accuracy : 99.75\n",
            "Epoch :  62, training loss : 0.0082, training accuracy : 99.76, test loss : 0.3536, test accuracy : 94.76\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0087, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0084, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0081, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0080, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.77\n",
            "Epoch :  63, training loss : 0.0079, training accuracy : 99.77, test loss : 0.3636, test accuracy : 94.76\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.0106, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0084, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0084, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0078, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0076, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.80\n",
            "Epoch :  64, training loss : 0.0074, training accuracy : 99.80, test loss : 0.3636, test accuracy : 94.66\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0072, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0079, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0080, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0083, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0079, accuracy : 99.77\n",
            "Epoch :  65, training loss : 0.0080, training accuracy : 99.76, test loss : 0.3567, test accuracy : 94.73\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0068, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0068, accuracy : 99.84\n",
            "Epoch :  66, training loss : 0.0068, training accuracy : 99.84, test loss : 0.3549, test accuracy : 94.68\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.0077, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0069, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0071, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0072, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0073, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0070, accuracy : 99.80\n",
            "Epoch :  67, training loss : 0.0070, training accuracy : 99.80, test loss : 0.3703, test accuracy : 94.53\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0072, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0071, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0069, accuracy : 99.82\n",
            "Epoch :  68, training loss : 0.0070, training accuracy : 99.81, test loss : 0.3622, test accuracy : 94.65\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0071, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0073, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0070, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0068, accuracy : 99.81\n",
            "Epoch :  69, training loss : 0.0069, training accuracy : 99.81, test loss : 0.3605, test accuracy : 94.66\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.0077, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0070, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0063, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0065, accuracy : 99.83\n",
            "Epoch :  70, training loss : 0.0064, training accuracy : 99.83, test loss : 0.3578, test accuracy : 94.70\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0067, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0067, accuracy : 99.79\n",
            "Epoch :  71, training loss : 0.0066, training accuracy : 99.80, test loss : 0.3677, test accuracy : 94.63\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.0062, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0067, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0064, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0063, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.83\n",
            "Epoch :  72, training loss : 0.0067, training accuracy : 99.82, test loss : 0.3634, test accuracy : 94.76\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0061, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.83\n",
            "Epoch :  73, training loss : 0.0062, training accuracy : 99.84, test loss : 0.3653, test accuracy : 94.80\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0060, accuracy : 99.83\n",
            "Epoch :  74, training loss : 0.0061, training accuracy : 99.82, test loss : 0.3691, test accuracy : 94.68\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0061, accuracy : 99.83\n",
            "Epoch :  75, training loss : 0.0060, training accuracy : 99.84, test loss : 0.3650, test accuracy : 94.58\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.83\n",
            "Epoch :  76, training loss : 0.0061, training accuracy : 99.83, test loss : 0.3716, test accuracy : 94.55\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.93\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.84\n",
            "Epoch :  77, training loss : 0.0063, training accuracy : 99.84, test loss : 0.3773, test accuracy : 94.63\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0070, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0059, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0061, accuracy : 99.85\n",
            "Epoch :  78, training loss : 0.0060, training accuracy : 99.85, test loss : 0.3706, test accuracy : 94.72\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0081, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0072, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0064, accuracy : 99.82\n",
            "Epoch :  79, training loss : 0.0062, training accuracy : 99.83, test loss : 0.3703, test accuracy : 94.66\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0058, accuracy : 99.83\n",
            "Epoch :  80, training loss : 0.0058, training accuracy : 99.83, test loss : 0.3735, test accuracy : 94.67\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0057, accuracy : 99.84\n",
            "Epoch :  81, training loss : 0.0057, training accuracy : 99.84, test loss : 0.3768, test accuracy : 94.61\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0070, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0075, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0066, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0060, accuracy : 99.83\n",
            "Epoch :  82, training loss : 0.0059, training accuracy : 99.84, test loss : 0.3716, test accuracy : 94.79\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.88\n",
            "Epoch :  83, training loss : 0.0054, training accuracy : 99.86, test loss : 0.3709, test accuracy : 94.76\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0062, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0061, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0059, accuracy : 99.85\n",
            "Epoch :  84, training loss : 0.0058, training accuracy : 99.86, test loss : 0.3722, test accuracy : 94.69\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.87\n",
            "Epoch :  85, training loss : 0.0052, training accuracy : 99.86, test loss : 0.3763, test accuracy : 94.64\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.88\n",
            "Epoch :  86, training loss : 0.0050, training accuracy : 99.87, test loss : 0.3800, test accuracy : 94.63\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.84\n",
            "Epoch :  87, training loss : 0.0053, training accuracy : 99.84, test loss : 0.3768, test accuracy : 94.69\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0058, accuracy : 99.83\n",
            "Epoch :  88, training loss : 0.0058, training accuracy : 99.83, test loss : 0.3772, test accuracy : 94.59\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch :  89, training loss : 0.0051, training accuracy : 99.85, test loss : 0.3848, test accuracy : 94.58\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.85\n",
            "Epoch :  90, training loss : 0.0055, training accuracy : 99.86, test loss : 0.3798, test accuracy : 94.65\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch :  91, training loss : 0.0052, training accuracy : 99.85, test loss : 0.3773, test accuracy : 94.69\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch :  92, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3820, test accuracy : 94.71\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch :  93, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3786, test accuracy : 94.70\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.86\n",
            "Epoch :  94, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3740, test accuracy : 94.66\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.89\n",
            "Epoch :  95, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3735, test accuracy : 94.70\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.86\n",
            "Epoch :  96, training loss : 0.0052, training accuracy : 99.86, test loss : 0.3842, test accuracy : 94.61\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.87\n",
            "Epoch :  97, training loss : 0.0049, training accuracy : 99.88, test loss : 0.3775, test accuracy : 94.71\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch :  98, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3699, test accuracy : 94.83\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.0080, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0069, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.85\n",
            "Epoch :  99, training loss : 0.0054, training accuracy : 99.86, test loss : 0.3731, test accuracy : 94.70\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.0033, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.85\n",
            "Epoch : 100, training loss : 0.0050, training accuracy : 99.85, test loss : 0.3813, test accuracy : 94.73\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.85\n",
            "Epoch : 101, training loss : 0.0052, training accuracy : 99.86, test loss : 0.3835, test accuracy : 94.50\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 102, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3829, test accuracy : 94.75\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 103, training loss : 0.0047, training accuracy : 99.86, test loss : 0.3859, test accuracy : 94.61\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 104, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3800, test accuracy : 94.65\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.86\n",
            "Epoch : 105, training loss : 0.0049, training accuracy : 99.85, test loss : 0.3770, test accuracy : 94.65\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.0082, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.86\n",
            "Epoch : 106, training loss : 0.0052, training accuracy : 99.86, test loss : 0.3852, test accuracy : 94.66\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch : 107, training loss : 0.0050, training accuracy : 99.85, test loss : 0.3824, test accuracy : 94.70\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 108, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3821, test accuracy : 94.56\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.85\n",
            "Epoch : 109, training loss : 0.0053, training accuracy : 99.84, test loss : 0.3803, test accuracy : 94.66\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 110, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3844, test accuracy : 94.67\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 111, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3836, test accuracy : 94.78\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 112, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3811, test accuracy : 94.86\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 113, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3795, test accuracy : 94.66\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0065, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.84\n",
            "Epoch : 114, training loss : 0.0053, training accuracy : 99.85, test loss : 0.3757, test accuracy : 94.78\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 115, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3803, test accuracy : 94.79\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.88\n",
            "Epoch : 116, training loss : 0.0045, training accuracy : 99.87, test loss : 0.3816, test accuracy : 94.63\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 117, training loss : 0.0051, training accuracy : 99.85, test loss : 0.3811, test accuracy : 94.79\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.0033, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0036, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.89\n",
            "Epoch : 118, training loss : 0.0044, training accuracy : 99.89, test loss : 0.3829, test accuracy : 94.65\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 119, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3883, test accuracy : 94.56\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 120, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3822, test accuracy : 94.73\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 121, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3927, test accuracy : 94.61\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.85\n",
            "Epoch : 122, training loss : 0.0052, training accuracy : 99.84, test loss : 0.3814, test accuracy : 94.69\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.87\n",
            "Epoch : 123, training loss : 0.0050, training accuracy : 99.87, test loss : 0.3878, test accuracy : 94.70\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 124, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3845, test accuracy : 94.61\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.90\n",
            "Epoch : 125, training loss : 0.0046, training accuracy : 99.89, test loss : 0.3777, test accuracy : 94.67\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.89\n",
            "Epoch : 126, training loss : 0.0045, training accuracy : 99.89, test loss : 0.3835, test accuracy : 94.77\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch : 127, training loss : 0.0052, training accuracy : 99.85, test loss : 0.3735, test accuracy : 94.74\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 128, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3764, test accuracy : 94.65\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.86\n",
            "Epoch : 129, training loss : 0.0045, training accuracy : 99.87, test loss : 0.3809, test accuracy : 94.69\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.85\n",
            "Epoch : 130, training loss : 0.0048, training accuracy : 99.85, test loss : 0.3816, test accuracy : 94.65\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0056, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.86\n",
            "Epoch : 131, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3842, test accuracy : 94.70\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0057, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.86\n",
            "Epoch : 132, training loss : 0.0053, training accuracy : 99.86, test loss : 0.3851, test accuracy : 94.68\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.0028, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0026, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0031, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0037, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0041, accuracy : 99.89\n",
            "Epoch : 133, training loss : 0.0043, training accuracy : 99.88, test loss : 0.3825, test accuracy : 94.71\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.86\n",
            "Epoch : 134, training loss : 0.0045, training accuracy : 99.86, test loss : 0.3749, test accuracy : 94.70\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 135, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3819, test accuracy : 94.67\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.86\n",
            "Epoch : 136, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3786, test accuracy : 94.66\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 137, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3842, test accuracy : 94.68\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 138, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3766, test accuracy : 94.76\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 139, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3753, test accuracy : 94.76\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 140, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3799, test accuracy : 94.70\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 141, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3830, test accuracy : 94.65\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 142, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3795, test accuracy : 94.74\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0033, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.86\n",
            "Epoch : 143, training loss : 0.0046, training accuracy : 99.86, test loss : 0.3835, test accuracy : 94.54\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 144, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3926, test accuracy : 94.55\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.88\n",
            "Epoch : 145, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3754, test accuracy : 94.75\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0072, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.85\n",
            "Epoch : 146, training loss : 0.0053, training accuracy : 99.86, test loss : 0.3836, test accuracy : 94.68\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 147, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3833, test accuracy : 94.67\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.83\n",
            "Epoch : 148, training loss : 0.0051, training accuracy : 99.83, test loss : 0.3721, test accuracy : 94.67\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 149, training loss : 0.0047, training accuracy : 99.86, test loss : 0.3871, test accuracy : 94.70\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0035, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0037, accuracy : 99.92\n",
            "iteration : 250, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.90\n",
            "Epoch : 150, training loss : 0.0045, training accuracy : 99.89, test loss : 0.3719, test accuracy : 94.81\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 151, training loss : 0.0049, training accuracy : 99.87, test loss : 0.3774, test accuracy : 94.64\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 152, training loss : 0.0047, training accuracy : 99.85, test loss : 0.3764, test accuracy : 94.72\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.85\n",
            "Epoch : 153, training loss : 0.0050, training accuracy : 99.85, test loss : 0.3727, test accuracy : 94.74\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 154, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3794, test accuracy : 94.78\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.85\n",
            "Epoch : 155, training loss : 0.0050, training accuracy : 99.85, test loss : 0.3804, test accuracy : 94.65\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.88\n",
            "Epoch : 156, training loss : 0.0049, training accuracy : 99.88, test loss : 0.3719, test accuracy : 94.58\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch : 157, training loss : 0.0051, training accuracy : 99.85, test loss : 0.3837, test accuracy : 94.62\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 158, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3782, test accuracy : 94.61\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.86\n",
            "Epoch : 159, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3733, test accuracy : 94.76\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.85\n",
            "Epoch : 160, training loss : 0.0047, training accuracy : 99.86, test loss : 0.3825, test accuracy : 94.80\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.86\n",
            "Epoch : 161, training loss : 0.0052, training accuracy : 99.86, test loss : 0.3792, test accuracy : 94.68\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.84\n",
            "Epoch : 162, training loss : 0.0051, training accuracy : 99.84, test loss : 0.3819, test accuracy : 94.69\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 163, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3812, test accuracy : 94.73\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0033, accuracy : 99.93\n",
            "iteration : 150, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 164, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3840, test accuracy : 94.61\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 165, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3847, test accuracy : 94.57\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.87\n",
            "Epoch : 166, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3755, test accuracy : 94.75\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.85\n",
            "Epoch : 167, training loss : 0.0052, training accuracy : 99.85, test loss : 0.3818, test accuracy : 94.80\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 168, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3782, test accuracy : 94.70\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.89\n",
            "Epoch : 169, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3840, test accuracy : 94.66\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch : 170, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3786, test accuracy : 94.66\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 171, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3753, test accuracy : 94.69\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 172, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3859, test accuracy : 94.72\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.88\n",
            "Epoch : 173, training loss : 0.0044, training accuracy : 99.88, test loss : 0.3799, test accuracy : 94.69\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0057, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.88\n",
            "Epoch : 174, training loss : 0.0053, training accuracy : 99.88, test loss : 0.3782, test accuracy : 94.55\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0034, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.89\n",
            "Epoch : 175, training loss : 0.0043, training accuracy : 99.89, test loss : 0.3853, test accuracy : 94.76\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch : 176, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3867, test accuracy : 94.65\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch : 177, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3780, test accuracy : 94.72\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.93\n",
            "iteration : 200, loss : 0.0038, accuracy : 99.93\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.91\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.89\n",
            "Epoch : 178, training loss : 0.0042, training accuracy : 99.89, test loss : 0.3838, test accuracy : 94.54\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.87\n",
            "Epoch : 179, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3809, test accuracy : 94.73\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.89\n",
            "Epoch : 180, training loss : 0.0049, training accuracy : 99.88, test loss : 0.3892, test accuracy : 94.66\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 181, training loss : 0.0049, training accuracy : 99.87, test loss : 0.3867, test accuracy : 94.63\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.87\n",
            "Epoch : 182, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3798, test accuracy : 94.61\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.92\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0040, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.89\n",
            "Epoch : 183, training loss : 0.0048, training accuracy : 99.88, test loss : 0.3798, test accuracy : 94.72\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0068, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0056, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.86\n",
            "Epoch : 184, training loss : 0.0053, training accuracy : 99.85, test loss : 0.3741, test accuracy : 94.78\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 185, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3815, test accuracy : 94.63\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.88\n",
            "Epoch : 186, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3820, test accuracy : 94.66\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 187, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3875, test accuracy : 94.55\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.89\n",
            "Epoch : 188, training loss : 0.0050, training accuracy : 99.88, test loss : 0.3828, test accuracy : 94.68\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 189, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3870, test accuracy : 94.66\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch : 190, training loss : 0.0052, training accuracy : 99.85, test loss : 0.3808, test accuracy : 94.56\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.84\n",
            "Epoch : 191, training loss : 0.0054, training accuracy : 99.84, test loss : 0.3865, test accuracy : 94.77\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 192, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3831, test accuracy : 94.75\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 193, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3805, test accuracy : 94.68\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 194, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3839, test accuracy : 94.63\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0057, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0057, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.82\n",
            "Epoch : 195, training loss : 0.0055, training accuracy : 99.83, test loss : 0.3849, test accuracy : 94.58\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 196, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3808, test accuracy : 94.61\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 197, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3849, test accuracy : 94.66\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.88\n",
            "Epoch : 198, training loss : 0.0049, training accuracy : 99.88, test loss : 0.3855, test accuracy : 94.71\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 199, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3836, test accuracy : 94.60\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.90\n",
            "Epoch : 200, training loss : 0.0042, training accuracy : 99.90, test loss : 0.3802, test accuracy : 94.62\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.88\n",
            "Epoch : 201, training loss : 0.0044, training accuracy : 99.88, test loss : 0.3815, test accuracy : 94.73\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 202, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3820, test accuracy : 94.72\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 203, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3853, test accuracy : 94.67\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.85\n",
            "Epoch : 204, training loss : 0.0055, training accuracy : 99.85, test loss : 0.3775, test accuracy : 94.68\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 205, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3871, test accuracy : 94.59\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.88\n",
            "Epoch : 206, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3774, test accuracy : 94.76\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 207, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3905, test accuracy : 94.69\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0040, accuracy : 99.92\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.90\n",
            "Epoch : 208, training loss : 0.0042, training accuracy : 99.89, test loss : 0.3807, test accuracy : 94.66\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.88\n",
            "Epoch : 209, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3815, test accuracy : 94.67\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.86\n",
            "Epoch : 210, training loss : 0.0054, training accuracy : 99.85, test loss : 0.3839, test accuracy : 94.69\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch : 211, training loss : 0.0047, training accuracy : 99.85, test loss : 0.3910, test accuracy : 94.65\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.88\n",
            "Epoch : 212, training loss : 0.0048, training accuracy : 99.88, test loss : 0.3865, test accuracy : 94.68\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.89\n",
            "Epoch : 213, training loss : 0.0046, training accuracy : 99.89, test loss : 0.3797, test accuracy : 94.79\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0040, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0035, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.88\n",
            "Epoch : 214, training loss : 0.0043, training accuracy : 99.88, test loss : 0.3823, test accuracy : 94.68\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 215, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3815, test accuracy : 94.78\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.87\n",
            "Epoch : 216, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3788, test accuracy : 94.68\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 217, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3761, test accuracy : 94.74\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 218, training loss : 0.0049, training accuracy : 99.87, test loss : 0.3740, test accuracy : 94.84\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0040, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0039, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0039, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0039, accuracy : 99.90\n",
            "Epoch : 219, training loss : 0.0040, training accuracy : 99.90, test loss : 0.3861, test accuracy : 94.73\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.84\n",
            "Epoch : 220, training loss : 0.0053, training accuracy : 99.84, test loss : 0.3735, test accuracy : 94.72\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.85\n",
            "Epoch : 221, training loss : 0.0054, training accuracy : 99.85, test loss : 0.3757, test accuracy : 94.70\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 222, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3836, test accuracy : 94.61\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 223, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3818, test accuracy : 94.57\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0030, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 224, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3755, test accuracy : 94.73\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 225, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3817, test accuracy : 94.71\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.87\n",
            "Epoch : 226, training loss : 0.0052, training accuracy : 99.87, test loss : 0.3789, test accuracy : 94.67\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.88\n",
            "Epoch : 227, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3852, test accuracy : 94.63\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 228, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3789, test accuracy : 94.64\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 229, training loss : 0.0044, training accuracy : 99.87, test loss : 0.3835, test accuracy : 94.66\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.84\n",
            "Epoch : 230, training loss : 0.0055, training accuracy : 99.84, test loss : 0.3935, test accuracy : 94.48\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 231, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3787, test accuracy : 94.68\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.85\n",
            "Epoch : 232, training loss : 0.0052, training accuracy : 99.85, test loss : 0.3792, test accuracy : 94.74\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 233, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3838, test accuracy : 94.60\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.87\n",
            "Epoch : 234, training loss : 0.0046, training accuracy : 99.86, test loss : 0.3794, test accuracy : 94.71\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.90\n",
            "Epoch : 235, training loss : 0.0045, training accuracy : 99.89, test loss : 0.3768, test accuracy : 94.76\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.84\n",
            "Epoch : 236, training loss : 0.0051, training accuracy : 99.85, test loss : 0.3790, test accuracy : 94.76\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 237, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3791, test accuracy : 94.76\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 238, training loss : 0.0044, training accuracy : 99.88, test loss : 0.3889, test accuracy : 94.62\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0040, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.89\n",
            "Epoch : 239, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3743, test accuracy : 94.72\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.88\n",
            "Epoch : 240, training loss : 0.0045, training accuracy : 99.87, test loss : 0.3834, test accuracy : 94.72\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 241, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3849, test accuracy : 94.66\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 242, training loss : 0.0049, training accuracy : 99.87, test loss : 0.3746, test accuracy : 94.67\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 243, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3824, test accuracy : 94.63\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.87\n",
            "Epoch : 244, training loss : 0.0044, training accuracy : 99.87, test loss : 0.3821, test accuracy : 94.71\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0028, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.91\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.89\n",
            "Epoch : 245, training loss : 0.0048, training accuracy : 99.89, test loss : 0.3865, test accuracy : 94.68\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.88\n",
            "Epoch : 246, training loss : 0.0049, training accuracy : 99.87, test loss : 0.3774, test accuracy : 94.70\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0064, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.85\n",
            "Epoch : 247, training loss : 0.0054, training accuracy : 99.85, test loss : 0.3737, test accuracy : 94.69\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.89\n",
            "Epoch : 248, training loss : 0.0048, training accuracy : 99.89, test loss : 0.3788, test accuracy : 94.70\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 249, training loss : 0.0048, training accuracy : 99.88, test loss : 0.3864, test accuracy : 94.61\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.89\n",
            "Epoch : 250, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3840, test accuracy : 94.67\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 251, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3823, test accuracy : 94.55\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 252, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3869, test accuracy : 94.60\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.88\n",
            "Epoch : 253, training loss : 0.0050, training accuracy : 99.88, test loss : 0.3875, test accuracy : 94.66\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0030, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 254, training loss : 0.0045, training accuracy : 99.89, test loss : 0.3786, test accuracy : 94.61\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0072, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch : 255, training loss : 0.0049, training accuracy : 99.85, test loss : 0.3735, test accuracy : 94.60\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 256, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3730, test accuracy : 94.74\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 257, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3816, test accuracy : 94.63\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0034, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.85\n",
            "Epoch : 258, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3767, test accuracy : 94.74\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0074, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 259, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3841, test accuracy : 94.78\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.87\n",
            "Epoch : 260, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3875, test accuracy : 94.60\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 261, training loss : 0.0047, training accuracy : 99.86, test loss : 0.3692, test accuracy : 94.76\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.87\n",
            "Epoch : 262, training loss : 0.0051, training accuracy : 99.88, test loss : 0.3754, test accuracy : 94.74\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 263, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3828, test accuracy : 94.65\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0057, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.86\n",
            "Epoch : 264, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3801, test accuracy : 94.59\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.88\n",
            "Epoch : 265, training loss : 0.0048, training accuracy : 99.88, test loss : 0.3871, test accuracy : 94.60\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch : 266, training loss : 0.0052, training accuracy : 99.85, test loss : 0.3755, test accuracy : 94.80\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.85\n",
            "Epoch : 267, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3841, test accuracy : 94.61\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.85\n",
            "Epoch : 268, training loss : 0.0050, training accuracy : 99.85, test loss : 0.3726, test accuracy : 94.71\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.84\n",
            "Epoch : 269, training loss : 0.0052, training accuracy : 99.84, test loss : 0.3794, test accuracy : 94.65\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 270, training loss : 0.0049, training accuracy : 99.87, test loss : 0.3838, test accuracy : 94.67\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 271, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3798, test accuracy : 94.75\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.86\n",
            "Epoch : 272, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3778, test accuracy : 94.72\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.89\n",
            "Epoch : 273, training loss : 0.0046, training accuracy : 99.89, test loss : 0.3821, test accuracy : 94.62\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 274, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3801, test accuracy : 94.66\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 275, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3791, test accuracy : 94.80\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 276, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3813, test accuracy : 94.64\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch : 277, training loss : 0.0051, training accuracy : 99.85, test loss : 0.3761, test accuracy : 94.66\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0064, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.88\n",
            "Epoch : 278, training loss : 0.0051, training accuracy : 99.87, test loss : 0.3808, test accuracy : 94.70\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 279, training loss : 0.0049, training accuracy : 99.88, test loss : 0.3796, test accuracy : 94.65\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 280, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3693, test accuracy : 94.84\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 281, training loss : 0.0050, training accuracy : 99.88, test loss : 0.3727, test accuracy : 94.70\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 282, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3811, test accuracy : 94.71\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.85\n",
            "Epoch : 283, training loss : 0.0053, training accuracy : 99.86, test loss : 0.3710, test accuracy : 94.77\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.89\n",
            "Epoch : 284, training loss : 0.0045, training accuracy : 99.89, test loss : 0.3803, test accuracy : 94.63\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.89\n",
            "Epoch : 285, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3810, test accuracy : 94.74\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.90\n",
            "Epoch : 286, training loss : 0.0043, training accuracy : 99.89, test loss : 0.3768, test accuracy : 94.70\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0036, accuracy : 99.92\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 287, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3750, test accuracy : 94.62\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.86\n",
            "Epoch : 288, training loss : 0.0045, training accuracy : 99.87, test loss : 0.3817, test accuracy : 94.73\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 289, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3736, test accuracy : 94.74\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0036, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.89\n",
            "Epoch : 290, training loss : 0.0043, training accuracy : 99.89, test loss : 0.3825, test accuracy : 94.67\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 291, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3820, test accuracy : 94.73\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 292, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3842, test accuracy : 94.69\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 293, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3735, test accuracy : 94.77\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 294, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3787, test accuracy : 94.66\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.88\n",
            "Epoch : 295, training loss : 0.0044, training accuracy : 99.88, test loss : 0.3772, test accuracy : 94.68\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0027, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0030, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 296, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3868, test accuracy : 94.61\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.84\n",
            "Epoch : 297, training loss : 0.0050, training accuracy : 99.84, test loss : 0.3816, test accuracy : 94.65\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0064, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.88\n",
            "Epoch : 298, training loss : 0.0048, training accuracy : 99.88, test loss : 0.3815, test accuracy : 94.72\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.87\n",
            "Epoch : 299, training loss : 0.0050, training accuracy : 99.87, test loss : 0.3853, test accuracy : 94.71\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0041, accuracy : 99.88\n",
            "Epoch : 300, training loss : 0.0041, training accuracy : 99.89, test loss : 0.3878, test accuracy : 94.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "id": "iUQVIKR-X3v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52d374a-405e-4a97-b3e1-f58dfb17ec63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.33536864510353875, 95.21358328211431)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : Step\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7CNz1iabSB21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "a2138b7f-80aa-4807-8f25-512df20c1cf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwc9Znn8c9jWYdl+ZQPfIHNZUyYYMAxYcAcMXe4QhJCmEyYhInJDGSATVjITg4ym911JpOEgUkgJOHIcgQC4dgAwcQxRwjYGGOIwSdgY+NLNr4kW5IlPfvHUy21pJYsyW615P6+X69+dXdVdf+eOvr31K+qq37m7oiIiAD0yXUAIiLScygpiIhIIyUFERFppKQgIiKNlBRERKSRkoKIiDRSUhARkUZKCpJVZvacmW0xs+JcxyJNzOwkM/uLmW0zsw/N7CUz+1gy7h/M7M+5jlFyQ0lBssbMxgPTAAcu6Oay+3ZneXurO+M1s4HA74FbgaHAGOB7QE13xSA9l5KCZNMXgVeAu4HL00eY2Tgz+52ZVZjZZjP7r7RxXzGzxWa2w8zeNrNjk+FuZoemTXe3mX0/eX2qma0xsxvMbD1wl5kNMbPfJ2VsSV6PTfv8UDO7y8zWJuMfS4YvMrPz06YrNLNNZnZMppk0swvNbKGZbTezd8zs7GT4SjM7PW26m8zs3uT1+GR+rjCz94E/mdnTZnZ1i+9+w8wuTl4fYWbPJnv2S83sks6sjDSHA7j7A+5e7+673H2Wu79pZpOA24ETzKzSzLYmZReb2X+Y2ftmtsHMbjezfi2W/f9IltNKM/u7LsYmOaakINn0ReC+5HGWmY0EMLMCYk91FTCe2FP9TTLus8BNyWcHEi2MzR0s7wBiz/cgYAaxfd+VvD8Q2AX8V9r0/xcoBT4CjAB+kgz/NfCFtOnOBda5++stCzSzqcn01wODgZOBlR2MF+AUYBJwFvAA8Pm07z4yif1JM+sPPAvcn8R6KfCzZJpWzOxGM/t9G2UuA+rN7B4zO8fMhqRGuPti4KvAy+5e5u6Dk1EziWQyGTiUWGffSfvOA4BhyfDLgTvMbGLHF4P0GO6uhx77/AGcBOwGhiXvlwDXJa9PACqAvhk+9wxwTRvf6cChae/vBr6fvD4VqAVK2olpMrAleT0KaACGZJhuNLADGJi8fxj4721858+Bn7QxbiVwetr7m4B7k9fjk/k5OG38AKAKOCh5/7+AO5PXnwNezFD2d7u4fiYly28NUAc8AYxMxv0D8Oe0aS2J65C0YScA76Ut+zqgf9r4h4Bv53o71KPzD7UUJFsuB2a5+6bk/f00HUIaB6xy97oMnxsHvNPFMivcvTr1xsxKzeznZrbKzLYDLwCDk5bKOOBDd9/S8kvcfS3wEvBpMxsMnEO0djLZm3gBVqeVuwN4kmgFQLQaUuUeBBxvZltTD+DviD30TnP3xe7+D+4+FjiKSIQ3tzH5cKJF9Vpa2X9IhqdscfeqtPerku+UXqZXnYyT3iE51nwJUJAc3wcoJirko4mK8EAz65shMawGDmnjq3cSlVPKAcSebkrLW/5+HZgIHO/u681sMvA6see7GhhqZoPdfWuGsu4B/pH4jbzs7h+0EVN78VZliLelljE/AHzXzF4ASoA5aeU87+5ntFFWl7n7EjO7G7iyjZg2EYfePtLOchhiZv3TEsOBwKJ9Hatkn1oKkg0XAfXAkcQhm8nE4YoXiXMF84B1wEwz629mJWZ2YvLZXwLfMLPjLBxqZgcl4xYCl5lZQXIy95Q9xDGAqMy2mtlQ4LupEe6+DniaOC4/JDmZfHLaZx8DjgWuIc4ZtOVXwJfMbLqZ9TGzMWZ2RFq8lybfPQX4zB7iBXiKaBX8G/Cguzckw38PHG5mf598X6GZfSw5MdwpyQnrr6dOupvZOKJV8koyyQZgrJkVASQx/AL4iZmNSD4zxszOavHV3zOzIjObBpwH/LazsUnuKSlINlwO3OXu77v7+tSDOMn7d8Se+vnECcv3ib39zwG4+2+JY+n3E8f1HyNOHkNU0OcDqUMnj+0hjpuBfsSe7ivEIY90f0+c91gCbASuTY1w913AI8AE4HdtFeDu84AvESeptwHPE5U6wLeJVsQW4i+f9+8hXty9Jinv9PTpk0NLZxKHltYC64EfEC2wVpJ/Aj3dRjE7gOOBuWZWRSybRUTLCuBPwFvAejNLHf67AVgBvJIcivsj0QpLWZ/M51rikNdX3X3JnuZXeh5zVyc7IpmY2XeAw939C3ucOI+Z2anECfSxe5pWej6dUxDJIDncdAXRmhDJGzp8JNKCmX2FOLH7tLu/kOt4RLqTDh+JiEgjtRRERKRRrz6nMGzYMB8/fnyuwxAR6VVee+21Te4+PNO4Xp0Uxo8fz/z583MdhohIr2Jmq9oap8NHIiLSSElBREQaKSmIiEgjJQUREWmkpCAiIo2ylhTM7E4z22hmi9KGDU26E1yePA9JhpuZ3WJmK8zsTUu6XxQRke6VzZbC3cDZLYbdCMx298OA2cl7iE5MDkseM4DbshiXiIi0IWvXKbj7C2Y2vsXgC4mu+yA6MXmOuCXvhcCvPe658YqZDTazUck972UfamiArVuhshKKi+Gtt2DYMKirg127oLYWysvj9bZtMbytO6G4t340NHRtGEBBAVRXx6OkBPr1g8LCmLahAfr0ieE1NTFNXV3TNDt3wu7d8R0ffghDk5ttm8GAAbBlS0xXWRnTjhsX31lVFd/bt2/zeUqPr2WsLd+XlMDAgVHOzp2wdm3EMmBADN+1Kz5TXBzjS0qiPLN4FBbGvGzeHPNTVBTvd++OZ4CysubDUp9NPfr0aft9W+OKi6G+Ppbljh0walTMz/r1MSx9XlPLqLAwnmtq4nV1dczTgAERo1lsN9XV8Zl98UgtGzPYvh3Gjo3lUFnZFFtq3lKqqqC0FAYNgo0b4zuKiuJRWBjbeWVlbCdlZbEc6uriOf11av3s3h3zPWhQDOvbN5bNli0xDmI9DxwYn+3TJ17X1MQjNSy13jdtivep5Zna/rZti9dFRTE+XWr+3KOsM86AY47Zu/ogk+6+eG1kWkW/HhiZvB5DWreExP31xxAdsTRjZjOI1gQHHnhg9iLtwdxjY9ywIX7A27c3VTh//SusWBEb4aRJUFEBCxfGRrhpU1SYDQ17LkNEerb+/fePpNDI3d3MOn03Pne/A7gDYMqUKfvl3fzq6mDevKjgV6+OPc9UAtiwIfZ8UnsnLZnFXjDAb34Te55TpsDf/E20AMrLYfjw2KC2b4ejjornwsLYs+rbN5JH//4weHDTnk1b2ttb7cwwiD2zoqKII9Vi2L27+R5jTU3saZeURKtg166YprS0aQ+wvDySplkkwO3bYciQ+P6yskiga9Y0zbN70/Jsb4+7rdhraqKMbdtieY8ZE/OxY0dTwoYoo6wspt+9u6mFVFMT3ztiRMxzai88tWeeatGk76nDnlsw7bV26uujnL59Y1n279+0TEaOjOWSPq+pZZRqrRQVxXNhYewRV1bG/LrH3nRJSVO5e/vo0yfWqXssv9WrYzmnWiapcqBpmZaWxk7S1q2xvZeUxLaRehQXxzSbNsU21LdvbE8FBc1fu8c8FxTE/O7Y0bQuzWK7Sq2P0tKm31J9fUxbXByP1LC6uoh12LCm70kt09Syq6+PGNNb6C1b66WlsQyyobuTwobUYSEzG0X0dgXwAdEBesrYZNh+7403YNkyePll+P3vY2PYujUqGIiN8YAD4oc6ciR89KPN348cGZV3qiKZNCl+4BDDiorar9T3VwMHtj9+Uqc7sWzfyJGth5WWZh7eU40e3fXPFhdHxd0djjyyY9MNHBi/lZTU7yJdWdm+iSm9zM7IVsW+N7o7KTxBdNU4M3l+PG341Wb2G6KbwG378/mEnTvhwQfhZz+D1K2biovhE5+IPYWyMjjzTDj++Pih9u3iWirO2FGjiEjbspYUzOwB4qTyMDNbQ3SaPhN4yMyuAFYBlySTPwWcS/QBu5Po83a/tG0bfOxjsHx57PHceiuccgpMmLDv91pERDorm/8++nwbo6ZnmNaBq7IVS09yzTXw7rvwxBNw3nn5eWhHRHquXn3r7N5mxw6491646io4//xcRyMi0ppuc9GNXnop/llw3nm5jkREJDMlhW70/PNx0vhv/zbXkYiIZKbDR91g50547jl4/PG4ZiDTX+NERHoCtRSyqK4OZs6Mv5V+8pOweDF8+tO5jkpEpG1qKWTJ++/DZZfFeYTzz4evfQ2mTo3rEEREeiolhX1s8WK47jqYMycuHrv/fvh8W3/OFRHpYXT4aB/ZuhUuuCDuJTR/frQMFi5UQhCR3kUthb3kHhei/e//Da+/DjfeCP/yL73rnjciIilKCnuhvh6uvhpuvz1uSnf//fCZz+Q6KhGRrtPhoy7atSsSwO23ww03RL8FSggi0tspKXSSOzz2WNzR9PHH4ZZb4m+nXb2TqYhIT6KqrBPq6uCf/gl++cu4d/xDD3WhdXD77bByJRx4YPSWs2BBfPEBB0R/iCefHPfNrq6OHnVGj45OESCOV6X3XFJS0rEy6+ubegx5993oeOHAA6PDhbbur/3OO9HRwwknxLGxtrz1VsR6xBGdvyqvoSEu8zaDadOa96TT3i1ja2rgmWei16FLLomTOaWlMHlyzN8RR7R9p8GGhviL2LBhHT/xs2ZN3LjqoIOinLq6pr2AVG8oqeXoHj0kDR4Mhx8e/UiuWhU96IxL6zJk50547bX4vmOPbYqtoCDW18KF0WPLSSfF+kqPv0+fKKeyMvrBTHFv3Sfle+9F5xEFBZnna9cuGD++eRkt1dXFo7i46furqmLehwxpPf3atTF+yJBYbiNHwuzZ0fPRJz8ZfWCmx5naPjNxb+oVp7Y21m9tbfwmJk5s6l2nrc93ROo7amuj28Jx46LcTZvid5L6/a1aFdv6xIlNn62uju4MR46MGCoq4nf9kY80TbNqVWwTn/xk9KazeXPcFnnr1thOUv2MpnrZqa2Ndfzuu3Dooc375ayvb+r9KEvM2+qAtxeYMmWKz091SJBlW7bAFVfAo4/Ct74F3/1uF1oHGzY0dYQLsXInToyKYd26GF9fHxXx++/DBx/Ej37y5NjwVqyIz6a6QzvppBh/zjnxQ1ywIBJFqheeN96Iz/3xj3DxxfH9L7wQZQ8fHhvw4YfDZz8bG+3KlTGjY8bEZyF6Afnc56LfvxdeiMp4+PAod9AgePHFmK6gILp3GzMmNuYJE+Iz770XlV9FRfwghg6NeRg6NH4UK1c2LYsjj4wf/9KlcPrpMb66On6chYWRMBcsiPuOb93aVG59fbweNKjp3uTjx8c/AD7+8Uhw1dVw3HHw9tvRdRdE8tixI2LZuDF+/Js3x7hU9259+kTlmSpr4sRYD1OnRkK66aao4MeMifWydWss54KCWE7r1zd99oILIq4PP4RZs2KdQ+xhbNsWifDSSyPJzZ0b4wYOjGRUVRWxbd8OhxwSj2eegYMPjjKnT49emg47LDoxBnj22VjmEydGUjruOPjzn2PYkUdGpx4NDbGOjz02LrevqopE//bbkUwOOwyefDJiLi+PZTx+fMS4ZUus59LSeN2vX2wXb77ZPMEVFUVFl9KnT1S8Z50Vy/bpp+O7x42LR1VVfOaUU+CnP43fwbRpsUyqq5u+Z+LE2Nbq6mI5lJVFJZv6HRQURLl1dTFv1dXRS1VpaSTdsrLYOXj00aZu03bsiFhqa+N1//5xWODQQ+GOO2KHZOLEWBdjx8YORlVVfOb442Mnp6oqfrPbt0eZ27dHAu/bt6nT7XHjYjssL491OW9e899lapmNHRvL9W/+JrbRuXPjO0pK4jDFP/5jZ2qgRmb2mrtPyThOSWHP3n47tsktW+DHP4Zrr+3iF91xB1x5ZfygRoyIH3z6HnFtLdx1F/zP/xkb43XXxf9bV6yIaVN7KKnu2d54o6mfTohKrr4+Nrbq6tjgd+6Myui55+LHeN11UcHOnx8/7tmzo9u3I46IH8yAAbGhn39+VCIPPwz33Rcb+qhRcTe/7dvje1etgnPPjQpywYLYsCsqonXz5pvxYx49Or5n7Ngod+PGSGirV0cC+NKX4oe2eHE0waqqoqW0dGkktpKS+KEvWxbTTJsWcX/607Ecnnkm9sCWL4+LQ44/Ps74L18e8/322/EDLSqK+CZOjOk//DCmHzYsXh9wQFSW5eVRmVRXx150TU18ftSoKP/11yOuhx6KH/rkyXDaaU19pJaVRSJatCiGHX98VOovvRT9o374YfzwJ0yIdbF2bXzn8OER81NPRQX1/e/HsFmzYpoBA2LPe9CgqHhefhm+/OUYt3t3TPepTzW1MCsrY2fh8MOj3OpqePXViHfYsPiOiy+Oad54IyqbBQuinEmTYnv4859jOzvxxPjcypXxvQsXRhI85ZRYvtXVEdv27bHMJk+O91u2RLzr1kU5AwbEctiyJfqaff752G6/9KX43KpVsc2UlUVZ69bFjcKOPjrm74wz4n1paUz34INR2Q8cGNtLat4/+tGm1lRxcfwmnn8+yn/99dj+PvrR2NY2boz1VVAQMZ92WvwGBwyI5fPqq/CHP0R5J54Y2/F778U2WFER6/EjH4kdpiVLIrlOmBDljRzZ1J/qxRfHOhsyJN6/+GLs1K1YEdvKuefGfC1dGq2Tysoo4/nnY/o33ojvO+GEqBt27YrvPOGELlVFSgp7oaEhtv3Fi2OHe/LkTnx40ya47baojK66Cn7yk9izWbas/eZfam+iI02Rurr4AZWVNfU9WFkZP46DD44fRHvf4x4/vlGj2o6pvj6+74ADmjdl97W1a6NiOeKIzHHW1bV/mCNd6jBLtmzdGhX8QQd1/NBF6rfW3rpPdaLcXuzuUaGldijcm9ZPeyoro0Ixy3zIJtvLrKX25nX79khKZ521d4eGWkq1Qjq6He2nlBS6aM2a2MFZtAjuvDN2aFpZujQmHDgw9raGDo0TDTU10fTeuDF+vP36xZ7Fv/5rtARERHKkvaSgE83t+P73Y6f+gQfisDrQ1CTu0yeawEcf3bQHmPLii9Gk3LAhDs/U1kZ2GT0arr++2+dDRKSjlBTaMHduHFr88pfh0s957OXPmwcXXhjH/o47LprihYVxQnPHjjix+qMfwc9/Hsf/pk6Nk1TuccnzKadEi0JEpIdSUsjgd7+L85gjRsAN19bAF74cJy/NogVw6qlxAnb9+rgV6llnNX34+9+Pf324ww9+EMPM4JvfzMm8iIh0hpJCC5s3x7UIxx0Hf/oTDLznjkgIV14Z/0D4j/+If7BcfXXc5OiGG5p/wdCh8S8SEZFeSEmhheuudUZuXsz9t9QwsP9H4eab429ft9/efMKJE+PvkCIi+xElhTSzZ8Pue3/Dm1wGlxLnBTZsgH//91yHJiLSLXTvo4Q7fPvb8PWiW2k45FC49db4Z9EvfxkXiYiI5AG1FBJz5kDVy28whZfhqh/HOYOrr851WCIi3UpJIXHffXBN4W14QQl2+eW5DkdEJCd0+Ii4dczsR7dzmd+LXXpp/INIRCQPqaVA3CvuE1sepoQq+OpXcx2OiEjO5KSlYGbXmNkiM3vLzK5Nhg01s2fNbHnynOFG7dnx/PNwHk/SMGZsXIUsIpKnuj0pmNlRwFeAqcDRwHlmdihwIzDb3Q8DZifvu8Xrc2s5s8+z9PnkuVntvEJEpKfLRUthEjDX3Xe6ex3wPHAxcCFwTzLNPcBF3RGMO/Sd+xJlDTvinuYiInksF0lhETDNzMrNrBQ4FxgHjHT3pCsq1gMd7Ctx77z7Lnxix2PUFZZE71UiInms2080u/tiM/sBMAuoAhYC9S2mcTPL2NGDmc0AZgAceOCBex3Pq3MbuJjfUXniWQxur19gEZE8kJMTze7+K3c/zt1PBrYAy4ANZjYKIHne2MZn73D3Ke4+Zfjw4Xsdy4b/N49xrKHsi5/e6+8SEentcvXvoxHJ84HE+YT7gSeA1FVjlwOPd0csxa9EP7F9L9D5BBGRXF2n8IiZlQO7gavcfauZzQQeMrMrgFXAJdkOor4eytYsZlv/UQwqL892cSIiPV5OkoK7T8swbDPQrWd6Fy+GQ+uWsOuwIxjUnQWLiPRQeX2bi9fmO5NYTNHkSbkORUSkR8jr21xsX7qOQWyndoqSgogI5HlLofjdxQAUHa2kICICeZ4Uyt+dFy8mKSmIiEA+J4W6Oqa9/XNeLT0ZRo/OdTQiIj1C/iaFJ59kxM5VPHbQtbmORESkx8jfpLB8OQDvTDg9x4GIiPQc+ZsUamoAKCsvznEgIiI9R/4mhdpaAAYNK8xxICIiPUfeXqdQv6uWOooYMlSd6oiIpORtUqjeXksDRQwdmutIRER6jrxNCjU7anGKGNJtPUGLiPR8eZsUaitrcLUURESayduksLsyDh+ppSAi0iRvk0Ldzlp2U6yWgohImrz9S2r9rlpqKWLw4FxHIiLSc+RtUrDdNdRSRFFRriMREek58jYp9KmLlkKfvF0CIiKt5W2VWFBXSw3FSgoiImnytkrsU6+WgohIS3lbJaYOH5nuciEi0ihvk0JBXY1aCiIiLeRtlVigw0ciIq3kbZWoE80iIq3lbZWYOtGscwoiIk3yNin0ra9hN7pyTUQkXd4mhYL6WnabkoKISLq8Tgq1pv6ZRUTS5XVSUEtBRKS5nCQFM7vOzN4ys0Vm9oCZlZjZBDOba2YrzOxBsyzW2PX19PEGJQURkRa6PSmY2RjgX4Ap7n4UUABcCvwA+Im7HwpsAa7IWhA1NQBKCiIiLeTq8FFfoJ+Z9QVKgXXAJ4CHk/H3ABdlrfTaWgDq+igpiIik6/ak4O4fAP8BvE8kg23Aa8BWd69LJlsDjMn0eTObYWbzzWx+RUVF14JIkoJONIuINJeLw0dDgAuBCcBooD9wdkc/7+53uPsUd58yfPjwrgWhloKISEa5OHx0OvCeu1e4+27gd8CJwODkcBLAWOCDrEWQOqegpCAi0kwuksL7wMfNrNTMDJgOvA3MAT6TTHM58HjWIki1FHSiWUSkmT0mBTM738z2WfJw97nECeUFwF+TGO4AbgD+m5mtAMqBX+2rMltJnVPoo3MKIiLp+u55Ej4H3GxmjwB3uvuSvS3U3b8LfLfF4HeBqXv73R2iloKISEZ7bAG4+xeAY4B3gLvN7OXkH0ADsh5dtuhEs4hIRh06LOTu24lDPr8BRgGfAhaY2deyGFv2JCea6wuUFERE0nXknMIFZvYo8BxQCEx193OAo4GvZze8LFFLQUQko46cU/g0cfuJF9IHuvtOM8verSiyKUkKu3WiWUSkmY4khZuIK48BMLN+wEh3X+nus7MVWFYlSUGHj0REmuvIOYXfAg1p7+uTYb1Xck5Bh49ERJrrSFLo6+61qTfJ695dm+qcgohIRh1JChVmdkHqjZldCGzKXkjdIJUUCnROQUQkXUfOKXwVuM/M/gswYDXwxaxGlW1qKYiIZLTHpODu7xD3KipL3ldmPaps04lmEZGMOtJSwMw+CXwEKIl72IG7/1sW48quQw7hlVGf0uEjEZEWOnLx2u3E/Y++Rhw++ixwUJbjyq6LLmLm1N/R0FctBRGRdB050fy37v5FYIu7fw84ATg8u2FlX0MD9MlVZ6QiIj1UR6rF6uR5p5mNBnYT9z/q1ZQURERa68g5hf9nZoOBHxJ9IDjwi6xG1Q0aGiA5PSIiIol2k0LSuc5sd98KPGJmvwdK3H1bt0SXRe5qKYiItNRutejuDcBP097X7A8JAXT4SEQkk45Ui7PN7NNm+9fBFiUFEZHWOlItXkncAK/GzLab2Q4z257luLJOSUFEpLWOXNHce7vdbIeSgohIa3tMCmZ2cqbhLTvd6W2UFEREWuvIX1KvT3tdAkwFXgM+kZWIuklDAxQU5DoKEZGepSOHj85Pf29m44CbsxZRN1FLQUSkta5Ui2uASfs6kO6m6xRERFrryDmFW4mrmCGSyGTiyuZeTS0FEZHWOnJOYX7a6zrgAXd/KUvxdBslBRGR1jqSFB4Gqt29HsDMCsys1N13Zje07FJSEBFprUNXNAP90t73A/6YnXC6j5KCiEhrHakWS9K74Exel2YvpO6hpCAi0lpHqsUqMzs29cbMjgN2dbVAM5toZgvTHtvN7FozG2pmz5rZ8uR5SFfL6AjdOltEpLWOnFO4Fvitma0luuM8gOies0vcfSnxDybMrAD4AHgUuJG4TfdMM7sxeX9DV8vZE7UURERa68jFa6+a2RHAxGTQUnffvY/Knw684+6rzOxC4NRk+D3Ac2QxKeg6BRGR1vZYLZrZVUB/d1/k7ouAMjP7531U/qXAA8nrke6+Lnm9HhjZRjwzzGy+mc2vqKjocsFqKYiItNaRavErSc9rALj7FuAre1uwmRUBFxC35W7G3Z2mC+ZajrvD3ae4+5Thw4d3uXwlBRGR1jpSLRakd7CTnAco2gdlnwMscPcNyfsNZjYqKWMUsHEflNEmJQURkdY6Ui3+AXjQzKab2XTicM/T+6Dsz9N06AjgCeDy5PXlwOP7oIw2KSmIiLTWkX8f3QDMAL6avH+T+AdSl5lZf+AMole3lJnAQ2Z2BbAKuGRvytgTJQURkdY68u+jBjObCxxCVNTDgEf2plB3rwLKWwzbTPwbqVvoOgURkdbaTApmdjhxiOfzwCbgQQB3P617Qssu/SVVRKS19loKS4AXgfPcfQWAmV3XLVF1Ax0+EhFprb1q8WJgHTDHzH6RnGTebw64KCmIiLTWZrXo7o+5+6XAEcAc4nYXI8zsNjM7s7sCzBYlBRGR1vZYLbp7lbvfn/TVPBZ4nSzefqK7KCmIiLTWqWrR3bckVxR327+EskVJQUSktbytFpUURERay9tqUdcpiIi0lrdJQdcpiIi0lrfVog4fiYi0lrfVopKCiEhreVstKimIiLSWt9WikoKISGt5Wy0qKYiItJa31aL+kioi0lpeJwW1FEREmsvLatE9npUURESay8tqUUlBRCSzvKwWGxriWUlBRKS5vKwWlRRERDLLy2pRSUFEJLO8rBaVFEREMsvLajGVFOveEMYAAA+WSURBVHSdgohIc3mdFNRSEBFpLi+rRf0lVUQks7ysFtVSEBHJLC+rRSUFEZHM8rJaVFIQEcksJ9WimQ02s4fNbImZLTazE8xsqJk9a2bLk+ch2SpfSUFEJLNcVYv/CfzB3Y8AjgYWAzcCs939MGB28j4rlBRERDLr9mrRzAYBJwO/AnD3WnffClwI3JNMdg9wUbZi0HUKIiKZ5WJfeQJQAdxlZq+b2S/NrD8w0t3XJdOsB0Zm+rCZzTCz+WY2v6KioksB6C+pIiKZ5aJa7AscC9zm7scAVbQ4VOTuDnimD7v7He4+xd2nDB8+vEsB6PCRiEhmuagW1wBr3H1u8v5hIklsMLNRAMnzxmwFoKQgIpJZt1eL7r4eWG1mE5NB04G3gSeAy5NhlwOPZysGJQURkcz65qjcrwH3mVkR8C7wJSJBPWRmVwCrgEuyVbiSgohIZjlJCu6+EJiSYdT07ihfSUFEJLO8rBaVFEREMsvLalHXKYiIZJaXSUHXKYiIZJaX1aIOH4mIZJaX1aKSgohIZnlZLSopiIhklpfVopKCiEhmeVktKimIiGSWl9Wi/pIqIpJZXicFtRRERJrLy2pR1ymIiGSWl9WiWgoiIpnlZbWopCAiklleVotKCiIimeWqP4WcUlIQyW+7d+9mzZo1VFdX5zqUrCopKWHs2LEUFhZ2+DNKCiKSd9asWcOAAQMYP348tp/+N93d2bx5M2vWrGHChAkd/lxeVou6TkEkv1VXV1NeXr7fJgQAM6O8vLzTraG8TgpqKYjkr/05IaR0ZR7zslrUdQoiIpnlZbWoloKI5NLWrVv52c9+1unPnXvuuWzdujULETXJy2pRSUFEcqmtpFBXV9fu55566ikGDx6crbAA/ftIRPLctdfCwoX79jsnT4abb257/I033sg777zD5MmTKSwspKSkhCFDhrBkyRKWLVvGRRddxOrVq6muruaaa65hxowZAIwfP5758+dTWVnJOeecw0knncRf/vIXxowZw+OPP06/fv32Ova8rBaVFEQkl2bOnMkhhxzCwoUL+eEPf8iCBQv4z//8T5YtWwbAnXfeyWuvvcb8+fO55ZZb2Lx5c6vvWL58OVdddRVvvfUWgwcP5pFHHtknsamlICJ5rb09+u4yderUZtcS3HLLLTz66KMArF69muXLl1NeXt7sMxMmTGDy5MkAHHfccaxcuXKfxJLXSSEP/pEmIr1A//79G18/99xz/PGPf+Tll1+mtLSUU089NeO1BsXFxY2vCwoK2LVr1z6JJS/3lfWXVBHJpQEDBrBjx46M47Zt28aQIUMoLS1lyZIlvPLKK90aW163FJQURCQXysvLOfHEEznqqKPo168fI0eObBx39tlnc/vttzNp0iQmTpzIxz/+8W6NTUlBRCQH7r///ozDi4uLefrppzOOS503GDZsGIsWLWoc/o1vfGOfxZWX1aKSgohIZjlpKZjZSmAHUA/UufsUMxsKPAiMB1YCl7j7lmyUr6QgIpJZLqvF09x9srtPSd7fCMx298OA2cn7rFBSEBHJrCdVixcC9ySv7wEuylZBSgoiIpnlqlp0YJaZvWZmM5JhI919XfJ6PTAy0wfNbIaZzTez+RUVFV0qXNcpiIhklqt/H53k7h+Y2QjgWTNbkj7S3d3MPNMH3f0O4A6AKVOmZJxmT3SdgohIZjmpFt39g+R5I/AoMBXYYGajAJLnjdkqX4ePRCSXunrrbICbb76ZnTt37uOImnR7tWhm/c1sQOo1cCawCHgCuDyZ7HLg8WzFoKQgIrnUk5NCLg4fjQQeTbqJ6wvc7+5/MLNXgYfM7ApgFXBJtgJQUhCRRjm4d3b6rbPPOOMMRowYwUMPPURNTQ2f+tSn+N73vkdVVRWXXHIJa9asob6+nm9/+9ts2LCBtWvXctpppzFs2DDmzJmzb+MmB0nB3d8Fjs4wfDMwvTtiUFIQkVyaOXMmixYtYuHChcyaNYuHH36YefPm4e5ccMEFvPDCC1RUVDB69GiefPJJIO6JNGjQIH784x8zZ84chg0blpXYdJsLEclvOb539qxZs5g1axbHHHMMAJWVlSxfvpxp06bx9a9/nRtuuIHzzjuPadOmdUs8SgoiIjnk7nzzm9/kyiuvbDVuwYIFPPXUU3zrW99i+vTpfOc738l6PHlZLeo6BRHJpfRbZ5911lnceeedVFZWAvDBBx+wceNG1q5dS2lpKV/4whe4/vrrWbBgQavPZkNethR0nYKI5FL6rbPPOeccLrvsMk444QQAysrKuPfee1mxYgXXX389ffr0obCwkNtuuw2AGTNmcPbZZzN69OisnGg29y5d/9UjTJkyxefPn9/pzz3xBNx7L/z611BSkoXARKRHW7x4MZMmTcp1GN0i07ya2Wtp951rJi9bChdcEA8REWlOB1BERKSRkoKI5KXefOi8o7oyj0oKIpJ3SkpK2Lx5836dGNydzZs3U9LJE6d5eU5BRPLb2LFjWbNmDV29/X5vUVJSwtixYzv1GSUFEck7hYWFTJgwIddh9Eg6fCQiIo2UFEREpJGSgoiINOrVVzSbWQXR90JXDAM27cNwcknz0jNpXnomzQsc5O7DM43o1Ulhb5jZ/LYu8+5tNC89k+alZ9K8tE+Hj0REpJGSgoiINMrnpHBHrgPYhzQvPZPmpWfSvLQjb88piIhIa/ncUhARkRaUFEREpFFeJgUzO9vMlprZCjO7MdfxdJaZrTSzv5rZQjObnwwbambPmtny5HlIruPMxMzuNLONZrYobVjG2C3ckqynN83s2NxF3lob83KTmX2QrJuFZnZu2rhvJvOy1MzOyk3UrZnZODObY2Zvm9lbZnZNMrzXrZd25qU3rpcSM5tnZm8k8/K9ZPgEM5ubxPygmRUlw4uT9yuS8eO7VLC759UDKADeAQ4GioA3gCNzHVcn52ElMKzFsH8Hbkxe3wj8INdxthH7ycCxwKI9xQ6cCzwNGPBxYG6u4+/AvNwEfCPDtEcm21oxMCHZBgtyPQ9JbKOAY5PXA4BlSby9br20My+9cb0YUJa8LgTmJsv7IeDSZPjtwD8lr/8ZuD15fSnwYFfKzceWwlRghbu/6+61wG+AC3Mc075wIXBP8voe4KIcxtImd38B+LDF4LZivxD4tYdXgMFmNqp7It2zNualLRcCv3H3Gnd/D1hBbIs55+7r3H1B8noHsBgYQy9cL+3MS1t68npxd69M3hYmDwc+ATycDG+5XlLr62FguplZZ8vNx6QwBlid9n4N7W80PZEDs8zsNTObkQwb6e7rktfrgZG5Ca1L2oq9t66rq5PDKnemHcbrFfOSHHI4htgr7dXrpcW8QC9cL2ZWYGYLgY3As0RLZqu71yWTpMfbOC/J+G1AeWfLzMeksD84yd2PBc4BrjKzk9NHerQfe+V/jXtz7InbgEOAycA64Ee5DafjzKwMeAS41t23p4/rbeslw7z0yvXi7vXuPhkYS7Rgjsh2mfmYFD4AxqW9H5sM6zXc/YPkeSPwKLGxbEg14ZPnjbmLsNPair3XrSt335D8kBuAX9B0KKJHz4uZFRKV6H3u/rtkcK9cL5nmpbeulxR33wrMAU4gDtelOkhLj7dxXpLxg4DNnS0rH5PCq8BhyRn8IuKEzBM5jqnDzKy/mQ1IvQbOBBYR83B5MtnlwOO5ibBL2or9CeCLyb9dPg5sSzuc0SO1OLb+KWLdQMzLpck/RCYAhwHzuju+TJLjzr8CFrv7j9NG9br10ta89NL1MtzMBiev+wFnEOdI5gCfSSZruV5S6+szwJ+SFl7n5PoMey4exL8nlhHH5/411/F0MvaDiX9LvAG8lYqfOHY4G1gO/BEYmutY24j/AaL5vps4HnpFW7ET/774abKe/gpMyXX8HZiX/5vE+mbyIx2VNv2/JvOyFDgn1/GnxXUScWjoTWBh8ji3N66XdualN66XjwKvJzEvAr6TDD+YSFwrgN8CxcnwkuT9imT8wV0pV7e5EBGRRvl4+EhERNqgpCAiIo2UFEREpJGSgoiINFJSEBGRRkoK0iuYmZvZj9Lef8PMbtpH3323mX1mz1PudTmfNbPFZjYn22W1KPcfzOy/urNM6b2UFKS3qAEuNrNhuQ4kXdqVpR1xBfAVdz8tW/GI7C0lBekt6oj+aK9rOaLlnr6ZVSbPp5rZ82b2uJm9a2YzzezvknvU/9XMDkn7mtPNbL6ZLTOz85LPF5jZD83s1eRGalemfe+LZvYE8HaGeD6ffP8iM/tBMuw7xIVVvzKzH2b4zPVp5aTumz/ezJaY2X1JC+NhMytNxk03s9eTcu40s+Jk+MfM7C8W9+Cfl7r6HRhtZn+w6Bvh39Pm7+4kzr+aWatlK/mnM3s5Irn2U+DNVKXWQUcDk4hbXL8L/NLdp1p0vvI14NpkuvHE/XAOAeaY2aHAF4lbOHwsqXRfMrNZyfTHAkd53G65kZmNBn4AHAdsIe5me5G7/5uZfYK4p//8Fp85k7i9wlTiauEnkpscvg9MBK5w95fM7E7gn5NDQXcD0919mZn9GvgnM/sZ8CDwOXd/1cwGAruSYiYTdwytAZaa2a3ACGCMux+VxDG4E8tV9lNqKUiv4XG3y18D/9KJj73qcY/9GuJWBqlK/a9EIkh5yN0b3H05kTyOIO4r9UWLWxfPJW77cFgy/byWCSHxMeA5d6/wuH3xfURnPO05M3m8DixIyk6Vs9rdX0pe30u0NiYC77n7smT4PUkZE4F17v4qxPLyplssz3b3be5eTbRuDkrm82Azu9XMzgaa3RlV8pNaCtLb3ExUnHelDasj2cExsz5Ej3opNWmvG9LeN9B8+295vxcn9tq/5u7PpI8ws1OBqq6Fn5EB/8fdf96inPFtxNUV6cuhHujr7lvM7GjgLOCrwCXAl7v4/bKfUEtBehV3/5DojvCKtMEricM1ABcQPVR11mfNrE9ynuFg4uZozxCHZQoBzOzw5M607ZkHnGJmw8ysAPg88PwePvMM8GWLPgAwszFmNiIZd6CZnZC8vgz4cxLb+OQQF8DfJ2UsBUaZ2ceS7xnQ3onw5KR9H3d/BPgWcUhM8pxaCtIb/Qi4Ou39L4DHzewN4A90bS/+faJCHwh81d2rzeyXxCGmBcktmSvYQzen7r7OzG4kbm9swJPu3u5tzN19lplNAl6OYqgEvkDs0S8lOlK6kzjsc1sS25eA3yaV/qtE37y1ZvY54NbkVsu7gNPbKXoMcFfSugL4ZntxSn7QXVJFeqjk8NHvUyeCRbqDDh+JiEgjtRRERKSRWgoiItJISUFERBopKYiISCMlBRERaaSkICIijf4/w0ELvVFp/NkAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : Step\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c505b0ec-1101-4a02-eb63-9b72a48093bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_step = {train_loss_list}\") \n",
        "print(f\"train_acc_list_step = {train_acc_list}\")\n",
        "print(f\"test_loss_list_step = {test_loss_list}\")\n",
        "print(f\"test_acc_list_step = {test_acc_list}\")"
      ],
      "metadata": {
        "id": "3eiY3bTlWipW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c6c9bc-136a-4856-9617-5c8c026a01e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_step = [1.5044204033810271, 0.49208608810817644, 0.37830603914209177, 0.33356322168415475, 0.30051283241529775, 0.2731129997548695, 0.25931339448464275, 0.24751879757416603, 0.23341218238762076, 0.22551919120114025, 0.21245744068246225, 0.20461816869696303, 0.19351729513385754, 0.18888680500591673, 0.1790498158513369, 0.17313903582794718, 0.16563977062863708, 0.15938490460599017, 0.1541340289319434, 0.14407484958246147, 0.13983443950815252, 0.13245242257630277, 0.12638695052443805, 0.12125968426304458, 0.11788717211605249, 0.11147532753424269, 0.10493720971996869, 0.10114793143257862, 0.0986312177905505, 0.09325535373060603, 0.06156983597173802, 0.050082290781091464, 0.04566037622696864, 0.042090283896077454, 0.037587769078418896, 0.03525592588446025, 0.034040165755286976, 0.031216950466235478, 0.030938032096463855, 0.029107897011765093, 0.02691679777953582, 0.024817724369887797, 0.02327188092970408, 0.022008026162917072, 0.021083637177603455, 0.020199724932410693, 0.020445302296316296, 0.01811823503557229, 0.016885934063002497, 0.016305384605991728, 0.015734232263797315, 0.01582022506202922, 0.01498541162761943, 0.013855328222454038, 0.012501976389670114, 0.011526370763357194, 0.012135067156394237, 0.011748958516110058, 0.01063293406628148, 0.010577014861956521, 0.009060478683824204, 0.00822005160518879, 0.007903504668233972, 0.0073660259777244665, 0.007953700653634155, 0.006759841606805863, 0.0069558188274406844, 0.007026840372753726, 0.006894602971896024, 0.0063807274687209935, 0.006584282455781946, 0.006663436767551165, 0.006154599042466102, 0.006103539849245962, 0.006048463034234704, 0.00614482418276391, 0.006322457585395453, 0.006028014029933677, 0.006245754332408476, 0.00581059281254909, 0.005709030747089944, 0.005923201890116833, 0.00537758718227695, 0.005807922808844647, 0.005243748521360034, 0.004974289045318631, 0.005323306708443634, 0.005775554073696229, 0.005093053230784864, 0.005536427160324011, 0.0052108629591284105, 0.004749226128792285, 0.005127630246161776, 0.005065120122888249, 0.004685268356975264, 0.005243820296808683, 0.0049121043785080205, 0.005093204319377071, 0.005370734221114888, 0.005008859195898547, 0.005249464887712999, 0.004638611593474268, 0.004728934781005616, 0.005035177127017733, 0.004871168427265863, 0.005197190184529169, 0.004969428170602996, 0.004836069228517858, 0.005308753678475464, 0.004877718586694237, 0.0046394178739258, 0.004715173941094246, 0.005071522070231865, 0.005326596872854554, 0.004782932753934018, 0.00452435254143787, 0.005128751716483132, 0.004405792564614916, 0.005072576416472445, 0.004683375086275378, 0.004745950062742184, 0.005199869099014116, 0.004980171865863766, 0.004840858398257126, 0.0046297258561474545, 0.004515150684752434, 0.005211585615480793, 0.0046955638231178315, 0.004485690257187711, 0.004824536544108968, 0.004908301922912914, 0.005288558260384356, 0.004324457162535601, 0.004497020012851448, 0.004832310812420976, 0.004629586771150122, 0.00469785524222389, 0.0051049072852895816, 0.004760202388700317, 0.00467975922823293, 0.004756802660750659, 0.004577458516745399, 0.0046352100029977686, 0.0045855785586362005, 0.0045302773612948275, 0.00527981271794404, 0.004842230399004512, 0.005128007588957292, 0.0047336897144657575, 0.00452429031405685, 0.004871112167589785, 0.004707888855899686, 0.004987144301444176, 0.004576647475924656, 0.0050210763291275344, 0.004869554478370365, 0.005145938421950075, 0.0049324849194394005, 0.005007630027365255, 0.004694439974090723, 0.005161294266647788, 0.005086089323139447, 0.0045723088694465674, 0.0049315163145030455, 0.005043683056833177, 0.004782498019489051, 0.005208640262013842, 0.004906343821498318, 0.004504839811582297, 0.004765471186647242, 0.004636792729359163, 0.0048650642755604795, 0.00440384292277286, 0.005331020985203442, 0.004309070902481564, 0.005132160800841049, 0.0048202084057818, 0.0042395592619208135, 0.004579358937877841, 0.004854094481356265, 0.004861267090400903, 0.004641431140822869, 0.004752275634260134, 0.005280723348089562, 0.0049866622946490744, 0.004713922302353166, 0.0049851976106240516, 0.0049522482596579456, 0.0047414097434517405, 0.005198805679831853, 0.005367297160049249, 0.004801289511232152, 0.004936103088396973, 0.004513620775553264, 0.005461517685133368, 0.005038318795404084, 0.004621399451298942, 0.004861979541443869, 0.004728190580562456, 0.00424972596940481, 0.004389661508080736, 0.004675524760544651, 0.004678315578823907, 0.005468495250993464, 0.004904287783485815, 0.004628283689890138, 0.004957924133196958, 0.004207717516179686, 0.004497190112673026, 0.0054004669855449725, 0.004738672002041044, 0.004832978636525801, 0.004593934473861766, 0.00428500973297249, 0.004800252157330026, 0.004579809123577946, 0.0047766751335504105, 0.004927973183699149, 0.003962443585616099, 0.005313499298157754, 0.005448946752569696, 0.004626204171514537, 0.00486521731347835, 0.00464669697920995, 0.00473364783223967, 0.0051715146905747635, 0.004559630797677459, 0.004807513805560901, 0.004414798407140251, 0.0055058369074104515, 0.004735360263094689, 0.00523965430700253, 0.004729446927025576, 0.0046202187999946675, 0.004473174700221971, 0.005083680975845281, 0.00473606128646736, 0.004358955291625135, 0.004462744230992882, 0.00445035025452959, 0.004661599569843557, 0.004881452044679059, 0.004753526896741532, 0.004433994836329803, 0.004789544894031266, 0.004920090539939323, 0.005374149948520697, 0.004787621462456112, 0.004773729829575745, 0.0044936602159881545, 0.004565336937149469, 0.004793149001421886, 0.004954056282359589, 0.00447947610881707, 0.004911790105014495, 0.004962967739427566, 0.004856595992693377, 0.0049429370942105965, 0.004495315723300048, 0.005116432885005868, 0.004663471084030195, 0.005105866048162296, 0.0048063792890729005, 0.004996042520567828, 0.0048235019935209174, 0.0052012274302681025, 0.004979272351725164, 0.004979745390991236, 0.005170751338667516, 0.004858217863982788, 0.004663840651548111, 0.005053441917670759, 0.004584236903877065, 0.004780888221675258, 0.004612757356412744, 0.004703525326821731, 0.005128545914503725, 0.005097627184756964, 0.004888794860495936, 0.004546376856262884, 0.0049637655482588335, 0.004738951851973477, 0.005256142429007358, 0.004507868475276601, 0.004676521270572799, 0.0042853716939203765, 0.004519075942008067, 0.004505532296253924, 0.004830542344546013, 0.004257976696643534, 0.004718432969060035, 0.004490681710655531, 0.004793198622190861, 0.004690485046958793, 0.004381172440937899, 0.004597538895479108, 0.0050219146489527905, 0.004799934776746264, 0.004984567811975109, 0.004093347564194513]\n",
            "train_acc_list_step = [47.394388565378506, 84.38962413975648, 88.32609846479619, 89.65802011646373, 90.93912122816305, 91.76071995764956, 92.30492323980943, 92.69878242456326, 93.1371095817893, 93.283218634198, 93.84859714134463, 93.97988353626258, 94.4203282159873, 94.56643726839597, 94.83324510322922, 95.06405505558496, 95.14452091053468, 95.38168343038645, 95.61461090524087, 95.86871360508205, 95.94282689253573, 96.23928004235044, 96.33456855479089, 96.51667548967708, 96.58231868713605, 96.79618845950239, 96.97829539438857, 97.11805187930122, 97.2006352567496, 97.23451561672843, 98.32927474854421, 98.71254632080466, 98.81630492323981, 98.94970884065643, 99.04499735309687, 99.05558496559026, 99.12969825304394, 99.22075172048703, 99.16993118051879, 99.24616199047115, 99.32027527792482, 99.33721545791424, 99.38168343038645, 99.40285865537321, 99.45156167284277, 99.46426680783483, 99.43885653785071, 99.49814716781366, 99.51932239280042, 99.55320275277924, 99.58073054526204, 99.56379036527264, 99.56379036527264, 99.6209634727369, 99.65907887771307, 99.68237162519851, 99.61884595023822, 99.62731604023293, 99.69507676019057, 99.72260455267337, 99.73742721016411, 99.7564849126522, 99.76919004764426, 99.79883536262572, 99.7649550026469, 99.83906829010058, 99.80307040762308, 99.81154049761778, 99.81154049761778, 99.83271572260455, 99.8009528851244, 99.82424563260984, 99.8369507676019, 99.82424563260984, 99.83906829010058, 99.83483324510323, 99.8369507676019, 99.84965590259397, 99.82636315510852, 99.83059820010588, 99.83906829010058, 99.84118581259926, 99.86236103758603, 99.85600847008999, 99.86236103758603, 99.87083112758073, 99.8369507676019, 99.8284806776072, 99.84753838009529, 99.85600847008999, 99.85389094759132, 99.87083112758073, 99.85812599258867, 99.86024351508735, 99.88353626257279, 99.85600847008999, 99.87506617257809, 99.85600847008999, 99.86236103758603, 99.84753838009529, 99.85600847008999, 99.87930121757543, 99.86236103758603, 99.8644785600847, 99.85177342509265, 99.86024351508735, 99.85389094759132, 99.85812599258867, 99.84330333509793, 99.86024351508735, 99.87718369507677, 99.86871360508205, 99.85812599258867, 99.84753838009529, 99.86659608258337, 99.87083112758073, 99.85389094759132, 99.88988883006881, 99.86024351508735, 99.87718369507677, 99.86659608258337, 99.84118581259926, 99.87083112758073, 99.85812599258867, 99.88777130757015, 99.88988883006881, 99.84542085759661, 99.87294865007941, 99.87083112758073, 99.85177342509265, 99.86024351508735, 99.86236103758603, 99.87506617257809, 99.8644785600847, 99.86024351508735, 99.86659608258337, 99.86871360508205, 99.85600847008999, 99.87083112758073, 99.87294865007941, 99.87294865007941, 99.88353626257279, 99.86236103758603, 99.87930121757543, 99.87930121757543, 99.86024351508735, 99.86024351508735, 99.83483324510323, 99.8644785600847, 99.89200635256749, 99.87083112758073, 99.84965590259397, 99.84965590259397, 99.88141874007411, 99.84753838009529, 99.87506617257809, 99.85389094759132, 99.85812599258867, 99.86236103758603, 99.85600847008999, 99.85600847008999, 99.84118581259926, 99.87930121757543, 99.86024351508735, 99.85812599258867, 99.86871360508205, 99.85389094759132, 99.86236103758603, 99.87718369507677, 99.85812599258867, 99.87083112758073, 99.85600847008999, 99.87506617257809, 99.87506617257809, 99.88777130757015, 99.85812599258867, 99.85600847008999, 99.88777130757015, 99.86659608258337, 99.87718369507677, 99.86659608258337, 99.86659608258337, 99.88141874007411, 99.85177342509265, 99.85600847008999, 99.87718369507677, 99.86236103758603, 99.87930121757543, 99.87506617257809, 99.85177342509265, 99.84118581259926, 99.86236103758603, 99.85812599258867, 99.87506617257809, 99.82636315510852, 99.85600847008999, 99.87718369507677, 99.88353626257279, 99.87294865007941, 99.89624139756485, 99.88353626257279, 99.87718369507677, 99.87083112758073, 99.84542085759661, 99.8644785600847, 99.88141874007411, 99.85600847008999, 99.89412387506617, 99.88141874007411, 99.85177342509265, 99.85389094759132, 99.87930121757543, 99.88565378507147, 99.88353626257279, 99.86871360508205, 99.86871360508205, 99.8644785600847, 99.86871360508205, 99.89624139756485, 99.84330333509793, 99.84542085759661, 99.88353626257279, 99.86236103758603, 99.87930121757543, 99.87506617257809, 99.87083112758073, 99.87294865007941, 99.87294865007941, 99.86871360508205, 99.84330333509793, 99.86659608258337, 99.84542085759661, 99.87506617257809, 99.8644785600847, 99.89200635256749, 99.84542085759661, 99.86871360508205, 99.87506617257809, 99.88141874007411, 99.87083112758073, 99.87083112758073, 99.86659608258337, 99.86659608258337, 99.86871360508205, 99.88565378507147, 99.87083112758073, 99.84753838009529, 99.88777130757015, 99.87718369507677, 99.88353626257279, 99.87083112758073, 99.86236103758603, 99.87718369507677, 99.88777130757015, 99.84965590259397, 99.85600847008999, 99.86236103758603, 99.85812599258867, 99.87718369507677, 99.8644785600847, 99.86024351508735, 99.87506617257809, 99.8644785600847, 99.8644785600847, 99.87930121757543, 99.85177342509265, 99.86024351508735, 99.85177342509265, 99.84118581259926, 99.86871360508205, 99.87718369507677, 99.86024351508735, 99.88988883006881, 99.8644785600847, 99.88141874007411, 99.87506617257809, 99.85177342509265, 99.86871360508205, 99.87506617257809, 99.87506617257809, 99.87930121757543, 99.86871360508205, 99.85812599258867, 99.88988883006881, 99.88353626257279, 99.89412387506617, 99.87930121757543, 99.86659608258337, 99.86659608258337, 99.88988883006881, 99.87083112758073, 99.87506617257809, 99.87083112758073, 99.87718369507677, 99.87930121757543, 99.87083112758073, 99.84330333509793, 99.87718369507677, 99.87294865007941, 99.88565378507147]\n",
            "test_loss_list_step = [0.7521707053278007, 0.41922322909037274, 0.3967930445191907, 0.35680990534670215, 0.3250766685049908, 0.30643289908766747, 0.27803952721696273, 0.26461896110399097, 0.26403681638047977, 0.2668783114309989, 0.2609864034708224, 0.25131638298797254, 0.24851202544774495, 0.24535505795011334, 0.23515790532909187, 0.23713360856488055, 0.2406249167215006, 0.24633904404061682, 0.22870721204169825, 0.23081522924350759, 0.23210598513776182, 0.23380596216256713, 0.23264140811036615, 0.2492445185597913, 0.24672274596477842, 0.23322659500819795, 0.2332557491848574, 0.2325650899324055, 0.2574915187433362, 0.23942345477567584, 0.21860766188953729, 0.22490006466122234, 0.22599033247588166, 0.2330661828027052, 0.23813083913980745, 0.2410290090677639, 0.25265564182408007, 0.25388093285408675, 0.25897152009694013, 0.2653597001095905, 0.267472947769634, 0.2757880237756991, 0.28352711534164116, 0.2891207381023788, 0.2937827925471699, 0.3016136303891008, 0.29683313752506296, 0.3059604737939605, 0.31242154799766986, 0.313900034299449, 0.3219801964347853, 0.33177046875889393, 0.3355082166837711, 0.33843776536192377, 0.34364481102309974, 0.3455076480613035, 0.34563379169569586, 0.35321818167051555, 0.35429473890576, 0.35551488774317297, 0.36010582940470354, 0.35364405492631096, 0.3636273516743791, 0.36364757276012327, 0.35665451670887277, 0.3549169595261999, 0.3703063836905594, 0.36219835961146246, 0.3605355029998749, 0.3578162646861564, 0.36774474406140106, 0.3633771698702784, 0.3653042366746448, 0.3691165424956411, 0.36504066177625577, 0.3715900047225695, 0.377250660302154, 0.3705682765610297, 0.3703488810170515, 0.37351591648606985, 0.37677567382343113, 0.37160208523638694, 0.3709423184650494, 0.3721590631769276, 0.3763345616011351, 0.3800408639488559, 0.37680745827874135, 0.3771674718214747, 0.3848353975142042, 0.3798498407590623, 0.3773468048494382, 0.3819653425073507, 0.3785710880149375, 0.37396970703654614, 0.37354924148131236, 0.38423583746029466, 0.3775230319622685, 0.36990976165614875, 0.3731101816017911, 0.3812584620443921, 0.3834761079017292, 0.38286398570327196, 0.3858671422372116, 0.3799955192816389, 0.3770175949378195, 0.3851937071338077, 0.38235137271968755, 0.38205398502303106, 0.38025479872400564, 0.3843826920378442, 0.38355313671533675, 0.3811237645043316, 0.37951354816665545, 0.3756716687428564, 0.38028262046110983, 0.38163883488296585, 0.38113248269712807, 0.38293040138395396, 0.38826611508414444, 0.3821931903378344, 0.3927168071534777, 0.3814124613087259, 0.38783683029788674, 0.384523262598497, 0.3777104146395098, 0.3834854347001323, 0.37351655306331083, 0.37643494215958256, 0.38091917734081837, 0.3816218973667014, 0.38416977602915436, 0.3851156278796421, 0.3824646536297366, 0.37491797317988146, 0.3818769982979432, 0.37861419655382633, 0.3842346754938583, 0.37655236199498177, 0.37527537754024654, 0.37987153332077844, 0.3830455071018899, 0.3794875234803733, 0.38347619624041457, 0.39262797689868834, 0.37537328259763764, 0.3836479137067263, 0.3833252173236699, 0.37210422399563386, 0.387080483238998, 0.3719306717816211, 0.37743640282446994, 0.3764378684846794, 0.37271541687568616, 0.37936363588361177, 0.3804472682075392, 0.371879458409168, 0.38374935142586336, 0.3781576618488294, 0.3732976428844838, 0.3825277503210065, 0.37921793430167083, 0.38190348801550034, 0.3811836604795912, 0.3839620574760963, 0.3847266497711341, 0.3755092674263698, 0.38182204500680755, 0.3782161524446279, 0.383978412504874, 0.3785792852560168, 0.37529283202728075, 0.38586872027200814, 0.37986113718144743, 0.3781509461410928, 0.385330743640296, 0.3866525995362477, 0.3780062812672672, 0.38378071400574315, 0.38093069549102115, 0.38920295537065935, 0.38671026251041424, 0.3797818386251582, 0.3798417299438049, 0.3740828680422376, 0.38145504804218516, 0.38200287843196123, 0.3875016655645096, 0.38280217152308016, 0.38698648213974984, 0.3808485120260978, 0.38649215569317924, 0.3831135995238654, 0.3805198727321683, 0.38389426964682105, 0.38486584148132336, 0.3807648518761876, 0.3849117392856701, 0.38554816227406263, 0.38357163151251333, 0.3802410184734446, 0.3815109624947403, 0.382021243195506, 0.3853015545938237, 0.377494075719048, 0.38711871966427447, 0.3774435311857173, 0.3905071147407095, 0.3806642469603057, 0.38146250298721535, 0.3838621310104488, 0.39097126777849944, 0.38649382303450625, 0.3796885114930132, 0.38229777593183895, 0.3814927838633166, 0.37877432031410874, 0.37605211639082897, 0.37402715621625676, 0.3860787514177169, 0.3734693876285033, 0.3757060868665576, 0.3836308258823028, 0.3818031575019453, 0.3754675344144012, 0.3817046451035376, 0.3788763102984019, 0.38524902579100695, 0.3789480691410455, 0.383543092720941, 0.39350814419780294, 0.3787268260402568, 0.3791520138837251, 0.3838299135951435, 0.3793616016559741, 0.3767985271874304, 0.3790097143866268, 0.37912912847583785, 0.38894346707007466, 0.3742955804382469, 0.38342078903909116, 0.3848988146238102, 0.37459168269061577, 0.38242888645561157, 0.3820874952601598, 0.38645922241951614, 0.37736037103276626, 0.37371054134450343, 0.3787742013148233, 0.3864014833873394, 0.38398269388605566, 0.3823276355716528, 0.38692381239368345, 0.38753888685731036, 0.37855833295878827, 0.37346514290673477, 0.37298866356814314, 0.3816173101830132, 0.3767242920311058, 0.3841047031056209, 0.38750297252965327, 0.369205586767445, 0.3753658247490724, 0.38275528518373475, 0.3800859667141648, 0.3870932775020015, 0.37551862878414494, 0.3840912418421723, 0.3726249620291021, 0.37941730108486454, 0.38384662088298915, 0.37976392629720707, 0.37775822057772207, 0.38208118103006306, 0.3800909294399853, 0.37907081163104844, 0.3812881442672555, 0.3761177863743083, 0.38076628101350485, 0.37963548230518607, 0.3693312196718419, 0.37269937157557875, 0.3810739266987452, 0.37098004634254705, 0.38030854867332997, 0.3809964715083148, 0.37681868622152537, 0.3750186215574835, 0.3816620926098788, 0.3736031973351012, 0.38245252669588026, 0.3819549302798787, 0.38419706475756626, 0.3735217913966991, 0.3787470100191878, 0.37715579814040195, 0.3867923977976555, 0.3815506379960068, 0.3814866198135503, 0.3853403181933305, 0.3877864530602214]\n",
            "test_acc_list_step = [75.81054087277197, 86.9660417947142, 87.58451137062077, 89.28242163491088, 90.1160110633067, 91.05331899200984, 91.73325138291334, 92.37092808850646, 92.390135218193, 92.24031960663798, 92.37092808850646, 92.9778733866011, 93.1737861094038, 93.04701905347265, 93.41963736939152, 93.37738168408113, 93.3159188690842, 93.0278119237861, 93.7077443146896, 93.72695144437616, 93.68853718500307, 93.78073140749846, 93.73847572218807, 93.26213890596189, 93.30055316533497, 93.93438844499079, 94.06883835279656, 94.11877688998156, 93.6040258143823, 94.0419483712354, 94.87553779963122, 94.83712354025815, 94.81023355869699, 94.87169637369392, 94.91779348494161, 94.89474492931777, 94.86785494775661, 94.84096496619546, 94.71419791026429, 94.56054087277197, 94.75645359557468, 94.73724646588813, 94.64889366933005, 94.64889366933005, 94.69499078057775, 94.72956361401353, 94.6757836508912, 94.74492931776275, 94.6220036877689, 94.65273509526736, 94.54901659496005, 94.53749231714812, 94.63352796558083, 94.61816226183159, 94.61432083589429, 94.5221266133989, 94.64121081745544, 94.56054087277197, 94.74492931776275, 94.61047940995698, 94.59127228027043, 94.75645359557468, 94.75645359557468, 94.66425937307929, 94.73340503995082, 94.6757836508912, 94.53365089121081, 94.64505224339274, 94.66041794714198, 94.70267363245236, 94.6258451137062, 94.75645359557468, 94.80255070682237, 94.67962507682851, 94.58358942839583, 94.54517516902274, 94.6258451137062, 94.7180393362016, 94.66425937307929, 94.6681007990166, 94.61432083589429, 94.78718500307313, 94.76029502151198, 94.68730792870313, 94.64121081745544, 94.62968653964352, 94.69114935464044, 94.58743085433314, 94.58358942839583, 94.64889366933005, 94.69114935464044, 94.70651505838967, 94.70267363245236, 94.66041794714198, 94.69883220651506, 94.61432083589429, 94.71419791026429, 94.82559926244622, 94.69883220651506, 94.72572218807622, 94.49523663183774, 94.75261216963737, 94.61047940995698, 94.64505224339274, 94.64889366933005, 94.66041794714198, 94.70267363245236, 94.55669944683467, 94.65657652120467, 94.6681007990166, 94.77566072526122, 94.85633066994468, 94.66041794714198, 94.78334357713584, 94.79102642901044, 94.63352796558083, 94.78718500307313, 94.64505224339274, 94.56438229870928, 94.73340503995082, 94.61047940995698, 94.68730792870313, 94.70267363245236, 94.61432083589429, 94.67194222495391, 94.77181929932391, 94.73724646588813, 94.64505224339274, 94.69499078057775, 94.65273509526736, 94.69883220651506, 94.6757836508912, 94.71035648432698, 94.69883220651506, 94.67194222495391, 94.66425937307929, 94.6757836508912, 94.76029502151198, 94.75645359557468, 94.70267363245236, 94.64505224339274, 94.74108789182544, 94.54133374308543, 94.54901659496005, 94.74877074370006, 94.67962507682851, 94.67194222495391, 94.67194222495391, 94.69883220651506, 94.8140749846343, 94.64121081745544, 94.7180393362016, 94.74492931776275, 94.78334357713584, 94.65273509526736, 94.5759065765212, 94.61816226183159, 94.61047940995698, 94.76413644744929, 94.79870928088506, 94.67962507682851, 94.69114935464044, 94.72572218807622, 94.60663798401967, 94.5720651505839, 94.75261216963737, 94.80255070682237, 94.70267363245236, 94.66041794714198, 94.66425937307929, 94.69114935464044, 94.72188076213891, 94.68730792870313, 94.55285802089736, 94.75645359557468, 94.64505224339274, 94.7180393362016, 94.53749231714812, 94.72572218807622, 94.65657652120467, 94.6258451137062, 94.61432083589429, 94.72188076213891, 94.77566072526122, 94.62968653964352, 94.66041794714198, 94.54901659496005, 94.6757836508912, 94.66425937307929, 94.55669944683467, 94.7679778733866, 94.74877074370006, 94.68346650276582, 94.63352796558083, 94.5759065765212, 94.61047940995698, 94.66425937307929, 94.71419791026429, 94.59895513214505, 94.61816226183159, 94.72956361401353, 94.7180393362016, 94.67194222495391, 94.68346650276582, 94.59127228027043, 94.76029502151198, 94.69499078057775, 94.66041794714198, 94.6681007990166, 94.69499078057775, 94.64889366933005, 94.67962507682851, 94.78718500307313, 94.67962507682851, 94.77950215119853, 94.6757836508912, 94.74492931776275, 94.84096496619546, 94.73340503995082, 94.72188076213891, 94.69883220651506, 94.61047940995698, 94.5720651505839, 94.72572218807622, 94.70651505838967, 94.6681007990166, 94.6258451137062, 94.64121081745544, 94.65657652120467, 94.48371235402581, 94.6757836508912, 94.73724646588813, 94.59895513214505, 94.70651505838967, 94.76029502151198, 94.75645359557468, 94.76413644744929, 94.6220036877689, 94.72188076213891, 94.7180393362016, 94.66425937307929, 94.6681007990166, 94.62968653964352, 94.71419791026429, 94.68346650276582, 94.69883220651506, 94.69114935464044, 94.70267363245236, 94.60663798401967, 94.67194222495391, 94.55285802089736, 94.59895513214505, 94.66041794714198, 94.61432083589429, 94.59895513214505, 94.73724646588813, 94.63352796558083, 94.73724646588813, 94.77566072526122, 94.60279655808236, 94.76413644744929, 94.74108789182544, 94.65273509526736, 94.58743085433314, 94.59511370620774, 94.80255070682237, 94.61047940995698, 94.71035648432698, 94.65273509526736, 94.6681007990166, 94.75261216963737, 94.72188076213891, 94.61816226183159, 94.65657652120467, 94.79870928088506, 94.64121081745544, 94.66425937307929, 94.69883220651506, 94.64889366933005, 94.83712354025815, 94.69883220651506, 94.71419791026429, 94.77181929932391, 94.63352796558083, 94.74492931776275, 94.69883220651506, 94.61816226183159, 94.73340503995082, 94.73724646588813, 94.67194222495391, 94.72956361401353, 94.69114935464044, 94.77181929932391, 94.65657652120467, 94.68346650276582, 94.60663798401967, 94.64889366933005, 94.72188076213891, 94.70651505838967, 94.70267363245236]\n"
          ]
        }
      ]
    }
  ]
}