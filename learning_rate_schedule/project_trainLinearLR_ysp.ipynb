{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e14be1a884ab45b8add1610551e09fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_176b9df4a60d4af0b9182515888b9c98",
              "IPY_MODEL_ed3d77cb1a484476a2da3be787d82aee",
              "IPY_MODEL_6ee540cd75394c6798893971e0ca0ba9"
            ],
            "layout": "IPY_MODEL_1fcc0f34fac14353b7891c932d1d9c40"
          }
        },
        "176b9df4a60d4af0b9182515888b9c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8586c2d318d4b16a79440ce9ada27f3",
            "placeholder": "​",
            "style": "IPY_MODEL_2c22802709604e8bbc24abe6f57f9153",
            "value": "100%"
          }
        },
        "ed3d77cb1a484476a2da3be787d82aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b51ec16feec04a1a9fa6ba295e89c8cf",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccd3910921a54b879692aced89502c94",
            "value": 182040794
          }
        },
        "6ee540cd75394c6798893971e0ca0ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aaf356942974bafbae00709518642ca",
            "placeholder": "​",
            "style": "IPY_MODEL_21735001403240cb947a4a1916459927",
            "value": " 182040794/182040794 [00:10&lt;00:00, 13252305.78it/s]"
          }
        },
        "1fcc0f34fac14353b7891c932d1d9c40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8586c2d318d4b16a79440ce9ada27f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c22802709604e8bbc24abe6f57f9153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b51ec16feec04a1a9fa6ba295e89c8cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccd3910921a54b879692aced89502c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5aaf356942974bafbae00709518642ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21735001403240cb947a4a1916459927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b4442a4aff4467ea96d1a9b48eb13e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dafa55a9b2ec49f0bc688f848e6aed91",
              "IPY_MODEL_4c2ded1116b04dd78a77551b8aa79380",
              "IPY_MODEL_bede0f0281d54e4ba19051a5e728a788"
            ],
            "layout": "IPY_MODEL_9a51ea636aca4c48ade66e8070a80e9e"
          }
        },
        "dafa55a9b2ec49f0bc688f848e6aed91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_097dcc7d6cb546d38f4800b9bc91b447",
            "placeholder": "​",
            "style": "IPY_MODEL_627fa52859014a978c66da7e54ff3a31",
            "value": "100%"
          }
        },
        "4c2ded1116b04dd78a77551b8aa79380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_186b7157b5604785bc59b35fd80ab85d",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3291acdd4f6b4c3abf475042e4b4033b",
            "value": 64275384
          }
        },
        "bede0f0281d54e4ba19051a5e728a788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a4d711676764e8d9885dfefae775b9b",
            "placeholder": "​",
            "style": "IPY_MODEL_901cd98efa0c47fb99dbd2c07ec89b1a",
            "value": " 64275384/64275384 [00:04&lt;00:00, 13537176.15it/s]"
          }
        },
        "9a51ea636aca4c48ade66e8070a80e9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "097dcc7d6cb546d38f4800b9bc91b447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "627fa52859014a978c66da7e54ff3a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "186b7157b5604785bc59b35fd80ab85d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3291acdd4f6b4c3abf475042e4b4033b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a4d711676764e8d9885dfefae775b9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "901cd98efa0c47fb99dbd2c07ec89b1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "e14be1a884ab45b8add1610551e09fc8",
            "176b9df4a60d4af0b9182515888b9c98",
            "ed3d77cb1a484476a2da3be787d82aee",
            "6ee540cd75394c6798893971e0ca0ba9",
            "1fcc0f34fac14353b7891c932d1d9c40",
            "a8586c2d318d4b16a79440ce9ada27f3",
            "2c22802709604e8bbc24abe6f57f9153",
            "b51ec16feec04a1a9fa6ba295e89c8cf",
            "ccd3910921a54b879692aced89502c94",
            "5aaf356942974bafbae00709518642ca",
            "21735001403240cb947a4a1916459927",
            "4b4442a4aff4467ea96d1a9b48eb13e9",
            "dafa55a9b2ec49f0bc688f848e6aed91",
            "4c2ded1116b04dd78a77551b8aa79380",
            "bede0f0281d54e4ba19051a5e728a788",
            "9a51ea636aca4c48ade66e8070a80e9e",
            "097dcc7d6cb546d38f4800b9bc91b447",
            "627fa52859014a978c66da7e54ff3a31",
            "186b7157b5604785bc59b35fd80ab85d",
            "3291acdd4f6b4c3abf475042e4b4033b",
            "9a4d711676764e8d9885dfefae775b9b",
            "901cd98efa0c47fb99dbd2c07ec89b1a"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "6a8082bb-e828-488e-d774-2efbfc2608df"
      },
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e14be1a884ab45b8add1610551e09fc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b4442a4aff4467ea96d1a9b48eb13e9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "bebcafc9-1b10-4fcb-e3eb-04079bcde5a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "cf545a13-451a-4d28-d13c-04761b3731cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "136b989f-2759-44a2-fb46-55be84f392b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArgupDVRwB8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f89d88-f374-421e-9c0f-68e4e6ed811b"
      },
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.2629, accuracy : 19.09\n",
            "iteration : 100, loss : 2.0071, accuracy : 28.93\n",
            "iteration : 150, loss : 1.7155, accuracy : 39.86\n",
            "iteration : 200, loss : 1.4762, accuracy : 48.68\n",
            "iteration : 250, loss : 1.2985, accuracy : 55.09\n",
            "iteration : 300, loss : 1.1735, accuracy : 59.54\n",
            "iteration : 350, loss : 1.0810, accuracy : 62.96\n",
            "Epoch :   1, training loss : 1.0505, training accuracy : 64.05, test loss : 0.6002, test accuracy : 81.17\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.4500, accuracy : 86.11\n",
            "iteration : 100, loss : 0.4446, accuracy : 86.12\n",
            "iteration : 150, loss : 0.4345, accuracy : 86.31\n",
            "iteration : 200, loss : 0.4331, accuracy : 86.35\n",
            "iteration : 250, loss : 0.4196, accuracy : 86.78\n",
            "iteration : 300, loss : 0.4127, accuracy : 87.09\n",
            "iteration : 350, loss : 0.4123, accuracy : 87.10\n",
            "Epoch :   2, training loss : 0.4103, training accuracy : 87.15, test loss : 0.4387, test accuracy : 86.57\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.3501, accuracy : 88.92\n",
            "iteration : 100, loss : 0.3578, accuracy : 88.84\n",
            "iteration : 150, loss : 0.3490, accuracy : 89.22\n",
            "iteration : 200, loss : 0.3443, accuracy : 89.33\n",
            "iteration : 250, loss : 0.3408, accuracy : 89.48\n",
            "iteration : 300, loss : 0.3417, accuracy : 89.49\n",
            "iteration : 350, loss : 0.3408, accuracy : 89.52\n",
            "Epoch :   3, training loss : 0.3402, training accuracy : 89.55, test loss : 0.3231, test accuracy : 90.26\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3134, accuracy : 90.44\n",
            "iteration : 100, loss : 0.3094, accuracy : 90.53\n",
            "iteration : 150, loss : 0.3017, accuracy : 90.76\n",
            "iteration : 200, loss : 0.3070, accuracy : 90.66\n",
            "iteration : 250, loss : 0.3079, accuracy : 90.65\n",
            "iteration : 300, loss : 0.3058, accuracy : 90.69\n",
            "iteration : 350, loss : 0.3036, accuracy : 90.77\n",
            "Epoch :   4, training loss : 0.3039, training accuracy : 90.76, test loss : 0.3437, test accuracy : 89.47\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.2614, accuracy : 91.69\n",
            "iteration : 100, loss : 0.2686, accuracy : 91.62\n",
            "iteration : 150, loss : 0.2729, accuracy : 91.50\n",
            "iteration : 200, loss : 0.2777, accuracy : 91.35\n",
            "iteration : 250, loss : 0.2807, accuracy : 91.38\n",
            "iteration : 300, loss : 0.2763, accuracy : 91.55\n",
            "iteration : 350, loss : 0.2769, accuracy : 91.60\n",
            "Epoch :   5, training loss : 0.2767, training accuracy : 91.62, test loss : 0.3047, test accuracy : 90.87\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.2607, accuracy : 92.61\n",
            "iteration : 100, loss : 0.2611, accuracy : 92.31\n",
            "iteration : 150, loss : 0.2612, accuracy : 92.24\n",
            "iteration : 200, loss : 0.2571, accuracy : 92.36\n",
            "iteration : 250, loss : 0.2593, accuracy : 92.21\n",
            "iteration : 300, loss : 0.2571, accuracy : 92.23\n",
            "iteration : 350, loss : 0.2556, accuracy : 92.29\n",
            "Epoch :   6, training loss : 0.2539, training accuracy : 92.33, test loss : 0.2850, test accuracy : 91.78\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.2339, accuracy : 92.84\n",
            "iteration : 100, loss : 0.2466, accuracy : 92.65\n",
            "iteration : 150, loss : 0.2480, accuracy : 92.54\n",
            "iteration : 200, loss : 0.2475, accuracy : 92.65\n",
            "iteration : 250, loss : 0.2437, accuracy : 92.80\n",
            "iteration : 300, loss : 0.2433, accuracy : 92.80\n",
            "iteration : 350, loss : 0.2431, accuracy : 92.87\n",
            "Epoch :   7, training loss : 0.2429, training accuracy : 92.87, test loss : 0.2638, test accuracy : 92.39\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.2315, accuracy : 93.19\n",
            "iteration : 100, loss : 0.2322, accuracy : 92.93\n",
            "iteration : 150, loss : 0.2323, accuracy : 93.02\n",
            "iteration : 200, loss : 0.2332, accuracy : 93.05\n",
            "iteration : 250, loss : 0.2335, accuracy : 93.09\n",
            "iteration : 300, loss : 0.2360, accuracy : 93.03\n",
            "iteration : 350, loss : 0.2323, accuracy : 93.12\n",
            "Epoch :   8, training loss : 0.2313, training accuracy : 93.14, test loss : 0.2683, test accuracy : 92.26\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.2252, accuracy : 93.20\n",
            "iteration : 100, loss : 0.2199, accuracy : 93.38\n",
            "iteration : 150, loss : 0.2212, accuracy : 93.38\n",
            "iteration : 200, loss : 0.2238, accuracy : 93.24\n",
            "iteration : 250, loss : 0.2226, accuracy : 93.23\n",
            "iteration : 300, loss : 0.2230, accuracy : 93.24\n",
            "iteration : 350, loss : 0.2228, accuracy : 93.24\n",
            "Epoch :   9, training loss : 0.2221, training accuracy : 93.26, test loss : 0.2729, test accuracy : 92.23\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2114, accuracy : 94.03\n",
            "iteration : 100, loss : 0.2071, accuracy : 94.06\n",
            "iteration : 150, loss : 0.2073, accuracy : 94.05\n",
            "iteration : 200, loss : 0.2081, accuracy : 93.96\n",
            "iteration : 250, loss : 0.2099, accuracy : 93.89\n",
            "iteration : 300, loss : 0.2090, accuracy : 93.85\n",
            "iteration : 350, loss : 0.2130, accuracy : 93.79\n",
            "Epoch :  10, training loss : 0.2128, training accuracy : 93.82, test loss : 0.2532, test accuracy : 92.61\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.1880, accuracy : 94.70\n",
            "iteration : 100, loss : 0.1965, accuracy : 94.52\n",
            "iteration : 150, loss : 0.1928, accuracy : 94.53\n",
            "iteration : 200, loss : 0.1942, accuracy : 94.43\n",
            "iteration : 250, loss : 0.2000, accuracy : 94.24\n",
            "iteration : 300, loss : 0.1996, accuracy : 94.19\n",
            "iteration : 350, loss : 0.2000, accuracy : 94.23\n",
            "Epoch :  11, training loss : 0.2002, training accuracy : 94.24, test loss : 0.2536, test accuracy : 92.72\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.1799, accuracy : 94.84\n",
            "iteration : 100, loss : 0.1837, accuracy : 94.75\n",
            "iteration : 150, loss : 0.1943, accuracy : 94.38\n",
            "iteration : 200, loss : 0.1956, accuracy : 94.38\n",
            "iteration : 250, loss : 0.1939, accuracy : 94.39\n",
            "iteration : 300, loss : 0.1931, accuracy : 94.39\n",
            "iteration : 350, loss : 0.1909, accuracy : 94.40\n",
            "Epoch :  12, training loss : 0.1927, training accuracy : 94.36, test loss : 0.2578, test accuracy : 92.74\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.1906, accuracy : 94.58\n",
            "iteration : 100, loss : 0.1915, accuracy : 94.52\n",
            "iteration : 150, loss : 0.1952, accuracy : 94.45\n",
            "iteration : 200, loss : 0.1919, accuracy : 94.52\n",
            "iteration : 250, loss : 0.1882, accuracy : 94.56\n",
            "iteration : 300, loss : 0.1878, accuracy : 94.57\n",
            "iteration : 350, loss : 0.1865, accuracy : 94.58\n",
            "Epoch :  13, training loss : 0.1852, training accuracy : 94.61, test loss : 0.2382, test accuracy : 93.29\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.1763, accuracy : 95.05\n",
            "iteration : 100, loss : 0.1764, accuracy : 94.95\n",
            "iteration : 150, loss : 0.1746, accuracy : 95.03\n",
            "iteration : 200, loss : 0.1783, accuracy : 94.96\n",
            "iteration : 250, loss : 0.1796, accuracy : 94.89\n",
            "iteration : 300, loss : 0.1813, accuracy : 94.76\n",
            "iteration : 350, loss : 0.1821, accuracy : 94.73\n",
            "Epoch :  14, training loss : 0.1822, training accuracy : 94.74, test loss : 0.2410, test accuracy : 93.27\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.1557, accuracy : 95.45\n",
            "iteration : 100, loss : 0.1602, accuracy : 95.33\n",
            "iteration : 150, loss : 0.1615, accuracy : 95.27\n",
            "iteration : 200, loss : 0.1675, accuracy : 95.06\n",
            "iteration : 250, loss : 0.1685, accuracy : 95.07\n",
            "iteration : 300, loss : 0.1716, accuracy : 94.99\n",
            "iteration : 350, loss : 0.1721, accuracy : 95.02\n",
            "Epoch :  15, training loss : 0.1728, training accuracy : 95.01, test loss : 0.2385, test accuracy : 93.41\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.1704, accuracy : 94.97\n",
            "iteration : 100, loss : 0.1694, accuracy : 95.04\n",
            "iteration : 150, loss : 0.1636, accuracy : 95.17\n",
            "iteration : 200, loss : 0.1620, accuracy : 95.21\n",
            "iteration : 250, loss : 0.1646, accuracy : 95.15\n",
            "iteration : 300, loss : 0.1658, accuracy : 95.16\n",
            "iteration : 350, loss : 0.1676, accuracy : 95.09\n",
            "Epoch :  16, training loss : 0.1676, training accuracy : 95.09, test loss : 0.2288, test accuracy : 93.58\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.1721, accuracy : 95.11\n",
            "iteration : 100, loss : 0.1625, accuracy : 95.29\n",
            "iteration : 150, loss : 0.1554, accuracy : 95.41\n",
            "iteration : 200, loss : 0.1575, accuracy : 95.35\n",
            "iteration : 250, loss : 0.1583, accuracy : 95.32\n",
            "iteration : 300, loss : 0.1595, accuracy : 95.29\n",
            "iteration : 350, loss : 0.1606, accuracy : 95.29\n",
            "Epoch :  17, training loss : 0.1608, training accuracy : 95.26, test loss : 0.2459, test accuracy : 93.14\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.1507, accuracy : 95.92\n",
            "iteration : 100, loss : 0.1613, accuracy : 95.61\n",
            "iteration : 150, loss : 0.1539, accuracy : 95.70\n",
            "iteration : 200, loss : 0.1500, accuracy : 95.80\n",
            "iteration : 250, loss : 0.1517, accuracy : 95.72\n",
            "iteration : 300, loss : 0.1525, accuracy : 95.67\n",
            "iteration : 350, loss : 0.1523, accuracy : 95.69\n",
            "Epoch :  18, training loss : 0.1527, training accuracy : 95.67, test loss : 0.2407, test accuracy : 93.36\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.1410, accuracy : 96.27\n",
            "iteration : 100, loss : 0.1429, accuracy : 95.95\n",
            "iteration : 150, loss : 0.1457, accuracy : 95.83\n",
            "iteration : 200, loss : 0.1440, accuracy : 95.79\n",
            "iteration : 250, loss : 0.1449, accuracy : 95.78\n",
            "iteration : 300, loss : 0.1470, accuracy : 95.72\n",
            "iteration : 350, loss : 0.1497, accuracy : 95.65\n",
            "Epoch :  19, training loss : 0.1504, training accuracy : 95.61, test loss : 0.2365, test accuracy : 93.61\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.1271, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1359, accuracy : 96.02\n",
            "iteration : 150, loss : 0.1407, accuracy : 95.84\n",
            "iteration : 200, loss : 0.1394, accuracy : 95.89\n",
            "iteration : 250, loss : 0.1413, accuracy : 95.81\n",
            "iteration : 300, loss : 0.1403, accuracy : 95.86\n",
            "iteration : 350, loss : 0.1436, accuracy : 95.77\n",
            "Epoch :  20, training loss : 0.1446, training accuracy : 95.74, test loss : 0.2352, test accuracy : 93.47\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.1384, accuracy : 96.00\n",
            "iteration : 100, loss : 0.1382, accuracy : 96.09\n",
            "iteration : 150, loss : 0.1335, accuracy : 96.22\n",
            "iteration : 200, loss : 0.1347, accuracy : 96.16\n",
            "iteration : 250, loss : 0.1363, accuracy : 96.10\n",
            "iteration : 300, loss : 0.1367, accuracy : 96.05\n",
            "iteration : 350, loss : 0.1362, accuracy : 96.01\n",
            "Epoch :  21, training loss : 0.1379, training accuracy : 95.99, test loss : 0.2393, test accuracy : 93.55\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.1267, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1335, accuracy : 96.17\n",
            "iteration : 150, loss : 0.1315, accuracy : 96.22\n",
            "iteration : 200, loss : 0.1315, accuracy : 96.11\n",
            "iteration : 250, loss : 0.1308, accuracy : 96.15\n",
            "iteration : 300, loss : 0.1307, accuracy : 96.17\n",
            "iteration : 350, loss : 0.1304, accuracy : 96.23\n",
            "Epoch :  22, training loss : 0.1307, training accuracy : 96.23, test loss : 0.2398, test accuracy : 93.74\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.1321, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1243, accuracy : 96.37\n",
            "iteration : 150, loss : 0.1251, accuracy : 96.29\n",
            "iteration : 200, loss : 0.1261, accuracy : 96.33\n",
            "iteration : 250, loss : 0.1238, accuracy : 96.40\n",
            "iteration : 300, loss : 0.1254, accuracy : 96.34\n",
            "iteration : 350, loss : 0.1262, accuracy : 96.31\n",
            "Epoch :  23, training loss : 0.1265, training accuracy : 96.30, test loss : 0.2286, test accuracy : 93.78\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.1236, accuracy : 96.52\n",
            "iteration : 100, loss : 0.1252, accuracy : 96.34\n",
            "iteration : 150, loss : 0.1211, accuracy : 96.41\n",
            "iteration : 200, loss : 0.1193, accuracy : 96.48\n",
            "iteration : 250, loss : 0.1195, accuracy : 96.55\n",
            "iteration : 300, loss : 0.1176, accuracy : 96.60\n",
            "iteration : 350, loss : 0.1207, accuracy : 96.52\n",
            "Epoch :  24, training loss : 0.1236, training accuracy : 96.46, test loss : 0.2504, test accuracy : 93.21\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.1032, accuracy : 96.69\n",
            "iteration : 100, loss : 0.1057, accuracy : 96.70\n",
            "iteration : 150, loss : 0.1084, accuracy : 96.57\n",
            "iteration : 200, loss : 0.1116, accuracy : 96.56\n",
            "iteration : 250, loss : 0.1141, accuracy : 96.49\n",
            "iteration : 300, loss : 0.1173, accuracy : 96.45\n",
            "iteration : 350, loss : 0.1189, accuracy : 96.44\n",
            "Epoch :  25, training loss : 0.1196, training accuracy : 96.43, test loss : 0.2523, test accuracy : 93.30\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.1070, accuracy : 96.75\n",
            "iteration : 100, loss : 0.1067, accuracy : 96.86\n",
            "iteration : 150, loss : 0.1077, accuracy : 96.89\n",
            "iteration : 200, loss : 0.1113, accuracy : 96.79\n",
            "iteration : 250, loss : 0.1113, accuracy : 96.82\n",
            "iteration : 300, loss : 0.1129, accuracy : 96.78\n",
            "iteration : 350, loss : 0.1157, accuracy : 96.65\n",
            "Epoch :  26, training loss : 0.1150, training accuracy : 96.65, test loss : 0.2286, test accuracy : 94.07\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.0974, accuracy : 97.06\n",
            "iteration : 100, loss : 0.1065, accuracy : 96.93\n",
            "iteration : 150, loss : 0.1105, accuracy : 96.83\n",
            "iteration : 200, loss : 0.1115, accuracy : 96.85\n",
            "iteration : 250, loss : 0.1119, accuracy : 96.84\n",
            "iteration : 300, loss : 0.1095, accuracy : 96.91\n",
            "iteration : 350, loss : 0.1089, accuracy : 96.88\n",
            "Epoch :  27, training loss : 0.1092, training accuracy : 96.87, test loss : 0.2523, test accuracy : 93.47\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.0887, accuracy : 97.25\n",
            "iteration : 100, loss : 0.0917, accuracy : 97.20\n",
            "iteration : 150, loss : 0.0954, accuracy : 97.17\n",
            "iteration : 200, loss : 0.1006, accuracy : 96.99\n",
            "iteration : 250, loss : 0.1046, accuracy : 96.91\n",
            "iteration : 300, loss : 0.1056, accuracy : 96.88\n",
            "iteration : 350, loss : 0.1048, accuracy : 96.89\n",
            "Epoch :  28, training loss : 0.1055, training accuracy : 96.87, test loss : 0.2344, test accuracy : 94.05\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.0866, accuracy : 97.50\n",
            "iteration : 100, loss : 0.0918, accuracy : 97.37\n",
            "iteration : 150, loss : 0.0988, accuracy : 97.26\n",
            "iteration : 200, loss : 0.1027, accuracy : 97.11\n",
            "iteration : 250, loss : 0.1035, accuracy : 97.08\n",
            "iteration : 300, loss : 0.1027, accuracy : 97.07\n",
            "iteration : 350, loss : 0.1023, accuracy : 97.07\n",
            "Epoch :  29, training loss : 0.1015, training accuracy : 97.09, test loss : 0.2511, test accuracy : 93.74\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.0869, accuracy : 97.50\n",
            "iteration : 100, loss : 0.0909, accuracy : 97.40\n",
            "iteration : 150, loss : 0.0896, accuracy : 97.39\n",
            "iteration : 200, loss : 0.0925, accuracy : 97.28\n",
            "iteration : 250, loss : 0.0928, accuracy : 97.28\n",
            "iteration : 300, loss : 0.0947, accuracy : 97.22\n",
            "iteration : 350, loss : 0.0982, accuracy : 97.14\n",
            "Epoch :  30, training loss : 0.0975, training accuracy : 97.16, test loss : 0.2367, test accuracy : 93.90\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.0886, accuracy : 97.64\n",
            "iteration : 100, loss : 0.0868, accuracy : 97.52\n",
            "iteration : 150, loss : 0.0882, accuracy : 97.43\n",
            "iteration : 200, loss : 0.0884, accuracy : 97.42\n",
            "iteration : 250, loss : 0.0926, accuracy : 97.31\n",
            "iteration : 300, loss : 0.0935, accuracy : 97.28\n",
            "iteration : 350, loss : 0.0952, accuracy : 97.25\n",
            "Epoch :  31, training loss : 0.0945, training accuracy : 97.26, test loss : 0.2577, test accuracy : 93.45\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.0806, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0858, accuracy : 97.43\n",
            "iteration : 150, loss : 0.0862, accuracy : 97.42\n",
            "iteration : 200, loss : 0.0860, accuracy : 97.44\n",
            "iteration : 250, loss : 0.0889, accuracy : 97.37\n",
            "iteration : 300, loss : 0.0889, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0905, accuracy : 97.34\n",
            "Epoch :  32, training loss : 0.0910, training accuracy : 97.34, test loss : 0.2464, test accuracy : 93.92\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.0778, accuracy : 97.70\n",
            "iteration : 100, loss : 0.0757, accuracy : 97.75\n",
            "iteration : 150, loss : 0.0760, accuracy : 97.76\n",
            "iteration : 200, loss : 0.0800, accuracy : 97.66\n",
            "iteration : 250, loss : 0.0851, accuracy : 97.47\n",
            "iteration : 300, loss : 0.0869, accuracy : 97.40\n",
            "iteration : 350, loss : 0.0871, accuracy : 97.35\n",
            "Epoch :  33, training loss : 0.0877, training accuracy : 97.35, test loss : 0.2499, test accuracy : 93.74\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.0798, accuracy : 97.77\n",
            "iteration : 100, loss : 0.0783, accuracy : 97.87\n",
            "iteration : 150, loss : 0.0826, accuracy : 97.71\n",
            "iteration : 200, loss : 0.0832, accuracy : 97.64\n",
            "iteration : 250, loss : 0.0819, accuracy : 97.62\n",
            "iteration : 300, loss : 0.0842, accuracy : 97.51\n",
            "iteration : 350, loss : 0.0838, accuracy : 97.51\n",
            "Epoch :  34, training loss : 0.0835, training accuracy : 97.51, test loss : 0.2596, test accuracy : 93.66\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.0820, accuracy : 97.50\n",
            "iteration : 100, loss : 0.0775, accuracy : 97.52\n",
            "iteration : 150, loss : 0.0804, accuracy : 97.46\n",
            "iteration : 200, loss : 0.0818, accuracy : 97.46\n",
            "iteration : 250, loss : 0.0837, accuracy : 97.45\n",
            "iteration : 300, loss : 0.0826, accuracy : 97.52\n",
            "iteration : 350, loss : 0.0838, accuracy : 97.52\n",
            "Epoch :  35, training loss : 0.0842, training accuracy : 97.51, test loss : 0.2510, test accuracy : 93.99\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.0721, accuracy : 97.73\n",
            "iteration : 100, loss : 0.0727, accuracy : 97.86\n",
            "iteration : 150, loss : 0.0721, accuracy : 97.89\n",
            "iteration : 200, loss : 0.0715, accuracy : 97.91\n",
            "iteration : 250, loss : 0.0736, accuracy : 97.87\n",
            "iteration : 300, loss : 0.0752, accuracy : 97.78\n",
            "iteration : 350, loss : 0.0759, accuracy : 97.75\n",
            "Epoch :  36, training loss : 0.0754, training accuracy : 97.77, test loss : 0.2564, test accuracy : 93.98\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.0813, accuracy : 97.84\n",
            "iteration : 100, loss : 0.0820, accuracy : 97.73\n",
            "iteration : 150, loss : 0.0801, accuracy : 97.77\n",
            "iteration : 200, loss : 0.0780, accuracy : 97.81\n",
            "iteration : 250, loss : 0.0766, accuracy : 97.82\n",
            "iteration : 300, loss : 0.0748, accuracy : 97.85\n",
            "iteration : 350, loss : 0.0768, accuracy : 97.75\n",
            "Epoch :  37, training loss : 0.0771, training accuracy : 97.73, test loss : 0.2515, test accuracy : 93.96\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.0610, accuracy : 98.22\n",
            "iteration : 100, loss : 0.0682, accuracy : 98.12\n",
            "iteration : 150, loss : 0.0668, accuracy : 98.10\n",
            "iteration : 200, loss : 0.0694, accuracy : 98.05\n",
            "iteration : 250, loss : 0.0692, accuracy : 98.00\n",
            "iteration : 300, loss : 0.0728, accuracy : 97.87\n",
            "iteration : 350, loss : 0.0734, accuracy : 97.87\n",
            "Epoch :  38, training loss : 0.0732, training accuracy : 97.88, test loss : 0.2636, test accuracy : 93.98\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.0642, accuracy : 98.02\n",
            "iteration : 100, loss : 0.0648, accuracy : 98.02\n",
            "iteration : 150, loss : 0.0675, accuracy : 97.94\n",
            "iteration : 200, loss : 0.0692, accuracy : 97.91\n",
            "iteration : 250, loss : 0.0712, accuracy : 97.85\n",
            "iteration : 300, loss : 0.0724, accuracy : 97.82\n",
            "iteration : 350, loss : 0.0716, accuracy : 97.83\n",
            "Epoch :  39, training loss : 0.0715, training accuracy : 97.83, test loss : 0.2691, test accuracy : 93.89\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.0575, accuracy : 98.45\n",
            "iteration : 100, loss : 0.0618, accuracy : 98.23\n",
            "iteration : 150, loss : 0.0625, accuracy : 98.14\n",
            "iteration : 200, loss : 0.0675, accuracy : 97.99\n",
            "iteration : 250, loss : 0.0681, accuracy : 97.94\n",
            "iteration : 300, loss : 0.0685, accuracy : 97.97\n",
            "iteration : 350, loss : 0.0691, accuracy : 97.96\n",
            "Epoch :  40, training loss : 0.0687, training accuracy : 97.97, test loss : 0.2728, test accuracy : 93.93\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.0497, accuracy : 98.48\n",
            "iteration : 100, loss : 0.0565, accuracy : 98.23\n",
            "iteration : 150, loss : 0.0579, accuracy : 98.26\n",
            "iteration : 200, loss : 0.0610, accuracy : 98.16\n",
            "iteration : 250, loss : 0.0627, accuracy : 98.10\n",
            "iteration : 300, loss : 0.0651, accuracy : 98.05\n",
            "iteration : 350, loss : 0.0657, accuracy : 98.01\n",
            "Epoch :  41, training loss : 0.0659, training accuracy : 98.00, test loss : 0.2878, test accuracy : 93.68\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.0557, accuracy : 98.33\n",
            "iteration : 100, loss : 0.0605, accuracy : 98.16\n",
            "iteration : 150, loss : 0.0574, accuracy : 98.25\n",
            "iteration : 200, loss : 0.0575, accuracy : 98.25\n",
            "iteration : 250, loss : 0.0587, accuracy : 98.22\n",
            "iteration : 300, loss : 0.0603, accuracy : 98.18\n",
            "iteration : 350, loss : 0.0598, accuracy : 98.17\n",
            "Epoch :  42, training loss : 0.0602, training accuracy : 98.17, test loss : 0.2628, test accuracy : 94.33\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.0578, accuracy : 98.25\n",
            "iteration : 100, loss : 0.0582, accuracy : 98.15\n",
            "iteration : 150, loss : 0.0571, accuracy : 98.27\n",
            "iteration : 200, loss : 0.0573, accuracy : 98.20\n",
            "iteration : 250, loss : 0.0567, accuracy : 98.20\n",
            "iteration : 300, loss : 0.0572, accuracy : 98.18\n",
            "iteration : 350, loss : 0.0594, accuracy : 98.17\n",
            "Epoch :  43, training loss : 0.0600, training accuracy : 98.14, test loss : 0.2891, test accuracy : 93.60\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.0528, accuracy : 98.48\n",
            "iteration : 100, loss : 0.0564, accuracy : 98.30\n",
            "iteration : 150, loss : 0.0557, accuracy : 98.30\n",
            "iteration : 200, loss : 0.0614, accuracy : 98.16\n",
            "iteration : 250, loss : 0.0636, accuracy : 98.07\n",
            "iteration : 300, loss : 0.0625, accuracy : 98.11\n",
            "iteration : 350, loss : 0.0623, accuracy : 98.11\n",
            "Epoch :  44, training loss : 0.0618, training accuracy : 98.12, test loss : 0.2637, test accuracy : 94.10\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.0530, accuracy : 98.34\n",
            "iteration : 100, loss : 0.0606, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0572, accuracy : 98.17\n",
            "iteration : 200, loss : 0.0552, accuracy : 98.22\n",
            "iteration : 250, loss : 0.0548, accuracy : 98.22\n",
            "iteration : 300, loss : 0.0545, accuracy : 98.24\n",
            "iteration : 350, loss : 0.0569, accuracy : 98.20\n",
            "Epoch :  45, training loss : 0.0571, training accuracy : 98.19, test loss : 0.2719, test accuracy : 94.15\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.0518, accuracy : 98.28\n",
            "iteration : 100, loss : 0.0475, accuracy : 98.49\n",
            "iteration : 150, loss : 0.0480, accuracy : 98.47\n",
            "iteration : 200, loss : 0.0499, accuracy : 98.41\n",
            "iteration : 250, loss : 0.0519, accuracy : 98.38\n",
            "iteration : 300, loss : 0.0525, accuracy : 98.40\n",
            "iteration : 350, loss : 0.0524, accuracy : 98.39\n",
            "Epoch :  46, training loss : 0.0530, training accuracy : 98.37, test loss : 0.3082, test accuracy : 93.80\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.0556, accuracy : 98.19\n",
            "iteration : 100, loss : 0.0571, accuracy : 98.19\n",
            "iteration : 150, loss : 0.0585, accuracy : 98.15\n",
            "iteration : 200, loss : 0.0602, accuracy : 98.07\n",
            "iteration : 250, loss : 0.0594, accuracy : 98.10\n",
            "iteration : 300, loss : 0.0585, accuracy : 98.15\n",
            "iteration : 350, loss : 0.0579, accuracy : 98.17\n",
            "Epoch :  47, training loss : 0.0571, training accuracy : 98.20, test loss : 0.2854, test accuracy : 94.17\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.0473, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0446, accuracy : 98.73\n",
            "iteration : 150, loss : 0.0480, accuracy : 98.61\n",
            "iteration : 200, loss : 0.0489, accuracy : 98.57\n",
            "iteration : 250, loss : 0.0491, accuracy : 98.56\n",
            "iteration : 300, loss : 0.0506, accuracy : 98.49\n",
            "iteration : 350, loss : 0.0526, accuracy : 98.44\n",
            "Epoch :  48, training loss : 0.0531, training accuracy : 98.44, test loss : 0.3029, test accuracy : 93.64\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.0526, accuracy : 98.47\n",
            "iteration : 100, loss : 0.0513, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0503, accuracy : 98.41\n",
            "iteration : 200, loss : 0.0518, accuracy : 98.37\n",
            "iteration : 250, loss : 0.0526, accuracy : 98.35\n",
            "iteration : 300, loss : 0.0527, accuracy : 98.31\n",
            "iteration : 350, loss : 0.0525, accuracy : 98.34\n",
            "Epoch :  49, training loss : 0.0527, training accuracy : 98.34, test loss : 0.2886, test accuracy : 94.00\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.0476, accuracy : 98.56\n",
            "iteration : 100, loss : 0.0487, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0492, accuracy : 98.44\n",
            "iteration : 200, loss : 0.0495, accuracy : 98.44\n",
            "iteration : 250, loss : 0.0491, accuracy : 98.45\n",
            "iteration : 300, loss : 0.0500, accuracy : 98.42\n",
            "iteration : 350, loss : 0.0507, accuracy : 98.41\n",
            "Epoch :  50, training loss : 0.0508, training accuracy : 98.42, test loss : 0.3013, test accuracy : 93.57\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.0509, accuracy : 98.48\n",
            "iteration : 100, loss : 0.0475, accuracy : 98.57\n",
            "iteration : 150, loss : 0.0472, accuracy : 98.53\n",
            "iteration : 200, loss : 0.0471, accuracy : 98.50\n",
            "iteration : 250, loss : 0.0486, accuracy : 98.44\n",
            "iteration : 300, loss : 0.0486, accuracy : 98.45\n",
            "iteration : 350, loss : 0.0487, accuracy : 98.44\n",
            "Epoch :  51, training loss : 0.0485, training accuracy : 98.43, test loss : 0.2918, test accuracy : 94.00\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.0381, accuracy : 98.80\n",
            "iteration : 100, loss : 0.0432, accuracy : 98.67\n",
            "iteration : 150, loss : 0.0458, accuracy : 98.65\n",
            "iteration : 200, loss : 0.0458, accuracy : 98.60\n",
            "iteration : 250, loss : 0.0452, accuracy : 98.58\n",
            "iteration : 300, loss : 0.0461, accuracy : 98.53\n",
            "iteration : 350, loss : 0.0468, accuracy : 98.50\n",
            "Epoch :  52, training loss : 0.0470, training accuracy : 98.49, test loss : 0.3129, test accuracy : 93.67\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.0518, accuracy : 98.47\n",
            "iteration : 100, loss : 0.0451, accuracy : 98.63\n",
            "iteration : 150, loss : 0.0442, accuracy : 98.67\n",
            "iteration : 200, loss : 0.0450, accuracy : 98.65\n",
            "iteration : 250, loss : 0.0467, accuracy : 98.61\n",
            "iteration : 300, loss : 0.0466, accuracy : 98.57\n",
            "iteration : 350, loss : 0.0474, accuracy : 98.55\n",
            "Epoch :  53, training loss : 0.0470, training accuracy : 98.56, test loss : 0.3016, test accuracy : 93.96\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.0347, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0428, accuracy : 98.60\n",
            "iteration : 150, loss : 0.0435, accuracy : 98.61\n",
            "iteration : 200, loss : 0.0437, accuracy : 98.59\n",
            "iteration : 250, loss : 0.0455, accuracy : 98.55\n",
            "iteration : 300, loss : 0.0470, accuracy : 98.48\n",
            "iteration : 350, loss : 0.0462, accuracy : 98.50\n",
            "Epoch :  54, training loss : 0.0465, training accuracy : 98.49, test loss : 0.3364, test accuracy : 93.51\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.0357, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0407, accuracy : 98.85\n",
            "iteration : 150, loss : 0.0400, accuracy : 98.82\n",
            "iteration : 200, loss : 0.0393, accuracy : 98.80\n",
            "iteration : 250, loss : 0.0417, accuracy : 98.71\n",
            "iteration : 300, loss : 0.0424, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0427, accuracy : 98.67\n",
            "Epoch :  55, training loss : 0.0427, training accuracy : 98.67, test loss : 0.3023, test accuracy : 94.00\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.0414, accuracy : 98.88\n",
            "iteration : 100, loss : 0.0393, accuracy : 98.85\n",
            "iteration : 150, loss : 0.0404, accuracy : 98.81\n",
            "iteration : 200, loss : 0.0402, accuracy : 98.77\n",
            "iteration : 250, loss : 0.0419, accuracy : 98.70\n",
            "iteration : 300, loss : 0.0416, accuracy : 98.69\n",
            "iteration : 350, loss : 0.0425, accuracy : 98.66\n",
            "Epoch :  56, training loss : 0.0430, training accuracy : 98.65, test loss : 0.3010, test accuracy : 94.33\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.0317, accuracy : 98.95\n",
            "iteration : 100, loss : 0.0387, accuracy : 98.76\n",
            "iteration : 150, loss : 0.0395, accuracy : 98.67\n",
            "iteration : 200, loss : 0.0401, accuracy : 98.68\n",
            "iteration : 250, loss : 0.0412, accuracy : 98.64\n",
            "iteration : 300, loss : 0.0412, accuracy : 98.64\n",
            "iteration : 350, loss : 0.0403, accuracy : 98.69\n",
            "Epoch :  57, training loss : 0.0402, training accuracy : 98.69, test loss : 0.3161, test accuracy : 93.98\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.0380, accuracy : 98.62\n",
            "iteration : 100, loss : 0.0354, accuracy : 98.75\n",
            "iteration : 150, loss : 0.0373, accuracy : 98.73\n",
            "iteration : 200, loss : 0.0378, accuracy : 98.73\n",
            "iteration : 250, loss : 0.0377, accuracy : 98.73\n",
            "iteration : 300, loss : 0.0377, accuracy : 98.74\n",
            "iteration : 350, loss : 0.0375, accuracy : 98.77\n",
            "Epoch :  58, training loss : 0.0378, training accuracy : 98.77, test loss : 0.3240, test accuracy : 93.78\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.0354, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0340, accuracy : 98.93\n",
            "iteration : 150, loss : 0.0342, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0376, accuracy : 98.78\n",
            "iteration : 250, loss : 0.0374, accuracy : 98.77\n",
            "iteration : 300, loss : 0.0393, accuracy : 98.74\n",
            "iteration : 350, loss : 0.0401, accuracy : 98.70\n",
            "Epoch :  59, training loss : 0.0404, training accuracy : 98.69, test loss : 0.2896, test accuracy : 94.17\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.0435, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0426, accuracy : 98.71\n",
            "iteration : 150, loss : 0.0408, accuracy : 98.72\n",
            "iteration : 200, loss : 0.0420, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0433, accuracy : 98.64\n",
            "iteration : 300, loss : 0.0426, accuracy : 98.66\n",
            "iteration : 350, loss : 0.0429, accuracy : 98.63\n",
            "Epoch :  60, training loss : 0.0428, training accuracy : 98.65, test loss : 0.2885, test accuracy : 94.10\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.0282, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0246, accuracy : 99.27\n",
            "iteration : 150, loss : 0.0296, accuracy : 99.06\n",
            "iteration : 200, loss : 0.0328, accuracy : 98.91\n",
            "iteration : 250, loss : 0.0334, accuracy : 98.89\n",
            "iteration : 300, loss : 0.0358, accuracy : 98.82\n",
            "iteration : 350, loss : 0.0367, accuracy : 98.79\n",
            "Epoch :  61, training loss : 0.0360, training accuracy : 98.82, test loss : 0.3081, test accuracy : 94.16\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.0357, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0361, accuracy : 98.76\n",
            "iteration : 150, loss : 0.0379, accuracy : 98.77\n",
            "iteration : 200, loss : 0.0389, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0390, accuracy : 98.72\n",
            "iteration : 300, loss : 0.0400, accuracy : 98.72\n",
            "iteration : 350, loss : 0.0391, accuracy : 98.74\n",
            "Epoch :  62, training loss : 0.0389, training accuracy : 98.74, test loss : 0.3103, test accuracy : 94.16\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.0335, accuracy : 98.98\n",
            "iteration : 100, loss : 0.0329, accuracy : 98.97\n",
            "iteration : 150, loss : 0.0320, accuracy : 98.93\n",
            "iteration : 200, loss : 0.0339, accuracy : 98.85\n",
            "iteration : 250, loss : 0.0342, accuracy : 98.82\n",
            "iteration : 300, loss : 0.0341, accuracy : 98.84\n",
            "iteration : 350, loss : 0.0344, accuracy : 98.82\n",
            "Epoch :  63, training loss : 0.0346, training accuracy : 98.82, test loss : 0.3132, test accuracy : 94.06\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.0344, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0332, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0334, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0356, accuracy : 98.82\n",
            "iteration : 250, loss : 0.0367, accuracy : 98.82\n",
            "iteration : 300, loss : 0.0359, accuracy : 98.83\n",
            "iteration : 350, loss : 0.0348, accuracy : 98.87\n",
            "Epoch :  64, training loss : 0.0348, training accuracy : 98.87, test loss : 0.3205, test accuracy : 94.12\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.0223, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0282, accuracy : 99.12\n",
            "iteration : 200, loss : 0.0319, accuracy : 99.00\n",
            "iteration : 250, loss : 0.0329, accuracy : 98.99\n",
            "iteration : 300, loss : 0.0341, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0349, accuracy : 98.91\n",
            "Epoch :  65, training loss : 0.0355, training accuracy : 98.88, test loss : 0.3239, test accuracy : 94.13\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.0394, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0379, accuracy : 98.65\n",
            "iteration : 150, loss : 0.0363, accuracy : 98.68\n",
            "iteration : 200, loss : 0.0360, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0354, accuracy : 98.76\n",
            "iteration : 300, loss : 0.0360, accuracy : 98.76\n",
            "iteration : 350, loss : 0.0351, accuracy : 98.78\n",
            "Epoch :  66, training loss : 0.0350, training accuracy : 98.79, test loss : 0.3075, test accuracy : 94.16\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.0354, accuracy : 98.77\n",
            "iteration : 100, loss : 0.0333, accuracy : 98.95\n",
            "iteration : 150, loss : 0.0332, accuracy : 98.92\n",
            "iteration : 200, loss : 0.0333, accuracy : 98.95\n",
            "iteration : 250, loss : 0.0338, accuracy : 98.91\n",
            "iteration : 300, loss : 0.0338, accuracy : 98.89\n",
            "iteration : 350, loss : 0.0341, accuracy : 98.89\n",
            "Epoch :  67, training loss : 0.0335, training accuracy : 98.90, test loss : 0.3133, test accuracy : 94.18\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.0282, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0289, accuracy : 99.06\n",
            "iteration : 150, loss : 0.0302, accuracy : 99.00\n",
            "iteration : 200, loss : 0.0316, accuracy : 98.98\n",
            "iteration : 250, loss : 0.0320, accuracy : 98.93\n",
            "iteration : 300, loss : 0.0329, accuracy : 98.90\n",
            "iteration : 350, loss : 0.0341, accuracy : 98.88\n",
            "Epoch :  68, training loss : 0.0337, training accuracy : 98.90, test loss : 0.3123, test accuracy : 94.18\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.0245, accuracy : 99.19\n",
            "iteration : 100, loss : 0.0256, accuracy : 99.15\n",
            "iteration : 150, loss : 0.0285, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0294, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0292, accuracy : 99.04\n",
            "iteration : 300, loss : 0.0295, accuracy : 99.03\n",
            "iteration : 350, loss : 0.0311, accuracy : 98.98\n",
            "Epoch :  69, training loss : 0.0315, training accuracy : 98.97, test loss : 0.3266, test accuracy : 94.19\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.0359, accuracy : 98.91\n",
            "iteration : 100, loss : 0.0315, accuracy : 99.03\n",
            "iteration : 150, loss : 0.0309, accuracy : 99.07\n",
            "iteration : 200, loss : 0.0299, accuracy : 99.07\n",
            "iteration : 250, loss : 0.0297, accuracy : 99.07\n",
            "iteration : 300, loss : 0.0301, accuracy : 99.04\n",
            "iteration : 350, loss : 0.0306, accuracy : 99.00\n",
            "Epoch :  70, training loss : 0.0309, training accuracy : 98.98, test loss : 0.3649, test accuracy : 94.03\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.0292, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0269, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0285, accuracy : 99.08\n",
            "iteration : 200, loss : 0.0294, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0298, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0295, accuracy : 99.04\n",
            "iteration : 350, loss : 0.0306, accuracy : 98.99\n",
            "Epoch :  71, training loss : 0.0310, training accuracy : 98.98, test loss : 0.3290, test accuracy : 94.14\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.0308, accuracy : 99.08\n",
            "iteration : 100, loss : 0.0317, accuracy : 99.01\n",
            "iteration : 150, loss : 0.0284, accuracy : 99.08\n",
            "iteration : 200, loss : 0.0285, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0278, accuracy : 99.11\n",
            "iteration : 300, loss : 0.0292, accuracy : 99.09\n",
            "iteration : 350, loss : 0.0303, accuracy : 99.04\n",
            "Epoch :  72, training loss : 0.0303, training accuracy : 99.04, test loss : 0.3448, test accuracy : 93.90\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.0220, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0248, accuracy : 99.13\n",
            "iteration : 150, loss : 0.0271, accuracy : 99.08\n",
            "iteration : 200, loss : 0.0267, accuracy : 99.10\n",
            "iteration : 250, loss : 0.0285, accuracy : 99.06\n",
            "iteration : 300, loss : 0.0296, accuracy : 99.03\n",
            "iteration : 350, loss : 0.0301, accuracy : 99.00\n",
            "Epoch :  73, training loss : 0.0299, training accuracy : 99.01, test loss : 0.3452, test accuracy : 93.81\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.0323, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0295, accuracy : 99.00\n",
            "iteration : 150, loss : 0.0279, accuracy : 99.06\n",
            "iteration : 200, loss : 0.0259, accuracy : 99.15\n",
            "iteration : 250, loss : 0.0252, accuracy : 99.16\n",
            "iteration : 300, loss : 0.0262, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0262, accuracy : 99.12\n",
            "Epoch :  74, training loss : 0.0265, training accuracy : 99.12, test loss : 0.3562, test accuracy : 93.93\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.0216, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0232, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0236, accuracy : 99.15\n",
            "iteration : 200, loss : 0.0260, accuracy : 99.07\n",
            "iteration : 250, loss : 0.0272, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0284, accuracy : 99.01\n",
            "iteration : 350, loss : 0.0302, accuracy : 98.99\n",
            "Epoch :  75, training loss : 0.0307, training accuracy : 98.99, test loss : 0.3455, test accuracy : 94.00\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.0243, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0247, accuracy : 99.19\n",
            "iteration : 150, loss : 0.0254, accuracy : 99.15\n",
            "iteration : 200, loss : 0.0256, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0250, accuracy : 99.17\n",
            "iteration : 300, loss : 0.0256, accuracy : 99.12\n",
            "iteration : 350, loss : 0.0263, accuracy : 99.12\n",
            "Epoch :  76, training loss : 0.0268, training accuracy : 99.11, test loss : 0.3508, test accuracy : 93.98\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.0299, accuracy : 99.03\n",
            "iteration : 100, loss : 0.0249, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0251, accuracy : 99.21\n",
            "iteration : 200, loss : 0.0254, accuracy : 99.19\n",
            "iteration : 250, loss : 0.0257, accuracy : 99.18\n",
            "iteration : 300, loss : 0.0270, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0279, accuracy : 99.10\n",
            "Epoch :  77, training loss : 0.0284, training accuracy : 99.09, test loss : 0.3557, test accuracy : 93.97\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.0371, accuracy : 98.83\n",
            "iteration : 100, loss : 0.0342, accuracy : 98.91\n",
            "iteration : 150, loss : 0.0313, accuracy : 99.01\n",
            "iteration : 200, loss : 0.0310, accuracy : 99.00\n",
            "iteration : 250, loss : 0.0304, accuracy : 99.02\n",
            "iteration : 300, loss : 0.0293, accuracy : 99.04\n",
            "iteration : 350, loss : 0.0290, accuracy : 99.06\n",
            "Epoch :  78, training loss : 0.0290, training accuracy : 99.05, test loss : 0.3487, test accuracy : 94.06\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.0291, accuracy : 98.98\n",
            "iteration : 100, loss : 0.0327, accuracy : 98.94\n",
            "iteration : 150, loss : 0.0292, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0280, accuracy : 99.10\n",
            "iteration : 250, loss : 0.0272, accuracy : 99.11\n",
            "iteration : 300, loss : 0.0268, accuracy : 99.12\n",
            "iteration : 350, loss : 0.0268, accuracy : 99.12\n",
            "Epoch :  79, training loss : 0.0269, training accuracy : 99.12, test loss : 0.3626, test accuracy : 94.00\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.0232, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0233, accuracy : 99.27\n",
            "iteration : 150, loss : 0.0227, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0225, accuracy : 99.29\n",
            "iteration : 250, loss : 0.0234, accuracy : 99.26\n",
            "iteration : 300, loss : 0.0242, accuracy : 99.24\n",
            "iteration : 350, loss : 0.0258, accuracy : 99.20\n",
            "Epoch :  80, training loss : 0.0258, training accuracy : 99.19, test loss : 0.3257, test accuracy : 94.26\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.0248, accuracy : 99.20\n",
            "iteration : 100, loss : 0.0254, accuracy : 99.16\n",
            "iteration : 150, loss : 0.0267, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0267, accuracy : 99.08\n",
            "iteration : 250, loss : 0.0270, accuracy : 99.09\n",
            "iteration : 300, loss : 0.0272, accuracy : 99.08\n",
            "iteration : 350, loss : 0.0273, accuracy : 99.07\n",
            "Epoch :  81, training loss : 0.0271, training accuracy : 99.09, test loss : 0.3328, test accuracy : 94.42\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.0219, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0236, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0248, accuracy : 99.16\n",
            "iteration : 200, loss : 0.0250, accuracy : 99.17\n",
            "iteration : 250, loss : 0.0267, accuracy : 99.11\n",
            "iteration : 300, loss : 0.0266, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0267, accuracy : 99.10\n",
            "Epoch :  82, training loss : 0.0269, training accuracy : 99.10, test loss : 0.3503, test accuracy : 94.02\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.0254, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0248, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0253, accuracy : 99.12\n",
            "iteration : 200, loss : 0.0254, accuracy : 99.11\n",
            "iteration : 250, loss : 0.0265, accuracy : 99.11\n",
            "iteration : 300, loss : 0.0264, accuracy : 99.12\n",
            "iteration : 350, loss : 0.0261, accuracy : 99.12\n",
            "Epoch :  83, training loss : 0.0270, training accuracy : 99.09, test loss : 0.3597, test accuracy : 93.78\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.0244, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0219, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0227, accuracy : 99.19\n",
            "iteration : 200, loss : 0.0243, accuracy : 99.16\n",
            "iteration : 250, loss : 0.0263, accuracy : 99.09\n",
            "iteration : 300, loss : 0.0263, accuracy : 99.11\n",
            "iteration : 350, loss : 0.0262, accuracy : 99.13\n",
            "Epoch :  84, training loss : 0.0264, training accuracy : 99.13, test loss : 0.3399, test accuracy : 94.15\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.0195, accuracy : 99.28\n",
            "iteration : 100, loss : 0.0202, accuracy : 99.26\n",
            "iteration : 150, loss : 0.0226, accuracy : 99.19\n",
            "iteration : 200, loss : 0.0249, accuracy : 99.15\n",
            "iteration : 250, loss : 0.0265, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0268, accuracy : 99.08\n",
            "iteration : 350, loss : 0.0261, accuracy : 99.11\n",
            "Epoch :  85, training loss : 0.0262, training accuracy : 99.11, test loss : 0.3275, test accuracy : 94.26\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.0187, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0205, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0185, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0180, accuracy : 99.47\n",
            "iteration : 250, loss : 0.0180, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0191, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0208, accuracy : 99.36\n",
            "Epoch :  86, training loss : 0.0213, training accuracy : 99.34, test loss : 0.3569, test accuracy : 94.05\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.0268, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0233, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0228, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0240, accuracy : 99.20\n",
            "iteration : 250, loss : 0.0233, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0237, accuracy : 99.23\n",
            "iteration : 350, loss : 0.0236, accuracy : 99.23\n",
            "Epoch :  87, training loss : 0.0239, training accuracy : 99.23, test loss : 0.3664, test accuracy : 93.90\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.0227, accuracy : 99.28\n",
            "iteration : 100, loss : 0.0223, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0221, accuracy : 99.22\n",
            "iteration : 200, loss : 0.0214, accuracy : 99.24\n",
            "iteration : 250, loss : 0.0220, accuracy : 99.22\n",
            "iteration : 300, loss : 0.0221, accuracy : 99.23\n",
            "iteration : 350, loss : 0.0227, accuracy : 99.19\n",
            "Epoch :  88, training loss : 0.0234, training accuracy : 99.18, test loss : 0.3611, test accuracy : 93.97\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.0230, accuracy : 99.28\n",
            "iteration : 100, loss : 0.0231, accuracy : 99.19\n",
            "iteration : 150, loss : 0.0248, accuracy : 99.16\n",
            "iteration : 200, loss : 0.0248, accuracy : 99.16\n",
            "iteration : 250, loss : 0.0244, accuracy : 99.16\n",
            "iteration : 300, loss : 0.0238, accuracy : 99.16\n",
            "iteration : 350, loss : 0.0258, accuracy : 99.11\n",
            "Epoch :  89, training loss : 0.0263, training accuracy : 99.10, test loss : 0.3437, test accuracy : 94.13\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.0321, accuracy : 98.95\n",
            "iteration : 100, loss : 0.0281, accuracy : 99.13\n",
            "iteration : 150, loss : 0.0264, accuracy : 99.14\n",
            "iteration : 200, loss : 0.0268, accuracy : 99.15\n",
            "iteration : 250, loss : 0.0259, accuracy : 99.19\n",
            "iteration : 300, loss : 0.0253, accuracy : 99.17\n",
            "iteration : 350, loss : 0.0248, accuracy : 99.18\n",
            "Epoch :  90, training loss : 0.0244, training accuracy : 99.20, test loss : 0.3434, test accuracy : 94.41\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.0218, accuracy : 99.28\n",
            "iteration : 100, loss : 0.0202, accuracy : 99.26\n",
            "iteration : 150, loss : 0.0202, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0203, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0207, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0209, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0210, accuracy : 99.28\n",
            "Epoch :  91, training loss : 0.0211, training accuracy : 99.27, test loss : 0.3388, test accuracy : 94.21\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.0232, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0232, accuracy : 99.16\n",
            "iteration : 150, loss : 0.0213, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0214, accuracy : 99.25\n",
            "iteration : 250, loss : 0.0213, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0220, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0224, accuracy : 99.24\n",
            "Epoch :  92, training loss : 0.0226, training accuracy : 99.24, test loss : 0.3383, test accuracy : 94.31\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.0206, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0185, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0178, accuracy : 99.45\n",
            "iteration : 200, loss : 0.0201, accuracy : 99.38\n",
            "iteration : 250, loss : 0.0213, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0218, accuracy : 99.31\n",
            "iteration : 350, loss : 0.0229, accuracy : 99.25\n",
            "Epoch :  93, training loss : 0.0229, training accuracy : 99.25, test loss : 0.3589, test accuracy : 93.77\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.0216, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0221, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0259, accuracy : 99.10\n",
            "iteration : 200, loss : 0.0272, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0268, accuracy : 99.06\n",
            "iteration : 300, loss : 0.0263, accuracy : 99.08\n",
            "iteration : 350, loss : 0.0266, accuracy : 99.08\n",
            "Epoch :  94, training loss : 0.0267, training accuracy : 99.08, test loss : 0.3676, test accuracy : 93.86\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.0196, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0186, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0204, accuracy : 99.30\n",
            "iteration : 200, loss : 0.0201, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0198, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0196, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0207, accuracy : 99.28\n",
            "Epoch :  95, training loss : 0.0215, training accuracy : 99.27, test loss : 0.3818, test accuracy : 93.78\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.0265, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0227, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0209, accuracy : 99.28\n",
            "iteration : 200, loss : 0.0197, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0215, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0219, accuracy : 99.26\n",
            "iteration : 350, loss : 0.0223, accuracy : 99.25\n",
            "Epoch :  96, training loss : 0.0226, training accuracy : 99.24, test loss : 0.3554, test accuracy : 94.22\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.0231, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0217, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0201, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0193, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0195, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0198, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0202, accuracy : 99.32\n",
            "Epoch :  97, training loss : 0.0203, training accuracy : 99.31, test loss : 0.3528, test accuracy : 94.33\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0159, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0173, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0167, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0164, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0189, accuracy : 99.33\n",
            "Epoch :  98, training loss : 0.0192, training accuracy : 99.32, test loss : 0.3716, test accuracy : 93.97\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.0232, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0249, accuracy : 99.18\n",
            "iteration : 150, loss : 0.0253, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0254, accuracy : 99.15\n",
            "iteration : 250, loss : 0.0245, accuracy : 99.19\n",
            "iteration : 300, loss : 0.0248, accuracy : 99.20\n",
            "iteration : 350, loss : 0.0249, accuracy : 99.18\n",
            "Epoch :  99, training loss : 0.0246, training accuracy : 99.19, test loss : 0.3585, test accuracy : 94.20\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.0231, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0244, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0230, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0218, accuracy : 99.26\n",
            "iteration : 250, loss : 0.0213, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0224, accuracy : 99.24\n",
            "iteration : 350, loss : 0.0223, accuracy : 99.24\n",
            "Epoch : 100, training loss : 0.0219, training accuracy : 99.25, test loss : 0.3432, test accuracy : 94.43\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.0146, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0150, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0152, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0158, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0159, accuracy : 99.46\n",
            "iteration : 300, loss : 0.0164, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0174, accuracy : 99.42\n",
            "Epoch : 101, training loss : 0.0175, training accuracy : 99.41, test loss : 0.3687, test accuracy : 94.20\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.0183, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0173, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0176, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0190, accuracy : 99.38\n",
            "iteration : 250, loss : 0.0192, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0185, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0190, accuracy : 99.38\n",
            "Epoch : 102, training loss : 0.0193, training accuracy : 99.36, test loss : 0.3779, test accuracy : 94.18\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.0209, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0191, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0183, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0183, accuracy : 99.40\n",
            "iteration : 250, loss : 0.0189, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0187, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0185, accuracy : 99.37\n",
            "Epoch : 103, training loss : 0.0192, training accuracy : 99.35, test loss : 0.3841, test accuracy : 93.82\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.0194, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0244, accuracy : 99.16\n",
            "iteration : 150, loss : 0.0260, accuracy : 99.11\n",
            "iteration : 200, loss : 0.0272, accuracy : 99.11\n",
            "iteration : 250, loss : 0.0257, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0254, accuracy : 99.12\n",
            "iteration : 350, loss : 0.0247, accuracy : 99.16\n",
            "Epoch : 104, training loss : 0.0245, training accuracy : 99.16, test loss : 0.3522, test accuracy : 94.38\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.0078, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0127, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0135, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0145, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0149, accuracy : 99.52\n",
            "Epoch : 105, training loss : 0.0151, training accuracy : 99.52, test loss : 0.3621, test accuracy : 94.31\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.0111, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0122, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0160, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0172, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0184, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0196, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0201, accuracy : 99.33\n",
            "Epoch : 106, training loss : 0.0204, training accuracy : 99.32, test loss : 0.3745, test accuracy : 94.13\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.0198, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0199, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0206, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0215, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0216, accuracy : 99.31\n",
            "iteration : 300, loss : 0.0216, accuracy : 99.30\n",
            "iteration : 350, loss : 0.0223, accuracy : 99.27\n",
            "Epoch : 107, training loss : 0.0220, training accuracy : 99.28, test loss : 0.3749, test accuracy : 94.18\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.0183, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0161, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0166, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0172, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0182, accuracy : 99.40\n",
            "iteration : 300, loss : 0.0189, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0187, accuracy : 99.38\n",
            "Epoch : 108, training loss : 0.0188, training accuracy : 99.37, test loss : 0.3660, test accuracy : 94.28\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.0180, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0193, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0201, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0206, accuracy : 99.29\n",
            "iteration : 250, loss : 0.0196, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0192, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0195, accuracy : 99.34\n",
            "Epoch : 109, training loss : 0.0194, training accuracy : 99.35, test loss : 0.3686, test accuracy : 94.15\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.0172, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0179, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0161, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0167, accuracy : 99.47\n",
            "iteration : 250, loss : 0.0180, accuracy : 99.40\n",
            "iteration : 300, loss : 0.0194, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0195, accuracy : 99.36\n",
            "Epoch : 110, training loss : 0.0191, training accuracy : 99.37, test loss : 0.3717, test accuracy : 94.31\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.0146, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0160, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0162, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0166, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0174, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0179, accuracy : 99.36\n",
            "Epoch : 111, training loss : 0.0178, training accuracy : 99.36, test loss : 0.3697, test accuracy : 94.41\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.0142, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0166, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0170, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0175, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0174, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.40\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.41\n",
            "Epoch : 112, training loss : 0.0174, training accuracy : 99.43, test loss : 0.3781, test accuracy : 94.35\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.0150, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0167, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0177, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0194, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0194, accuracy : 99.31\n",
            "iteration : 300, loss : 0.0212, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0219, accuracy : 99.23\n",
            "Epoch : 113, training loss : 0.0218, training accuracy : 99.22, test loss : 0.3566, test accuracy : 94.10\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.0206, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0198, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0191, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0181, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0185, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0183, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0179, accuracy : 99.42\n",
            "Epoch : 114, training loss : 0.0179, training accuracy : 99.42, test loss : 0.3536, test accuracy : 94.33\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0122, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0117, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0137, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0158, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0164, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0175, accuracy : 99.41\n",
            "Epoch : 115, training loss : 0.0180, training accuracy : 99.39, test loss : 0.4062, test accuracy : 93.75\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0195, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0176, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0182, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0168, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0166, accuracy : 99.46\n",
            "iteration : 300, loss : 0.0162, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0165, accuracy : 99.46\n",
            "Epoch : 116, training loss : 0.0164, training accuracy : 99.46, test loss : 0.3737, test accuracy : 94.24\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.0147, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0183, accuracy : 99.39\n",
            "iteration : 150, loss : 0.0179, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0178, accuracy : 99.40\n",
            "iteration : 250, loss : 0.0176, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0176, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.40\n",
            "Epoch : 117, training loss : 0.0178, training accuracy : 99.39, test loss : 0.3697, test accuracy : 94.29\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.0131, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0187, accuracy : 99.39\n",
            "iteration : 150, loss : 0.0187, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0193, accuracy : 99.33\n",
            "iteration : 250, loss : 0.0189, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0192, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0190, accuracy : 99.33\n",
            "Epoch : 118, training loss : 0.0190, training accuracy : 99.34, test loss : 0.3746, test accuracy : 93.95\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.0129, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0143, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0138, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0128, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0140, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0149, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0150, accuracy : 99.50\n",
            "Epoch : 119, training loss : 0.0158, training accuracy : 99.48, test loss : 0.4071, test accuracy : 93.89\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.0202, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0195, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0225, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0215, accuracy : 99.33\n",
            "iteration : 250, loss : 0.0209, accuracy : 99.35\n",
            "iteration : 300, loss : 0.0213, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0221, accuracy : 99.30\n",
            "Epoch : 120, training loss : 0.0226, training accuracy : 99.28, test loss : 0.3882, test accuracy : 93.83\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.0215, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0212, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0205, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0210, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0197, accuracy : 99.35\n",
            "iteration : 300, loss : 0.0198, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0195, accuracy : 99.36\n",
            "Epoch : 121, training loss : 0.0193, training accuracy : 99.36, test loss : 0.3734, test accuracy : 94.08\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.0102, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0117, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0141, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0141, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0141, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0142, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0147, accuracy : 99.56\n",
            "Epoch : 122, training loss : 0.0148, training accuracy : 99.56, test loss : 0.3670, test accuracy : 94.37\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.0095, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0112, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0128, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0129, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0141, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0149, accuracy : 99.51\n",
            "Epoch : 123, training loss : 0.0150, training accuracy : 99.50, test loss : 0.3697, test accuracy : 94.17\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.0178, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0170, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0188, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0187, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0182, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0187, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0185, accuracy : 99.40\n",
            "Epoch : 124, training loss : 0.0185, training accuracy : 99.42, test loss : 0.3702, test accuracy : 94.45\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.0136, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0174, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0187, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0189, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0193, accuracy : 99.35\n",
            "iteration : 300, loss : 0.0187, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0179, accuracy : 99.39\n",
            "Epoch : 125, training loss : 0.0181, training accuracy : 99.39, test loss : 0.3819, test accuracy : 94.15\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.0144, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0127, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0132, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0138, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0135, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0138, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0138, accuracy : 99.56\n",
            "Epoch : 126, training loss : 0.0141, training accuracy : 99.55, test loss : 0.3749, test accuracy : 94.38\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0129, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0141, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0145, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0150, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0149, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0154, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0157, accuracy : 99.50\n",
            "Epoch : 127, training loss : 0.0160, training accuracy : 99.49, test loss : 0.3891, test accuracy : 94.22\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0169, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0168, accuracy : 99.36\n",
            "iteration : 150, loss : 0.0169, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0172, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0177, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0180, accuracy : 99.40\n",
            "iteration : 350, loss : 0.0183, accuracy : 99.37\n",
            "Epoch : 128, training loss : 0.0181, training accuracy : 99.38, test loss : 0.3678, test accuracy : 94.22\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.0113, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0154, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0160, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0158, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0159, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0161, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0164, accuracy : 99.47\n",
            "Epoch : 129, training loss : 0.0161, training accuracy : 99.48, test loss : 0.3668, test accuracy : 94.53\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0124, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0138, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0140, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0148, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0150, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0150, accuracy : 99.51\n",
            "Epoch : 130, training loss : 0.0148, training accuracy : 99.52, test loss : 0.3927, test accuracy : 94.15\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.0111, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0127, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0123, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0126, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0139, accuracy : 99.56\n",
            "Epoch : 131, training loss : 0.0139, training accuracy : 99.55, test loss : 0.4038, test accuracy : 94.16\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.0148, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0180, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0171, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0168, accuracy : 99.47\n",
            "iteration : 250, loss : 0.0177, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0175, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0183, accuracy : 99.42\n",
            "Epoch : 132, training loss : 0.0181, training accuracy : 99.42, test loss : 0.4014, test accuracy : 93.91\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.0211, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0186, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0178, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0176, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0170, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0167, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0172, accuracy : 99.46\n",
            "Epoch : 133, training loss : 0.0173, training accuracy : 99.45, test loss : 0.3804, test accuracy : 93.98\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0201, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0188, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0181, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0166, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0161, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0148, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0142, accuracy : 99.52\n",
            "Epoch : 134, training loss : 0.0143, training accuracy : 99.52, test loss : 0.3879, test accuracy : 94.28\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.0142, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0171, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0173, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0157, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0150, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0145, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0153, accuracy : 99.49\n",
            "Epoch : 135, training loss : 0.0157, training accuracy : 99.47, test loss : 0.3762, test accuracy : 94.20\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0174, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0201, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0188, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0185, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0181, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0173, accuracy : 99.40\n",
            "iteration : 350, loss : 0.0167, accuracy : 99.42\n",
            "Epoch : 136, training loss : 0.0166, training accuracy : 99.42, test loss : 0.3729, test accuracy : 94.21\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0119, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0140, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0135, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0139, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0138, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0142, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0142, accuracy : 99.53\n",
            "Epoch : 137, training loss : 0.0143, training accuracy : 99.53, test loss : 0.3661, test accuracy : 94.33\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.0173, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0154, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0169, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0152, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0147, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0161, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0168, accuracy : 99.43\n",
            "Epoch : 138, training loss : 0.0170, training accuracy : 99.41, test loss : 0.3800, test accuracy : 93.98\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0166, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0163, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0141, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0138, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0143, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0144, accuracy : 99.49\n",
            "Epoch : 139, training loss : 0.0146, training accuracy : 99.49, test loss : 0.3786, test accuracy : 94.19\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.0129, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0129, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0121, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0139, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0146, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0161, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0168, accuracy : 99.47\n",
            "Epoch : 140, training loss : 0.0168, training accuracy : 99.47, test loss : 0.3903, test accuracy : 94.03\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0155, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0160, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0152, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0135, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0136, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0135, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0140, accuracy : 99.52\n",
            "Epoch : 141, training loss : 0.0137, training accuracy : 99.53, test loss : 0.3959, test accuracy : 94.28\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0105, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0114, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0119, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0125, accuracy : 99.58\n",
            "Epoch : 142, training loss : 0.0125, training accuracy : 99.57, test loss : 0.4233, test accuracy : 93.83\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0146, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0155, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0159, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0145, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0153, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0157, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0155, accuracy : 99.49\n",
            "Epoch : 143, training loss : 0.0155, training accuracy : 99.49, test loss : 0.4184, test accuracy : 94.12\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0098, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0117, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0128, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0125, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0130, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0131, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0133, accuracy : 99.54\n",
            "Epoch : 144, training loss : 0.0136, training accuracy : 99.54, test loss : 0.4038, test accuracy : 94.02\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0163, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0145, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0159, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0162, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0152, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0151, accuracy : 99.50\n",
            "Epoch : 145, training loss : 0.0151, training accuracy : 99.50, test loss : 0.3937, test accuracy : 94.13\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0201, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0179, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0157, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0159, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0158, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0150, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0145, accuracy : 99.50\n",
            "Epoch : 146, training loss : 0.0145, training accuracy : 99.50, test loss : 0.4071, test accuracy : 94.09\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0175, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0171, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0179, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0167, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0153, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0147, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0145, accuracy : 99.53\n",
            "Epoch : 147, training loss : 0.0144, training accuracy : 99.53, test loss : 0.3832, test accuracy : 94.53\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0104, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0104, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0112, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0113, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0112, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0116, accuracy : 99.62\n",
            "Epoch : 148, training loss : 0.0117, training accuracy : 99.62, test loss : 0.3994, test accuracy : 94.09\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0154, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0191, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0169, accuracy : 99.45\n",
            "iteration : 200, loss : 0.0168, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0156, accuracy : 99.46\n",
            "iteration : 300, loss : 0.0153, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0156, accuracy : 99.47\n",
            "Epoch : 149, training loss : 0.0161, training accuracy : 99.46, test loss : 0.4123, test accuracy : 94.03\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0106, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0111, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0099, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0099, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0112, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0120, accuracy : 99.60\n",
            "Epoch : 150, training loss : 0.0119, training accuracy : 99.60, test loss : 0.4056, test accuracy : 94.33\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0088, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0112, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0117, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0125, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0123, accuracy : 99.56\n",
            "Epoch : 151, training loss : 0.0125, training accuracy : 99.55, test loss : 0.3918, test accuracy : 94.31\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0181, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0177, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0161, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0160, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0154, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0145, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0140, accuracy : 99.54\n",
            "Epoch : 152, training loss : 0.0136, training accuracy : 99.56, test loss : 0.3998, test accuracy : 94.43\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0122, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0103, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0124, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0137, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0146, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0159, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0156, accuracy : 99.49\n",
            "Epoch : 153, training loss : 0.0159, training accuracy : 99.47, test loss : 0.3857, test accuracy : 94.40\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0158, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0149, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0139, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0137, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0132, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0127, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.62\n",
            "Epoch : 154, training loss : 0.0116, training accuracy : 99.63, test loss : 0.3893, test accuracy : 94.42\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0079, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0116, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0106, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0103, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0099, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0095, accuracy : 99.69\n",
            "Epoch : 155, training loss : 0.0098, training accuracy : 99.68, test loss : 0.4581, test accuracy : 94.00\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0132, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0127, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0126, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0126, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0127, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0125, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0132, accuracy : 99.57\n",
            "Epoch : 156, training loss : 0.0134, training accuracy : 99.57, test loss : 0.4118, test accuracy : 94.17\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0135, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0142, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0142, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0162, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0173, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0161, accuracy : 99.45\n",
            "Epoch : 157, training loss : 0.0158, training accuracy : 99.46, test loss : 0.3847, test accuracy : 94.35\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0149, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0135, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0142, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0156, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0157, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0150, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0144, accuracy : 99.54\n",
            "Epoch : 158, training loss : 0.0144, training accuracy : 99.53, test loss : 0.3895, test accuracy : 94.23\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0149, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0141, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0142, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0150, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0152, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0149, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0141, accuracy : 99.50\n",
            "Epoch : 159, training loss : 0.0140, training accuracy : 99.51, test loss : 0.3886, test accuracy : 94.26\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0116, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0101, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0105, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0103, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0110, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0119, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0120, accuracy : 99.62\n",
            "Epoch : 160, training loss : 0.0120, training accuracy : 99.62, test loss : 0.4141, test accuracy : 94.40\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0141, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0137, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0155, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0154, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0154, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0161, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0164, accuracy : 99.45\n",
            "Epoch : 161, training loss : 0.0161, training accuracy : 99.46, test loss : 0.3931, test accuracy : 94.49\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0150, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0142, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0129, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0126, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0128, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0130, accuracy : 99.59\n",
            "Epoch : 162, training loss : 0.0128, training accuracy : 99.60, test loss : 0.4089, test accuracy : 94.26\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0105, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0113, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0141, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0154, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0154, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0146, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0140, accuracy : 99.56\n",
            "Epoch : 163, training loss : 0.0136, training accuracy : 99.57, test loss : 0.3917, test accuracy : 94.51\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0088, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0078, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0079, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0081, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0097, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.62\n",
            "Epoch : 164, training loss : 0.0117, training accuracy : 99.62, test loss : 0.4033, test accuracy : 94.40\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0133, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0145, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0141, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0141, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0142, accuracy : 99.53\n",
            "Epoch : 165, training loss : 0.0141, training accuracy : 99.53, test loss : 0.4069, test accuracy : 94.16\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0150, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0132, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0121, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0108, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0115, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0121, accuracy : 99.60\n",
            "Epoch : 166, training loss : 0.0129, training accuracy : 99.59, test loss : 0.4021, test accuracy : 94.16\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0183, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0161, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0159, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0160, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0163, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0161, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0154, accuracy : 99.49\n",
            "Epoch : 167, training loss : 0.0150, training accuracy : 99.50, test loss : 0.3761, test accuracy : 94.42\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0098, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.59\n",
            "Epoch : 168, training loss : 0.0122, training accuracy : 99.57, test loss : 0.4119, test accuracy : 94.18\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0142, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0134, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0148, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0132, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0137, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0145, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0143, accuracy : 99.53\n",
            "Epoch : 169, training loss : 0.0144, training accuracy : 99.54, test loss : 0.3946, test accuracy : 94.32\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0104, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0119, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0116, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0116, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.58\n",
            "Epoch : 170, training loss : 0.0111, training accuracy : 99.58, test loss : 0.4065, test accuracy : 94.32\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0093, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0087, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0089, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0098, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0097, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.63\n",
            "Epoch : 171, training loss : 0.0112, training accuracy : 99.62, test loss : 0.4281, test accuracy : 94.01\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0162, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0165, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0167, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0158, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0167, accuracy : 99.45\n",
            "iteration : 300, loss : 0.0167, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0167, accuracy : 99.46\n",
            "Epoch : 172, training loss : 0.0164, training accuracy : 99.47, test loss : 0.3944, test accuracy : 94.33\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0107, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0100, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0099, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0086, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0082, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.73\n",
            "Epoch : 173, training loss : 0.0079, training accuracy : 99.72, test loss : 0.4146, test accuracy : 94.35\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0087, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0077, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0119, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0127, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0123, accuracy : 99.58\n",
            "Epoch : 174, training loss : 0.0119, training accuracy : 99.59, test loss : 0.3962, test accuracy : 94.33\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0073, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0105, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0101, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0108, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0104, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0120, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0123, accuracy : 99.57\n",
            "Epoch : 175, training loss : 0.0125, training accuracy : 99.56, test loss : 0.4008, test accuracy : 94.25\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0113, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0130, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0139, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0134, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0134, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0132, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0131, accuracy : 99.56\n",
            "Epoch : 176, training loss : 0.0130, training accuracy : 99.56, test loss : 0.4100, test accuracy : 94.51\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0107, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0097, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0105, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0115, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0127, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0124, accuracy : 99.61\n",
            "Epoch : 177, training loss : 0.0123, training accuracy : 99.61, test loss : 0.3959, test accuracy : 94.39\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0103, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0113, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0107, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0117, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0111, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0111, accuracy : 99.65\n",
            "Epoch : 178, training loss : 0.0111, training accuracy : 99.65, test loss : 0.4048, test accuracy : 94.27\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0097, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0118, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0119, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0114, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.61\n",
            "Epoch : 179, training loss : 0.0119, training accuracy : 99.60, test loss : 0.3898, test accuracy : 94.43\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0121, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0129, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0125, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0124, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0128, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.56\n",
            "Epoch : 180, training loss : 0.0128, training accuracy : 99.56, test loss : 0.3940, test accuracy : 94.33\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0095, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0100, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0109, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0112, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0119, accuracy : 99.63\n",
            "Epoch : 181, training loss : 0.0122, training accuracy : 99.61, test loss : 0.4186, test accuracy : 94.23\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0079, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0104, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0111, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0135, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0126, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0118, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.62\n",
            "Epoch : 182, training loss : 0.0116, training accuracy : 99.62, test loss : 0.4002, test accuracy : 94.24\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0080, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0096, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0090, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.64\n",
            "Epoch : 183, training loss : 0.0107, training accuracy : 99.64, test loss : 0.4257, test accuracy : 93.87\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0137, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0112, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0105, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0112, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0116, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0117, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.59\n",
            "Epoch : 184, training loss : 0.0136, training accuracy : 99.57, test loss : 0.4077, test accuracy : 94.13\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0121, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0120, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0132, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0132, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0128, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.58\n",
            "Epoch : 185, training loss : 0.0126, training accuracy : 99.58, test loss : 0.4035, test accuracy : 94.29\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0078, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0076, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0089, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0095, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0100, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0100, accuracy : 99.67\n",
            "Epoch : 186, training loss : 0.0100, training accuracy : 99.67, test loss : 0.4168, test accuracy : 94.36\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0068, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0096, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0115, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0109, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0116, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0115, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0120, accuracy : 99.59\n",
            "Epoch : 187, training loss : 0.0120, training accuracy : 99.59, test loss : 0.4404, test accuracy : 94.20\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0131, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0125, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0120, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0116, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0119, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0121, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.62\n",
            "Epoch : 188, training loss : 0.0116, training accuracy : 99.63, test loss : 0.4126, test accuracy : 94.38\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0071, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0090, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0111, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0115, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0126, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0125, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.58\n",
            "Epoch : 189, training loss : 0.0126, training accuracy : 99.57, test loss : 0.4149, test accuracy : 94.33\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0118, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0117, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0113, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0107, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0114, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0125, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0135, accuracy : 99.59\n",
            "Epoch : 190, training loss : 0.0133, training accuracy : 99.60, test loss : 0.3932, test accuracy : 94.25\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0112, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0096, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0085, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0081, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0085, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.65\n",
            "Epoch : 191, training loss : 0.0107, training accuracy : 99.64, test loss : 0.4000, test accuracy : 94.28\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0078, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0101, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0105, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0107, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0115, accuracy : 99.61\n",
            "Epoch : 192, training loss : 0.0115, training accuracy : 99.61, test loss : 0.4002, test accuracy : 94.23\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0117, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0114, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0107, accuracy : 99.63\n",
            "Epoch : 193, training loss : 0.0107, training accuracy : 99.63, test loss : 0.4017, test accuracy : 94.27\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0101, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0106, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0111, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0105, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0102, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0098, accuracy : 99.66\n",
            "Epoch : 194, training loss : 0.0097, training accuracy : 99.66, test loss : 0.4004, test accuracy : 94.69\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0080, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0092, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0096, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0095, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0107, accuracy : 99.66\n",
            "Epoch : 195, training loss : 0.0106, training accuracy : 99.66, test loss : 0.4116, test accuracy : 94.16\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0107, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0115, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0122, accuracy : 99.59\n",
            "Epoch : 196, training loss : 0.0122, training accuracy : 99.59, test loss : 0.3964, test accuracy : 94.47\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0089, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0081, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0111, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0114, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0118, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0116, accuracy : 99.62\n",
            "Epoch : 197, training loss : 0.0115, training accuracy : 99.62, test loss : 0.4053, test accuracy : 94.27\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0127, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0120, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0105, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.68\n",
            "Epoch : 198, training loss : 0.0105, training accuracy : 99.68, test loss : 0.4004, test accuracy : 94.37\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0080, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0083, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0089, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0092, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0091, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0093, accuracy : 99.67\n",
            "Epoch : 199, training loss : 0.0097, training accuracy : 99.66, test loss : 0.4361, test accuracy : 94.11\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0145, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0132, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0133, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0123, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0118, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.63\n",
            "Epoch : 200, training loss : 0.0111, training accuracy : 99.63, test loss : 0.4210, test accuracy : 94.43\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0106, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0089, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0092, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0097, accuracy : 99.70\n",
            "Epoch : 201, training loss : 0.0101, training accuracy : 99.68, test loss : 0.4292, test accuracy : 94.24\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0140, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0136, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0137, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0140, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0150, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0149, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0145, accuracy : 99.57\n",
            "Epoch : 202, training loss : 0.0144, training accuracy : 99.57, test loss : 0.4017, test accuracy : 94.46\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0090, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0094, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0085, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0088, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0094, accuracy : 99.66\n",
            "Epoch : 203, training loss : 0.0093, training accuracy : 99.67, test loss : 0.4237, test accuracy : 94.32\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0087, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0063, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0061, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0061, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0069, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0069, accuracy : 99.81\n",
            "Epoch : 204, training loss : 0.0071, training accuracy : 99.80, test loss : 0.4398, test accuracy : 94.29\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0124, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0132, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0133, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0137, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0140, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0139, accuracy : 99.55\n",
            "Epoch : 205, training loss : 0.0138, training accuracy : 99.56, test loss : 0.4031, test accuracy : 94.11\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0128, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0098, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0097, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0119, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0117, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0126, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0128, accuracy : 99.56\n",
            "Epoch : 206, training loss : 0.0126, training accuracy : 99.57, test loss : 0.4022, test accuracy : 94.17\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0132, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0111, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0106, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0115, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0123, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0122, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.63\n",
            "Epoch : 207, training loss : 0.0116, training accuracy : 99.63, test loss : 0.4053, test accuracy : 94.43\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0083, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0077, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0072, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0081, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0087, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.70\n",
            "Epoch : 208, training loss : 0.0089, training accuracy : 99.70, test loss : 0.4139, test accuracy : 94.31\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0125, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0103, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0095, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0092, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0097, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0100, accuracy : 99.68\n",
            "Epoch : 209, training loss : 0.0100, training accuracy : 99.67, test loss : 0.4238, test accuracy : 94.20\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0109, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0099, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0098, accuracy : 99.67\n",
            "Epoch : 210, training loss : 0.0099, training accuracy : 99.67, test loss : 0.4160, test accuracy : 94.46\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0109, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0114, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0110, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0107, accuracy : 99.63\n",
            "Epoch : 211, training loss : 0.0108, training accuracy : 99.64, test loss : 0.4309, test accuracy : 94.18\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0160, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0135, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0126, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0134, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0131, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0126, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0123, accuracy : 99.58\n",
            "Epoch : 212, training loss : 0.0119, training accuracy : 99.60, test loss : 0.4159, test accuracy : 94.53\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0089, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0097, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0101, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0101, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0109, accuracy : 99.64\n",
            "Epoch : 213, training loss : 0.0109, training accuracy : 99.64, test loss : 0.4251, test accuracy : 94.17\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0113, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0109, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.65\n",
            "Epoch : 214, training loss : 0.0112, training accuracy : 99.66, test loss : 0.4288, test accuracy : 94.29\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0089, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0078, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0077, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0105, accuracy : 99.65\n",
            "Epoch : 215, training loss : 0.0104, training accuracy : 99.64, test loss : 0.4061, test accuracy : 94.30\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0068, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0083, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0088, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.72\n",
            "Epoch : 216, training loss : 0.0085, training accuracy : 99.73, test loss : 0.4108, test accuracy : 94.50\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0070, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0078, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0080, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0084, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0088, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0087, accuracy : 99.70\n",
            "Epoch : 217, training loss : 0.0093, training accuracy : 99.69, test loss : 0.4237, test accuracy : 94.26\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0108, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0132, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0129, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0120, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0120, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0121, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0122, accuracy : 99.61\n",
            "Epoch : 218, training loss : 0.0124, training accuracy : 99.61, test loss : 0.4147, test accuracy : 94.10\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0100, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0081, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0069, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0073, accuracy : 99.78\n",
            "Epoch : 219, training loss : 0.0073, training accuracy : 99.77, test loss : 0.4100, test accuracy : 94.65\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0062, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0075, accuracy : 99.76\n",
            "Epoch : 220, training loss : 0.0079, training accuracy : 99.75, test loss : 0.4473, test accuracy : 94.30\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0100, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0127, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0139, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0134, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0133, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0131, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0131, accuracy : 99.57\n",
            "Epoch : 221, training loss : 0.0131, training accuracy : 99.56, test loss : 0.4076, test accuracy : 94.43\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0070, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0076, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0079, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0077, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0080, accuracy : 99.75\n",
            "Epoch : 222, training loss : 0.0080, training accuracy : 99.75, test loss : 0.4096, test accuracy : 94.45\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.68\n",
            "Epoch : 223, training loss : 0.0092, training accuracy : 99.68, test loss : 0.4399, test accuracy : 94.20\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0114, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0120, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0115, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0116, accuracy : 99.64\n",
            "Epoch : 224, training loss : 0.0118, training accuracy : 99.63, test loss : 0.4397, test accuracy : 94.19\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0104, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0082, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0089, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0094, accuracy : 99.71\n",
            "Epoch : 225, training loss : 0.0095, training accuracy : 99.70, test loss : 0.4217, test accuracy : 94.45\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0130, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0124, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0122, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0111, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0102, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0101, accuracy : 99.66\n",
            "Epoch : 226, training loss : 0.0103, training accuracy : 99.65, test loss : 0.4525, test accuracy : 94.16\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0081, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0100, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0111, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0119, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0124, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.59\n",
            "Epoch : 227, training loss : 0.0126, training accuracy : 99.60, test loss : 0.4040, test accuracy : 94.33\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0114, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0087, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0080, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0081, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0079, accuracy : 99.77\n",
            "Epoch : 228, training loss : 0.0080, training accuracy : 99.76, test loss : 0.4275, test accuracy : 94.55\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0076, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0068, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0077, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0076, accuracy : 99.73\n",
            "Epoch : 229, training loss : 0.0075, training accuracy : 99.73, test loss : 0.4156, test accuracy : 94.47\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0094, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0108, accuracy : 99.61\n",
            "iteration : 150, loss : 0.0127, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0120, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0118, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0119, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.60\n",
            "Epoch : 230, training loss : 0.0118, training accuracy : 99.61, test loss : 0.4244, test accuracy : 94.31\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0113, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0103, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0107, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.65\n",
            "Epoch : 231, training loss : 0.0106, training accuracy : 99.65, test loss : 0.4347, test accuracy : 94.40\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0126, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0119, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0117, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0111, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0120, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0119, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0115, accuracy : 99.63\n",
            "Epoch : 232, training loss : 0.0116, training accuracy : 99.63, test loss : 0.4042, test accuracy : 94.49\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0078, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0068, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0069, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0069, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0082, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0084, accuracy : 99.72\n",
            "Epoch : 233, training loss : 0.0085, training accuracy : 99.72, test loss : 0.4213, test accuracy : 94.38\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0076, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0085, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0082, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0080, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0081, accuracy : 99.74\n",
            "Epoch : 234, training loss : 0.0083, training accuracy : 99.73, test loss : 0.4277, test accuracy : 94.45\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0111, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0096, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0094, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0103, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0107, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0107, accuracy : 99.66\n",
            "Epoch : 235, training loss : 0.0105, training accuracy : 99.67, test loss : 0.4169, test accuracy : 94.31\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0071, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0074, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0077, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0077, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0073, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0071, accuracy : 99.76\n",
            "Epoch : 236, training loss : 0.0072, training accuracy : 99.76, test loss : 0.4240, test accuracy : 94.46\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0102, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0103, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0100, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0102, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0093, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0094, accuracy : 99.71\n",
            "Epoch : 237, training loss : 0.0093, training accuracy : 99.71, test loss : 0.4350, test accuracy : 94.30\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0087, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0087, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0104, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0111, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0109, accuracy : 99.64\n",
            "Epoch : 238, training loss : 0.0112, training accuracy : 99.63, test loss : 0.4087, test accuracy : 94.55\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0065, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0069, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0082, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.74\n",
            "Epoch : 239, training loss : 0.0087, training accuracy : 99.74, test loss : 0.3963, test accuracy : 94.43\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0073, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0078, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0072, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0077, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0084, accuracy : 99.71\n",
            "Epoch : 240, training loss : 0.0084, training accuracy : 99.71, test loss : 0.4179, test accuracy : 94.41\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0102, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0099, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0098, accuracy : 99.68\n",
            "Epoch : 241, training loss : 0.0099, training accuracy : 99.67, test loss : 0.4271, test accuracy : 94.35\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0107, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0112, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0117, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0118, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0119, accuracy : 99.58\n",
            "Epoch : 242, training loss : 0.0116, training accuracy : 99.59, test loss : 0.3986, test accuracy : 94.43\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0086, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0095, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0095, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0095, accuracy : 99.69\n",
            "Epoch : 243, training loss : 0.0092, training accuracy : 99.70, test loss : 0.4018, test accuracy : 94.49\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0063, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0064, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0064, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0064, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0067, accuracy : 99.75\n",
            "Epoch : 244, training loss : 0.0070, training accuracy : 99.74, test loss : 0.4377, test accuracy : 94.27\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0078, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0092, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0096, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0097, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0099, accuracy : 99.69\n",
            "Epoch : 245, training loss : 0.0102, training accuracy : 99.68, test loss : 0.4086, test accuracy : 94.48\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0064, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0078, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0085, accuracy : 99.72\n",
            "Epoch : 246, training loss : 0.0087, training accuracy : 99.72, test loss : 0.4204, test accuracy : 94.26\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0137, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0099, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0092, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0090, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0085, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0085, accuracy : 99.72\n",
            "Epoch : 247, training loss : 0.0091, training accuracy : 99.71, test loss : 0.4250, test accuracy : 94.25\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0072, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0068, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0069, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0076, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0080, accuracy : 99.73\n",
            "Epoch : 248, training loss : 0.0081, training accuracy : 99.73, test loss : 0.4273, test accuracy : 94.55\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0108, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0099, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0092, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0100, accuracy : 99.66\n",
            "Epoch : 249, training loss : 0.0102, training accuracy : 99.65, test loss : 0.4183, test accuracy : 94.33\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0107, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0113, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0120, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0123, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0115, accuracy : 99.60\n",
            "Epoch : 250, training loss : 0.0114, training accuracy : 99.61, test loss : 0.4211, test accuracy : 94.30\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0071, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0079, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0077, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0070, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0070, accuracy : 99.80\n",
            "Epoch : 251, training loss : 0.0071, training accuracy : 99.79, test loss : 0.3999, test accuracy : 94.61\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0083, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0086, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0088, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0081, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.71\n",
            "Epoch : 252, training loss : 0.0088, training accuracy : 99.71, test loss : 0.4379, test accuracy : 94.12\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0066, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0070, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0088, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0089, accuracy : 99.70\n",
            "Epoch : 253, training loss : 0.0092, training accuracy : 99.69, test loss : 0.4252, test accuracy : 94.40\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0083, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0093, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0097, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0099, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0098, accuracy : 99.70\n",
            "Epoch : 254, training loss : 0.0096, training accuracy : 99.71, test loss : 0.4179, test accuracy : 94.27\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0118, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0130, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0122, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0103, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0097, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0092, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0084, accuracy : 99.76\n",
            "Epoch : 255, training loss : 0.0084, training accuracy : 99.76, test loss : 0.4189, test accuracy : 94.60\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0059, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.76\n",
            "Epoch : 256, training loss : 0.0077, training accuracy : 99.76, test loss : 0.4217, test accuracy : 94.28\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0099, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0105, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0101, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0101, accuracy : 99.67\n",
            "Epoch : 257, training loss : 0.0101, training accuracy : 99.67, test loss : 0.4152, test accuracy : 94.39\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0072, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0079, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0066, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0063, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0064, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0069, accuracy : 99.78\n",
            "Epoch : 258, training loss : 0.0072, training accuracy : 99.77, test loss : 0.4278, test accuracy : 94.40\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0068, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0071, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0087, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0105, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0104, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0102, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.65\n",
            "Epoch : 259, training loss : 0.0105, training accuracy : 99.64, test loss : 0.4215, test accuracy : 94.22\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0102, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0092, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0086, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0096, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0099, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0105, accuracy : 99.67\n",
            "Epoch : 260, training loss : 0.0104, training accuracy : 99.67, test loss : 0.3970, test accuracy : 94.50\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0081, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0083, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0085, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0082, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0077, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0082, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0087, accuracy : 99.72\n",
            "Epoch : 261, training loss : 0.0089, training accuracy : 99.72, test loss : 0.4247, test accuracy : 94.25\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0081, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0090, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0083, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0072, accuracy : 99.76\n",
            "Epoch : 262, training loss : 0.0071, training accuracy : 99.76, test loss : 0.4301, test accuracy : 94.51\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0067, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0087, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0085, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0083, accuracy : 99.73\n",
            "Epoch : 263, training loss : 0.0084, training accuracy : 99.73, test loss : 0.4101, test accuracy : 94.59\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0057, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0064, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0074, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0073, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.76\n",
            "Epoch : 264, training loss : 0.0081, training accuracy : 99.75, test loss : 0.4379, test accuracy : 94.43\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0066, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0072, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0081, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0083, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0089, accuracy : 99.70\n",
            "Epoch : 265, training loss : 0.0093, training accuracy : 99.69, test loss : 0.4121, test accuracy : 94.32\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0097, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0086, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0077, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0080, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0086, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.69\n",
            "Epoch : 266, training loss : 0.0090, training accuracy : 99.68, test loss : 0.4338, test accuracy : 94.47\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0098, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0118, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0122, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0115, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.66\n",
            "Epoch : 267, training loss : 0.0104, training accuracy : 99.66, test loss : 0.4062, test accuracy : 94.44\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0061, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0068, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0073, accuracy : 99.73\n",
            "Epoch : 268, training loss : 0.0075, training accuracy : 99.72, test loss : 0.4237, test accuracy : 94.44\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0073, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0082, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0082, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0090, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0088, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0085, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0087, accuracy : 99.69\n",
            "Epoch : 269, training loss : 0.0087, training accuracy : 99.69, test loss : 0.4324, test accuracy : 94.37\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0107, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0106, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0105, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0099, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0096, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.70\n",
            "Epoch : 270, training loss : 0.0087, training accuracy : 99.71, test loss : 0.4284, test accuracy : 94.34\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0081, accuracy : 99.73\n",
            "Epoch : 271, training loss : 0.0082, training accuracy : 99.72, test loss : 0.4480, test accuracy : 94.00\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0098, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0087, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0090, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0085, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.77\n",
            "Epoch : 272, training loss : 0.0079, training accuracy : 99.76, test loss : 0.4221, test accuracy : 94.38\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0083, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0097, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0089, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0083, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0080, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0076, accuracy : 99.79\n",
            "Epoch : 273, training loss : 0.0075, training accuracy : 99.79, test loss : 0.4178, test accuracy : 94.58\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0073, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0085, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0089, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0086, accuracy : 99.70\n",
            "Epoch : 274, training loss : 0.0087, training accuracy : 99.70, test loss : 0.4266, test accuracy : 94.58\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0092, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0096, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0106, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0102, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0099, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.69\n",
            "Epoch : 275, training loss : 0.0090, training accuracy : 99.68, test loss : 0.4232, test accuracy : 94.42\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0068, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0073, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0073, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0074, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0075, accuracy : 99.76\n",
            "Epoch : 276, training loss : 0.0082, training accuracy : 99.73, test loss : 0.4484, test accuracy : 94.08\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0112, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0081, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0072, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0073, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0081, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0086, accuracy : 99.74\n",
            "Epoch : 277, training loss : 0.0088, training accuracy : 99.74, test loss : 0.4473, test accuracy : 94.10\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0067, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0066, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0071, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0067, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0070, accuracy : 99.75\n",
            "Epoch : 278, training loss : 0.0068, training accuracy : 99.76, test loss : 0.4283, test accuracy : 94.38\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0066, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0085, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0081, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0078, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.75\n",
            "Epoch : 279, training loss : 0.0079, training accuracy : 99.75, test loss : 0.4324, test accuracy : 94.31\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0077, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0085, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0097, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.67\n",
            "Epoch : 280, training loss : 0.0106, training accuracy : 99.67, test loss : 0.4345, test accuracy : 94.30\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0092, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0080, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0080, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0081, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0086, accuracy : 99.71\n",
            "Epoch : 281, training loss : 0.0089, training accuracy : 99.70, test loss : 0.4196, test accuracy : 94.38\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0065, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.76\n",
            "Epoch : 282, training loss : 0.0077, training accuracy : 99.76, test loss : 0.4024, test accuracy : 94.53\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0080, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0091, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0086, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0074, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0073, accuracy : 99.76\n",
            "Epoch : 283, training loss : 0.0075, training accuracy : 99.76, test loss : 0.4264, test accuracy : 94.27\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0073, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0063, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0060, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0057, accuracy : 99.79\n",
            "Epoch : 284, training loss : 0.0058, training accuracy : 99.79, test loss : 0.4510, test accuracy : 94.35\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0097, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0085, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0077, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0081, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0083, accuracy : 99.72\n",
            "Epoch : 285, training loss : 0.0081, training accuracy : 99.72, test loss : 0.4451, test accuracy : 94.36\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0139, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0137, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0135, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0124, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0121, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0118, accuracy : 99.63\n",
            "Epoch : 286, training loss : 0.0114, training accuracy : 99.64, test loss : 0.4251, test accuracy : 94.37\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0060, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0059, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0058, accuracy : 99.79\n",
            "Epoch : 287, training loss : 0.0058, training accuracy : 99.79, test loss : 0.4429, test accuracy : 94.39\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0080, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0090, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0092, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0092, accuracy : 99.70\n",
            "Epoch : 288, training loss : 0.0094, training accuracy : 99.69, test loss : 0.4293, test accuracy : 94.36\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0062, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0063, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0061, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0058, accuracy : 99.80\n",
            "Epoch : 289, training loss : 0.0060, training accuracy : 99.80, test loss : 0.4464, test accuracy : 94.37\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0079, accuracy : 99.76\n",
            "Epoch : 290, training loss : 0.0079, training accuracy : 99.76, test loss : 0.4292, test accuracy : 94.35\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0125, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0103, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0080, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.75\n",
            "Epoch : 291, training loss : 0.0092, training accuracy : 99.73, test loss : 0.4365, test accuracy : 94.40\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0078, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0082, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0079, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0089, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0097, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0099, accuracy : 99.67\n",
            "Epoch : 292, training loss : 0.0100, training accuracy : 99.66, test loss : 0.4075, test accuracy : 94.29\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0083, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0078, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0076, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0078, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0081, accuracy : 99.75\n",
            "Epoch : 293, training loss : 0.0080, training accuracy : 99.75, test loss : 0.4073, test accuracy : 94.47\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0098, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0064, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0065, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0068, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0073, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0071, accuracy : 99.78\n",
            "Epoch : 294, training loss : 0.0070, training accuracy : 99.78, test loss : 0.4307, test accuracy : 94.56\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0064, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0065, accuracy : 99.80\n",
            "Epoch : 295, training loss : 0.0065, training accuracy : 99.80, test loss : 0.4238, test accuracy : 94.63\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0081, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0084, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0089, accuracy : 99.69\n",
            "Epoch : 296, training loss : 0.0088, training accuracy : 99.70, test loss : 0.4178, test accuracy : 94.35\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0080, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0079, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0085, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0085, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0079, accuracy : 99.69\n",
            "Epoch : 297, training loss : 0.0078, training accuracy : 99.70, test loss : 0.4168, test accuracy : 94.44\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0069, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0063, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0064, accuracy : 99.77\n",
            "Epoch : 298, training loss : 0.0064, training accuracy : 99.77, test loss : 0.4319, test accuracy : 94.40\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0107, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0105, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0095, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0089, accuracy : 99.71\n",
            "Epoch : 299, training loss : 0.0093, training accuracy : 99.71, test loss : 0.4225, test accuracy : 94.26\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0076, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0072, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0070, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0073, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0072, accuracy : 99.76\n",
            "Epoch : 300, training loss : 0.0074, training accuracy : 99.75, test loss : 0.4653, test accuracy : 93.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "id": "iUQVIKR-X3v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71eb65d3-37f5-433f-dab5-f92df8df1db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4382414690711919, 94.27243392747388)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : Linear\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7CNz1iabSB21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ecd6bb1d-27fd-47a1-a354-abf4b691edbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : Linear\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ff1902ca-4365-4254-f5b2-0f5fddbc92dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_linear = {train_loss_list}\") \n",
        "print(f\"train_acc_list_linear = {train_acc_list}\")\n",
        "print(f\"test_loss_list_linear = {test_loss_list}\")\n",
        "print(f\"test_acc_list_linear = {test_acc_list}\")"
      ],
      "metadata": {
        "id": "3eiY3bTlWipW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897d4be2-0e9a-4bc5-e1f0-dc020a719aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_linear = [1.0505396097010067, 0.41031305952285363, 0.34015994405924144, 0.3038505317194029, 0.27671615290771007, 0.2539030461090044, 0.24293845730422312, 0.2313264879753919, 0.22205986383403867, 0.21279181394635177, 0.20017173882260877, 0.19267895106014196, 0.18516845405505603, 0.18218227440546844, 0.17281363284604012, 0.16757936995264475, 0.1608044776490064, 0.1526946394343363, 0.1503949542921087, 0.14460325269861435, 0.1379334318267338, 0.13074936200347212, 0.12652073063834654, 0.12361395870888137, 0.11955785878043027, 0.11495751238915171, 0.10918548092407586, 0.10549298947964741, 0.10151599690843081, 0.09746526491886312, 0.09452159306524084, 0.09095732697353857, 0.08772212413056352, 0.08348307219462667, 0.08420775942554884, 0.07539896097104885, 0.07707164484154443, 0.07315899130855473, 0.07151561531902047, 0.06873627238071506, 0.06587666986752452, 0.060232834865876696, 0.059981180607343754, 0.06175297403930204, 0.05708458852451951, 0.05296678836066225, 0.057111382709063976, 0.053133565748266894, 0.05266129370491073, 0.050845057070684346, 0.04849550221115351, 0.0469664877891379, 0.04695639163596419, 0.04652371517443136, 0.04273645039004221, 0.042981587787635805, 0.040151388551610795, 0.03779632822411049, 0.04038962408859879, 0.04276739005321198, 0.035970243897294085, 0.0389290425395164, 0.034595519276827996, 0.034754975707721986, 0.03550548981976138, 0.034968214722219, 0.033513369056052934, 0.033747713374943145, 0.03146642715614011, 0.03088276023026253, 0.03096695340504784, 0.030255960882325888, 0.029922214430077934, 0.026458163596317426, 0.030674233368527256, 0.026803990912770092, 0.028383832357679988, 0.029045142340322395, 0.026921882543196714, 0.025758919344589033, 0.027054627095336514, 0.026907914471629563, 0.02701804478293749, 0.026367548554233428, 0.0261913293705708, 0.02126370853306549, 0.023870387598386333, 0.02343241305678558, 0.026290576351723865, 0.024439686884867598, 0.02106365902915024, 0.022621461096346544, 0.022915108798849105, 0.026723837544939667, 0.021479926218290063, 0.022599881692684752, 0.02025709556021947, 0.019150306689010323, 0.0245601841217654, 0.02191897199554167, 0.01751039288937257, 0.019330726730524307, 0.019164910448808477, 0.024538845911642237, 0.015059424295682626, 0.020382445553086728, 0.022040061389306198, 0.018767614002140155, 0.019355695793985044, 0.019107737408797596, 0.017818392037986942, 0.017417942697963446, 0.021768966118750215, 0.017869346996114355, 0.017951215525590914, 0.01640004834187341, 0.01777675135988997, 0.019044938084980482, 0.015788522874557873, 0.022552578193564623, 0.019296770346089745, 0.014811130384146363, 0.015027894850487148, 0.018469126043487265, 0.018098258266041212, 0.014053442152699729, 0.015989337644331865, 0.018125206405116463, 0.016060621967932922, 0.014774189744400855, 0.01385672290374157, 0.018077892058706895, 0.01727216446030504, 0.014268980369684513, 0.015724211079724697, 0.016640492273927766, 0.014277730624145401, 0.016971004403142326, 0.014619164786088283, 0.016830329990812937, 0.01374343925469295, 0.012524507506886185, 0.01549116191725552, 0.013598875393053052, 0.0151209402418879, 0.014543565561276227, 0.01439005304061679, 0.01173475706005888, 0.016134371224896974, 0.011917180406560032, 0.012485065904832735, 0.0135999512963786, 0.01591276152397482, 0.011587380154317354, 0.009772636422980987, 0.01344329923276031, 0.015756404174912624, 0.014378462400149205, 0.014014234783006126, 0.012014912663851542, 0.01611480384725613, 0.012844832076313767, 0.013578993814619104, 0.011672752483787808, 0.014098815763512592, 0.012935566670426325, 0.015032568787632202, 0.012224024389351091, 0.014372231721552152, 0.011108738588597336, 0.011174887407980897, 0.016397042357398143, 0.007919598232576864, 0.011932518933157513, 0.012465056306042874, 0.012968409797498517, 0.012325529049170838, 0.011099239793170491, 0.011867815633636468, 0.012848886264346762, 0.012182228031996032, 0.011562556203482023, 0.010714672763688681, 0.013554639555182596, 0.012572727286869529, 0.01000095399527329, 0.011995266647266443, 0.011573533052920085, 0.012627172824654724, 0.013339461832399402, 0.010677528724291735, 0.011543188395016422, 0.010650319310586942, 0.009674460366150607, 0.010641892690030602, 0.012230279986791725, 0.01145689182891605, 0.01050705676828164, 0.00970218700554869, 0.011088927940358716, 0.010147759896973753, 0.0144252521625949, 0.009280225094541092, 0.0071248399373703905, 0.013807516299508516, 0.01255990872180107, 0.011576532107942895, 0.008876261175843192, 0.009988300831966212, 0.00985363164915767, 0.01084158176923064, 0.011923880645647825, 0.010890684340816107, 0.011154608579179295, 0.010417346872992216, 0.00848533340091224, 0.009280508183635208, 0.012399288785308195, 0.0072954312946646145, 0.007857306191068237, 0.013135696996073112, 0.007985612870672613, 0.009203406998578557, 0.011825267453099425, 0.009498726522558978, 0.010254976841146922, 0.012558721663513307, 0.007996204799592626, 0.007471766532690587, 0.01177365344033834, 0.010632048703338191, 0.01163114502161057, 0.008471069070895856, 0.008329393120841747, 0.010546008776983772, 0.007213423388353836, 0.009344507342992069, 0.01120725078885267, 0.008723668579122892, 0.008438797396795254, 0.009862698597215867, 0.011641287900165775, 0.009182740705942006, 0.00702334287046593, 0.010164535000019891, 0.008744396335386742, 0.009129313499368456, 0.008084669785307412, 0.010208096987843342, 0.011367271971373962, 0.007117051272343865, 0.008831601460626934, 0.00922884266564445, 0.009587360389017516, 0.008383878850809764, 0.007664171105706518, 0.010088089144474219, 0.007183663350923383, 0.010528553138330638, 0.010388069153280813, 0.008873608166863086, 0.007134666277026189, 0.00842848956719368, 0.008079520347828268, 0.009280271344362845, 0.008955260842407897, 0.010437573963957794, 0.007464409461843186, 0.00868264229136562, 0.008687793158964795, 0.008216140737795655, 0.007891892086514802, 0.007512410591053153, 0.008665417990315836, 0.008976994133145053, 0.008162402152780846, 0.008810677293791461, 0.006835815719566462, 0.007850608918254282, 0.010575828147088042, 0.008934358710317107, 0.0076927311314263745, 0.007455576667292384, 0.005771527193367357, 0.008088972039366073, 0.011438270648105747, 0.005829402718917685, 0.009437662637821316, 0.006043440636937449, 0.007854135620251515, 0.009180552848274084, 0.010004485826416422, 0.007954190094466415, 0.0069836680795018575, 0.006469878575636705, 0.008808609063802835, 0.007791922163181544, 0.006387679697369947, 0.009282423594551585, 0.007415674569260415]\n",
            "train_acc_list_linear = [64.05293806246691, 87.14875595553202, 89.55002646903124, 90.75913181577555, 91.62308099523557, 92.33456855479089, 92.87030174695606, 93.13922710428798, 93.25780836421387, 93.82106934886183, 94.23822128110112, 94.36315510852303, 94.60667019587083, 94.74007411328745, 95.01111699311805, 95.09158284806776, 95.25674960296453, 95.67178401270513, 95.61461090524087, 95.74377977766014, 95.9915299100053, 96.22869242985706, 96.29645314981472, 96.45526733721546, 96.42985706723134, 96.65431445209106, 96.86818422445738, 96.87241926945474, 97.08628904182108, 97.16040232927475, 97.2641609317099, 97.3361566966649, 97.35309687665432, 97.51191106405506, 97.51191106405506, 97.76601376389624, 97.7342509264161, 97.88035997882477, 97.82953943885654, 97.96717840127052, 98.00317628374802, 98.16622551614611, 98.13658020116463, 98.11540497617787, 98.1937533086289, 98.37374272101641, 98.19798835362626, 98.44150344097406, 98.33562731604023, 98.41821069348862, 98.43303335097936, 98.4923239809423, 98.5643197458973, 98.4923239809423, 98.66807834833246, 98.64902064584436, 98.69348861831656, 98.76971942826893, 98.68925357331922, 98.64690312334568, 98.82053996823716, 98.74219163578613, 98.81842244573849, 98.86500794070938, 98.88194812069878, 98.78877713075701, 98.90100582318688, 98.90312334568554, 98.96664902064585, 98.97723663313923, 98.98358920063525, 99.0428798305982, 99.01111699311805, 99.12122816304924, 98.98570672313393, 99.10852302805718, 99.09158284806776, 99.05134992059291, 99.1233456855479, 99.1868713605082, 99.0873478030704, 99.10217046056114, 99.08523028057174, 99.13181577554262, 99.11064055055584, 99.33721545791424, 99.22710428798305, 99.17628374801482, 99.0979354155638, 99.20169401799895, 99.2694547379566, 99.2419269454738, 99.25463208046585, 99.08099523557438, 99.26733721545791, 99.2419269454738, 99.30968766543144, 99.3223928004235, 99.1868713605082, 99.25463208046585, 99.40921122286925, 99.36262572789836, 99.3499205929063, 99.16357861302276, 99.52355743779778, 99.31815775542616, 99.2779248279513, 99.37109581789306, 99.3499205929063, 99.37321334039174, 99.36050820539968, 99.42826892535733, 99.22075172048703, 99.41979883536263, 99.3943885653785, 99.46426680783483, 99.39015352038115, 99.33509793541556, 99.47697194282689, 99.28427739544733, 99.36050820539968, 99.5574377977766, 99.5044997353097, 99.41556379036527, 99.38803599788248, 99.5489677077819, 99.49179460031763, 99.37956590788777, 99.47697194282689, 99.5214399152991, 99.55108523028058, 99.42403388035999, 99.45156167284277, 99.51720487030175, 99.47273689782953, 99.42403388035999, 99.5299100052938, 99.41132874536792, 99.48755955532027, 99.46638433033351, 99.53202752779248, 99.57014293276866, 99.49179460031763, 99.53626257278984, 99.50026469031233, 99.50026469031233, 99.5299100052938, 99.61884595023822, 99.45579671784013, 99.59978824775013, 99.55108523028058, 99.55532027527792, 99.47273689782953, 99.62519851773425, 99.67813658020117, 99.57437797776602, 99.45579671784013, 99.5299100052938, 99.50873478030704, 99.61672842773955, 99.46214928533615, 99.59555320275278, 99.57014293276866, 99.6209634727369, 99.53414505029116, 99.58920063525674, 99.5044997353097, 99.57014293276866, 99.53626257278984, 99.58073054526204, 99.6209634727369, 99.46850185283219, 99.72260455267337, 99.58920063525674, 99.55955532027528, 99.56379036527264, 99.60825833774484, 99.64849126521969, 99.60402329274748, 99.5574377977766, 99.60825833774484, 99.61884595023822, 99.63790365272631, 99.56802541026998, 99.58284806776072, 99.66754896770779, 99.58708311275808, 99.62519851773425, 99.57226045526734, 99.59767072525146, 99.64213869772367, 99.60614081524616, 99.63366860772896, 99.66331392271043, 99.65907887771307, 99.58920063525674, 99.61884595023822, 99.67813658020117, 99.66119640021175, 99.62731604023293, 99.68237162519851, 99.57437797776602, 99.66543144520911, 99.80307040762308, 99.5574377977766, 99.5659078877713, 99.63366860772896, 99.69931180518793, 99.66966649020645, 99.67178401270513, 99.63578613022763, 99.59767072525146, 99.63790365272631, 99.66119640021175, 99.64425622022235, 99.72683959767072, 99.68660667019587, 99.61249338274219, 99.7734250926416, 99.7480148226575, 99.56379036527264, 99.74589730015882, 99.67813658020117, 99.62731604023293, 99.69931180518793, 99.65484383271573, 99.59767072525146, 99.7649550026469, 99.73319216516676, 99.61037586024352, 99.65060878771837, 99.62731604023293, 99.72260455267337, 99.73107464266808, 99.66543144520911, 99.76071995764956, 99.71413446267867, 99.62731604023293, 99.74166225516146, 99.71201694017999, 99.67178401270513, 99.58708311275808, 99.69507676019057, 99.73954473266278, 99.68025410269983, 99.71836950767602, 99.70566437268396, 99.73107464266808, 99.65272631021705, 99.61037586024352, 99.79460031762838, 99.71201694017999, 99.68872419269455, 99.70778189518263, 99.7564849126522, 99.7564849126522, 99.66543144520911, 99.76919004764426, 99.64213869772367, 99.66966649020645, 99.71836950767602, 99.76071995764956, 99.73319216516676, 99.75224986765484, 99.6929592376919, 99.67813658020117, 99.65907887771307, 99.72472207517205, 99.68872419269455, 99.71413446267867, 99.72472207517205, 99.76071995764956, 99.7924827951297, 99.69719428268925, 99.68448914769719, 99.73107464266808, 99.73742721016411, 99.76283748014822, 99.75224986765484, 99.66543144520911, 99.70354685018528, 99.7564849126522, 99.76283748014822, 99.79460031762838, 99.72260455267337, 99.64002117522499, 99.79036527263102, 99.69084171519323, 99.8009528851244, 99.75860243515088, 99.73107464266808, 99.6569613552144, 99.75013234515616, 99.77977766013764, 99.79883536262572, 99.70142932768661, 99.69507676019057, 99.7734250926416, 99.70989941768131, 99.7480148226575]\n",
            "test_loss_list_linear = [0.6001535044873462, 0.4386981564993952, 0.32313791202271686, 0.3436826935001448, 0.3047397654576629, 0.28500792238057826, 0.26378655251042515, 0.2683426228297107, 0.27285771396960695, 0.2532431707516605, 0.25363739693135606, 0.25782927618745494, 0.2381599084200228, 0.24098271157081222, 0.23853215436432876, 0.22876223501767598, 0.2459365823762674, 0.24074384005849853, 0.23645990466078123, 0.23522190126937395, 0.23930488064812094, 0.23977934446770185, 0.22856533881642072, 0.2503986795633739, 0.2523015177096514, 0.2286284007497278, 0.2522817155821066, 0.23435633048853455, 0.25106761294106644, 0.23668714861075082, 0.25767872133748787, 0.2464016547937896, 0.24990028208669493, 0.25964026233437015, 0.2509775684014255, 0.25643125852095144, 0.25149472640352505, 0.2636343431750349, 0.2690670864802657, 0.2728121512952973, 0.2877597091719508, 0.2628066184400928, 0.289077399882908, 0.2637001336643509, 0.2718517802947876, 0.30822780637034014, 0.2853905560412243, 0.3029168626914422, 0.2886001042948634, 0.30125397385335434, 0.2918206257928236, 0.31290997323744435, 0.3016124471894228, 0.3364011060409978, 0.30226883032888757, 0.30097986143264993, 0.3160975992350894, 0.324009220079318, 0.2896068644545534, 0.28848243194321793, 0.30809056048518885, 0.31027250127026845, 0.31317287329219134, 0.3204711799728958, 0.323861251740406, 0.30745547297684583, 0.313345615642474, 0.31225476020435783, 0.32664194748755176, 0.3649349614393477, 0.3289772248781268, 0.34484266127715363, 0.34519970382326376, 0.3562348909566508, 0.34551431653181125, 0.3507686852495752, 0.3557186407749267, 0.34874562685396154, 0.3626312471414898, 0.32565479080977977, 0.3328241787111277, 0.3502718099739914, 0.35965223179436195, 0.33991847547026827, 0.32748869253212914, 0.3568636128751963, 0.366436490642966, 0.36107295554350405, 0.34370773008056715, 0.343383331070928, 0.3387923375794701, 0.33830804925631075, 0.3589262347485797, 0.3675877243079537, 0.3818419510498643, 0.35536084654649686, 0.35281344309595286, 0.3716038850629154, 0.35846942082485733, 0.34315043032242387, 0.36869288876871853, 0.3778895241226636, 0.38406652862242624, 0.352233542257226, 0.3620971898712656, 0.37449508531055614, 0.37493528568131085, 0.36601132105159406, 0.36861560567665624, 0.3717369966793294, 0.369726698087784, 0.3781300349087984, 0.35664984070714195, 0.3536169459751132, 0.406203392372631, 0.3737115380445532, 0.36973171939562055, 0.37459768973948326, 0.40706704461983606, 0.3881958369896108, 0.37344259142364356, 0.3670044747710812, 0.3696818688847855, 0.37015472998952165, 0.38189552233134416, 0.3749138325744984, 0.3891030573742647, 0.3678013856029686, 0.36681280497863306, 0.3926687809620418, 0.4037954283184281, 0.4014408321065061, 0.3803745116673264, 0.38791271775741787, 0.37617705127808687, 0.3729255223537193, 0.36608954668775495, 0.3800407153572522, 0.37864029130843635, 0.3903196998415332, 0.39593674976597815, 0.4233312764953749, 0.41837985076092404, 0.4037898524353902, 0.39368393412772934, 0.40711697678574743, 0.3831657852119237, 0.39936033349630295, 0.4122653530234946, 0.4055917443908459, 0.391819414573119, 0.3998463283493823, 0.3856644867325896, 0.38932094629853964, 0.45811930652159977, 0.4117749379582557, 0.3846923142269838, 0.38948512334814844, 0.3886060610632686, 0.4141315968488069, 0.39312611405244646, 0.40893315564037536, 0.3917342342217179, 0.4033093715232669, 0.40686407040658534, 0.402124198153615, 0.3760559221218322, 0.41189432279298116, 0.3945868524777539, 0.4064665962668026, 0.4281328501380688, 0.3944055610610282, 0.4145982712644207, 0.39619766517231864, 0.4008463170269833, 0.4100334781368135, 0.3959438812681565, 0.4048078361277779, 0.3898382809506181, 0.3940078625638111, 0.41857934771699135, 0.40016016322553305, 0.42573420590191496, 0.4076968614815497, 0.40349324788971275, 0.41680151768320917, 0.4404344489323158, 0.4126104597257925, 0.41489401694464806, 0.39322311091510687, 0.39995222047482637, 0.4001541677862406, 0.40168759471955984, 0.4003755842317261, 0.4116448443930815, 0.3964004161512004, 0.40526396525092423, 0.40040231844885094, 0.436148803023731, 0.42100324081804824, 0.42921065237811384, 0.4016612854548821, 0.4237121128860642, 0.439825722326835, 0.40309608828586835, 0.4022408921143734, 0.40526854241376414, 0.4139130270638156, 0.42377193091327653, 0.4159748113506437, 0.43089511207140546, 0.41587188575124623, 0.42511460325662415, 0.42877695572507735, 0.4061464395730154, 0.4108063448129185, 0.4237177971266575, 0.4147245909142144, 0.4100258317867331, 0.44727529514599224, 0.40763759292552576, 0.4096112189969669, 0.4398777195818576, 0.4397024883450392, 0.42169872507014694, 0.4525160300362782, 0.4039532243748944, 0.42747145336048276, 0.41558541071500776, 0.4243580105614063, 0.4347197997529863, 0.40417124533697085, 0.4213481634563091, 0.42768024219492196, 0.41689553120922224, 0.42403014976640835, 0.43499788091353636, 0.40865986559576556, 0.39625785907949596, 0.41788156942793114, 0.42710175029203, 0.39860118992453186, 0.40182735008534554, 0.43774937425612237, 0.4086107819651564, 0.42035614554861594, 0.42495340735231546, 0.42730405055943477, 0.41829457241749646, 0.4210991954211803, 0.39986408456210415, 0.43789072635163573, 0.4252349973893633, 0.4179426681058591, 0.4189265777229094, 0.42167082347327334, 0.4152061831133038, 0.4278368429582128, 0.42154261065354826, 0.39703031598279875, 0.42471260858663157, 0.4301079974747172, 0.4101321147526523, 0.4378960633869557, 0.412081252308745, 0.4338206772089881, 0.4061669551530013, 0.4237026169088067, 0.4324107744557527, 0.4283796655671561, 0.4480068746077664, 0.42207027151815446, 0.4178310304186216, 0.42660820346289113, 0.423226847276822, 0.44835605301127274, 0.4472741484733335, 0.4282900923771747, 0.4323618656535651, 0.4345184702519784, 0.4196194001455225, 0.4023636024691822, 0.4263699218977754, 0.45100487594940136, 0.4451090616046214, 0.4251364004779972, 0.4428678105822673, 0.42934233425459, 0.4463761615374025, 0.42917647955519167, 0.43650849757935195, 0.40754444616865, 0.4072841090405835, 0.43068673907249583, 0.42376545924857695, 0.4177722467438263, 0.41679786920105794, 0.4319006550381435, 0.42252690807057947, 0.4652784176572573]\n",
            "test_acc_list_linear = [81.16548862937923, 86.56653349723418, 90.2581438229871, 89.47065150583897, 90.86893054701905, 91.77934849416103, 92.38629379225569, 92.26336816226183, 92.23263675476336, 92.60525507068223, 92.71665642286416, 92.73586355255071, 93.29287031346036, 93.27366318377382, 93.4081130915796, 93.58481868469576, 93.13921327596803, 93.3620159803319, 93.6078672403196, 93.46957590657652, 93.5540872771973, 93.73847572218807, 93.77688998156115, 93.21220036877689, 93.30055316533497, 94.06883835279656, 93.46957590657652, 94.04963122311001, 93.73847572218807, 93.8959741856177, 93.44652735095268, 93.91518131530424, 93.73847572218807, 93.6578057775046, 93.9881684081131, 93.98432698217579, 93.95743700061463, 93.98432698217579, 93.8921327596804, 93.92670559311617, 93.68469575906576, 94.3338967424708, 93.6040258143823, 94.10341118623234, 94.14950829748003, 93.79609711124769, 94.17255685310387, 93.64244007375538, 93.9958512599877, 93.56945298094652, 94.00353411186232, 93.67317148125385, 93.96127842655194, 93.50799016594961, 93.99969268592501, 94.3300553165335, 93.97664413030117, 93.78457283343577, 94.16871542716656, 94.10341118623234, 94.16103257529196, 94.16487400122925, 94.05731407498463, 94.11877688998156, 94.12645974185618, 94.16487400122925, 94.17639827904118, 94.17639827904118, 94.18792255685311, 94.02658266748617, 94.1379840196681, 93.90365703749232, 93.80762138905962, 93.93054701905348, 94.00353411186232, 93.98432698217579, 93.97280270436386, 94.06499692685925, 93.99969268592501, 94.25706822372464, 94.41840811309157, 94.01505838967425, 93.78457283343577, 94.14566687154272, 94.25706822372464, 94.04963122311001, 93.8959741856177, 93.97280270436386, 94.13030116779349, 94.41072526121697, 94.20712968653964, 94.31084818684695, 93.77304855562384, 93.85755992624462, 93.78073140749846, 94.21865396435157, 94.3262138905962, 93.96896127842655, 94.20328826060233, 94.43377381684081, 94.19944683466503, 94.17639827904118, 93.82298709280884, 94.3761524277812, 94.31084818684695, 94.13030116779349, 94.18023970497849, 94.27627535341118, 94.14950829748003, 94.30700676090964, 94.41456668715428, 94.35310387215735, 94.10341118623234, 94.3262138905962, 93.75, 94.23786109403811, 94.28779963122311, 93.94975414874001, 93.8921327596804, 93.83066994468346, 94.08420405654579, 94.37231100184388, 94.17255685310387, 94.44913952059004, 94.14950829748003, 94.3761524277812, 94.22249539028887, 94.21865396435157, 94.53365089121081, 94.14566687154272, 94.16487400122925, 93.91133988936693, 93.98432698217579, 94.2839582052858, 94.19944683466503, 94.21481253841426, 94.3338967424708, 93.98432698217579, 94.18792255685311, 94.03042409342348, 94.27627535341118, 93.83066994468346, 94.12261831591887, 94.02274124154886, 94.13030116779349, 94.0880454824831, 94.52980946527352, 94.0880454824831, 94.02658266748617, 94.3262138905962, 94.31084818684695, 94.42609096496619, 94.39535955746773, 94.42224953902888, 93.99969268592501, 94.16871542716656, 94.35310387215735, 94.23017824216349, 94.26475107559926, 94.39535955746773, 94.49139520590043, 94.26090964966195, 94.51444376152428, 94.40304240934235, 94.15719114935465, 94.16103257529196, 94.41840811309157, 94.17639827904118, 94.31853103872157, 94.31853103872157, 94.00737553779963, 94.3338967424708, 94.34926244622004, 94.3338967424708, 94.24554394591273, 94.51060233558697, 94.39151813153042, 94.27243392747388, 94.43377381684081, 94.3262138905962, 94.23017824216349, 94.24170251997542, 93.86908420405655, 94.13030116779349, 94.29164105716042, 94.35694529809466, 94.19560540872772, 94.3799938537185, 94.3338967424708, 94.25322679778733, 94.2839582052858, 94.2340196681008, 94.26859250153657, 94.69114935464044, 94.16487400122925, 94.46834665027659, 94.27243392747388, 94.36846957590657, 94.10725261216963, 94.43377381684081, 94.23786109403811, 94.46450522433928, 94.32237246465888, 94.28779963122311, 94.11493546404425, 94.16871542716656, 94.4299323909035, 94.30700676090964, 94.19560540872772, 94.46066379840197, 94.17639827904118, 94.53365089121081, 94.16871542716656, 94.29164105716042, 94.29548248309773, 94.49523663183774, 94.25706822372464, 94.10341118623234, 94.64889366933005, 94.30316533497235, 94.42609096496619, 94.44529809465274, 94.19944683466503, 94.1917639827904, 94.44529809465274, 94.16103257529196, 94.3338967424708, 94.54901659496005, 94.4721880762139, 94.31084818684695, 94.39920098340504, 94.49139520590043, 94.3761524277812, 94.45298094652735, 94.31468961278426, 94.45682237246466, 94.30316533497235, 94.55285802089736, 94.4299323909035, 94.40688383527966, 94.34542102028273, 94.4299323909035, 94.49139520590043, 94.27243392747388, 94.47602950215119, 94.26090964966195, 94.24554394591273, 94.54517516902274, 94.3300553165335, 94.29548248309773, 94.61432083589429, 94.11877688998156, 94.40304240934235, 94.27243392747388, 94.59511370620774, 94.2839582052858, 94.39151813153042, 94.39535955746773, 94.21865396435157, 94.49523663183774, 94.24554394591273, 94.51060233558697, 94.59127228027043, 94.43377381684081, 94.31853103872157, 94.46834665027659, 94.43761524277812, 94.43761524277812, 94.37231100184388, 94.34157959434542, 94.00353411186232, 94.38383527965581, 94.58358942839583, 94.5759065765212, 94.41840811309157, 94.07652120467118, 94.09956976029503, 94.3799938537185, 94.30700676090964, 94.29932390903504, 94.38383527965581, 94.5259680393362, 94.26859250153657, 94.35310387215735, 94.36462814996926, 94.37231100184388, 94.38767670559312, 94.35694529809466, 94.36846957590657, 94.34542102028273, 94.40304240934235, 94.28779963122311, 94.46834665027659, 94.56054087277197, 94.62968653964352, 94.35310387215735, 94.43761524277812, 94.39535955746773, 94.26475107559926, 93.79609711124769]\n"
          ]
        }
      ]
    }
  ]
}