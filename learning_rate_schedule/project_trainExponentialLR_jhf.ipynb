{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgAiImV0uURP",
        "outputId": "bef035df-b142-47f4-8a2a-42caf944cfbd"
      },
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./data/train_32x32.mat\n",
            "Using downloaded and verified file: ./data/test_32x32.mat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "862c0e7f-4fc4-4a5b-8785-294161b008e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "a9df5999-cfc2-4d27-9a16-5ead1b50e71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "753c6f3f-7f51-4c70-e00f-1f903bc8c81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArgupDVRwB8i",
        "outputId": "e9e4293d-a7c9-43b7-9041-8774a785e490"
      },
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3475, accuracy : 16.53\n",
            "iteration : 100, loss : 2.2805, accuracy : 18.83\n",
            "iteration : 150, loss : 2.2068, accuracy : 21.65\n",
            "iteration : 200, loss : 2.0536, accuracy : 26.98\n",
            "iteration : 250, loss : 1.8730, accuracy : 33.77\n",
            "iteration : 300, loss : 1.7006, accuracy : 40.18\n",
            "iteration : 350, loss : 1.5581, accuracy : 45.48\n",
            "Epoch :   1, training loss : 1.5108, training accuracy : 47.22, test loss : 0.7600, test accuracy : 75.59\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.5415, accuracy : 82.64\n",
            "iteration : 100, loss : 0.5114, accuracy : 83.78\n",
            "iteration : 150, loss : 0.4903, accuracy : 84.47\n",
            "iteration : 200, loss : 0.4795, accuracy : 84.80\n",
            "iteration : 250, loss : 0.4672, accuracy : 85.25\n",
            "iteration : 300, loss : 0.4582, accuracy : 85.60\n",
            "iteration : 350, loss : 0.4577, accuracy : 85.60\n",
            "Epoch :   2, training loss : 0.4557, training accuracy : 85.66, test loss : 0.4139, test accuracy : 87.07\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.3953, accuracy : 87.36\n",
            "iteration : 100, loss : 0.4025, accuracy : 87.20\n",
            "iteration : 150, loss : 0.3912, accuracy : 87.80\n",
            "iteration : 200, loss : 0.3902, accuracy : 87.84\n",
            "iteration : 250, loss : 0.3887, accuracy : 87.92\n",
            "iteration : 300, loss : 0.3904, accuracy : 87.90\n",
            "iteration : 350, loss : 0.3888, accuracy : 87.94\n",
            "Epoch :   3, training loss : 0.3896, training accuracy : 87.93, test loss : 0.3922, test accuracy : 87.53\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3852, accuracy : 87.86\n",
            "iteration : 100, loss : 0.3813, accuracy : 88.00\n",
            "iteration : 150, loss : 0.3784, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3840, accuracy : 88.03\n",
            "iteration : 250, loss : 0.3835, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3826, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3809, accuracy : 88.11\n",
            "Epoch :   4, training loss : 0.3820, training accuracy : 88.12, test loss : 0.3909, test accuracy : 87.89\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.3585, accuracy : 88.67\n",
            "iteration : 100, loss : 0.3656, accuracy : 88.52\n",
            "iteration : 150, loss : 0.3728, accuracy : 88.27\n",
            "iteration : 200, loss : 0.3786, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3820, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3794, accuracy : 88.24\n",
            "Epoch :   5, training loss : 0.3792, training accuracy : 88.25, test loss : 0.3910, test accuracy : 87.77\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.3717, accuracy : 88.55\n",
            "iteration : 100, loss : 0.3755, accuracy : 88.23\n",
            "iteration : 150, loss : 0.3789, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3772, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3802, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.06\n",
            "Epoch :   6, training loss : 0.3785, training accuracy : 88.08, test loss : 0.3915, test accuracy : 87.75\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.3859, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3841, accuracy : 87.97\n",
            "iteration : 150, loss : 0.3816, accuracy : 88.04\n",
            "iteration : 200, loss : 0.3807, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3795, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3790, accuracy : 88.20\n",
            "Epoch :   7, training loss : 0.3779, training accuracy : 88.24, test loss : 0.3894, test accuracy : 87.99\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.3857, accuracy : 87.81\n",
            "iteration : 100, loss : 0.3873, accuracy : 87.88\n",
            "iteration : 150, loss : 0.3855, accuracy : 88.00\n",
            "iteration : 200, loss : 0.3787, accuracy : 88.23\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.19\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.23\n",
            "Epoch :   8, training loss : 0.3783, training accuracy : 88.18, test loss : 0.3928, test accuracy : 87.76\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.3961, accuracy : 87.89\n",
            "iteration : 100, loss : 0.3895, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3880, accuracy : 87.90\n",
            "iteration : 200, loss : 0.3876, accuracy : 87.83\n",
            "iteration : 250, loss : 0.3847, accuracy : 87.92\n",
            "iteration : 300, loss : 0.3825, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.09\n",
            "Epoch :   9, training loss : 0.3789, training accuracy : 88.14, test loss : 0.3913, test accuracy : 87.88\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.3730, accuracy : 88.55\n",
            "iteration : 100, loss : 0.3747, accuracy : 88.17\n",
            "iteration : 150, loss : 0.3775, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3770, accuracy : 88.08\n",
            "iteration : 250, loss : 0.3771, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3770, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.08\n",
            "Epoch :  10, training loss : 0.3799, training accuracy : 88.10, test loss : 0.3898, test accuracy : 87.72\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.3786, accuracy : 88.17\n",
            "iteration : 100, loss : 0.3897, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3826, accuracy : 88.18\n",
            "iteration : 200, loss : 0.3799, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3851, accuracy : 87.99\n",
            "iteration : 300, loss : 0.3816, accuracy : 88.05\n",
            "iteration : 350, loss : 0.3814, accuracy : 88.02\n",
            "Epoch :  11, training loss : 0.3802, training accuracy : 88.07, test loss : 0.3919, test accuracy : 87.75\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.3683, accuracy : 88.72\n",
            "iteration : 100, loss : 0.3739, accuracy : 88.41\n",
            "iteration : 150, loss : 0.3792, accuracy : 88.21\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3799, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3771, accuracy : 88.18\n",
            "Epoch :  12, training loss : 0.3786, training accuracy : 88.14, test loss : 0.3930, test accuracy : 87.80\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.3940, accuracy : 87.80\n",
            "iteration : 100, loss : 0.3949, accuracy : 87.71\n",
            "iteration : 150, loss : 0.3971, accuracy : 87.69\n",
            "iteration : 200, loss : 0.3912, accuracy : 87.81\n",
            "iteration : 250, loss : 0.3849, accuracy : 87.88\n",
            "iteration : 300, loss : 0.3833, accuracy : 87.99\n",
            "iteration : 350, loss : 0.3826, accuracy : 87.96\n",
            "Epoch :  13, training loss : 0.3801, training accuracy : 88.01, test loss : 0.3876, test accuracy : 87.96\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.3895, accuracy : 87.88\n",
            "iteration : 100, loss : 0.3836, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3813, accuracy : 88.17\n",
            "iteration : 200, loss : 0.3832, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3812, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3821, accuracy : 87.97\n",
            "iteration : 350, loss : 0.3821, accuracy : 88.01\n",
            "Epoch :  14, training loss : 0.3819, training accuracy : 87.99, test loss : 0.3887, test accuracy : 87.81\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.3623, accuracy : 88.89\n",
            "iteration : 100, loss : 0.3658, accuracy : 88.84\n",
            "iteration : 150, loss : 0.3617, accuracy : 88.73\n",
            "iteration : 200, loss : 0.3693, accuracy : 88.52\n",
            "iteration : 250, loss : 0.3694, accuracy : 88.49\n",
            "iteration : 300, loss : 0.3732, accuracy : 88.38\n",
            "iteration : 350, loss : 0.3755, accuracy : 88.30\n",
            "Epoch :  15, training loss : 0.3756, training accuracy : 88.28, test loss : 0.3932, test accuracy : 87.81\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.3997, accuracy : 87.48\n",
            "iteration : 100, loss : 0.3872, accuracy : 87.85\n",
            "iteration : 150, loss : 0.3819, accuracy : 87.88\n",
            "iteration : 200, loss : 0.3766, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.01\n",
            "iteration : 300, loss : 0.3769, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3785, accuracy : 88.05\n",
            "Epoch :  16, training loss : 0.3785, training accuracy : 88.07, test loss : 0.3915, test accuracy : 87.81\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.3906, accuracy : 87.83\n",
            "iteration : 100, loss : 0.3891, accuracy : 87.70\n",
            "iteration : 150, loss : 0.3811, accuracy : 88.11\n",
            "iteration : 200, loss : 0.3799, accuracy : 88.08\n",
            "iteration : 250, loss : 0.3796, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.13\n",
            "Epoch :  17, training loss : 0.3810, training accuracy : 88.09, test loss : 0.3903, test accuracy : 87.94\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.3778, accuracy : 88.11\n",
            "iteration : 100, loss : 0.3849, accuracy : 88.03\n",
            "iteration : 150, loss : 0.3807, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3779, accuracy : 88.29\n",
            "iteration : 250, loss : 0.3784, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3785, accuracy : 88.16\n",
            "Epoch :  18, training loss : 0.3791, training accuracy : 88.11, test loss : 0.3896, test accuracy : 87.93\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.3749, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3712, accuracy : 88.31\n",
            "iteration : 150, loss : 0.3807, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3757, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.00\n",
            "iteration : 300, loss : 0.3779, accuracy : 88.00\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.01\n",
            "Epoch :  19, training loss : 0.3803, training accuracy : 87.96, test loss : 0.3941, test accuracy : 87.77\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.3714, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3843, accuracy : 87.91\n",
            "iteration : 150, loss : 0.3791, accuracy : 87.99\n",
            "iteration : 200, loss : 0.3753, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3767, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3747, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3767, accuracy : 88.11\n",
            "Epoch :  20, training loss : 0.3771, training accuracy : 88.11, test loss : 0.3924, test accuracy : 87.78\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.3909, accuracy : 87.88\n",
            "iteration : 100, loss : 0.3919, accuracy : 87.61\n",
            "iteration : 150, loss : 0.3857, accuracy : 87.84\n",
            "iteration : 200, loss : 0.3830, accuracy : 87.92\n",
            "iteration : 250, loss : 0.3811, accuracy : 88.00\n",
            "iteration : 300, loss : 0.3820, accuracy : 88.05\n",
            "iteration : 350, loss : 0.3785, accuracy : 88.15\n",
            "Epoch :  21, training loss : 0.3791, training accuracy : 88.13, test loss : 0.3913, test accuracy : 87.66\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.3730, accuracy : 87.78\n",
            "iteration : 100, loss : 0.3769, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3815, accuracy : 87.97\n",
            "iteration : 200, loss : 0.3845, accuracy : 87.88\n",
            "iteration : 250, loss : 0.3785, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3790, accuracy : 88.07\n",
            "iteration : 350, loss : 0.3790, accuracy : 88.12\n",
            "Epoch :  22, training loss : 0.3796, training accuracy : 88.14, test loss : 0.3941, test accuracy : 87.75\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.3680, accuracy : 88.64\n",
            "iteration : 100, loss : 0.3767, accuracy : 88.39\n",
            "iteration : 150, loss : 0.3791, accuracy : 88.34\n",
            "iteration : 200, loss : 0.3785, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3765, accuracy : 88.34\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3773, accuracy : 88.25\n",
            "Epoch :  23, training loss : 0.3777, training accuracy : 88.24, test loss : 0.3901, test accuracy : 87.75\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.3716, accuracy : 88.41\n",
            "iteration : 100, loss : 0.3788, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3763, accuracy : 88.27\n",
            "iteration : 200, loss : 0.3747, accuracy : 88.31\n",
            "iteration : 250, loss : 0.3761, accuracy : 88.30\n",
            "iteration : 300, loss : 0.3743, accuracy : 88.33\n",
            "iteration : 350, loss : 0.3767, accuracy : 88.21\n",
            "Epoch :  24, training loss : 0.3792, training accuracy : 88.17, test loss : 0.3911, test accuracy : 87.86\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.3696, accuracy : 87.81\n",
            "iteration : 100, loss : 0.3669, accuracy : 88.07\n",
            "iteration : 150, loss : 0.3717, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3739, accuracy : 88.05\n",
            "iteration : 250, loss : 0.3733, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3763, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3755, accuracy : 88.10\n",
            "Epoch :  25, training loss : 0.3764, training accuracy : 88.09, test loss : 0.3896, test accuracy : 87.67\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.3787, accuracy : 88.09\n",
            "iteration : 100, loss : 0.3743, accuracy : 88.25\n",
            "iteration : 150, loss : 0.3713, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3767, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3765, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3771, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3770, accuracy : 88.20\n",
            "Epoch :  26, training loss : 0.3762, training accuracy : 88.20, test loss : 0.3906, test accuracy : 87.71\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.3543, accuracy : 88.75\n",
            "iteration : 100, loss : 0.3740, accuracy : 88.34\n",
            "iteration : 150, loss : 0.3787, accuracy : 88.24\n",
            "iteration : 200, loss : 0.3819, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3805, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.14\n",
            "Epoch :  27, training loss : 0.3767, training accuracy : 88.20, test loss : 0.3924, test accuracy : 87.65\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.3909, accuracy : 87.66\n",
            "iteration : 100, loss : 0.3796, accuracy : 87.94\n",
            "iteration : 150, loss : 0.3814, accuracy : 87.99\n",
            "iteration : 200, loss : 0.3819, accuracy : 87.92\n",
            "iteration : 250, loss : 0.3813, accuracy : 87.91\n",
            "iteration : 300, loss : 0.3793, accuracy : 87.95\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.03\n",
            "Epoch :  28, training loss : 0.3780, training accuracy : 88.01, test loss : 0.3930, test accuracy : 87.66\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.3635, accuracy : 88.75\n",
            "iteration : 100, loss : 0.3803, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3819, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3859, accuracy : 87.80\n",
            "iteration : 250, loss : 0.3837, accuracy : 87.85\n",
            "iteration : 300, loss : 0.3836, accuracy : 87.92\n",
            "iteration : 350, loss : 0.3818, accuracy : 87.95\n",
            "Epoch :  29, training loss : 0.3797, training accuracy : 88.01, test loss : 0.3910, test accuracy : 87.74\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.3611, accuracy : 88.20\n",
            "iteration : 100, loss : 0.3754, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3710, accuracy : 88.08\n",
            "iteration : 200, loss : 0.3742, accuracy : 88.13\n",
            "iteration : 250, loss : 0.3758, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3768, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3792, accuracy : 88.13\n",
            "Epoch :  30, training loss : 0.3769, training accuracy : 88.20, test loss : 0.3926, test accuracy : 87.75\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.3877, accuracy : 87.95\n",
            "iteration : 100, loss : 0.3801, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3752, accuracy : 88.37\n",
            "iteration : 200, loss : 0.3734, accuracy : 88.46\n",
            "iteration : 250, loss : 0.3765, accuracy : 88.30\n",
            "iteration : 300, loss : 0.3779, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.10\n",
            "Epoch :  31, training loss : 0.3782, training accuracy : 88.15, test loss : 0.3910, test accuracy : 87.86\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.3666, accuracy : 88.70\n",
            "iteration : 100, loss : 0.3749, accuracy : 88.34\n",
            "iteration : 150, loss : 0.3747, accuracy : 88.21\n",
            "iteration : 200, loss : 0.3734, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3791, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3765, accuracy : 88.29\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.25\n",
            "Epoch :  32, training loss : 0.3786, training accuracy : 88.22, test loss : 0.3919, test accuracy : 87.90\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.3788, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3787, accuracy : 87.91\n",
            "iteration : 150, loss : 0.3769, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3791, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3821, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3817, accuracy : 88.00\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.09\n",
            "Epoch :  33, training loss : 0.3801, training accuracy : 88.05, test loss : 0.3932, test accuracy : 87.65\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.3795, accuracy : 87.97\n",
            "iteration : 100, loss : 0.3745, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3817, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3819, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.16\n",
            "iteration : 300, loss : 0.3804, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3791, accuracy : 88.09\n",
            "Epoch :  34, training loss : 0.3801, training accuracy : 88.07, test loss : 0.3947, test accuracy : 87.61\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.3894, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3849, accuracy : 88.04\n",
            "iteration : 150, loss : 0.3769, accuracy : 88.30\n",
            "iteration : 200, loss : 0.3767, accuracy : 88.37\n",
            "iteration : 250, loss : 0.3778, accuracy : 88.29\n",
            "iteration : 300, loss : 0.3773, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.28\n",
            "Epoch :  35, training loss : 0.3785, training accuracy : 88.29, test loss : 0.3895, test accuracy : 88.02\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.3783, accuracy : 87.86\n",
            "iteration : 100, loss : 0.3801, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3808, accuracy : 88.01\n",
            "iteration : 200, loss : 0.3809, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3808, accuracy : 88.21\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.14\n",
            "Epoch :  36, training loss : 0.3776, training accuracy : 88.19, test loss : 0.3913, test accuracy : 87.86\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.3953, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3930, accuracy : 88.02\n",
            "iteration : 150, loss : 0.3903, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3842, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3812, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3797, accuracy : 88.09\n",
            "Epoch :  37, training loss : 0.3798, training accuracy : 88.11, test loss : 0.3929, test accuracy : 87.50\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.3854, accuracy : 87.86\n",
            "iteration : 100, loss : 0.3867, accuracy : 87.90\n",
            "iteration : 150, loss : 0.3793, accuracy : 88.20\n",
            "iteration : 200, loss : 0.3809, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3804, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.19\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.16\n",
            "Epoch :  38, training loss : 0.3784, training accuracy : 88.18, test loss : 0.3889, test accuracy : 87.92\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.3852, accuracy : 88.36\n",
            "iteration : 100, loss : 0.3799, accuracy : 88.30\n",
            "iteration : 150, loss : 0.3834, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3778, accuracy : 88.30\n",
            "iteration : 250, loss : 0.3781, accuracy : 88.31\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3790, accuracy : 88.18\n",
            "Epoch :  39, training loss : 0.3778, training accuracy : 88.21, test loss : 0.3907, test accuracy : 87.83\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.3762, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3738, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3707, accuracy : 88.21\n",
            "iteration : 200, loss : 0.3715, accuracy : 88.22\n",
            "iteration : 250, loss : 0.3749, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3725, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.09\n",
            "Epoch :  40, training loss : 0.3784, training accuracy : 88.08, test loss : 0.3918, test accuracy : 87.81\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.3648, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3713, accuracy : 88.23\n",
            "iteration : 150, loss : 0.3734, accuracy : 88.11\n",
            "iteration : 200, loss : 0.3819, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3834, accuracy : 87.98\n",
            "iteration : 300, loss : 0.3829, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3820, accuracy : 88.08\n",
            "Epoch :  41, training loss : 0.3813, training accuracy : 88.07, test loss : 0.3900, test accuracy : 87.83\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.3784, accuracy : 88.28\n",
            "iteration : 100, loss : 0.3796, accuracy : 88.42\n",
            "iteration : 150, loss : 0.3781, accuracy : 88.19\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3778, accuracy : 88.33\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.29\n",
            "iteration : 350, loss : 0.3777, accuracy : 88.27\n",
            "Epoch :  42, training loss : 0.3776, training accuracy : 88.23, test loss : 0.3890, test accuracy : 87.94\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.3738, accuracy : 88.50\n",
            "iteration : 100, loss : 0.3783, accuracy : 88.33\n",
            "iteration : 150, loss : 0.3755, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3747, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3758, accuracy : 88.15\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.18\n",
            "iteration : 350, loss : 0.3768, accuracy : 88.25\n",
            "Epoch :  43, training loss : 0.3778, training accuracy : 88.23, test loss : 0.3931, test accuracy : 87.69\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.3613, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3658, accuracy : 88.24\n",
            "iteration : 150, loss : 0.3712, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3759, accuracy : 88.22\n",
            "iteration : 250, loss : 0.3808, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3809, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3788, accuracy : 88.17\n",
            "Epoch :  44, training loss : 0.3786, training accuracy : 88.16, test loss : 0.3919, test accuracy : 87.72\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.3716, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3847, accuracy : 88.02\n",
            "iteration : 150, loss : 0.3794, accuracy : 88.04\n",
            "iteration : 200, loss : 0.3789, accuracy : 88.16\n",
            "iteration : 250, loss : 0.3756, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3754, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3753, accuracy : 88.33\n",
            "Epoch :  45, training loss : 0.3763, training accuracy : 88.27, test loss : 0.3893, test accuracy : 87.82\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.3974, accuracy : 87.70\n",
            "iteration : 100, loss : 0.3873, accuracy : 87.96\n",
            "iteration : 150, loss : 0.3811, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3795, accuracy : 88.23\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3814, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3777, accuracy : 88.29\n",
            "Epoch :  46, training loss : 0.3766, training accuracy : 88.32, test loss : 0.3919, test accuracy : 87.81\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.3956, accuracy : 87.78\n",
            "iteration : 100, loss : 0.3861, accuracy : 87.87\n",
            "iteration : 150, loss : 0.3829, accuracy : 88.02\n",
            "iteration : 200, loss : 0.3795, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3819, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3810, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3827, accuracy : 88.02\n",
            "Epoch :  47, training loss : 0.3826, training accuracy : 88.06, test loss : 0.3910, test accuracy : 87.88\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.3662, accuracy : 88.22\n",
            "iteration : 100, loss : 0.3708, accuracy : 88.21\n",
            "iteration : 150, loss : 0.3752, accuracy : 88.23\n",
            "iteration : 200, loss : 0.3770, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3806, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3814, accuracy : 88.07\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.17\n",
            "Epoch :  48, training loss : 0.3788, training accuracy : 88.18, test loss : 0.3937, test accuracy : 87.66\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.3816, accuracy : 88.08\n",
            "iteration : 100, loss : 0.3813, accuracy : 87.97\n",
            "iteration : 150, loss : 0.3758, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3756, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3755, accuracy : 88.19\n",
            "iteration : 300, loss : 0.3793, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3783, accuracy : 88.17\n",
            "Epoch :  49, training loss : 0.3783, training accuracy : 88.17, test loss : 0.3921, test accuracy : 87.84\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.3788, accuracy : 88.08\n",
            "iteration : 100, loss : 0.3858, accuracy : 87.91\n",
            "iteration : 150, loss : 0.3864, accuracy : 87.85\n",
            "iteration : 200, loss : 0.3817, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3802, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3798, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3787, accuracy : 88.12\n",
            "Epoch :  50, training loss : 0.3785, training accuracy : 88.16, test loss : 0.3897, test accuracy : 87.77\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.3681, accuracy : 88.69\n",
            "iteration : 100, loss : 0.3776, accuracy : 88.25\n",
            "iteration : 150, loss : 0.3776, accuracy : 88.18\n",
            "iteration : 200, loss : 0.3797, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3819, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3808, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.12\n",
            "Epoch :  51, training loss : 0.3779, training accuracy : 88.14, test loss : 0.3898, test accuracy : 87.88\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.3743, accuracy : 88.22\n",
            "iteration : 100, loss : 0.3766, accuracy : 88.28\n",
            "iteration : 150, loss : 0.3772, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3795, accuracy : 87.96\n",
            "iteration : 250, loss : 0.3781, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3786, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.15\n",
            "Epoch :  52, training loss : 0.3780, training accuracy : 88.15, test loss : 0.3902, test accuracy : 87.78\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.3779, accuracy : 87.88\n",
            "iteration : 100, loss : 0.3827, accuracy : 87.79\n",
            "iteration : 150, loss : 0.3778, accuracy : 87.94\n",
            "iteration : 200, loss : 0.3793, accuracy : 87.91\n",
            "iteration : 250, loss : 0.3811, accuracy : 87.97\n",
            "iteration : 300, loss : 0.3768, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3788, accuracy : 88.12\n",
            "Epoch :  53, training loss : 0.3780, training accuracy : 88.12, test loss : 0.3899, test accuracy : 87.79\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.3670, accuracy : 88.41\n",
            "iteration : 100, loss : 0.3724, accuracy : 88.32\n",
            "iteration : 150, loss : 0.3775, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3818, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3851, accuracy : 87.94\n",
            "iteration : 300, loss : 0.3828, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3818, accuracy : 88.02\n",
            "Epoch :  54, training loss : 0.3814, training accuracy : 88.02, test loss : 0.3912, test accuracy : 87.80\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.3726, accuracy : 88.39\n",
            "iteration : 100, loss : 0.3762, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3689, accuracy : 88.43\n",
            "iteration : 200, loss : 0.3734, accuracy : 88.34\n",
            "iteration : 250, loss : 0.3759, accuracy : 88.33\n",
            "iteration : 300, loss : 0.3799, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3795, accuracy : 88.13\n",
            "Epoch :  55, training loss : 0.3780, training accuracy : 88.19, test loss : 0.3901, test accuracy : 87.82\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.3721, accuracy : 88.48\n",
            "iteration : 100, loss : 0.3691, accuracy : 88.41\n",
            "iteration : 150, loss : 0.3690, accuracy : 88.43\n",
            "iteration : 200, loss : 0.3681, accuracy : 88.39\n",
            "iteration : 250, loss : 0.3758, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3783, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.22\n",
            "Epoch :  56, training loss : 0.3792, training accuracy : 88.19, test loss : 0.3881, test accuracy : 87.89\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.3691, accuracy : 88.75\n",
            "iteration : 100, loss : 0.3718, accuracy : 88.50\n",
            "iteration : 150, loss : 0.3735, accuracy : 88.47\n",
            "iteration : 200, loss : 0.3745, accuracy : 88.49\n",
            "iteration : 250, loss : 0.3794, accuracy : 88.41\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.34\n",
            "iteration : 350, loss : 0.3773, accuracy : 88.32\n",
            "Epoch :  57, training loss : 0.3767, training accuracy : 88.31, test loss : 0.3908, test accuracy : 87.93\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.3958, accuracy : 87.75\n",
            "iteration : 100, loss : 0.3778, accuracy : 88.26\n",
            "iteration : 150, loss : 0.3782, accuracy : 88.27\n",
            "iteration : 200, loss : 0.3768, accuracy : 88.26\n",
            "iteration : 250, loss : 0.3821, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3768, accuracy : 88.23\n",
            "Epoch :  58, training loss : 0.3780, training accuracy : 88.16, test loss : 0.3912, test accuracy : 87.95\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.3775, accuracy : 88.34\n",
            "iteration : 100, loss : 0.3758, accuracy : 88.33\n",
            "iteration : 150, loss : 0.3748, accuracy : 88.40\n",
            "iteration : 200, loss : 0.3809, accuracy : 88.28\n",
            "iteration : 250, loss : 0.3788, accuracy : 88.26\n",
            "iteration : 300, loss : 0.3767, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3760, accuracy : 88.27\n",
            "Epoch :  59, training loss : 0.3787, training accuracy : 88.17, test loss : 0.3905, test accuracy : 87.78\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.3714, accuracy : 88.31\n",
            "iteration : 100, loss : 0.3847, accuracy : 87.80\n",
            "iteration : 150, loss : 0.3778, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3798, accuracy : 87.99\n",
            "iteration : 250, loss : 0.3832, accuracy : 87.89\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3771, accuracy : 88.18\n",
            "Epoch :  60, training loss : 0.3765, training accuracy : 88.19, test loss : 0.3921, test accuracy : 87.89\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.3769, accuracy : 88.67\n",
            "iteration : 100, loss : 0.3713, accuracy : 88.77\n",
            "iteration : 150, loss : 0.3772, accuracy : 88.43\n",
            "iteration : 200, loss : 0.3763, accuracy : 88.46\n",
            "iteration : 250, loss : 0.3774, accuracy : 88.44\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.32\n",
            "iteration : 350, loss : 0.3805, accuracy : 88.23\n",
            "Epoch :  61, training loss : 0.3788, training accuracy : 88.26, test loss : 0.3916, test accuracy : 87.79\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.3594, accuracy : 88.77\n",
            "iteration : 100, loss : 0.3610, accuracy : 88.64\n",
            "iteration : 150, loss : 0.3724, accuracy : 88.35\n",
            "iteration : 200, loss : 0.3790, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3781, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3815, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3815, accuracy : 88.08\n",
            "Epoch :  62, training loss : 0.3799, training accuracy : 88.12, test loss : 0.3869, test accuracy : 88.00\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.3818, accuracy : 88.09\n",
            "iteration : 100, loss : 0.3839, accuracy : 88.06\n",
            "iteration : 150, loss : 0.3840, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3818, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3789, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3767, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3763, accuracy : 88.17\n",
            "Epoch :  63, training loss : 0.3771, training accuracy : 88.14, test loss : 0.3899, test accuracy : 87.88\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.3691, accuracy : 88.36\n",
            "iteration : 100, loss : 0.3749, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3808, accuracy : 87.88\n",
            "iteration : 200, loss : 0.3787, accuracy : 88.01\n",
            "iteration : 250, loss : 0.3802, accuracy : 88.00\n",
            "iteration : 300, loss : 0.3770, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.11\n",
            "Epoch :  64, training loss : 0.3775, training accuracy : 88.16, test loss : 0.3904, test accuracy : 87.79\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.3651, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3648, accuracy : 88.41\n",
            "iteration : 150, loss : 0.3708, accuracy : 88.35\n",
            "iteration : 200, loss : 0.3777, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3806, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3811, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3790, accuracy : 88.10\n",
            "Epoch :  65, training loss : 0.3801, training accuracy : 88.09, test loss : 0.3894, test accuracy : 87.68\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.3869, accuracy : 87.83\n",
            "iteration : 100, loss : 0.3897, accuracy : 87.80\n",
            "iteration : 150, loss : 0.3829, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3868, accuracy : 88.05\n",
            "iteration : 250, loss : 0.3813, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3817, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3795, accuracy : 88.19\n",
            "Epoch :  66, training loss : 0.3791, training accuracy : 88.25, test loss : 0.3909, test accuracy : 87.81\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.3694, accuracy : 88.41\n",
            "iteration : 100, loss : 0.3720, accuracy : 88.33\n",
            "iteration : 150, loss : 0.3783, accuracy : 88.08\n",
            "iteration : 200, loss : 0.3801, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3802, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3800, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3812, accuracy : 88.08\n",
            "Epoch :  67, training loss : 0.3805, training accuracy : 88.12, test loss : 0.3910, test accuracy : 87.72\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.3717, accuracy : 88.47\n",
            "iteration : 100, loss : 0.3709, accuracy : 88.57\n",
            "iteration : 150, loss : 0.3724, accuracy : 88.45\n",
            "iteration : 200, loss : 0.3762, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3732, accuracy : 88.32\n",
            "iteration : 300, loss : 0.3753, accuracy : 88.25\n",
            "iteration : 350, loss : 0.3771, accuracy : 88.14\n",
            "Epoch :  68, training loss : 0.3770, training accuracy : 88.11, test loss : 0.3894, test accuracy : 87.82\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.3806, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3818, accuracy : 88.09\n",
            "iteration : 150, loss : 0.3822, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3808, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3768, accuracy : 88.19\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.14\n",
            "Epoch :  69, training loss : 0.3789, training accuracy : 88.09, test loss : 0.3913, test accuracy : 87.79\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.3798, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3761, accuracy : 88.19\n",
            "iteration : 150, loss : 0.3823, accuracy : 88.08\n",
            "iteration : 200, loss : 0.3790, accuracy : 88.07\n",
            "iteration : 250, loss : 0.3795, accuracy : 87.99\n",
            "iteration : 300, loss : 0.3794, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3779, accuracy : 88.08\n",
            "Epoch :  70, training loss : 0.3784, training accuracy : 88.10, test loss : 0.3887, test accuracy : 87.82\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.3644, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3706, accuracy : 88.07\n",
            "iteration : 150, loss : 0.3779, accuracy : 88.04\n",
            "iteration : 200, loss : 0.3794, accuracy : 87.98\n",
            "iteration : 250, loss : 0.3788, accuracy : 88.04\n",
            "iteration : 300, loss : 0.3818, accuracy : 87.96\n",
            "iteration : 350, loss : 0.3819, accuracy : 88.01\n",
            "Epoch :  71, training loss : 0.3806, training accuracy : 88.04, test loss : 0.3922, test accuracy : 87.83\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.3725, accuracy : 88.45\n",
            "iteration : 100, loss : 0.3775, accuracy : 88.18\n",
            "iteration : 150, loss : 0.3702, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3746, accuracy : 88.25\n",
            "iteration : 250, loss : 0.3755, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.18\n",
            "Epoch :  72, training loss : 0.3778, training accuracy : 88.18, test loss : 0.3925, test accuracy : 87.78\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.3837, accuracy : 87.81\n",
            "iteration : 100, loss : 0.3851, accuracy : 87.84\n",
            "iteration : 150, loss : 0.3818, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3830, accuracy : 87.94\n",
            "iteration : 250, loss : 0.3837, accuracy : 87.95\n",
            "iteration : 300, loss : 0.3821, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3817, accuracy : 88.06\n",
            "Epoch :  73, training loss : 0.3809, training accuracy : 88.10, test loss : 0.3933, test accuracy : 87.78\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.3917, accuracy : 87.80\n",
            "iteration : 100, loss : 0.3790, accuracy : 88.19\n",
            "iteration : 150, loss : 0.3741, accuracy : 88.32\n",
            "iteration : 200, loss : 0.3723, accuracy : 88.34\n",
            "iteration : 250, loss : 0.3743, accuracy : 88.29\n",
            "iteration : 300, loss : 0.3795, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.11\n",
            "Epoch :  74, training loss : 0.3794, training accuracy : 88.14, test loss : 0.3923, test accuracy : 87.72\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.3901, accuracy : 87.77\n",
            "iteration : 100, loss : 0.3873, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3833, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3849, accuracy : 87.91\n",
            "iteration : 250, loss : 0.3840, accuracy : 87.98\n",
            "iteration : 300, loss : 0.3830, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3815, accuracy : 88.06\n",
            "Epoch :  75, training loss : 0.3814, training accuracy : 88.08, test loss : 0.3923, test accuracy : 87.73\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.3858, accuracy : 87.98\n",
            "iteration : 100, loss : 0.3845, accuracy : 88.20\n",
            "iteration : 150, loss : 0.3816, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3776, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3748, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3767, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3790, accuracy : 88.11\n",
            "Epoch :  76, training loss : 0.3796, training accuracy : 88.09, test loss : 0.3902, test accuracy : 87.82\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.3782, accuracy : 88.00\n",
            "iteration : 100, loss : 0.3730, accuracy : 88.37\n",
            "iteration : 150, loss : 0.3755, accuracy : 88.41\n",
            "iteration : 200, loss : 0.3712, accuracy : 88.44\n",
            "iteration : 250, loss : 0.3735, accuracy : 88.35\n",
            "iteration : 300, loss : 0.3767, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3770, accuracy : 88.30\n",
            "Epoch :  77, training loss : 0.3784, training accuracy : 88.24, test loss : 0.3913, test accuracy : 87.81\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.3801, accuracy : 88.70\n",
            "iteration : 100, loss : 0.3870, accuracy : 88.11\n",
            "iteration : 150, loss : 0.3840, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3800, accuracy : 88.19\n",
            "iteration : 250, loss : 0.3807, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3789, accuracy : 88.19\n",
            "iteration : 350, loss : 0.3773, accuracy : 88.21\n",
            "Epoch :  78, training loss : 0.3789, training accuracy : 88.13, test loss : 0.3891, test accuracy : 87.88\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.3686, accuracy : 88.14\n",
            "iteration : 100, loss : 0.3825, accuracy : 87.90\n",
            "iteration : 150, loss : 0.3765, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3789, accuracy : 88.07\n",
            "iteration : 250, loss : 0.3801, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3806, accuracy : 88.08\n",
            "Epoch :  79, training loss : 0.3796, training accuracy : 88.10, test loss : 0.3926, test accuracy : 87.83\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.3781, accuracy : 88.31\n",
            "iteration : 100, loss : 0.3819, accuracy : 88.09\n",
            "iteration : 150, loss : 0.3776, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3744, accuracy : 88.31\n",
            "iteration : 250, loss : 0.3748, accuracy : 88.32\n",
            "iteration : 300, loss : 0.3759, accuracy : 88.33\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.27\n",
            "Epoch :  80, training loss : 0.3785, training accuracy : 88.26, test loss : 0.3907, test accuracy : 87.85\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.3862, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3835, accuracy : 88.02\n",
            "iteration : 150, loss : 0.3826, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3792, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3782, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3766, accuracy : 88.29\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.24\n",
            "Epoch :  81, training loss : 0.3768, training accuracy : 88.25, test loss : 0.3922, test accuracy : 87.91\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.3865, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3859, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3878, accuracy : 88.01\n",
            "iteration : 200, loss : 0.3790, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3757, accuracy : 88.31\n",
            "iteration : 300, loss : 0.3747, accuracy : 88.29\n",
            "iteration : 350, loss : 0.3767, accuracy : 88.17\n",
            "Epoch :  82, training loss : 0.3770, training accuracy : 88.16, test loss : 0.3910, test accuracy : 87.73\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.3755, accuracy : 88.42\n",
            "iteration : 100, loss : 0.3783, accuracy : 88.08\n",
            "iteration : 150, loss : 0.3787, accuracy : 87.97\n",
            "iteration : 200, loss : 0.3776, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3795, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3821, accuracy : 88.00\n",
            "iteration : 350, loss : 0.3812, accuracy : 88.07\n",
            "Epoch :  83, training loss : 0.3815, training accuracy : 88.06, test loss : 0.3908, test accuracy : 87.81\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.3762, accuracy : 87.75\n",
            "iteration : 100, loss : 0.3705, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3699, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3764, accuracy : 88.03\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3766, accuracy : 88.18\n",
            "Epoch :  84, training loss : 0.3775, training accuracy : 88.12, test loss : 0.3899, test accuracy : 87.73\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.3847, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3854, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3785, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3800, accuracy : 88.10\n",
            "iteration : 250, loss : 0.3789, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3807, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.11\n",
            "Epoch :  85, training loss : 0.3793, training accuracy : 88.15, test loss : 0.3929, test accuracy : 87.74\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.3840, accuracy : 87.94\n",
            "iteration : 100, loss : 0.3689, accuracy : 88.33\n",
            "iteration : 150, loss : 0.3719, accuracy : 88.26\n",
            "iteration : 200, loss : 0.3706, accuracy : 88.38\n",
            "iteration : 250, loss : 0.3729, accuracy : 88.35\n",
            "iteration : 300, loss : 0.3752, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3765, accuracy : 88.24\n",
            "Epoch :  86, training loss : 0.3770, training accuracy : 88.25, test loss : 0.3889, test accuracy : 87.73\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.4015, accuracy : 87.61\n",
            "iteration : 100, loss : 0.3788, accuracy : 88.17\n",
            "iteration : 150, loss : 0.3742, accuracy : 88.29\n",
            "iteration : 200, loss : 0.3741, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3777, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3783, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3758, accuracy : 88.13\n",
            "Epoch :  87, training loss : 0.3764, training accuracy : 88.15, test loss : 0.3889, test accuracy : 87.89\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.3805, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3863, accuracy : 88.08\n",
            "iteration : 150, loss : 0.3886, accuracy : 87.99\n",
            "iteration : 200, loss : 0.3801, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3791, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3802, accuracy : 88.00\n",
            "Epoch :  88, training loss : 0.3792, training accuracy : 88.05, test loss : 0.3926, test accuracy : 87.67\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.3534, accuracy : 88.72\n",
            "iteration : 100, loss : 0.3571, accuracy : 88.74\n",
            "iteration : 150, loss : 0.3663, accuracy : 88.55\n",
            "iteration : 200, loss : 0.3709, accuracy : 88.35\n",
            "iteration : 250, loss : 0.3726, accuracy : 88.31\n",
            "iteration : 300, loss : 0.3742, accuracy : 88.27\n",
            "iteration : 350, loss : 0.3775, accuracy : 88.16\n",
            "Epoch :  89, training loss : 0.3787, training accuracy : 88.16, test loss : 0.3897, test accuracy : 87.84\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.3650, accuracy : 88.22\n",
            "iteration : 100, loss : 0.3776, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3821, accuracy : 88.02\n",
            "iteration : 200, loss : 0.3827, accuracy : 88.01\n",
            "iteration : 250, loss : 0.3828, accuracy : 88.01\n",
            "iteration : 300, loss : 0.3784, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3792, accuracy : 88.10\n",
            "Epoch :  90, training loss : 0.3787, training accuracy : 88.12, test loss : 0.3923, test accuracy : 87.79\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.3759, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3818, accuracy : 88.32\n",
            "iteration : 150, loss : 0.3852, accuracy : 88.23\n",
            "iteration : 200, loss : 0.3825, accuracy : 88.19\n",
            "iteration : 250, loss : 0.3801, accuracy : 88.30\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.31\n",
            "Epoch :  91, training loss : 0.3778, training accuracy : 88.33, test loss : 0.3928, test accuracy : 87.70\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.3778, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3835, accuracy : 88.07\n",
            "iteration : 150, loss : 0.3778, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3789, accuracy : 88.17\n",
            "iteration : 250, loss : 0.3760, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.19\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.20\n",
            "Epoch :  92, training loss : 0.3772, training accuracy : 88.21, test loss : 0.3948, test accuracy : 87.80\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.3938, accuracy : 87.94\n",
            "iteration : 100, loss : 0.3857, accuracy : 88.13\n",
            "iteration : 150, loss : 0.3831, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3807, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3795, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.28\n",
            "iteration : 350, loss : 0.3765, accuracy : 88.29\n",
            "Epoch :  93, training loss : 0.3777, training accuracy : 88.24, test loss : 0.3924, test accuracy : 87.78\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.3609, accuracy : 88.88\n",
            "iteration : 100, loss : 0.3639, accuracy : 88.90\n",
            "iteration : 150, loss : 0.3659, accuracy : 88.77\n",
            "iteration : 200, loss : 0.3693, accuracy : 88.75\n",
            "iteration : 250, loss : 0.3731, accuracy : 88.55\n",
            "iteration : 300, loss : 0.3750, accuracy : 88.45\n",
            "iteration : 350, loss : 0.3752, accuracy : 88.36\n",
            "Epoch :  94, training loss : 0.3766, training accuracy : 88.27, test loss : 0.3879, test accuracy : 87.90\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.3646, accuracy : 88.58\n",
            "iteration : 100, loss : 0.3656, accuracy : 88.70\n",
            "iteration : 150, loss : 0.3761, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3783, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3761, accuracy : 88.39\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.39\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.27\n",
            "Epoch :  95, training loss : 0.3802, training accuracy : 88.24, test loss : 0.3911, test accuracy : 87.75\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.3865, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3770, accuracy : 88.01\n",
            "iteration : 150, loss : 0.3777, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3779, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3765, accuracy : 87.97\n",
            "iteration : 300, loss : 0.3770, accuracy : 87.96\n",
            "iteration : 350, loss : 0.3775, accuracy : 87.96\n",
            "Epoch :  96, training loss : 0.3798, training accuracy : 87.92, test loss : 0.3914, test accuracy : 87.83\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.3685, accuracy : 87.97\n",
            "iteration : 100, loss : 0.3699, accuracy : 88.21\n",
            "iteration : 150, loss : 0.3681, accuracy : 88.36\n",
            "iteration : 200, loss : 0.3736, accuracy : 88.30\n",
            "iteration : 250, loss : 0.3764, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3802, accuracy : 88.03\n",
            "Epoch :  97, training loss : 0.3792, training accuracy : 88.05, test loss : 0.3890, test accuracy : 87.87\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.3670, accuracy : 88.48\n",
            "iteration : 100, loss : 0.3735, accuracy : 88.27\n",
            "iteration : 150, loss : 0.3755, accuracy : 88.21\n",
            "iteration : 200, loss : 0.3738, accuracy : 88.28\n",
            "iteration : 250, loss : 0.3744, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3735, accuracy : 88.28\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.16\n",
            "Epoch :  98, training loss : 0.3785, training accuracy : 88.16, test loss : 0.3895, test accuracy : 87.87\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.3815, accuracy : 88.39\n",
            "iteration : 100, loss : 0.3767, accuracy : 88.43\n",
            "iteration : 150, loss : 0.3835, accuracy : 88.30\n",
            "iteration : 200, loss : 0.3802, accuracy : 88.26\n",
            "iteration : 250, loss : 0.3803, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3779, accuracy : 88.27\n",
            "Epoch :  99, training loss : 0.3775, training accuracy : 88.27, test loss : 0.3915, test accuracy : 87.79\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.3651, accuracy : 88.95\n",
            "iteration : 100, loss : 0.3774, accuracy : 88.41\n",
            "iteration : 150, loss : 0.3790, accuracy : 88.27\n",
            "iteration : 200, loss : 0.3828, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3809, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3801, accuracy : 88.18\n",
            "iteration : 350, loss : 0.3806, accuracy : 88.11\n",
            "Epoch : 100, training loss : 0.3807, training accuracy : 88.11, test loss : 0.3905, test accuracy : 87.85\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.3695, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3766, accuracy : 88.29\n",
            "iteration : 150, loss : 0.3773, accuracy : 88.14\n",
            "iteration : 200, loss : 0.3772, accuracy : 88.23\n",
            "iteration : 250, loss : 0.3775, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.15\n",
            "Epoch : 101, training loss : 0.3792, training accuracy : 88.13, test loss : 0.3892, test accuracy : 87.92\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.3714, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3700, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3756, accuracy : 88.20\n",
            "iteration : 200, loss : 0.3770, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.27\n",
            "iteration : 350, loss : 0.3777, accuracy : 88.29\n",
            "Epoch : 102, training loss : 0.3783, training accuracy : 88.28, test loss : 0.3926, test accuracy : 87.85\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.4007, accuracy : 87.42\n",
            "iteration : 100, loss : 0.3921, accuracy : 87.66\n",
            "iteration : 150, loss : 0.3814, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3819, accuracy : 88.07\n",
            "iteration : 250, loss : 0.3816, accuracy : 88.16\n",
            "iteration : 300, loss : 0.3815, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3798, accuracy : 88.21\n",
            "Epoch : 103, training loss : 0.3801, training accuracy : 88.22, test loss : 0.3931, test accuracy : 87.83\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.3685, accuracy : 88.41\n",
            "iteration : 100, loss : 0.3770, accuracy : 88.00\n",
            "iteration : 150, loss : 0.3775, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3778, accuracy : 88.13\n",
            "iteration : 250, loss : 0.3779, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3777, accuracy : 88.20\n",
            "Epoch : 104, training loss : 0.3779, training accuracy : 88.20, test loss : 0.3926, test accuracy : 87.80\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.3715, accuracy : 88.48\n",
            "iteration : 100, loss : 0.3778, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3792, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3843, accuracy : 88.04\n",
            "iteration : 250, loss : 0.3843, accuracy : 88.00\n",
            "iteration : 300, loss : 0.3825, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3815, accuracy : 88.14\n",
            "Epoch : 105, training loss : 0.3803, training accuracy : 88.18, test loss : 0.3940, test accuracy : 87.71\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.3837, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3872, accuracy : 88.19\n",
            "iteration : 150, loss : 0.3746, accuracy : 88.40\n",
            "iteration : 200, loss : 0.3738, accuracy : 88.26\n",
            "iteration : 250, loss : 0.3750, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.11\n",
            "Epoch : 106, training loss : 0.3792, training accuracy : 88.10, test loss : 0.3908, test accuracy : 87.75\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.3928, accuracy : 87.45\n",
            "iteration : 100, loss : 0.3866, accuracy : 87.79\n",
            "iteration : 150, loss : 0.3828, accuracy : 87.93\n",
            "iteration : 200, loss : 0.3814, accuracy : 87.96\n",
            "iteration : 250, loss : 0.3807, accuracy : 87.98\n",
            "iteration : 300, loss : 0.3810, accuracy : 87.99\n",
            "iteration : 350, loss : 0.3820, accuracy : 87.97\n",
            "Epoch : 107, training loss : 0.3811, training accuracy : 88.01, test loss : 0.3892, test accuracy : 87.98\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.3850, accuracy : 87.69\n",
            "iteration : 100, loss : 0.3811, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3794, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3763, accuracy : 88.04\n",
            "iteration : 250, loss : 0.3783, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3761, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.13\n",
            "Epoch : 108, training loss : 0.3782, training accuracy : 88.09, test loss : 0.3915, test accuracy : 87.70\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.3813, accuracy : 88.33\n",
            "iteration : 100, loss : 0.3790, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3745, accuracy : 88.17\n",
            "iteration : 200, loss : 0.3708, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3736, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3759, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3777, accuracy : 88.06\n",
            "Epoch : 109, training loss : 0.3784, training accuracy : 88.10, test loss : 0.3905, test accuracy : 87.85\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.3786, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3820, accuracy : 87.79\n",
            "iteration : 150, loss : 0.3802, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3800, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3790, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3801, accuracy : 88.08\n",
            "Epoch : 110, training loss : 0.3807, training accuracy : 88.10, test loss : 0.3912, test accuracy : 87.88\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.3898, accuracy : 87.66\n",
            "iteration : 100, loss : 0.3875, accuracy : 87.83\n",
            "iteration : 150, loss : 0.3867, accuracy : 87.92\n",
            "iteration : 200, loss : 0.3841, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3815, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3818, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3823, accuracy : 88.03\n",
            "Epoch : 111, training loss : 0.3807, training accuracy : 88.06, test loss : 0.3944, test accuracy : 87.74\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.3835, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3735, accuracy : 88.36\n",
            "iteration : 150, loss : 0.3798, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3782, accuracy : 88.08\n",
            "iteration : 250, loss : 0.3795, accuracy : 87.96\n",
            "iteration : 300, loss : 0.3799, accuracy : 88.02\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.10\n",
            "Epoch : 112, training loss : 0.3776, training accuracy : 88.16, test loss : 0.3913, test accuracy : 87.81\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.3816, accuracy : 87.56\n",
            "iteration : 100, loss : 0.3825, accuracy : 88.08\n",
            "iteration : 150, loss : 0.3823, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3808, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3790, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3795, accuracy : 88.07\n",
            "Epoch : 113, training loss : 0.3794, training accuracy : 88.11, test loss : 0.3938, test accuracy : 87.65\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.3749, accuracy : 88.69\n",
            "iteration : 100, loss : 0.3727, accuracy : 88.48\n",
            "iteration : 150, loss : 0.3734, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3746, accuracy : 88.28\n",
            "iteration : 250, loss : 0.3779, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3791, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.11\n",
            "Epoch : 114, training loss : 0.3799, training accuracy : 88.12, test loss : 0.3891, test accuracy : 87.75\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.3855, accuracy : 87.91\n",
            "iteration : 100, loss : 0.3880, accuracy : 87.93\n",
            "iteration : 150, loss : 0.3837, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3829, accuracy : 88.03\n",
            "iteration : 250, loss : 0.3822, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3814, accuracy : 88.02\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.09\n",
            "Epoch : 115, training loss : 0.3791, training accuracy : 88.09, test loss : 0.3961, test accuracy : 87.52\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.3761, accuracy : 88.61\n",
            "iteration : 100, loss : 0.3757, accuracy : 88.36\n",
            "iteration : 150, loss : 0.3753, accuracy : 88.59\n",
            "iteration : 200, loss : 0.3792, accuracy : 88.40\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.41\n",
            "iteration : 300, loss : 0.3785, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.29\n",
            "Epoch : 116, training loss : 0.3767, training accuracy : 88.29, test loss : 0.3884, test accuracy : 88.06\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.3748, accuracy : 88.11\n",
            "iteration : 100, loss : 0.3828, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3803, accuracy : 88.00\n",
            "iteration : 200, loss : 0.3788, accuracy : 87.99\n",
            "iteration : 250, loss : 0.3770, accuracy : 88.15\n",
            "iteration : 300, loss : 0.3769, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.10\n",
            "Epoch : 117, training loss : 0.3784, training accuracy : 88.08, test loss : 0.3920, test accuracy : 87.80\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.3744, accuracy : 87.97\n",
            "iteration : 100, loss : 0.3840, accuracy : 87.74\n",
            "iteration : 150, loss : 0.3782, accuracy : 88.01\n",
            "iteration : 200, loss : 0.3815, accuracy : 87.99\n",
            "iteration : 250, loss : 0.3777, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3804, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3810, accuracy : 88.00\n",
            "Epoch : 118, training loss : 0.3804, training accuracy : 88.01, test loss : 0.3887, test accuracy : 87.92\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.3663, accuracy : 88.55\n",
            "iteration : 100, loss : 0.3723, accuracy : 88.22\n",
            "iteration : 150, loss : 0.3703, accuracy : 88.30\n",
            "iteration : 200, loss : 0.3721, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3748, accuracy : 88.27\n",
            "iteration : 300, loss : 0.3759, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3764, accuracy : 88.21\n",
            "Epoch : 119, training loss : 0.3770, training accuracy : 88.17, test loss : 0.3946, test accuracy : 87.57\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.3585, accuracy : 88.89\n",
            "iteration : 100, loss : 0.3658, accuracy : 88.52\n",
            "iteration : 150, loss : 0.3716, accuracy : 88.35\n",
            "iteration : 200, loss : 0.3733, accuracy : 88.36\n",
            "iteration : 250, loss : 0.3744, accuracy : 88.38\n",
            "iteration : 300, loss : 0.3751, accuracy : 88.39\n",
            "iteration : 350, loss : 0.3767, accuracy : 88.27\n",
            "Epoch : 120, training loss : 0.3762, training accuracy : 88.28, test loss : 0.3901, test accuracy : 87.82\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.3760, accuracy : 88.02\n",
            "iteration : 100, loss : 0.3765, accuracy : 88.15\n",
            "iteration : 150, loss : 0.3783, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3775, accuracy : 88.23\n",
            "iteration : 250, loss : 0.3800, accuracy : 88.21\n",
            "iteration : 300, loss : 0.3825, accuracy : 88.07\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.16\n",
            "Epoch : 121, training loss : 0.3803, training accuracy : 88.11, test loss : 0.3927, test accuracy : 87.73\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.3518, accuracy : 88.91\n",
            "iteration : 100, loss : 0.3683, accuracy : 88.55\n",
            "iteration : 150, loss : 0.3657, accuracy : 88.56\n",
            "iteration : 200, loss : 0.3709, accuracy : 88.27\n",
            "iteration : 250, loss : 0.3725, accuracy : 88.23\n",
            "iteration : 300, loss : 0.3727, accuracy : 88.22\n",
            "iteration : 350, loss : 0.3753, accuracy : 88.16\n",
            "Epoch : 122, training loss : 0.3769, training accuracy : 88.12, test loss : 0.3912, test accuracy : 87.76\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.3710, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3734, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3772, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3776, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3810, accuracy : 87.96\n",
            "iteration : 300, loss : 0.3811, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3816, accuracy : 87.95\n",
            "Epoch : 123, training loss : 0.3809, training accuracy : 87.99, test loss : 0.3919, test accuracy : 87.81\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.3758, accuracy : 88.58\n",
            "iteration : 100, loss : 0.3836, accuracy : 88.23\n",
            "iteration : 150, loss : 0.3807, accuracy : 88.22\n",
            "iteration : 200, loss : 0.3754, accuracy : 88.30\n",
            "iteration : 250, loss : 0.3770, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3798, accuracy : 88.18\n",
            "Epoch : 124, training loss : 0.3785, training accuracy : 88.22, test loss : 0.3916, test accuracy : 87.85\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.3841, accuracy : 87.50\n",
            "iteration : 100, loss : 0.3825, accuracy : 87.84\n",
            "iteration : 150, loss : 0.3759, accuracy : 87.99\n",
            "iteration : 200, loss : 0.3790, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3769, accuracy : 88.04\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3775, accuracy : 88.17\n",
            "Epoch : 125, training loss : 0.3802, training accuracy : 88.13, test loss : 0.3960, test accuracy : 87.75\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.3530, accuracy : 89.12\n",
            "iteration : 100, loss : 0.3740, accuracy : 88.57\n",
            "iteration : 150, loss : 0.3739, accuracy : 88.46\n",
            "iteration : 200, loss : 0.3765, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3778, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3785, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3815, accuracy : 88.11\n",
            "Epoch : 126, training loss : 0.3799, training accuracy : 88.12, test loss : 0.3925, test accuracy : 87.68\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.3789, accuracy : 87.88\n",
            "iteration : 100, loss : 0.3812, accuracy : 88.01\n",
            "iteration : 150, loss : 0.3823, accuracy : 87.85\n",
            "iteration : 200, loss : 0.3798, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3789, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3774, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3780, accuracy : 88.09\n",
            "Epoch : 127, training loss : 0.3770, training accuracy : 88.14, test loss : 0.3923, test accuracy : 87.63\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.3789, accuracy : 87.98\n",
            "iteration : 100, loss : 0.3808, accuracy : 88.06\n",
            "iteration : 150, loss : 0.3744, accuracy : 88.17\n",
            "iteration : 200, loss : 0.3735, accuracy : 88.29\n",
            "iteration : 250, loss : 0.3737, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3756, accuracy : 88.22\n",
            "iteration : 350, loss : 0.3785, accuracy : 88.16\n",
            "Epoch : 128, training loss : 0.3781, training accuracy : 88.17, test loss : 0.3896, test accuracy : 87.83\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.3648, accuracy : 88.33\n",
            "iteration : 100, loss : 0.3807, accuracy : 88.07\n",
            "iteration : 150, loss : 0.3771, accuracy : 88.20\n",
            "iteration : 200, loss : 0.3796, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.15\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3772, accuracy : 88.16\n",
            "Epoch : 129, training loss : 0.3764, training accuracy : 88.14, test loss : 0.3903, test accuracy : 87.91\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.3841, accuracy : 87.69\n",
            "iteration : 100, loss : 0.3817, accuracy : 87.94\n",
            "iteration : 150, loss : 0.3803, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3773, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3798, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.06\n",
            "Epoch : 130, training loss : 0.3790, training accuracy : 88.10, test loss : 0.3892, test accuracy : 87.77\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.3555, accuracy : 89.00\n",
            "iteration : 100, loss : 0.3727, accuracy : 88.55\n",
            "iteration : 150, loss : 0.3787, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3782, accuracy : 88.10\n",
            "iteration : 250, loss : 0.3764, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.22\n",
            "Epoch : 131, training loss : 0.3790, training accuracy : 88.19, test loss : 0.3902, test accuracy : 87.81\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.3829, accuracy : 87.83\n",
            "iteration : 100, loss : 0.3895, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3814, accuracy : 88.24\n",
            "iteration : 200, loss : 0.3827, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.31\n",
            "iteration : 300, loss : 0.3782, accuracy : 88.27\n",
            "iteration : 350, loss : 0.3809, accuracy : 88.15\n",
            "Epoch : 132, training loss : 0.3789, training accuracy : 88.20, test loss : 0.3889, test accuracy : 88.05\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.3788, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3785, accuracy : 88.25\n",
            "iteration : 150, loss : 0.3747, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.01\n",
            "iteration : 250, loss : 0.3816, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3802, accuracy : 88.12\n",
            "Epoch : 133, training loss : 0.3787, training accuracy : 88.15, test loss : 0.3914, test accuracy : 87.82\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.3640, accuracy : 88.47\n",
            "iteration : 100, loss : 0.3768, accuracy : 88.33\n",
            "iteration : 150, loss : 0.3778, accuracy : 88.34\n",
            "iteration : 200, loss : 0.3795, accuracy : 88.30\n",
            "iteration : 250, loss : 0.3813, accuracy : 88.15\n",
            "iteration : 300, loss : 0.3821, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3808, accuracy : 88.03\n",
            "Epoch : 134, training loss : 0.3802, training accuracy : 88.07, test loss : 0.3924, test accuracy : 87.83\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.3873, accuracy : 88.56\n",
            "iteration : 100, loss : 0.3874, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3823, accuracy : 87.95\n",
            "iteration : 200, loss : 0.3825, accuracy : 87.99\n",
            "iteration : 250, loss : 0.3823, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3808, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3802, accuracy : 88.12\n",
            "Epoch : 135, training loss : 0.3800, training accuracy : 88.12, test loss : 0.3911, test accuracy : 87.69\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.3916, accuracy : 88.17\n",
            "iteration : 100, loss : 0.3784, accuracy : 88.17\n",
            "iteration : 150, loss : 0.3826, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3830, accuracy : 88.16\n",
            "iteration : 250, loss : 0.3798, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3785, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.24\n",
            "Epoch : 136, training loss : 0.3767, training accuracy : 88.26, test loss : 0.3887, test accuracy : 87.76\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.3809, accuracy : 88.16\n",
            "iteration : 100, loss : 0.3865, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3867, accuracy : 87.95\n",
            "iteration : 200, loss : 0.3821, accuracy : 88.03\n",
            "iteration : 250, loss : 0.3803, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.14\n",
            "Epoch : 137, training loss : 0.3791, training accuracy : 88.15, test loss : 0.3895, test accuracy : 87.74\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.3939, accuracy : 87.67\n",
            "iteration : 100, loss : 0.3837, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3810, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3802, accuracy : 88.17\n",
            "iteration : 250, loss : 0.3801, accuracy : 88.15\n",
            "iteration : 300, loss : 0.3786, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3763, accuracy : 88.27\n",
            "Epoch : 138, training loss : 0.3769, training accuracy : 88.26, test loss : 0.3938, test accuracy : 87.80\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.3847, accuracy : 87.67\n",
            "iteration : 100, loss : 0.3784, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3847, accuracy : 87.81\n",
            "iteration : 200, loss : 0.3878, accuracy : 87.82\n",
            "iteration : 250, loss : 0.3849, accuracy : 87.93\n",
            "iteration : 300, loss : 0.3829, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3801, accuracy : 88.06\n",
            "Epoch : 139, training loss : 0.3782, training accuracy : 88.12, test loss : 0.3892, test accuracy : 87.96\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.3603, accuracy : 88.64\n",
            "iteration : 100, loss : 0.3671, accuracy : 88.59\n",
            "iteration : 150, loss : 0.3709, accuracy : 88.28\n",
            "iteration : 200, loss : 0.3705, accuracy : 88.33\n",
            "iteration : 250, loss : 0.3750, accuracy : 88.26\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3801, accuracy : 88.09\n",
            "Epoch : 140, training loss : 0.3796, training accuracy : 88.10, test loss : 0.3928, test accuracy : 87.84\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.3682, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3776, accuracy : 88.09\n",
            "iteration : 150, loss : 0.3830, accuracy : 87.95\n",
            "iteration : 200, loss : 0.3790, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3781, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3790, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3792, accuracy : 88.10\n",
            "Epoch : 141, training loss : 0.3791, training accuracy : 88.11, test loss : 0.3930, test accuracy : 87.65\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.3613, accuracy : 88.47\n",
            "iteration : 100, loss : 0.3813, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3766, accuracy : 88.25\n",
            "iteration : 200, loss : 0.3749, accuracy : 88.25\n",
            "iteration : 250, loss : 0.3743, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3762, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3756, accuracy : 88.25\n",
            "Epoch : 142, training loss : 0.3767, training accuracy : 88.18, test loss : 0.3918, test accuracy : 87.85\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.3654, accuracy : 88.42\n",
            "iteration : 100, loss : 0.3724, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3765, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3819, accuracy : 87.96\n",
            "iteration : 250, loss : 0.3800, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3785, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.13\n",
            "Epoch : 143, training loss : 0.3786, training accuracy : 88.15, test loss : 0.3902, test accuracy : 87.92\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.3721, accuracy : 88.70\n",
            "iteration : 100, loss : 0.3774, accuracy : 88.56\n",
            "iteration : 150, loss : 0.3865, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3781, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3785, accuracy : 88.26\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.35\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.27\n",
            "Epoch : 144, training loss : 0.3785, training accuracy : 88.27, test loss : 0.3916, test accuracy : 87.68\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.3632, accuracy : 88.48\n",
            "iteration : 100, loss : 0.3689, accuracy : 88.38\n",
            "iteration : 150, loss : 0.3780, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3749, accuracy : 88.26\n",
            "iteration : 250, loss : 0.3761, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.20\n",
            "Epoch : 145, training loss : 0.3786, training accuracy : 88.16, test loss : 0.3908, test accuracy : 87.75\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.3886, accuracy : 87.94\n",
            "iteration : 100, loss : 0.3782, accuracy : 88.36\n",
            "iteration : 150, loss : 0.3834, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3838, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3866, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3849, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3803, accuracy : 88.17\n",
            "Epoch : 146, training loss : 0.3798, training accuracy : 88.17, test loss : 0.3900, test accuracy : 87.78\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.3734, accuracy : 88.23\n",
            "iteration : 100, loss : 0.3694, accuracy : 88.13\n",
            "iteration : 150, loss : 0.3788, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3810, accuracy : 88.05\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.17\n",
            "Epoch : 147, training loss : 0.3790, training accuracy : 88.14, test loss : 0.3919, test accuracy : 87.82\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.3834, accuracy : 88.16\n",
            "iteration : 100, loss : 0.3799, accuracy : 88.36\n",
            "iteration : 150, loss : 0.3798, accuracy : 88.17\n",
            "iteration : 200, loss : 0.3822, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3819, accuracy : 87.96\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.08\n",
            "Epoch : 148, training loss : 0.3773, training accuracy : 88.09, test loss : 0.3893, test accuracy : 87.98\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.3938, accuracy : 87.70\n",
            "iteration : 100, loss : 0.3970, accuracy : 87.55\n",
            "iteration : 150, loss : 0.3887, accuracy : 87.69\n",
            "iteration : 200, loss : 0.3874, accuracy : 87.84\n",
            "iteration : 250, loss : 0.3847, accuracy : 87.95\n",
            "iteration : 300, loss : 0.3824, accuracy : 87.98\n",
            "iteration : 350, loss : 0.3831, accuracy : 88.00\n",
            "Epoch : 149, training loss : 0.3811, training accuracy : 88.04, test loss : 0.3916, test accuracy : 87.88\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.3798, accuracy : 88.12\n",
            "iteration : 100, loss : 0.3854, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3752, accuracy : 88.21\n",
            "iteration : 200, loss : 0.3742, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3732, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3732, accuracy : 88.27\n",
            "iteration : 350, loss : 0.3759, accuracy : 88.20\n",
            "Epoch : 150, training loss : 0.3777, training accuracy : 88.15, test loss : 0.3920, test accuracy : 87.64\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.3773, accuracy : 88.20\n",
            "iteration : 100, loss : 0.3812, accuracy : 88.04\n",
            "iteration : 150, loss : 0.3861, accuracy : 87.99\n",
            "iteration : 200, loss : 0.3854, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3850, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.18\n",
            "Epoch : 151, training loss : 0.3799, training accuracy : 88.15, test loss : 0.3898, test accuracy : 87.93\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.3519, accuracy : 89.11\n",
            "iteration : 100, loss : 0.3704, accuracy : 88.53\n",
            "iteration : 150, loss : 0.3720, accuracy : 88.36\n",
            "iteration : 200, loss : 0.3798, accuracy : 88.17\n",
            "iteration : 250, loss : 0.3805, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3774, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3768, accuracy : 88.34\n",
            "Epoch : 152, training loss : 0.3776, training accuracy : 88.29, test loss : 0.3942, test accuracy : 87.65\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.3980, accuracy : 88.20\n",
            "iteration : 100, loss : 0.3851, accuracy : 88.20\n",
            "iteration : 150, loss : 0.3808, accuracy : 88.28\n",
            "iteration : 200, loss : 0.3867, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3862, accuracy : 87.95\n",
            "iteration : 300, loss : 0.3840, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.16\n",
            "Epoch : 153, training loss : 0.3794, training accuracy : 88.14, test loss : 0.3912, test accuracy : 87.68\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.3973, accuracy : 87.20\n",
            "iteration : 100, loss : 0.3892, accuracy : 87.77\n",
            "iteration : 150, loss : 0.3812, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3794, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3785, accuracy : 88.26\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.27\n",
            "Epoch : 154, training loss : 0.3774, training accuracy : 88.29, test loss : 0.3932, test accuracy : 87.60\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.3782, accuracy : 88.44\n",
            "iteration : 100, loss : 0.3855, accuracy : 88.08\n",
            "iteration : 150, loss : 0.3824, accuracy : 88.34\n",
            "iteration : 200, loss : 0.3841, accuracy : 88.27\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.24\n",
            "Epoch : 155, training loss : 0.3787, training accuracy : 88.26, test loss : 0.3956, test accuracy : 87.66\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.3889, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3773, accuracy : 88.47\n",
            "iteration : 150, loss : 0.3751, accuracy : 88.54\n",
            "iteration : 200, loss : 0.3780, accuracy : 88.49\n",
            "iteration : 250, loss : 0.3807, accuracy : 88.31\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.22\n",
            "iteration : 350, loss : 0.3788, accuracy : 88.24\n",
            "Epoch : 156, training loss : 0.3796, training accuracy : 88.22, test loss : 0.3907, test accuracy : 87.65\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.3738, accuracy : 88.28\n",
            "iteration : 100, loss : 0.3811, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3871, accuracy : 87.95\n",
            "iteration : 200, loss : 0.3811, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3805, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3794, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3765, accuracy : 88.26\n",
            "Epoch : 157, training loss : 0.3777, training accuracy : 88.22, test loss : 0.3902, test accuracy : 87.78\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.3813, accuracy : 88.52\n",
            "iteration : 100, loss : 0.3694, accuracy : 88.52\n",
            "iteration : 150, loss : 0.3798, accuracy : 88.32\n",
            "iteration : 200, loss : 0.3773, accuracy : 88.29\n",
            "iteration : 250, loss : 0.3785, accuracy : 88.26\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.25\n",
            "Epoch : 158, training loss : 0.3766, training accuracy : 88.27, test loss : 0.3919, test accuracy : 87.78\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.3729, accuracy : 87.97\n",
            "iteration : 100, loss : 0.3725, accuracy : 88.19\n",
            "iteration : 150, loss : 0.3726, accuracy : 88.20\n",
            "iteration : 200, loss : 0.3736, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3746, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3761, accuracy : 88.19\n",
            "iteration : 350, loss : 0.3745, accuracy : 88.26\n",
            "Epoch : 159, training loss : 0.3749, training accuracy : 88.27, test loss : 0.3922, test accuracy : 87.58\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.3976, accuracy : 87.48\n",
            "iteration : 100, loss : 0.3923, accuracy : 87.69\n",
            "iteration : 150, loss : 0.3846, accuracy : 87.95\n",
            "iteration : 200, loss : 0.3812, accuracy : 88.13\n",
            "iteration : 250, loss : 0.3826, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3793, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3797, accuracy : 88.12\n",
            "Epoch : 160, training loss : 0.3787, training accuracy : 88.11, test loss : 0.3897, test accuracy : 87.98\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.3833, accuracy : 88.08\n",
            "iteration : 100, loss : 0.3785, accuracy : 87.90\n",
            "iteration : 150, loss : 0.3745, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3731, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3766, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3800, accuracy : 87.97\n",
            "Epoch : 161, training loss : 0.3791, training accuracy : 88.03, test loss : 0.3905, test accuracy : 87.83\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.3705, accuracy : 88.33\n",
            "iteration : 100, loss : 0.3766, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3769, accuracy : 88.02\n",
            "iteration : 200, loss : 0.3757, accuracy : 88.13\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3795, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3809, accuracy : 87.99\n",
            "Epoch : 162, training loss : 0.3798, training accuracy : 88.03, test loss : 0.3903, test accuracy : 87.73\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.3881, accuracy : 87.78\n",
            "iteration : 100, loss : 0.3877, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3883, accuracy : 87.88\n",
            "iteration : 200, loss : 0.3852, accuracy : 87.96\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.15\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3803, accuracy : 88.11\n",
            "Epoch : 163, training loss : 0.3802, training accuracy : 88.08, test loss : 0.3909, test accuracy : 87.85\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.3878, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3830, accuracy : 87.99\n",
            "iteration : 150, loss : 0.3794, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3798, accuracy : 88.16\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.13\n",
            "Epoch : 164, training loss : 0.3778, training accuracy : 88.15, test loss : 0.3925, test accuracy : 87.76\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.3804, accuracy : 87.50\n",
            "iteration : 100, loss : 0.3767, accuracy : 88.01\n",
            "iteration : 150, loss : 0.3764, accuracy : 88.00\n",
            "iteration : 200, loss : 0.3790, accuracy : 87.92\n",
            "iteration : 250, loss : 0.3771, accuracy : 88.04\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3783, accuracy : 88.10\n",
            "Epoch : 165, training loss : 0.3792, training accuracy : 88.10, test loss : 0.3903, test accuracy : 87.76\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.3686, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3738, accuracy : 88.21\n",
            "iteration : 150, loss : 0.3790, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3785, accuracy : 88.17\n",
            "iteration : 250, loss : 0.3760, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3762, accuracy : 88.28\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.21\n",
            "Epoch : 166, training loss : 0.3789, training accuracy : 88.19, test loss : 0.3945, test accuracy : 87.64\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.3904, accuracy : 87.69\n",
            "iteration : 100, loss : 0.3834, accuracy : 87.73\n",
            "iteration : 150, loss : 0.3792, accuracy : 88.04\n",
            "iteration : 200, loss : 0.3781, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3789, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3801, accuracy : 88.07\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.16\n",
            "Epoch : 167, training loss : 0.3776, training accuracy : 88.19, test loss : 0.3892, test accuracy : 87.91\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.3814, accuracy : 87.94\n",
            "iteration : 100, loss : 0.3766, accuracy : 88.18\n",
            "iteration : 150, loss : 0.3814, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3785, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3764, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3786, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3795, accuracy : 88.18\n",
            "Epoch : 168, training loss : 0.3797, training accuracy : 88.18, test loss : 0.3900, test accuracy : 87.75\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.3881, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3833, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3842, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3809, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3828, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3772, accuracy : 88.14\n",
            "Epoch : 169, training loss : 0.3785, training accuracy : 88.12, test loss : 0.3926, test accuracy : 87.57\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.3738, accuracy : 88.42\n",
            "iteration : 100, loss : 0.3724, accuracy : 88.30\n",
            "iteration : 150, loss : 0.3786, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3784, accuracy : 88.05\n",
            "iteration : 250, loss : 0.3803, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.11\n",
            "Epoch : 170, training loss : 0.3821, training accuracy : 88.06, test loss : 0.3920, test accuracy : 87.71\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.3859, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3770, accuracy : 88.01\n",
            "iteration : 150, loss : 0.3754, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3774, accuracy : 87.95\n",
            "iteration : 250, loss : 0.3778, accuracy : 87.95\n",
            "iteration : 300, loss : 0.3751, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3759, accuracy : 88.08\n",
            "Epoch : 171, training loss : 0.3769, training accuracy : 88.05, test loss : 0.3911, test accuracy : 87.73\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.3660, accuracy : 88.23\n",
            "iteration : 100, loss : 0.3794, accuracy : 88.03\n",
            "iteration : 150, loss : 0.3770, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3715, accuracy : 88.33\n",
            "iteration : 250, loss : 0.3726, accuracy : 88.36\n",
            "iteration : 300, loss : 0.3761, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3762, accuracy : 88.25\n",
            "Epoch : 172, training loss : 0.3763, training accuracy : 88.27, test loss : 0.3924, test accuracy : 87.78\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.3753, accuracy : 88.11\n",
            "iteration : 100, loss : 0.3787, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3775, accuracy : 88.01\n",
            "iteration : 200, loss : 0.3753, accuracy : 88.07\n",
            "iteration : 250, loss : 0.3740, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3753, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.07\n",
            "Epoch : 173, training loss : 0.3791, training accuracy : 88.07, test loss : 0.3939, test accuracy : 87.77\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.3781, accuracy : 87.95\n",
            "iteration : 100, loss : 0.3813, accuracy : 88.19\n",
            "iteration : 150, loss : 0.3826, accuracy : 88.26\n",
            "iteration : 200, loss : 0.3798, accuracy : 88.33\n",
            "iteration : 250, loss : 0.3793, accuracy : 88.26\n",
            "iteration : 300, loss : 0.3795, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.24\n",
            "Epoch : 174, training loss : 0.3801, training accuracy : 88.22, test loss : 0.3900, test accuracy : 88.02\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.3676, accuracy : 88.38\n",
            "iteration : 100, loss : 0.3719, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3725, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3744, accuracy : 88.13\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3794, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3803, accuracy : 88.09\n",
            "Epoch : 175, training loss : 0.3801, training accuracy : 88.09, test loss : 0.3922, test accuracy : 87.85\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.3944, accuracy : 88.08\n",
            "iteration : 100, loss : 0.3917, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3860, accuracy : 87.92\n",
            "iteration : 200, loss : 0.3852, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3835, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3815, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3806, accuracy : 88.13\n",
            "Epoch : 176, training loss : 0.3797, training accuracy : 88.12, test loss : 0.3933, test accuracy : 87.73\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.3879, accuracy : 88.33\n",
            "iteration : 100, loss : 0.3841, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3813, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3822, accuracy : 88.27\n",
            "iteration : 250, loss : 0.3812, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3823, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3805, accuracy : 88.16\n",
            "Epoch : 177, training loss : 0.3787, training accuracy : 88.19, test loss : 0.3925, test accuracy : 87.88\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.3845, accuracy : 88.00\n",
            "iteration : 100, loss : 0.3779, accuracy : 88.02\n",
            "iteration : 150, loss : 0.3771, accuracy : 88.00\n",
            "iteration : 200, loss : 0.3780, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3771, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3757, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3768, accuracy : 88.13\n",
            "Epoch : 178, training loss : 0.3778, training accuracy : 88.11, test loss : 0.3926, test accuracy : 87.70\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.3640, accuracy : 88.73\n",
            "iteration : 100, loss : 0.3621, accuracy : 88.53\n",
            "iteration : 150, loss : 0.3712, accuracy : 88.34\n",
            "iteration : 200, loss : 0.3745, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3770, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3776, accuracy : 88.27\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.21\n",
            "Epoch : 179, training loss : 0.3780, training accuracy : 88.22, test loss : 0.3874, test accuracy : 87.88\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.3669, accuracy : 88.02\n",
            "iteration : 100, loss : 0.3732, accuracy : 87.94\n",
            "iteration : 150, loss : 0.3741, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3779, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3808, accuracy : 87.88\n",
            "iteration : 300, loss : 0.3785, accuracy : 87.95\n",
            "iteration : 350, loss : 0.3795, accuracy : 87.99\n",
            "Epoch : 180, training loss : 0.3796, training accuracy : 88.00, test loss : 0.3920, test accuracy : 87.88\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.3725, accuracy : 88.53\n",
            "iteration : 100, loss : 0.3704, accuracy : 88.53\n",
            "iteration : 150, loss : 0.3718, accuracy : 88.34\n",
            "iteration : 200, loss : 0.3725, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3712, accuracy : 88.38\n",
            "iteration : 300, loss : 0.3757, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3772, accuracy : 88.13\n",
            "Epoch : 181, training loss : 0.3773, training accuracy : 88.12, test loss : 0.3949, test accuracy : 87.68\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.3913, accuracy : 88.00\n",
            "iteration : 100, loss : 0.3794, accuracy : 88.30\n",
            "iteration : 150, loss : 0.3856, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.25\n",
            "iteration : 250, loss : 0.3816, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3783, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3780, accuracy : 88.25\n",
            "Epoch : 182, training loss : 0.3774, training accuracy : 88.28, test loss : 0.3899, test accuracy : 87.93\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.3650, accuracy : 88.70\n",
            "iteration : 100, loss : 0.3740, accuracy : 88.48\n",
            "iteration : 150, loss : 0.3781, accuracy : 88.38\n",
            "iteration : 200, loss : 0.3751, accuracy : 88.44\n",
            "iteration : 250, loss : 0.3762, accuracy : 88.45\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.35\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.31\n",
            "Epoch : 183, training loss : 0.3775, training accuracy : 88.30, test loss : 0.3894, test accuracy : 87.96\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.3920, accuracy : 88.08\n",
            "iteration : 100, loss : 0.3768, accuracy : 88.09\n",
            "iteration : 150, loss : 0.3773, accuracy : 88.27\n",
            "iteration : 200, loss : 0.3767, accuracy : 88.27\n",
            "iteration : 250, loss : 0.3776, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3780, accuracy : 88.24\n",
            "Epoch : 184, training loss : 0.3774, training accuracy : 88.25, test loss : 0.3864, test accuracy : 87.91\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.3770, accuracy : 87.95\n",
            "iteration : 100, loss : 0.3766, accuracy : 88.15\n",
            "iteration : 150, loss : 0.3740, accuracy : 88.26\n",
            "iteration : 200, loss : 0.3778, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3791, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3795, accuracy : 88.13\n",
            "Epoch : 185, training loss : 0.3787, training accuracy : 88.15, test loss : 0.3910, test accuracy : 87.68\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.3861, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3768, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3730, accuracy : 88.18\n",
            "iteration : 200, loss : 0.3762, accuracy : 88.14\n",
            "iteration : 250, loss : 0.3807, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3812, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3790, accuracy : 88.13\n",
            "Epoch : 186, training loss : 0.3800, training accuracy : 88.12, test loss : 0.3909, test accuracy : 87.76\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.3842, accuracy : 87.61\n",
            "iteration : 100, loss : 0.3907, accuracy : 87.52\n",
            "iteration : 150, loss : 0.3830, accuracy : 87.89\n",
            "iteration : 200, loss : 0.3844, accuracy : 87.95\n",
            "iteration : 250, loss : 0.3839, accuracy : 87.92\n",
            "iteration : 300, loss : 0.3832, accuracy : 87.96\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.13\n",
            "Epoch : 187, training loss : 0.3793, training accuracy : 88.11, test loss : 0.3916, test accuracy : 87.78\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.3756, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3847, accuracy : 87.88\n",
            "iteration : 150, loss : 0.3811, accuracy : 87.94\n",
            "iteration : 200, loss : 0.3764, accuracy : 88.08\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3765, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.04\n",
            "Epoch : 188, training loss : 0.3793, training accuracy : 88.02, test loss : 0.3938, test accuracy : 87.67\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.3796, accuracy : 88.22\n",
            "iteration : 100, loss : 0.3827, accuracy : 87.91\n",
            "iteration : 150, loss : 0.3818, accuracy : 88.01\n",
            "iteration : 200, loss : 0.3791, accuracy : 88.22\n",
            "iteration : 250, loss : 0.3751, accuracy : 88.42\n",
            "iteration : 300, loss : 0.3760, accuracy : 88.36\n",
            "iteration : 350, loss : 0.3785, accuracy : 88.28\n",
            "Epoch : 189, training loss : 0.3787, training accuracy : 88.25, test loss : 0.3898, test accuracy : 87.68\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.3841, accuracy : 88.02\n",
            "iteration : 100, loss : 0.3765, accuracy : 88.00\n",
            "iteration : 150, loss : 0.3809, accuracy : 87.85\n",
            "iteration : 200, loss : 0.3814, accuracy : 87.88\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.00\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3813, accuracy : 88.07\n",
            "Epoch : 190, training loss : 0.3805, training accuracy : 88.07, test loss : 0.3912, test accuracy : 87.88\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.3778, accuracy : 88.55\n",
            "iteration : 100, loss : 0.3704, accuracy : 88.59\n",
            "iteration : 150, loss : 0.3775, accuracy : 88.40\n",
            "iteration : 200, loss : 0.3765, accuracy : 88.44\n",
            "iteration : 250, loss : 0.3758, accuracy : 88.35\n",
            "iteration : 300, loss : 0.3758, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3783, accuracy : 88.20\n",
            "Epoch : 191, training loss : 0.3785, training accuracy : 88.24, test loss : 0.3916, test accuracy : 87.80\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.3802, accuracy : 87.95\n",
            "iteration : 100, loss : 0.3786, accuracy : 87.93\n",
            "iteration : 150, loss : 0.3776, accuracy : 88.04\n",
            "iteration : 200, loss : 0.3766, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3793, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3767, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.04\n",
            "Epoch : 192, training loss : 0.3791, training accuracy : 88.06, test loss : 0.3923, test accuracy : 87.66\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.3749, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3815, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3846, accuracy : 87.92\n",
            "iteration : 200, loss : 0.3820, accuracy : 87.92\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3783, accuracy : 88.12\n",
            "Epoch : 193, training loss : 0.3786, training accuracy : 88.14, test loss : 0.3911, test accuracy : 87.83\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.3755, accuracy : 88.25\n",
            "iteration : 100, loss : 0.3766, accuracy : 88.31\n",
            "iteration : 150, loss : 0.3791, accuracy : 88.29\n",
            "iteration : 200, loss : 0.3801, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3798, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3762, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3758, accuracy : 88.25\n",
            "Epoch : 194, training loss : 0.3773, training accuracy : 88.22, test loss : 0.3902, test accuracy : 87.86\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.3961, accuracy : 88.09\n",
            "iteration : 100, loss : 0.3798, accuracy : 88.42\n",
            "iteration : 150, loss : 0.3786, accuracy : 88.41\n",
            "iteration : 200, loss : 0.3825, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3811, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3766, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3769, accuracy : 88.24\n",
            "Epoch : 195, training loss : 0.3779, training accuracy : 88.19, test loss : 0.3944, test accuracy : 87.78\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.3532, accuracy : 89.20\n",
            "iteration : 100, loss : 0.3701, accuracy : 88.67\n",
            "iteration : 150, loss : 0.3782, accuracy : 88.32\n",
            "iteration : 200, loss : 0.3772, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3781, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3779, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.26\n",
            "Epoch : 196, training loss : 0.3778, training accuracy : 88.26, test loss : 0.3916, test accuracy : 87.67\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.3806, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3774, accuracy : 88.20\n",
            "iteration : 150, loss : 0.3806, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3790, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3791, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3816, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.16\n",
            "Epoch : 197, training loss : 0.3789, training accuracy : 88.16, test loss : 0.3920, test accuracy : 87.81\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.3699, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3716, accuracy : 88.68\n",
            "iteration : 150, loss : 0.3750, accuracy : 88.47\n",
            "iteration : 200, loss : 0.3759, accuracy : 88.40\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3783, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.17\n",
            "Epoch : 198, training loss : 0.3769, training accuracy : 88.20, test loss : 0.3936, test accuracy : 87.64\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.3790, accuracy : 87.67\n",
            "iteration : 100, loss : 0.3607, accuracy : 88.55\n",
            "iteration : 150, loss : 0.3690, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3680, accuracy : 88.46\n",
            "iteration : 250, loss : 0.3700, accuracy : 88.44\n",
            "iteration : 300, loss : 0.3724, accuracy : 88.28\n",
            "iteration : 350, loss : 0.3751, accuracy : 88.19\n",
            "Epoch : 199, training loss : 0.3765, training accuracy : 88.14, test loss : 0.3911, test accuracy : 87.70\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.3621, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3664, accuracy : 88.47\n",
            "iteration : 150, loss : 0.3741, accuracy : 88.38\n",
            "iteration : 200, loss : 0.3778, accuracy : 88.23\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.23\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.32\n",
            "iteration : 350, loss : 0.3773, accuracy : 88.28\n",
            "Epoch : 200, training loss : 0.3774, training accuracy : 88.28, test loss : 0.3919, test accuracy : 87.71\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.3811, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3779, accuracy : 88.24\n",
            "iteration : 150, loss : 0.3751, accuracy : 88.34\n",
            "iteration : 200, loss : 0.3686, accuracy : 88.50\n",
            "iteration : 250, loss : 0.3727, accuracy : 88.33\n",
            "iteration : 300, loss : 0.3745, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3754, accuracy : 88.27\n",
            "Epoch : 201, training loss : 0.3762, training accuracy : 88.25, test loss : 0.3912, test accuracy : 87.85\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.3768, accuracy : 88.39\n",
            "iteration : 100, loss : 0.3737, accuracy : 88.32\n",
            "iteration : 150, loss : 0.3755, accuracy : 88.22\n",
            "iteration : 200, loss : 0.3774, accuracy : 88.26\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3767, accuracy : 88.34\n",
            "iteration : 350, loss : 0.3757, accuracy : 88.34\n",
            "Epoch : 202, training loss : 0.3759, training accuracy : 88.33, test loss : 0.3898, test accuracy : 87.82\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.3842, accuracy : 87.66\n",
            "iteration : 100, loss : 0.3750, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3825, accuracy : 87.82\n",
            "iteration : 200, loss : 0.3799, accuracy : 87.98\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3779, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.04\n",
            "Epoch : 203, training loss : 0.3795, training accuracy : 88.02, test loss : 0.3926, test accuracy : 87.79\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.3898, accuracy : 87.66\n",
            "iteration : 100, loss : 0.3831, accuracy : 87.87\n",
            "iteration : 150, loss : 0.3797, accuracy : 88.17\n",
            "iteration : 200, loss : 0.3771, accuracy : 88.30\n",
            "iteration : 250, loss : 0.3772, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3810, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.21\n",
            "Epoch : 204, training loss : 0.3793, training accuracy : 88.16, test loss : 0.3901, test accuracy : 87.71\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.3835, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3814, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3812, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3765, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3760, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3797, accuracy : 88.05\n",
            "iteration : 350, loss : 0.3801, accuracy : 88.08\n",
            "Epoch : 205, training loss : 0.3799, training accuracy : 88.07, test loss : 0.3890, test accuracy : 87.83\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.3946, accuracy : 87.66\n",
            "iteration : 100, loss : 0.3803, accuracy : 88.06\n",
            "iteration : 150, loss : 0.3809, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3806, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3811, accuracy : 88.04\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.14\n",
            "Epoch : 206, training loss : 0.3774, training accuracy : 88.17, test loss : 0.3910, test accuracy : 87.89\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.4017, accuracy : 87.48\n",
            "iteration : 100, loss : 0.3782, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3818, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3811, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3825, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3838, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3805, accuracy : 88.19\n",
            "Epoch : 207, training loss : 0.3801, training accuracy : 88.15, test loss : 0.3908, test accuracy : 87.76\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.3706, accuracy : 88.97\n",
            "iteration : 100, loss : 0.3669, accuracy : 88.75\n",
            "iteration : 150, loss : 0.3710, accuracy : 88.45\n",
            "iteration : 200, loss : 0.3771, accuracy : 88.24\n",
            "iteration : 250, loss : 0.3778, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3802, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3805, accuracy : 88.12\n",
            "Epoch : 208, training loss : 0.3804, training accuracy : 88.12, test loss : 0.3921, test accuracy : 87.73\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.3705, accuracy : 88.45\n",
            "iteration : 100, loss : 0.3662, accuracy : 88.34\n",
            "iteration : 150, loss : 0.3703, accuracy : 88.28\n",
            "iteration : 200, loss : 0.3711, accuracy : 88.33\n",
            "iteration : 250, loss : 0.3753, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.17\n",
            "Epoch : 209, training loss : 0.3790, training accuracy : 88.16, test loss : 0.3912, test accuracy : 87.76\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.3840, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3778, accuracy : 88.03\n",
            "iteration : 150, loss : 0.3789, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3779, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3766, accuracy : 88.19\n",
            "iteration : 300, loss : 0.3746, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3765, accuracy : 88.22\n",
            "Epoch : 210, training loss : 0.3781, training accuracy : 88.18, test loss : 0.3915, test accuracy : 87.84\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.3781, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3798, accuracy : 88.15\n",
            "iteration : 150, loss : 0.3863, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3838, accuracy : 87.96\n",
            "iteration : 250, loss : 0.3813, accuracy : 88.04\n",
            "iteration : 300, loss : 0.3802, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3813, accuracy : 88.03\n",
            "Epoch : 211, training loss : 0.3802, training accuracy : 88.07, test loss : 0.3884, test accuracy : 87.75\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.3685, accuracy : 88.34\n",
            "iteration : 100, loss : 0.3784, accuracy : 88.11\n",
            "iteration : 150, loss : 0.3860, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3806, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3802, accuracy : 88.05\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.06\n",
            "Epoch : 212, training loss : 0.3795, training accuracy : 88.08, test loss : 0.3938, test accuracy : 87.72\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.3849, accuracy : 88.14\n",
            "iteration : 100, loss : 0.3842, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3761, accuracy : 88.14\n",
            "iteration : 200, loss : 0.3762, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3777, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3771, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3783, accuracy : 88.07\n",
            "Epoch : 213, training loss : 0.3772, training accuracy : 88.12, test loss : 0.3903, test accuracy : 87.85\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.3728, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3678, accuracy : 88.46\n",
            "iteration : 150, loss : 0.3679, accuracy : 88.53\n",
            "iteration : 200, loss : 0.3746, accuracy : 88.25\n",
            "iteration : 250, loss : 0.3758, accuracy : 88.29\n",
            "iteration : 300, loss : 0.3815, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3819, accuracy : 88.06\n",
            "Epoch : 214, training loss : 0.3793, training accuracy : 88.14, test loss : 0.3955, test accuracy : 87.60\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.3789, accuracy : 88.70\n",
            "iteration : 100, loss : 0.3711, accuracy : 88.67\n",
            "iteration : 150, loss : 0.3706, accuracy : 88.51\n",
            "iteration : 200, loss : 0.3701, accuracy : 88.52\n",
            "iteration : 250, loss : 0.3750, accuracy : 88.35\n",
            "iteration : 300, loss : 0.3782, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3812, accuracy : 88.12\n",
            "Epoch : 215, training loss : 0.3812, training accuracy : 88.14, test loss : 0.3932, test accuracy : 87.70\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.3898, accuracy : 87.66\n",
            "iteration : 100, loss : 0.3789, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3769, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3779, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3804, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3798, accuracy : 88.12\n",
            "Epoch : 216, training loss : 0.3786, training accuracy : 88.19, test loss : 0.3933, test accuracy : 87.62\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.3793, accuracy : 88.66\n",
            "iteration : 100, loss : 0.3788, accuracy : 88.60\n",
            "iteration : 150, loss : 0.3789, accuracy : 88.32\n",
            "iteration : 200, loss : 0.3717, accuracy : 88.55\n",
            "iteration : 250, loss : 0.3755, accuracy : 88.42\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.32\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.23\n",
            "Epoch : 217, training loss : 0.3783, training accuracy : 88.19, test loss : 0.3866, test accuracy : 88.04\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.3813, accuracy : 87.56\n",
            "iteration : 100, loss : 0.3846, accuracy : 87.90\n",
            "iteration : 150, loss : 0.3897, accuracy : 87.81\n",
            "iteration : 200, loss : 0.3891, accuracy : 87.88\n",
            "iteration : 250, loss : 0.3840, accuracy : 88.01\n",
            "iteration : 300, loss : 0.3802, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3802, accuracy : 88.10\n",
            "Epoch : 218, training loss : 0.3792, training accuracy : 88.14, test loss : 0.3902, test accuracy : 87.78\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.3788, accuracy : 88.02\n",
            "iteration : 100, loss : 0.3754, accuracy : 88.23\n",
            "iteration : 150, loss : 0.3774, accuracy : 88.23\n",
            "iteration : 200, loss : 0.3773, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3737, accuracy : 88.33\n",
            "iteration : 300, loss : 0.3745, accuracy : 88.33\n",
            "iteration : 350, loss : 0.3765, accuracy : 88.30\n",
            "Epoch : 219, training loss : 0.3775, training accuracy : 88.25, test loss : 0.3952, test accuracy : 87.74\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.3878, accuracy : 87.80\n",
            "iteration : 100, loss : 0.3809, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3801, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3740, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3767, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3766, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3773, accuracy : 88.20\n",
            "Epoch : 220, training loss : 0.3756, training accuracy : 88.22, test loss : 0.3926, test accuracy : 87.81\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.3744, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3693, accuracy : 88.41\n",
            "iteration : 150, loss : 0.3754, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3744, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3776, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3779, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.17\n",
            "Epoch : 221, training loss : 0.3771, training accuracy : 88.18, test loss : 0.3933, test accuracy : 87.57\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.3701, accuracy : 88.50\n",
            "iteration : 100, loss : 0.3839, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3817, accuracy : 88.02\n",
            "iteration : 200, loss : 0.3787, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3799, accuracy : 88.07\n",
            "iteration : 350, loss : 0.3792, accuracy : 88.10\n",
            "Epoch : 222, training loss : 0.3798, training accuracy : 88.08, test loss : 0.3950, test accuracy : 87.63\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.3914, accuracy : 87.70\n",
            "iteration : 100, loss : 0.3872, accuracy : 87.89\n",
            "iteration : 150, loss : 0.3848, accuracy : 88.04\n",
            "iteration : 200, loss : 0.3835, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3847, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3836, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3807, accuracy : 88.09\n",
            "Epoch : 223, training loss : 0.3790, training accuracy : 88.14, test loss : 0.3911, test accuracy : 87.70\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.3730, accuracy : 88.50\n",
            "iteration : 100, loss : 0.3685, accuracy : 88.38\n",
            "iteration : 150, loss : 0.3683, accuracy : 88.60\n",
            "iteration : 200, loss : 0.3743, accuracy : 88.37\n",
            "iteration : 250, loss : 0.3778, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.15\n",
            "Epoch : 224, training loss : 0.3787, training accuracy : 88.10, test loss : 0.3922, test accuracy : 87.72\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.3590, accuracy : 88.77\n",
            "iteration : 100, loss : 0.3700, accuracy : 88.42\n",
            "iteration : 150, loss : 0.3798, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3799, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.19\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.30\n",
            "Epoch : 225, training loss : 0.3768, training accuracy : 88.31, test loss : 0.3913, test accuracy : 87.90\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.3623, accuracy : 88.52\n",
            "iteration : 100, loss : 0.3655, accuracy : 88.75\n",
            "iteration : 150, loss : 0.3694, accuracy : 88.60\n",
            "iteration : 200, loss : 0.3719, accuracy : 88.43\n",
            "iteration : 250, loss : 0.3733, accuracy : 88.45\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.20\n",
            "Epoch : 226, training loss : 0.3783, training accuracy : 88.19, test loss : 0.3906, test accuracy : 87.83\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.3757, accuracy : 88.42\n",
            "iteration : 100, loss : 0.3726, accuracy : 88.46\n",
            "iteration : 150, loss : 0.3716, accuracy : 88.40\n",
            "iteration : 200, loss : 0.3734, accuracy : 88.38\n",
            "iteration : 250, loss : 0.3746, accuracy : 88.36\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.23\n",
            "Epoch : 227, training loss : 0.3774, training accuracy : 88.23, test loss : 0.3907, test accuracy : 87.76\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.3690, accuracy : 88.34\n",
            "iteration : 100, loss : 0.3800, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3761, accuracy : 88.26\n",
            "iteration : 200, loss : 0.3819, accuracy : 88.04\n",
            "iteration : 250, loss : 0.3795, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3811, accuracy : 88.07\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.16\n",
            "Epoch : 228, training loss : 0.3790, training accuracy : 88.10, test loss : 0.3923, test accuracy : 87.63\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.3482, accuracy : 89.11\n",
            "iteration : 100, loss : 0.3591, accuracy : 88.67\n",
            "iteration : 150, loss : 0.3680, accuracy : 88.35\n",
            "iteration : 200, loss : 0.3740, accuracy : 88.25\n",
            "iteration : 250, loss : 0.3756, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.11\n",
            "Epoch : 229, training loss : 0.3793, training accuracy : 88.13, test loss : 0.3917, test accuracy : 87.80\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.3848, accuracy : 87.80\n",
            "iteration : 100, loss : 0.3736, accuracy : 88.24\n",
            "iteration : 150, loss : 0.3756, accuracy : 88.18\n",
            "iteration : 200, loss : 0.3786, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3799, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3794, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3779, accuracy : 88.20\n",
            "Epoch : 230, training loss : 0.3788, training accuracy : 88.19, test loss : 0.3913, test accuracy : 87.65\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.3694, accuracy : 88.25\n",
            "iteration : 100, loss : 0.3718, accuracy : 88.27\n",
            "iteration : 150, loss : 0.3758, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3786, accuracy : 88.14\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3772, accuracy : 88.13\n",
            "Epoch : 231, training loss : 0.3773, training accuracy : 88.11, test loss : 0.3890, test accuracy : 87.82\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.3734, accuracy : 88.80\n",
            "iteration : 100, loss : 0.3806, accuracy : 88.02\n",
            "iteration : 150, loss : 0.3806, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3787, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3808, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3782, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.11\n",
            "Epoch : 232, training loss : 0.3800, training accuracy : 88.13, test loss : 0.3918, test accuracy : 87.85\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.3900, accuracy : 87.83\n",
            "iteration : 100, loss : 0.3787, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3757, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3785, accuracy : 88.19\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.22\n",
            "iteration : 350, loss : 0.3760, accuracy : 88.26\n",
            "Epoch : 233, training loss : 0.3775, training accuracy : 88.22, test loss : 0.3901, test accuracy : 87.85\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.3891, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3829, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3782, accuracy : 88.31\n",
            "iteration : 200, loss : 0.3795, accuracy : 88.24\n",
            "iteration : 250, loss : 0.3769, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.23\n",
            "Epoch : 234, training loss : 0.3784, training accuracy : 88.18, test loss : 0.3931, test accuracy : 87.80\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.3694, accuracy : 88.28\n",
            "iteration : 100, loss : 0.3766, accuracy : 88.07\n",
            "iteration : 150, loss : 0.3779, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3777, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3813, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3804, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3808, accuracy : 88.14\n",
            "Epoch : 235, training loss : 0.3801, training accuracy : 88.13, test loss : 0.3900, test accuracy : 87.88\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.3946, accuracy : 87.86\n",
            "iteration : 100, loss : 0.3881, accuracy : 87.97\n",
            "iteration : 150, loss : 0.3867, accuracy : 87.87\n",
            "iteration : 200, loss : 0.3833, accuracy : 87.98\n",
            "iteration : 250, loss : 0.3813, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3826, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3809, accuracy : 88.11\n",
            "Epoch : 236, training loss : 0.3803, training accuracy : 88.13, test loss : 0.3931, test accuracy : 87.85\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.3767, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3876, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3810, accuracy : 88.31\n",
            "iteration : 200, loss : 0.3806, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3831, accuracy : 88.19\n",
            "iteration : 300, loss : 0.3790, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3779, accuracy : 88.27\n",
            "Epoch : 237, training loss : 0.3782, training accuracy : 88.25, test loss : 0.3888, test accuracy : 87.96\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.3787, accuracy : 88.34\n",
            "iteration : 100, loss : 0.3820, accuracy : 88.29\n",
            "iteration : 150, loss : 0.3848, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3803, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3818, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3806, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.12\n",
            "Epoch : 238, training loss : 0.3790, training accuracy : 88.17, test loss : 0.3910, test accuracy : 87.85\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.3621, accuracy : 88.36\n",
            "iteration : 100, loss : 0.3733, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3756, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3805, accuracy : 87.92\n",
            "iteration : 250, loss : 0.3770, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3781, accuracy : 87.97\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.08\n",
            "Epoch : 239, training loss : 0.3783, training accuracy : 88.07, test loss : 0.3900, test accuracy : 87.72\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.3587, accuracy : 88.81\n",
            "iteration : 100, loss : 0.3714, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3733, accuracy : 88.11\n",
            "iteration : 200, loss : 0.3731, accuracy : 88.22\n",
            "iteration : 250, loss : 0.3745, accuracy : 88.32\n",
            "iteration : 300, loss : 0.3761, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3767, accuracy : 88.27\n",
            "Epoch : 240, training loss : 0.3765, training accuracy : 88.25, test loss : 0.3897, test accuracy : 87.86\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.3881, accuracy : 87.92\n",
            "iteration : 100, loss : 0.3799, accuracy : 88.08\n",
            "iteration : 150, loss : 0.3836, accuracy : 87.89\n",
            "iteration : 200, loss : 0.3866, accuracy : 87.91\n",
            "iteration : 250, loss : 0.3785, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3793, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3787, accuracy : 88.06\n",
            "Epoch : 241, training loss : 0.3781, training accuracy : 88.12, test loss : 0.3927, test accuracy : 87.78\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.3880, accuracy : 87.80\n",
            "iteration : 100, loss : 0.3837, accuracy : 88.04\n",
            "iteration : 150, loss : 0.3878, accuracy : 87.91\n",
            "iteration : 200, loss : 0.3907, accuracy : 87.76\n",
            "iteration : 250, loss : 0.3859, accuracy : 87.90\n",
            "iteration : 300, loss : 0.3842, accuracy : 87.98\n",
            "iteration : 350, loss : 0.3806, accuracy : 88.12\n",
            "Epoch : 242, training loss : 0.3794, training accuracy : 88.15, test loss : 0.3901, test accuracy : 87.78\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.3711, accuracy : 88.52\n",
            "iteration : 100, loss : 0.3752, accuracy : 88.36\n",
            "iteration : 150, loss : 0.3808, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3800, accuracy : 88.14\n",
            "iteration : 250, loss : 0.3763, accuracy : 88.27\n",
            "iteration : 300, loss : 0.3794, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3768, accuracy : 88.25\n",
            "Epoch : 243, training loss : 0.3768, training accuracy : 88.23, test loss : 0.3926, test accuracy : 87.62\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.3560, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3687, accuracy : 88.13\n",
            "iteration : 150, loss : 0.3744, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3776, accuracy : 88.08\n",
            "iteration : 250, loss : 0.3771, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3787, accuracy : 88.06\n",
            "Epoch : 244, training loss : 0.3785, training accuracy : 88.07, test loss : 0.3906, test accuracy : 87.86\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.3695, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3668, accuracy : 88.33\n",
            "iteration : 150, loss : 0.3784, accuracy : 87.95\n",
            "iteration : 200, loss : 0.3770, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3779, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3785, accuracy : 88.05\n",
            "iteration : 350, loss : 0.3797, accuracy : 88.04\n",
            "Epoch : 245, training loss : 0.3789, training accuracy : 88.08, test loss : 0.3895, test accuracy : 87.96\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.3645, accuracy : 88.78\n",
            "iteration : 100, loss : 0.3727, accuracy : 88.26\n",
            "iteration : 150, loss : 0.3758, accuracy : 88.17\n",
            "iteration : 200, loss : 0.3770, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3782, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3754, accuracy : 88.25\n",
            "Epoch : 246, training loss : 0.3758, training accuracy : 88.24, test loss : 0.3907, test accuracy : 87.86\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.3812, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3812, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3819, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3781, accuracy : 88.16\n",
            "iteration : 250, loss : 0.3772, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3803, accuracy : 87.97\n",
            "iteration : 350, loss : 0.3798, accuracy : 88.02\n",
            "Epoch : 247, training loss : 0.3790, training accuracy : 88.07, test loss : 0.3908, test accuracy : 87.76\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.3722, accuracy : 88.47\n",
            "iteration : 100, loss : 0.3686, accuracy : 88.48\n",
            "iteration : 150, loss : 0.3750, accuracy : 88.30\n",
            "iteration : 200, loss : 0.3775, accuracy : 88.24\n",
            "iteration : 250, loss : 0.3763, accuracy : 88.33\n",
            "iteration : 300, loss : 0.3776, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3759, accuracy : 88.22\n",
            "Epoch : 248, training loss : 0.3772, training accuracy : 88.22, test loss : 0.3907, test accuracy : 87.85\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.3782, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3840, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3842, accuracy : 88.01\n",
            "iteration : 200, loss : 0.3807, accuracy : 88.08\n",
            "iteration : 250, loss : 0.3817, accuracy : 88.04\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3798, accuracy : 88.19\n",
            "Epoch : 249, training loss : 0.3796, training accuracy : 88.16, test loss : 0.3914, test accuracy : 87.79\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.3760, accuracy : 88.41\n",
            "iteration : 100, loss : 0.3755, accuracy : 88.21\n",
            "iteration : 150, loss : 0.3793, accuracy : 88.18\n",
            "iteration : 200, loss : 0.3804, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3791, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3793, accuracy : 88.18\n",
            "iteration : 350, loss : 0.3809, accuracy : 88.13\n",
            "Epoch : 250, training loss : 0.3802, training accuracy : 88.15, test loss : 0.3907, test accuracy : 87.85\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.3890, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3790, accuracy : 88.18\n",
            "iteration : 150, loss : 0.3741, accuracy : 88.19\n",
            "iteration : 200, loss : 0.3720, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3718, accuracy : 88.21\n",
            "iteration : 300, loss : 0.3724, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.10\n",
            "Epoch : 251, training loss : 0.3773, training accuracy : 88.12, test loss : 0.3904, test accuracy : 87.70\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.3919, accuracy : 87.41\n",
            "iteration : 100, loss : 0.3858, accuracy : 87.63\n",
            "iteration : 150, loss : 0.3839, accuracy : 87.93\n",
            "iteration : 200, loss : 0.3841, accuracy : 87.88\n",
            "iteration : 250, loss : 0.3828, accuracy : 88.01\n",
            "iteration : 300, loss : 0.3814, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.09\n",
            "Epoch : 252, training loss : 0.3791, training accuracy : 88.11, test loss : 0.3949, test accuracy : 87.71\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.3689, accuracy : 88.12\n",
            "iteration : 100, loss : 0.3697, accuracy : 88.62\n",
            "iteration : 150, loss : 0.3756, accuracy : 88.38\n",
            "iteration : 200, loss : 0.3764, accuracy : 88.37\n",
            "iteration : 250, loss : 0.3778, accuracy : 88.32\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.28\n",
            "Epoch : 253, training loss : 0.3785, training accuracy : 88.28, test loss : 0.3924, test accuracy : 87.66\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.3937, accuracy : 87.36\n",
            "iteration : 100, loss : 0.3841, accuracy : 87.84\n",
            "iteration : 150, loss : 0.3879, accuracy : 87.70\n",
            "iteration : 200, loss : 0.3800, accuracy : 87.95\n",
            "iteration : 250, loss : 0.3794, accuracy : 87.99\n",
            "iteration : 300, loss : 0.3815, accuracy : 87.97\n",
            "iteration : 350, loss : 0.3791, accuracy : 88.00\n",
            "Epoch : 254, training loss : 0.3790, training accuracy : 88.00, test loss : 0.3934, test accuracy : 87.62\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.3798, accuracy : 88.12\n",
            "iteration : 100, loss : 0.3796, accuracy : 88.00\n",
            "iteration : 150, loss : 0.3841, accuracy : 87.76\n",
            "iteration : 200, loss : 0.3805, accuracy : 88.01\n",
            "iteration : 250, loss : 0.3754, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3744, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3754, accuracy : 88.25\n",
            "Epoch : 255, training loss : 0.3779, training accuracy : 88.17, test loss : 0.3916, test accuracy : 87.72\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.3986, accuracy : 87.73\n",
            "iteration : 100, loss : 0.3830, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3868, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3844, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3826, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3794, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.15\n",
            "Epoch : 256, training loss : 0.3796, training accuracy : 88.18, test loss : 0.3898, test accuracy : 87.97\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.3833, accuracy : 88.20\n",
            "iteration : 100, loss : 0.3715, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3740, accuracy : 88.20\n",
            "iteration : 200, loss : 0.3766, accuracy : 88.19\n",
            "iteration : 250, loss : 0.3836, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3820, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3804, accuracy : 88.16\n",
            "Epoch : 257, training loss : 0.3805, training accuracy : 88.14, test loss : 0.3907, test accuracy : 87.98\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.3650, accuracy : 88.61\n",
            "iteration : 100, loss : 0.3708, accuracy : 88.39\n",
            "iteration : 150, loss : 0.3701, accuracy : 88.38\n",
            "iteration : 200, loss : 0.3725, accuracy : 88.28\n",
            "iteration : 250, loss : 0.3766, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3779, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3766, accuracy : 88.23\n",
            "Epoch : 258, training loss : 0.3778, training accuracy : 88.19, test loss : 0.3934, test accuracy : 87.76\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.4101, accuracy : 87.38\n",
            "iteration : 100, loss : 0.3912, accuracy : 87.84\n",
            "iteration : 150, loss : 0.3833, accuracy : 87.83\n",
            "iteration : 200, loss : 0.3813, accuracy : 87.98\n",
            "iteration : 250, loss : 0.3782, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3812, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3809, accuracy : 88.06\n",
            "Epoch : 259, training loss : 0.3802, training accuracy : 88.09, test loss : 0.3920, test accuracy : 87.75\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.3698, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3773, accuracy : 88.26\n",
            "iteration : 150, loss : 0.3801, accuracy : 88.08\n",
            "iteration : 200, loss : 0.3749, accuracy : 88.19\n",
            "iteration : 250, loss : 0.3781, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3782, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.10\n",
            "Epoch : 260, training loss : 0.3788, training accuracy : 88.11, test loss : 0.3918, test accuracy : 87.81\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.3808, accuracy : 87.81\n",
            "iteration : 100, loss : 0.3794, accuracy : 87.76\n",
            "iteration : 150, loss : 0.3857, accuracy : 87.66\n",
            "iteration : 200, loss : 0.3867, accuracy : 87.79\n",
            "iteration : 250, loss : 0.3804, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3807, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3783, accuracy : 88.10\n",
            "Epoch : 261, training loss : 0.3779, training accuracy : 88.09, test loss : 0.3897, test accuracy : 87.89\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.3733, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3780, accuracy : 88.02\n",
            "iteration : 150, loss : 0.3723, accuracy : 88.36\n",
            "iteration : 200, loss : 0.3759, accuracy : 88.19\n",
            "iteration : 250, loss : 0.3765, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3769, accuracy : 88.25\n",
            "iteration : 350, loss : 0.3757, accuracy : 88.29\n",
            "Epoch : 262, training loss : 0.3769, training accuracy : 88.23, test loss : 0.3894, test accuracy : 87.83\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.3865, accuracy : 88.25\n",
            "iteration : 100, loss : 0.3903, accuracy : 88.17\n",
            "iteration : 150, loss : 0.3855, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3850, accuracy : 88.13\n",
            "iteration : 250, loss : 0.3782, accuracy : 88.31\n",
            "iteration : 300, loss : 0.3770, accuracy : 88.28\n",
            "iteration : 350, loss : 0.3770, accuracy : 88.24\n",
            "Epoch : 263, training loss : 0.3782, training accuracy : 88.21, test loss : 0.3899, test accuracy : 87.88\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.3904, accuracy : 87.97\n",
            "iteration : 100, loss : 0.3818, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3837, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3822, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3831, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3794, accuracy : 88.16\n",
            "Epoch : 264, training loss : 0.3794, training accuracy : 88.14, test loss : 0.3917, test accuracy : 87.68\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.3798, accuracy : 87.91\n",
            "iteration : 100, loss : 0.3747, accuracy : 88.00\n",
            "iteration : 150, loss : 0.3812, accuracy : 87.82\n",
            "iteration : 200, loss : 0.3789, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3821, accuracy : 87.97\n",
            "iteration : 300, loss : 0.3831, accuracy : 88.02\n",
            "iteration : 350, loss : 0.3808, accuracy : 88.12\n",
            "Epoch : 265, training loss : 0.3815, training accuracy : 88.12, test loss : 0.3905, test accuracy : 87.89\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.3794, accuracy : 88.16\n",
            "iteration : 100, loss : 0.3814, accuracy : 88.15\n",
            "iteration : 150, loss : 0.3793, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3817, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3796, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3757, accuracy : 88.29\n",
            "Epoch : 266, training loss : 0.3768, training accuracy : 88.29, test loss : 0.3918, test accuracy : 87.63\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.3646, accuracy : 88.41\n",
            "iteration : 100, loss : 0.3639, accuracy : 88.41\n",
            "iteration : 150, loss : 0.3701, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3754, accuracy : 88.27\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3793, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3763, accuracy : 88.20\n",
            "Epoch : 267, training loss : 0.3760, training accuracy : 88.20, test loss : 0.3922, test accuracy : 87.77\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.3543, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3618, accuracy : 88.52\n",
            "iteration : 150, loss : 0.3656, accuracy : 88.42\n",
            "iteration : 200, loss : 0.3707, accuracy : 88.35\n",
            "iteration : 250, loss : 0.3713, accuracy : 88.36\n",
            "iteration : 300, loss : 0.3750, accuracy : 88.29\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.22\n",
            "Epoch : 268, training loss : 0.3787, training accuracy : 88.19, test loss : 0.3944, test accuracy : 87.60\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.3622, accuracy : 88.61\n",
            "iteration : 100, loss : 0.3670, accuracy : 88.35\n",
            "iteration : 150, loss : 0.3669, accuracy : 88.44\n",
            "iteration : 200, loss : 0.3731, accuracy : 88.24\n",
            "iteration : 250, loss : 0.3710, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3731, accuracy : 88.27\n",
            "iteration : 350, loss : 0.3764, accuracy : 88.17\n",
            "Epoch : 269, training loss : 0.3778, training accuracy : 88.14, test loss : 0.3945, test accuracy : 87.66\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.3639, accuracy : 88.53\n",
            "iteration : 100, loss : 0.3780, accuracy : 88.17\n",
            "iteration : 150, loss : 0.3848, accuracy : 87.87\n",
            "iteration : 200, loss : 0.3853, accuracy : 87.83\n",
            "iteration : 250, loss : 0.3834, accuracy : 87.89\n",
            "iteration : 300, loss : 0.3809, accuracy : 87.94\n",
            "iteration : 350, loss : 0.3791, accuracy : 88.02\n",
            "Epoch : 270, training loss : 0.3781, training accuracy : 88.04, test loss : 0.3913, test accuracy : 87.98\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.3701, accuracy : 88.59\n",
            "iteration : 100, loss : 0.3713, accuracy : 88.39\n",
            "iteration : 150, loss : 0.3749, accuracy : 88.22\n",
            "iteration : 200, loss : 0.3764, accuracy : 88.28\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.18\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.17\n",
            "Epoch : 271, training loss : 0.3789, training accuracy : 88.15, test loss : 0.3867, test accuracy : 87.88\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.3671, accuracy : 88.70\n",
            "iteration : 100, loss : 0.3694, accuracy : 88.55\n",
            "iteration : 150, loss : 0.3738, accuracy : 88.47\n",
            "iteration : 200, loss : 0.3729, accuracy : 88.46\n",
            "iteration : 250, loss : 0.3767, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.15\n",
            "Epoch : 272, training loss : 0.3792, training accuracy : 88.14, test loss : 0.3895, test accuracy : 87.87\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.3766, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3735, accuracy : 88.03\n",
            "iteration : 150, loss : 0.3756, accuracy : 87.97\n",
            "iteration : 200, loss : 0.3775, accuracy : 88.07\n",
            "iteration : 250, loss : 0.3788, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3813, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.03\n",
            "Epoch : 273, training loss : 0.3796, training accuracy : 88.08, test loss : 0.3912, test accuracy : 87.77\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.3635, accuracy : 89.12\n",
            "iteration : 100, loss : 0.3689, accuracy : 88.89\n",
            "iteration : 150, loss : 0.3748, accuracy : 88.74\n",
            "iteration : 200, loss : 0.3764, accuracy : 88.62\n",
            "iteration : 250, loss : 0.3752, accuracy : 88.61\n",
            "iteration : 300, loss : 0.3786, accuracy : 88.44\n",
            "iteration : 350, loss : 0.3814, accuracy : 88.28\n",
            "Epoch : 274, training loss : 0.3803, training accuracy : 88.30, test loss : 0.3884, test accuracy : 87.94\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.3550, accuracy : 89.05\n",
            "iteration : 100, loss : 0.3644, accuracy : 88.48\n",
            "iteration : 150, loss : 0.3707, accuracy : 88.38\n",
            "iteration : 200, loss : 0.3735, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3724, accuracy : 88.30\n",
            "iteration : 300, loss : 0.3759, accuracy : 88.22\n",
            "iteration : 350, loss : 0.3760, accuracy : 88.24\n",
            "Epoch : 275, training loss : 0.3771, training accuracy : 88.21, test loss : 0.3898, test accuracy : 87.91\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.3890, accuracy : 88.09\n",
            "iteration : 100, loss : 0.3816, accuracy : 88.25\n",
            "iteration : 150, loss : 0.3821, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3803, accuracy : 88.17\n",
            "iteration : 250, loss : 0.3794, accuracy : 88.16\n",
            "iteration : 300, loss : 0.3768, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.16\n",
            "Epoch : 276, training loss : 0.3779, training accuracy : 88.17, test loss : 0.3917, test accuracy : 87.79\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.3916, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3805, accuracy : 88.32\n",
            "iteration : 150, loss : 0.3780, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.27\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3808, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3761, accuracy : 88.36\n",
            "Epoch : 277, training loss : 0.3758, training accuracy : 88.35, test loss : 0.3907, test accuracy : 87.70\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.3748, accuracy : 88.31\n",
            "iteration : 100, loss : 0.3787, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3799, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3824, accuracy : 87.82\n",
            "iteration : 250, loss : 0.3805, accuracy : 87.94\n",
            "iteration : 300, loss : 0.3785, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.02\n",
            "Epoch : 278, training loss : 0.3796, training accuracy : 88.00, test loss : 0.3934, test accuracy : 87.78\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.3707, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3815, accuracy : 87.94\n",
            "iteration : 150, loss : 0.3852, accuracy : 87.92\n",
            "iteration : 200, loss : 0.3867, accuracy : 87.71\n",
            "iteration : 250, loss : 0.3868, accuracy : 87.74\n",
            "iteration : 300, loss : 0.3847, accuracy : 87.83\n",
            "iteration : 350, loss : 0.3809, accuracy : 87.89\n",
            "Epoch : 279, training loss : 0.3801, training accuracy : 87.91, test loss : 0.3894, test accuracy : 87.85\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.3930, accuracy : 87.78\n",
            "iteration : 100, loss : 0.3873, accuracy : 88.06\n",
            "iteration : 150, loss : 0.3849, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3796, accuracy : 88.16\n",
            "iteration : 250, loss : 0.3766, accuracy : 88.21\n",
            "iteration : 300, loss : 0.3760, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3757, accuracy : 88.16\n",
            "Epoch : 280, training loss : 0.3757, training accuracy : 88.15, test loss : 0.3914, test accuracy : 87.67\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.3808, accuracy : 87.95\n",
            "iteration : 100, loss : 0.3825, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3768, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3787, accuracy : 88.01\n",
            "iteration : 250, loss : 0.3776, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3762, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3769, accuracy : 88.12\n",
            "Epoch : 281, training loss : 0.3770, training accuracy : 88.16, test loss : 0.3898, test accuracy : 87.81\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.3665, accuracy : 87.94\n",
            "iteration : 100, loss : 0.3730, accuracy : 87.99\n",
            "iteration : 150, loss : 0.3739, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3755, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3784, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3780, accuracy : 88.13\n",
            "Epoch : 282, training loss : 0.3781, training accuracy : 88.16, test loss : 0.3938, test accuracy : 87.74\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.3669, accuracy : 88.48\n",
            "iteration : 100, loss : 0.3767, accuracy : 88.11\n",
            "iteration : 150, loss : 0.3828, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3841, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3801, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3812, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.10\n",
            "Epoch : 283, training loss : 0.3787, training accuracy : 88.14, test loss : 0.3907, test accuracy : 87.72\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.3709, accuracy : 87.92\n",
            "iteration : 100, loss : 0.3737, accuracy : 88.30\n",
            "iteration : 150, loss : 0.3785, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3788, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3775, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.05\n",
            "Epoch : 284, training loss : 0.3799, training accuracy : 88.08, test loss : 0.3921, test accuracy : 87.65\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.3824, accuracy : 87.56\n",
            "iteration : 100, loss : 0.3684, accuracy : 88.40\n",
            "iteration : 150, loss : 0.3739, accuracy : 88.51\n",
            "iteration : 200, loss : 0.3763, accuracy : 88.34\n",
            "iteration : 250, loss : 0.3762, accuracy : 88.33\n",
            "iteration : 300, loss : 0.3773, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3761, accuracy : 88.35\n",
            "Epoch : 285, training loss : 0.3762, training accuracy : 88.37, test loss : 0.3921, test accuracy : 87.68\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.3645, accuracy : 88.88\n",
            "iteration : 100, loss : 0.3699, accuracy : 88.65\n",
            "iteration : 150, loss : 0.3678, accuracy : 88.46\n",
            "iteration : 200, loss : 0.3740, accuracy : 88.40\n",
            "iteration : 250, loss : 0.3735, accuracy : 88.42\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.18\n",
            "iteration : 350, loss : 0.3787, accuracy : 88.19\n",
            "Epoch : 286, training loss : 0.3776, training accuracy : 88.23, test loss : 0.3916, test accuracy : 87.87\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.3825, accuracy : 87.47\n",
            "iteration : 100, loss : 0.3793, accuracy : 87.67\n",
            "iteration : 150, loss : 0.3786, accuracy : 87.93\n",
            "iteration : 200, loss : 0.3760, accuracy : 88.04\n",
            "iteration : 250, loss : 0.3751, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3794, accuracy : 88.08\n",
            "Epoch : 287, training loss : 0.3777, training accuracy : 88.14, test loss : 0.3911, test accuracy : 87.82\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.3753, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3758, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3791, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3810, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3774, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3771, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.16\n",
            "Epoch : 288, training loss : 0.3768, training accuracy : 88.18, test loss : 0.3954, test accuracy : 87.78\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.3803, accuracy : 88.25\n",
            "iteration : 100, loss : 0.3767, accuracy : 88.23\n",
            "iteration : 150, loss : 0.3751, accuracy : 88.18\n",
            "iteration : 200, loss : 0.3760, accuracy : 88.25\n",
            "iteration : 250, loss : 0.3737, accuracy : 88.40\n",
            "iteration : 300, loss : 0.3753, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.18\n",
            "Epoch : 289, training loss : 0.3778, training accuracy : 88.15, test loss : 0.3894, test accuracy : 87.73\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.3787, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3795, accuracy : 88.19\n",
            "iteration : 150, loss : 0.3782, accuracy : 88.28\n",
            "iteration : 200, loss : 0.3801, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3823, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3811, accuracy : 88.05\n",
            "iteration : 350, loss : 0.3804, accuracy : 88.15\n",
            "Epoch : 290, training loss : 0.3788, training accuracy : 88.18, test loss : 0.3904, test accuracy : 87.98\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.3813, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3727, accuracy : 88.53\n",
            "iteration : 150, loss : 0.3780, accuracy : 88.29\n",
            "iteration : 200, loss : 0.3774, accuracy : 88.26\n",
            "iteration : 250, loss : 0.3744, accuracy : 88.40\n",
            "iteration : 300, loss : 0.3756, accuracy : 88.39\n",
            "iteration : 350, loss : 0.3761, accuracy : 88.30\n",
            "Epoch : 291, training loss : 0.3775, training accuracy : 88.28, test loss : 0.3921, test accuracy : 87.67\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.3923, accuracy : 87.91\n",
            "iteration : 100, loss : 0.3836, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3762, accuracy : 88.32\n",
            "iteration : 200, loss : 0.3707, accuracy : 88.45\n",
            "iteration : 250, loss : 0.3721, accuracy : 88.35\n",
            "iteration : 300, loss : 0.3749, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.16\n",
            "Epoch : 292, training loss : 0.3782, training accuracy : 88.17, test loss : 0.3941, test accuracy : 87.70\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.3771, accuracy : 87.77\n",
            "iteration : 100, loss : 0.3772, accuracy : 88.01\n",
            "iteration : 150, loss : 0.3735, accuracy : 88.28\n",
            "iteration : 200, loss : 0.3746, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3766, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3794, accuracy : 88.07\n",
            "Epoch : 293, training loss : 0.3770, training accuracy : 88.15, test loss : 0.3913, test accuracy : 87.75\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.3752, accuracy : 88.25\n",
            "iteration : 100, loss : 0.3741, accuracy : 88.48\n",
            "iteration : 150, loss : 0.3758, accuracy : 88.35\n",
            "iteration : 200, loss : 0.3810, accuracy : 88.29\n",
            "iteration : 250, loss : 0.3797, accuracy : 88.29\n",
            "iteration : 300, loss : 0.3808, accuracy : 88.19\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.28\n",
            "Epoch : 294, training loss : 0.3788, training accuracy : 88.26, test loss : 0.3903, test accuracy : 87.91\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.3926, accuracy : 87.56\n",
            "iteration : 100, loss : 0.3836, accuracy : 87.88\n",
            "iteration : 150, loss : 0.3805, accuracy : 88.11\n",
            "iteration : 200, loss : 0.3770, accuracy : 88.24\n",
            "iteration : 250, loss : 0.3798, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3762, accuracy : 88.29\n",
            "iteration : 350, loss : 0.3779, accuracy : 88.17\n",
            "Epoch : 295, training loss : 0.3795, training accuracy : 88.14, test loss : 0.3932, test accuracy : 87.90\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.3729, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3786, accuracy : 88.09\n",
            "iteration : 150, loss : 0.3727, accuracy : 88.40\n",
            "iteration : 200, loss : 0.3739, accuracy : 88.38\n",
            "iteration : 250, loss : 0.3766, accuracy : 88.30\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3768, accuracy : 88.29\n",
            "Epoch : 296, training loss : 0.3774, training accuracy : 88.26, test loss : 0.3924, test accuracy : 87.72\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.3572, accuracy : 88.89\n",
            "iteration : 100, loss : 0.3768, accuracy : 88.21\n",
            "iteration : 150, loss : 0.3841, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3849, accuracy : 88.10\n",
            "iteration : 250, loss : 0.3833, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3841, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3825, accuracy : 88.03\n",
            "Epoch : 297, training loss : 0.3810, training accuracy : 88.09, test loss : 0.3888, test accuracy : 87.91\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.3649, accuracy : 88.53\n",
            "iteration : 100, loss : 0.3704, accuracy : 88.39\n",
            "iteration : 150, loss : 0.3706, accuracy : 88.38\n",
            "iteration : 200, loss : 0.3786, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3761, accuracy : 88.16\n",
            "iteration : 300, loss : 0.3793, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.10\n",
            "Epoch : 298, training loss : 0.3796, training accuracy : 88.10, test loss : 0.3888, test accuracy : 87.88\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.3799, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3813, accuracy : 87.83\n",
            "iteration : 150, loss : 0.3846, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3847, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3853, accuracy : 87.95\n",
            "iteration : 300, loss : 0.3833, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.19\n",
            "Epoch : 299, training loss : 0.3808, training accuracy : 88.17, test loss : 0.3895, test accuracy : 87.91\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.3877, accuracy : 87.47\n",
            "iteration : 100, loss : 0.3806, accuracy : 87.82\n",
            "iteration : 150, loss : 0.3795, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3785, accuracy : 87.98\n",
            "iteration : 250, loss : 0.3797, accuracy : 87.94\n",
            "iteration : 300, loss : 0.3776, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3760, accuracy : 88.15\n",
            "Epoch : 300, training loss : 0.3770, training accuracy : 88.13, test loss : 0.3917, test accuracy : 87.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUQVIKR-X3v6",
        "outputId": "2ecdd7cb-c1b4-4d99-d2e6-353a610a6d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3929436405499776, 87.68822987092808)"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : Exponential\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7CNz1iabSB21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "98e71224-b023-492a-e3d9-57d7a35d085c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : Exponential\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ed74fb9a-e437-4a18-c35a-5b46cdbc5f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_exp = {train_loss_list}\") \n",
        "print(f\"train_acc_list_exp = {train_acc_list}\")\n",
        "print(f\"test_loss_list_exp = {test_loss_list}\")\n",
        "print(f\"test_acc_list_exp = {test_acc_list}\")"
      ],
      "metadata": {
        "id": "3eiY3bTlWipW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957c16f6-1c6a-4cc8-dccb-aaf64760be57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_exp = [1.5108453227575556, 0.45569133027620756, 0.38955149030297753, 0.38201055714108434, 0.3791554563736851, 0.37846633355791975, 0.37789585104156637, 0.37833547551780533, 0.37885424543202406, 0.3798585463830126, 0.3801606303146538, 0.3785690202864851, 0.380058984244419, 0.38191306041831247, 0.3755665107309657, 0.37845069464790787, 0.3810173268240642, 0.37906573632060675, 0.38025176412044825, 0.37710853672124506, 0.37910957913088605, 0.3795646985129612, 0.37768419939004955, 0.37915716253645054, 0.3763544963304266, 0.37615088787343764, 0.3767496614356028, 0.3780435439209305, 0.37970296291477956, 0.3768931365190806, 0.3782231926433439, 0.37863624968179843, 0.3801447201146666, 0.38008787327504095, 0.3784985868184547, 0.3775513489152681, 0.3797982176387213, 0.378436125835106, 0.37775709227656284, 0.3784049479618951, 0.3812676381047179, 0.377607957215167, 0.37778531927564923, 0.3785970114110931, 0.3763440459320539, 0.37659880587563604, 0.38260392961786366, 0.3788121801404772, 0.3782783241937477, 0.37851579224837184, 0.37789375260270386, 0.37802613358995135, 0.3780238080800064, 0.38144587775879113, 0.3779912972595634, 0.3791698661600025, 0.3766854087996289, 0.37804330437163997, 0.3786587099718854, 0.37654282059772876, 0.3787634488609102, 0.3799068733524824, 0.37713607115958764, 0.37751451530430696, 0.3801226066299247, 0.37911422368956776, 0.3805197463610631, 0.37700172774190827, 0.3788724349847008, 0.3784166157326401, 0.38057625192775313, 0.37780569122251134, 0.38091700400924944, 0.3794297932528545, 0.3814112742338077, 0.37960589101644066, 0.37835461256626823, 0.37888884483798735, 0.3795582245439695, 0.37849634607148364, 0.37682519744082194, 0.37698636250444223, 0.3814956169464401, 0.3775139360570003, 0.379286357057773, 0.37696852730864755, 0.3763741816123973, 0.37921798265561824, 0.3787178158921601, 0.3787029756682352, 0.37783306570557074, 0.3772085670452454, 0.377733004771597, 0.3765660253401371, 0.3802021978912638, 0.37975252381346736, 0.3792028047528047, 0.3785102287039847, 0.3774594428739574, 0.3806928960773034, 0.3792232088441771, 0.37827566011649805, 0.38008303063994825, 0.3778988100325835, 0.3803016372974003, 0.3791544923614357, 0.38108806501882186, 0.37818956795100594, 0.37843327103106955, 0.38068289166382013, 0.38067130910025704, 0.37755737811084683, 0.37938234137325755, 0.3798758971497295, 0.37914535011540906, 0.37674440685811084, 0.37839110369281714, 0.3803737539505248, 0.3769939240966709, 0.37618244623104086, 0.3802966764625818, 0.376914295521855, 0.38093100847590583, 0.3785027291797364, 0.38015424458153524, 0.3799492670753138, 0.3770360491140102, 0.3781222916311688, 0.37643939103214397, 0.37902817707559283, 0.37899113222350916, 0.3788869072428241, 0.37866583002130516, 0.38024979207896925, 0.37998249028433306, 0.37665241209633626, 0.3791340853221371, 0.3768656629776244, 0.3781943121738227, 0.37956079872966136, 0.3790555945660687, 0.3767165967280949, 0.3785593254860178, 0.3785436793071468, 0.3786095209157241, 0.37977585394369556, 0.37895426606421223, 0.37726105127715803, 0.3811360982456181, 0.3777372095239195, 0.37985739951857384, 0.3775850888190231, 0.37942486108964696, 0.3774096647575296, 0.37866538746893247, 0.37956950698441605, 0.37765448986676325, 0.3765703885535884, 0.37493878506063444, 0.3787174464322041, 0.3790939331539278, 0.3798395115024029, 0.3802033216972661, 0.3778447469237051, 0.37922620373528176, 0.3788863822696655, 0.3776389633010073, 0.37965354569720705, 0.37847416466329153, 0.38210083584636856, 0.3769269231858292, 0.376283608316406, 0.3791458204105941, 0.3800827085891067, 0.3800524467940576, 0.379729083156198, 0.3786869500145357, 0.37784998191566, 0.37798632447150987, 0.37959555187199495, 0.3772686913326827, 0.37737370587299834, 0.3774894487647829, 0.3773925192149351, 0.37866038096144916, 0.3799699583673865, 0.37930802181161194, 0.37929151229419034, 0.37872534558217374, 0.38051237880699035, 0.3785012537224829, 0.37907243408969427, 0.3786347356188265, 0.3772817448306536, 0.37789186423386983, 0.377803510807071, 0.37891541296227516, 0.3768928705596019, 0.3764786839808229, 0.37735731827049723, 0.37620092403436417, 0.37594923223583354, 0.37946950848186567, 0.37934725721515616, 0.3799315985383057, 0.37742675118006985, 0.3800657158137014, 0.38043200723362486, 0.3790072623468673, 0.37812678907621844, 0.380248889528962, 0.3795400029679301, 0.37716066687895355, 0.37926767851279036, 0.3811982564002195, 0.3785968935344277, 0.37829239452434427, 0.3791591095003655, 0.37750050801087204, 0.3755794913788152, 0.37707439347657407, 0.3798365030023787, 0.37901620768757693, 0.3787338199815776, 0.3768027005237616, 0.378330920527621, 0.3773879486774688, 0.37904529988281127, 0.3793277707364824, 0.37877471587522243, 0.3773436505135482, 0.38000925620235404, 0.37747299065434836, 0.3783611831707037, 0.38008849536823386, 0.3802644786069064, 0.37820831300604957, 0.37898807813158525, 0.37832132050500006, 0.37651883493755567, 0.3780955279745707, 0.3794448867157546, 0.37681427719147226, 0.37846876878725483, 0.37892088152690306, 0.3757547806433546, 0.3790287462272618, 0.3771775669764051, 0.37960463952081314, 0.38023487892415786, 0.3773157599818739, 0.3790636116734688, 0.37847773605568946, 0.3790109474969104, 0.3778621018094422, 0.3795850692483468, 0.38052020777208695, 0.3777503734681664, 0.38023589062819957, 0.37878056504539037, 0.3778544519813403, 0.3769152445117956, 0.37817029428837423, 0.3794189135879682, 0.3814815942268708, 0.3767595124357761, 0.376037456559618, 0.3787438214067521, 0.37781988387185383, 0.3780568046621514, 0.3789186481295562, 0.3792363849031893, 0.3795603767723895, 0.38027571985715125, 0.37710057130350977, 0.37790428061633896, 0.3758348726887044, 0.37957349484205893, 0.3801340398184329, 0.37570041014250055, 0.37696901714898706, 0.3780564808748602, 0.37865450675409984, 0.37989892315896867, 0.37623435913062675, 0.3776249932402841, 0.3777484252120098, 0.37681428954853274, 0.3777670302203677, 0.37880285431537525, 0.3775242522641572, 0.37820892516513505, 0.37701137581976446, 0.378751233704691, 0.37953954490865793, 0.3773617081364319, 0.3809620241324107, 0.37960273697770386, 0.3808248356428896, 0.37698043551709914]\n",
            "train_acc_list_exp = [47.21651667548968, 85.66437268395977, 87.928004235045, 88.11858125992589, 88.25410269984118, 88.07623080995235, 88.23928004235044, 88.18422445738486, 88.14187400741133, 88.09740603493913, 88.06564319745897, 88.14399152991001, 88.0084700899947, 87.9915299100053, 88.28163049232398, 88.06776071995765, 88.09105346744309, 88.11434621492853, 87.96188459502382, 88.11222869242985, 88.12916887241927, 88.13975648491265, 88.2435150873478, 88.16728427739545, 88.09105346744309, 88.20116463737428, 88.20328215987296, 88.01482265749074, 88.0084700899947, 88.20116463737428, 88.15034409740603, 88.22445738485972, 88.04870301746956, 88.06776071995765, 88.28586553732133, 88.19269454737956, 88.11011116993119, 88.18422445738486, 88.20539968237162, 88.08046585494971, 88.071995764955, 88.23292747485442, 88.23292747485442, 88.15881418740074, 88.26680783483324, 88.32398094229751, 88.06140815246162, 88.18210693488618, 88.17363684489148, 88.15881418740074, 88.13975648491265, 88.15034409740603, 88.12281630492323, 88.01694017998942, 88.19481206987824, 88.18634197988354, 88.3070407623081, 88.16093170989942, 88.16516675489677, 88.19481206987824, 88.25622022233986, 88.12493382742191, 88.1355214399153, 88.16093170989942, 88.09105346744309, 88.24563260984648, 88.11858125992589, 88.10799364743251, 88.09317098994177, 88.09528851244045, 88.0359978824775, 88.17787188988883, 88.10375860243515, 88.1355214399153, 88.08470089994707, 88.09105346744309, 88.23928004235044, 88.12916887241927, 88.10375860243515, 88.26045526733722, 88.24775013234516, 88.16093170989942, 88.05717310746427, 88.11858125992589, 88.15034409740603, 88.24563260984648, 88.14822657490735, 88.05293806246691, 88.1630492323981, 88.12281630492323, 88.32821598729487, 88.21386977236634, 88.23928004235044, 88.26892535733192, 88.23928004235044, 87.91529910005293, 88.05293806246691, 88.16093170989942, 88.26892535733192, 88.10587612493383, 88.13128639491795, 88.28374801482266, 88.21810481736368, 88.20328215987296, 88.18422445738486, 88.09740603493913, 88.00635256749602, 88.08681842244575, 88.10375860243515, 88.09528851244045, 88.06140815246162, 88.16093170989942, 88.10799364743251, 88.12493382742191, 88.08681842244575, 88.29433562731604, 88.08258337744839, 88.0084700899947, 88.17363684489148, 88.27527792482795, 88.10587612493383, 88.12069878242457, 87.98941238750662, 88.22022233986236, 88.12916887241927, 88.12281630492323, 88.14399152991001, 88.16516675489677, 88.14399152991001, 88.09528851244045, 88.18845950238222, 88.19692959237692, 88.15246161990471, 88.06564319745897, 88.12069878242457, 88.25833774483854, 88.15246161990471, 88.25622022233986, 88.11646373742721, 88.10375860243515, 88.11434621492853, 88.18422445738486, 88.14610905240868, 88.27316040232928, 88.15881418740074, 88.16728427739545, 88.13763896241397, 88.08681842244575, 88.03811540497618, 88.15457914240339, 88.15034409740603, 88.28586553732133, 88.13975648491265, 88.29010058231869, 88.25622022233986, 88.21810481736368, 88.22445738485972, 88.27316040232928, 88.2710428798306, 88.11222869242985, 88.02541026998412, 88.02541026998412, 88.08258337744839, 88.14610905240868, 88.09740603493913, 88.18634197988354, 88.18634197988354, 88.17998941238751, 88.11646373742721, 88.06140815246162, 88.05293806246691, 88.2710428798306, 88.071995764955, 88.22233986236104, 88.09317098994177, 88.12493382742191, 88.18634197988354, 88.11434621492853, 88.21810481736368, 88.00211752249868, 88.11858125992589, 88.27527792482795, 88.30280571731075, 88.24563260984648, 88.15246161990471, 88.12493382742191, 88.11434621492853, 88.02117522498676, 88.24563260984648, 88.06987824245633, 88.23716251985178, 88.0635256749603, 88.14399152991001, 88.22445738485972, 88.19269454737956, 88.26045526733722, 88.1630492323981, 88.1990471148756, 88.14399152991001, 88.27527792482795, 88.24563260984648, 88.32609846479619, 88.02117522498676, 88.15669666490207, 88.071995764955, 88.16728427739545, 88.15034409740603, 88.12281630492323, 88.15881418740074, 88.17787188988883, 88.06987824245633, 88.08470089994707, 88.11646373742721, 88.13763896241397, 88.1355214399153, 88.18845950238222, 88.18845950238222, 88.14399152991001, 88.25410269984118, 88.22445738485972, 88.17998941238751, 88.07623080995235, 88.14187400741133, 88.09952355743779, 88.31127580730545, 88.19481206987824, 88.2265749073584, 88.10164107993647, 88.12705134992059, 88.1905770248809, 88.11434621492853, 88.13128639491795, 88.22233986236104, 88.17998941238751, 88.13128639491795, 88.12705134992059, 88.24563260984648, 88.16940179989412, 88.06987824245633, 88.24986765484384, 88.12493382742191, 88.15457914240339, 88.22869242985706, 88.06987824245633, 88.08470089994707, 88.23928004235044, 88.07411328745368, 88.215987294865, 88.15881418740074, 88.14822657490735, 88.12069878242457, 88.10799364743251, 88.27527792482795, 88.00423504499736, 88.17363684489148, 88.18422445738486, 88.1355214399153, 88.18634197988354, 88.08893594494441, 88.11011116993119, 88.09105346744309, 88.2265749073584, 88.20539968237162, 88.13763896241397, 88.11646373742721, 88.29221810481737, 88.20116463737428, 88.18845950238222, 88.14399152991001, 88.0359978824775, 88.15246161990471, 88.14399152991001, 88.07834833245103, 88.30068819481207, 88.21386977236634, 88.1715193223928, 88.34515616728427, 87.99788247750132, 87.91106405505559, 88.15034409740603, 88.15669666490207, 88.1630492323981, 88.13975648491265, 88.08470089994707, 88.3705664372684, 88.22869242985706, 88.1355214399153, 88.17787188988883, 88.14610905240868, 88.18210693488618, 88.27527792482795, 88.17363684489148, 88.15034409740603, 88.26469031233457, 88.14187400741133, 88.26045526733722, 88.08681842244575, 88.09952355743779, 88.16728427739545, 88.12916887241927]\n",
            "test_loss_list_exp = [0.7599668520338395, 0.41391250622623105, 0.392150557216476, 0.3908533724207504, 0.3910430484980929, 0.3915113131059151, 0.3893689420439449, 0.39278720669886646, 0.3913472319642703, 0.3898209324654411, 0.3919214537622882, 0.3930370972729197, 0.3875842901567618, 0.3887199363579937, 0.3931986349178295, 0.3915082841527228, 0.39034265929869577, 0.38962281656031517, 0.39410267273585003, 0.3923792628680958, 0.3913208600498882, 0.3940731910806076, 0.3901035269978, 0.3910912872091228, 0.38958175787154364, 0.3905664257266942, 0.3923723243323027, 0.3930160653795682, 0.3910279740013328, 0.39261947053612445, 0.39096589437594603, 0.391885179076709, 0.3932027472730945, 0.39465218753206965, 0.38952960351518556, 0.3913427036182553, 0.39291733357251857, 0.38893387598149914, 0.3907097995865579, 0.39178018352272465, 0.38997077613192443, 0.38897235888768644, 0.39309656094102297, 0.3918713939686616, 0.38930126476813764, 0.3919200319431576, 0.3909892667742336, 0.3936515018782195, 0.3921337866900014, 0.3897011807444049, 0.389769053050116, 0.3902200594106141, 0.38991651070468564, 0.3911624582228707, 0.3900841569491461, 0.38808364508783116, 0.39078373330480914, 0.3911517692693308, 0.3905084923494096, 0.392090796986047, 0.3915708720245782, 0.3868667685664168, 0.3898552243469977, 0.3904008638186782, 0.38936665559224054, 0.3908520120323873, 0.3909977381574173, 0.3894340494538055, 0.3913114227938886, 0.38867319550584345, 0.3922208594340904, 0.39250611046365663, 0.3933124074748918, 0.3923281282916957, 0.3923263843445217, 0.3901771641537255, 0.39134884019400556, 0.38913627334085166, 0.39259269424513277, 0.39072176375809836, 0.3921530346806143, 0.39100976697370116, 0.39079241250075547, 0.3899397780643959, 0.3929170014373228, 0.3889102427398457, 0.38894158966985404, 0.39256049473496046, 0.3897372295020842, 0.39233241295989824, 0.39279353041567056, 0.394793822411813, 0.3923672336865874, 0.38789207575952306, 0.391120845853698, 0.3914312515042576, 0.3890334714715387, 0.38947177671042144, 0.3914659348334752, 0.39052898429480254, 0.38922284959870224, 0.3926088816541083, 0.3930806327684253, 0.392626012584158, 0.3940140974580073, 0.3908073946687521, 0.3892480945762466, 0.3914746798428835, 0.3904998555925547, 0.39119182118013796, 0.39440940546931, 0.39126060596283746, 0.39380357363352586, 0.3891267492344566, 0.3960636570027061, 0.3884448780879086, 0.39201007885675804, 0.3887008518418845, 0.3946489935704306, 0.39012787235425966, 0.3926868131201641, 0.391174876587648, 0.39185114438627283, 0.3916316426121721, 0.39595334516728625, 0.3925237079315326, 0.3923376457510041, 0.3896145831574412, 0.3903454670719072, 0.38917045835770814, 0.3901947463552157, 0.38894958538459795, 0.39138510881685745, 0.3923698270729944, 0.391075956543871, 0.3886808121905607, 0.3895190705855687, 0.3937717227666986, 0.3892171628334943, 0.3928420579462659, 0.3929640487128613, 0.39179296280239145, 0.39022867213569434, 0.3915944700585861, 0.3908092556338684, 0.38998721832153843, 0.3919305083360158, 0.38930609106433156, 0.3915616778620318, 0.39198073642510994, 0.3898476078083702, 0.3942314390750492, 0.39122209778311207, 0.39321639089315547, 0.39556199356037025, 0.39069073004465477, 0.3901906652631713, 0.3918502590089452, 0.3921655998656563, 0.3897442833027419, 0.39050330440787706, 0.39026205139417275, 0.3908781868716081, 0.39245978443353785, 0.3903481831007144, 0.39445868105280635, 0.3892170712351799, 0.390042986282531, 0.3925706706941128, 0.39200084298556925, 0.3911351076528138, 0.39240403268851487, 0.3938527383348521, 0.3900003031480546, 0.3922225654709573, 0.3932921053001694, 0.39253465655971975, 0.3926194997540876, 0.3874349548097919, 0.39200115612908903, 0.3948955665908608, 0.3898700127998988, 0.38938687602971117, 0.3864065995257275, 0.39098570473930416, 0.3909385864641152, 0.3916171494067884, 0.39381099733359676, 0.3898175518740626, 0.3911994169740116, 0.39155133265782804, 0.39230194549058, 0.39114902234252763, 0.39017920955723406, 0.3944379236622184, 0.3916288679283039, 0.39200210498244153, 0.39363550146420795, 0.3911365672361617, 0.39186333141782703, 0.39117470471297994, 0.38984077394593, 0.39264219977399883, 0.3900822729018389, 0.38898184559508864, 0.3910406724202867, 0.3907987319809549, 0.39212526841198697, 0.39124271427007284, 0.3914998725202738, 0.38839635442869336, 0.3937654222781752, 0.3902626017875531, 0.3955241435883092, 0.39323169840317146, 0.39330630157800284, 0.38655056892072454, 0.39017754106544983, 0.3952399448436849, 0.39261257144458156, 0.3932858498073092, 0.39499423879326556, 0.3910665978111473, 0.3922448316041161, 0.39132243789294185, 0.3905644398547855, 0.39066209877822916, 0.39234474681171716, 0.3916584701806891, 0.39128884439374884, 0.38904376978091165, 0.39183107588221044, 0.3900741520611679, 0.3930635173969409, 0.38995324082526506, 0.39311547854951784, 0.3887742017121876, 0.3909773443113355, 0.38996322007448064, 0.38965144855718986, 0.39268765284442436, 0.390111534545819, 0.3925885803559247, 0.39064669915858435, 0.3895316495030534, 0.3907163414154567, 0.3907886814399093, 0.39067917981860684, 0.3914143468673323, 0.39072976911477014, 0.39038341521632436, 0.39487345085716713, 0.39242782728636966, 0.39340144921751585, 0.391638440317383, 0.38982952050134245, 0.39072682591629965, 0.39341319571523103, 0.39195014886996327, 0.3917578032203749, 0.38969211536003096, 0.3894471714601797, 0.3898716885961738, 0.39170538400318106, 0.39051353675769823, 0.39179608466870647, 0.3921622397998969, 0.39439454747765673, 0.39445579307628614, 0.39132673534400325, 0.3867330071972866, 0.3894542139388767, 0.39122700530524346, 0.3883669243431559, 0.38984382444737004, 0.3917064682817927, 0.3907441727670969, 0.39336200226463525, 0.3893578349083078, 0.3913763885696729, 0.3897980788320887, 0.3937839002293699, 0.39068301957027585, 0.3921215317997278, 0.39208485865417647, 0.3916279115513259, 0.3911278141918136, 0.39544760121726524, 0.38938475297946556, 0.39043475139667005, 0.39209672136634005, 0.39414026142627584, 0.3913420901871195, 0.39028101735839654, 0.3931575333516972, 0.39237413212072614, 0.38880399651094977, 0.3887612786801422, 0.3894933131979961, 0.3917112931901333]\n",
            "test_acc_list_exp = [75.58773816840811, 87.06976029502151, 87.53073140749846, 87.8918254456054, 87.76889981561156, 87.74969268592501, 87.98786109403811, 87.76121696373694, 87.87645974185618, 87.71896127842655, 87.74969268592501, 87.79578979717272, 87.96481253841426, 87.81115550092194, 87.81499692685925, 87.80731407498463, 87.9417639827904, 87.93023970497849, 87.76505838967425, 87.78042409342348, 87.66133988936693, 87.7458512599877, 87.7458512599877, 87.85725261216963, 87.66518131530424, 87.71127842655194, 87.65365703749232, 87.66133988936693, 87.74200983405039, 87.7458512599877, 87.85725261216963, 87.89566687154272, 87.64981561155501, 87.61140135218193, 88.02243392747388, 87.85725261216963, 87.50384142593731, 87.91871542716656, 87.82652120467118, 87.80731407498463, 87.82652120467118, 87.93792255685311, 87.6920712968654, 87.72280270436386, 87.82267977873387, 87.81499692685925, 87.87645974185618, 87.66133988936693, 87.8418869084204, 87.76889981561156, 87.88030116779349, 87.78042409342348, 87.7919483712354, 87.79963122311001, 87.81883835279656, 87.8918254456054, 87.92639827904118, 87.94944683466503, 87.78426551936079, 87.8918254456054, 87.7881069452981, 88.00322679778733, 87.87645974185618, 87.7881069452981, 87.68054701905348, 87.81499692685925, 87.71896127842655, 87.82267977873387, 87.7919483712354, 87.81883835279656, 87.83036263060848, 87.78042409342348, 87.78042409342348, 87.71511985248924, 87.72664413030117, 87.82267977873387, 87.80731407498463, 87.88030116779349, 87.83036263060848, 87.84956976029503, 87.91487400122925, 87.73048555623848, 87.81115550092194, 87.73048555623848, 87.7381684081131, 87.72664413030117, 87.8918254456054, 87.66518131530424, 87.8418869084204, 87.7919483712354, 87.69975414874001, 87.79578979717272, 87.77658266748617, 87.90334972341734, 87.75353411186232, 87.83036263060848, 87.86877688998156, 87.86877688998156, 87.7919483712354, 87.84572833435772, 87.91871542716656, 87.84956976029503, 87.83036263060848, 87.80347264904732, 87.70743700061463, 87.75353411186232, 87.9840196681008, 87.70359557467732, 87.84956976029503, 87.87645974185618, 87.7381684081131, 87.81115550092194, 87.6459741856177, 87.7458512599877, 87.51920712968654, 88.05700676090964, 87.79578979717272, 87.91871542716656, 87.56914566687155, 87.82267977873387, 87.73432698217579, 87.75737553779963, 87.81115550092194, 87.85341118623234, 87.74969268592501, 87.68054701905348, 87.63444990780577, 87.83420405654579, 87.91103257529196, 87.76505838967425, 87.80731407498463, 88.04548248309773, 87.81883835279656, 87.82652120467118, 87.6920712968654, 87.76121696373694, 87.74200983405039, 87.80347264904732, 87.96481253841426, 87.8380454824831, 87.6459741856177, 87.85341118623234, 87.92255685310387, 87.68438844499079, 87.7458512599877, 87.78426551936079, 87.81883835279656, 87.98017824216349, 87.88030116779349, 87.6421327596804, 87.9340811309158, 87.65365703749232, 87.68438844499079, 87.60371850030731, 87.66133988936693, 87.6459741856177, 87.78042409342348, 87.77658266748617, 87.58451137062077, 87.97633681622618, 87.83420405654579, 87.73048555623848, 87.84572833435772, 87.75737553779963, 87.76121696373694, 87.63829133374308, 87.91103257529196, 87.75353411186232, 87.57298709280884, 87.70743700061463, 87.73048555623848, 87.77658266748617, 87.76505838967425, 88.02243392747388, 87.84572833435772, 87.73048555623848, 87.88414259373079, 87.69975414874001, 87.88030116779349, 87.88030116779349, 87.68054701905348, 87.92639827904118, 87.95712968653964, 87.91103257529196, 87.68054701905348, 87.76121696373694, 87.78426551936079, 87.67286416717886, 87.67670559311617, 87.88414259373079, 87.79578979717272, 87.66133988936693, 87.83036263060848, 87.86493546404425, 87.78426551936079, 87.67286416717886, 87.81115550092194, 87.6421327596804, 87.70359557467732, 87.71127842655194, 87.84956976029503, 87.82267977873387, 87.7919483712354, 87.71127842655194, 87.83036263060848, 87.8879840196681, 87.76121696373694, 87.73048555623848, 87.76121696373694, 87.8380454824831, 87.75353411186232, 87.72280270436386, 87.84956976029503, 87.59987707437, 87.6959127228027, 87.62292562999386, 88.03779963122311, 87.77658266748617, 87.7381684081131, 87.80731407498463, 87.56530424093424, 87.63060848186846, 87.69975414874001, 87.71896127842655, 87.89566687154272, 87.83420405654579, 87.76121696373694, 87.63060848186846, 87.79578979717272, 87.65365703749232, 87.82267977873387, 87.85341118623234, 87.85341118623234, 87.80347264904732, 87.87645974185618, 87.84572833435772, 87.96097111247695, 87.84572833435772, 87.71511985248924, 87.85725261216963, 87.78426551936079, 87.78426551936079, 87.61908420405655, 87.86493546404425, 87.96097111247695, 87.86109403810694, 87.75737553779963, 87.84956976029503, 87.7881069452981, 87.84572833435772, 87.70359557467732, 87.71127842655194, 87.65749846342962, 87.62292562999386, 87.71896127842655, 87.96865396435157, 87.97633681622618, 87.76121696373694, 87.75353411186232, 87.81115550092194, 87.8879840196681, 87.82652120467118, 87.87645974185618, 87.68438844499079, 87.8918254456054, 87.63060848186846, 87.76505838967425, 87.5960356484327, 87.66133988936693, 87.9840196681008, 87.88030116779349, 87.86877688998156, 87.77274124154886, 87.9417639827904, 87.90719114935465, 87.7881069452981, 87.69975414874001, 87.77658266748617, 87.84572833435772, 87.67286416717886, 87.80731407498463, 87.74200983405039, 87.71896127842655, 87.6459741856177, 87.68438844499079, 87.86877688998156, 87.82267977873387, 87.78426551936079, 87.72664413030117, 87.98017824216349, 87.66518131530424, 87.6959127228027, 87.75353411186232, 87.91487400122925, 87.89566687154272, 87.72280270436386, 87.91487400122925, 87.88030116779349, 87.91103257529196, 87.75353411186232]\n"
          ]
        }
      ]
    }
  ]
}