{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4964385645a24b1faff977c5b9d5adac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d881792c1564227ac6d2ea305bd5b13",
              "IPY_MODEL_8cb7468137364219917ebaaf5ef85e3b",
              "IPY_MODEL_d1b1ac2f95e94046a308c5fd89baf137"
            ],
            "layout": "IPY_MODEL_1db72ad0f4734e2eb4b31c6f72148c1d"
          }
        },
        "2d881792c1564227ac6d2ea305bd5b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8825f9630bf34fe5b01ba7f4ef9c300b",
            "placeholder": "​",
            "style": "IPY_MODEL_176902798b4d485db4e5ddd403ab455d",
            "value": "100%"
          }
        },
        "8cb7468137364219917ebaaf5ef85e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28aeec1fe3b547bd958f522067cd7541",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9361b5f4c3d844fbb00603f4243ceb63",
            "value": 182040794
          }
        },
        "d1b1ac2f95e94046a308c5fd89baf137": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa0724452e964f99a86ba29fcb574b1c",
            "placeholder": "​",
            "style": "IPY_MODEL_b922a3d4528749d28d024d97529f616d",
            "value": " 182040794/182040794 [00:12&lt;00:00, 18968202.92it/s]"
          }
        },
        "1db72ad0f4734e2eb4b31c6f72148c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8825f9630bf34fe5b01ba7f4ef9c300b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "176902798b4d485db4e5ddd403ab455d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28aeec1fe3b547bd958f522067cd7541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9361b5f4c3d844fbb00603f4243ceb63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa0724452e964f99a86ba29fcb574b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b922a3d4528749d28d024d97529f616d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43c89db4bd18434e98c69149b8aecc4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50e118520b884313a5dc9666ae3ba9b3",
              "IPY_MODEL_eef5cb5938b44439b894f0fad2441c1c",
              "IPY_MODEL_bc0d013e77fd4d4994f54d4c1968e268"
            ],
            "layout": "IPY_MODEL_4e6bc8b3b6814c759486734bc42789b3"
          }
        },
        "50e118520b884313a5dc9666ae3ba9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ab950774fd94851b09a1664145f9876",
            "placeholder": "​",
            "style": "IPY_MODEL_8d237a8ae1004b18adefaf7d25985be5",
            "value": "100%"
          }
        },
        "eef5cb5938b44439b894f0fad2441c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8b03617e35346f59e7c3a01a1856e4e",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_766c5aec612044e9bde15b85023bfaf9",
            "value": 64275384
          }
        },
        "bc0d013e77fd4d4994f54d4c1968e268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bbae6f096414ca5aece57e0929792a6",
            "placeholder": "​",
            "style": "IPY_MODEL_c0413fdca50048bca532a9ebbe5cd1bf",
            "value": " 64275384/64275384 [00:07&lt;00:00, 18666255.78it/s]"
          }
        },
        "4e6bc8b3b6814c759486734bc42789b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ab950774fd94851b09a1664145f9876": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d237a8ae1004b18adefaf7d25985be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8b03617e35346f59e7c3a01a1856e4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "766c5aec612044e9bde15b85023bfaf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3bbae6f096414ca5aece57e0929792a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0413fdca50048bca532a9ebbe5cd1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Source code for unpickle function: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "# def unpickle(file):\n",
        "#     import pickle\n",
        "#     with open(file, 'rb') as fo:\n",
        "#         dict = pickle.load(fo, encoding='bytes')\n",
        "#     return dict\n",
        "\n",
        "# import numpy as np\n",
        "# def get_mean_color():\n",
        "#     d=unpickle('./data/cifar-10-batches-py/data_batch_1')\n",
        "#     channels = d[b'data']\n",
        "#     for i in range(2,6):\n",
        "#         d=unpickle('./data/cifar-10-batches-py/data_batch_'+str(i))\n",
        "#         channels=np.concatenate((channels, d[b'data']), axis=0)\n",
        "#     r=np.mean(channels[:,:1024])/255  \n",
        "#     g=np.mean(channels[:,1024:2048])/255\n",
        "#     b=np.mean(channels[:,2048:])/255\n",
        "#     return(r,g,b)\n",
        "# get_mean_color()"
      ],
      "metadata": {
        "id": "8ntKy6oKQGJv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "4964385645a24b1faff977c5b9d5adac",
            "2d881792c1564227ac6d2ea305bd5b13",
            "8cb7468137364219917ebaaf5ef85e3b",
            "d1b1ac2f95e94046a308c5fd89baf137",
            "1db72ad0f4734e2eb4b31c6f72148c1d",
            "8825f9630bf34fe5b01ba7f4ef9c300b",
            "176902798b4d485db4e5ddd403ab455d",
            "28aeec1fe3b547bd958f522067cd7541",
            "9361b5f4c3d844fbb00603f4243ceb63",
            "aa0724452e964f99a86ba29fcb574b1c",
            "b922a3d4528749d28d024d97529f616d",
            "43c89db4bd18434e98c69149b8aecc4e",
            "50e118520b884313a5dc9666ae3ba9b3",
            "eef5cb5938b44439b894f0fad2441c1c",
            "bc0d013e77fd4d4994f54d4c1968e268",
            "4e6bc8b3b6814c759486734bc42789b3",
            "4ab950774fd94851b09a1664145f9876",
            "8d237a8ae1004b18adefaf7d25985be5",
            "d8b03617e35346f59e7c3a01a1856e4e",
            "766c5aec612044e9bde15b85023bfaf9",
            "3bbae6f096414ca5aece57e0929792a6",
            "c0413fdca50048bca532a9ebbe5cd1bf"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "c399e72f-3088-4427-81a7-dfa22b0c029a"
      },
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    #transforms.RandomErasing(value=get_mean_color()),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4964385645a24b1faff977c5b9d5adac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43c89db4bd18434e98c69149b8aecc4e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "69527763-6469-404a-96cf-6ff9bec1d2ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te71lQ17B1L_"
      },
      "source": [
        "#Data Augmentation\n",
        "Data augmentation performs random modifications of the image as a preprocessing step. It serves the following purposes:\n",
        "1. It increases the amount of data for training.\n",
        "2. By deleting features, it prevents the network from relying on a narrow set of features, which may not generalize.\n",
        "3. By changing features while maintaining the same output, it helps the network become tolerant of changes that do not change the image lab. \n",
        "\n",
        "In short, data augmentation desensitivizes the network, so it extracts features that are invariant to changes that should not affect the prediction. \n",
        "\n",
        "We showcase a few random data augmentation provided by PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyG26xoJC0Pa"
      },
      "source": [
        "# import torch.nn as nn\n",
        "# transforms = torch.nn.Sequential(\n",
        "#     T.Resize(256), # resize the short edge to 256.\n",
        "#     T.RandomCrop(224), #randomly crop a 224x224 region from the image\n",
        "#     T.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
        "#     #T.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0.1)\n",
        "#     #T.AutoAugment()\n",
        "# )\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# dog1 = dog1.to(device)\n",
        "# # dog2 = dog2.to(device)\n",
        "\n",
        "# # transformed_dog1 = transforms(dog1)\n",
        "# transformed_dog1 = transforms(dog1)\n",
        "# show([transformed_dog1])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # This is the \"stem\"\n",
        "        # For CIFAR (32x32 images), it does not perform downsampling\n",
        "        # It should downsample for ImageNet\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "03ced685-fe5a-4ce2-d2f5-7505086fe4dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "08a28cf5-7b8e-445b-d4a6-b9bdac604ddc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArgupDVRwB8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750c31a0-d085-4458-ffa7-a41ef7ea8a26"
      },
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3450, accuracy : 16.69\n",
            "iteration : 100, loss : 2.2695, accuracy : 19.49\n",
            "iteration : 150, loss : 2.1872, accuracy : 22.55\n",
            "iteration : 200, loss : 2.0258, accuracy : 28.01\n",
            "iteration : 250, loss : 1.8504, accuracy : 34.45\n",
            "iteration : 300, loss : 1.6890, accuracy : 40.47\n",
            "iteration : 350, loss : 1.5511, accuracy : 45.71\n",
            "Epoch :   1, training loss : 1.5044, training accuracy : 47.39, test loss : 0.7522, test accuracy : 75.81\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.5928, accuracy : 80.27\n",
            "iteration : 100, loss : 0.5669, accuracy : 81.55\n",
            "iteration : 150, loss : 0.5442, accuracy : 82.55\n",
            "iteration : 200, loss : 0.5317, accuracy : 83.05\n",
            "iteration : 250, loss : 0.5165, accuracy : 83.46\n",
            "iteration : 300, loss : 0.5028, accuracy : 84.00\n",
            "iteration : 350, loss : 0.4965, accuracy : 84.21\n",
            "Epoch :   2, training loss : 0.4921, training accuracy : 84.39, test loss : 0.4192, test accuracy : 86.97\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.3989, accuracy : 87.34\n",
            "iteration : 100, loss : 0.4083, accuracy : 87.34\n",
            "iteration : 150, loss : 0.3962, accuracy : 87.71\n",
            "iteration : 200, loss : 0.3895, accuracy : 87.89\n",
            "iteration : 250, loss : 0.3847, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3831, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3794, accuracy : 88.28\n",
            "Epoch :   3, training loss : 0.3783, training accuracy : 88.33, test loss : 0.3968, test accuracy : 87.58\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3446, accuracy : 89.33\n",
            "iteration : 100, loss : 0.3432, accuracy : 89.54\n",
            "iteration : 150, loss : 0.3376, accuracy : 89.58\n",
            "iteration : 200, loss : 0.3417, accuracy : 89.41\n",
            "iteration : 250, loss : 0.3389, accuracy : 89.53\n",
            "iteration : 300, loss : 0.3369, accuracy : 89.55\n",
            "iteration : 350, loss : 0.3342, accuracy : 89.65\n",
            "Epoch :   4, training loss : 0.3336, training accuracy : 89.66, test loss : 0.3568, test accuracy : 89.28\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.2865, accuracy : 91.12\n",
            "iteration : 100, loss : 0.2914, accuracy : 91.11\n",
            "iteration : 150, loss : 0.2981, accuracy : 90.82\n",
            "iteration : 200, loss : 0.3041, accuracy : 90.71\n",
            "iteration : 250, loss : 0.3051, accuracy : 90.74\n",
            "iteration : 300, loss : 0.3018, accuracy : 90.83\n",
            "iteration : 350, loss : 0.3010, accuracy : 90.89\n",
            "Epoch :   5, training loss : 0.3005, training accuracy : 90.94, test loss : 0.3251, test accuracy : 90.12\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.2799, accuracy : 92.05\n",
            "iteration : 100, loss : 0.2811, accuracy : 91.74\n",
            "iteration : 150, loss : 0.2823, accuracy : 91.59\n",
            "iteration : 200, loss : 0.2800, accuracy : 91.58\n",
            "iteration : 250, loss : 0.2800, accuracy : 91.55\n",
            "iteration : 300, loss : 0.2775, accuracy : 91.62\n",
            "iteration : 350, loss : 0.2746, accuracy : 91.73\n",
            "Epoch :   6, training loss : 0.2731, training accuracy : 91.76, test loss : 0.3064, test accuracy : 91.05\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.2495, accuracy : 92.53\n",
            "iteration : 100, loss : 0.2633, accuracy : 91.99\n",
            "iteration : 150, loss : 0.2638, accuracy : 92.07\n",
            "iteration : 200, loss : 0.2628, accuracy : 92.17\n",
            "iteration : 250, loss : 0.2600, accuracy : 92.26\n",
            "iteration : 300, loss : 0.2600, accuracy : 92.22\n",
            "iteration : 350, loss : 0.2597, accuracy : 92.29\n",
            "Epoch :   7, training loss : 0.2593, training accuracy : 92.30, test loss : 0.2780, test accuracy : 91.73\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.2404, accuracy : 93.11\n",
            "iteration : 100, loss : 0.2487, accuracy : 92.79\n",
            "iteration : 150, loss : 0.2486, accuracy : 92.82\n",
            "iteration : 200, loss : 0.2468, accuracy : 92.81\n",
            "iteration : 250, loss : 0.2482, accuracy : 92.80\n",
            "iteration : 300, loss : 0.2526, accuracy : 92.66\n",
            "iteration : 350, loss : 0.2491, accuracy : 92.68\n",
            "Epoch :   8, training loss : 0.2475, training accuracy : 92.70, test loss : 0.2646, test accuracy : 92.37\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.2335, accuracy : 93.30\n",
            "iteration : 100, loss : 0.2277, accuracy : 93.45\n",
            "iteration : 150, loss : 0.2329, accuracy : 93.16\n",
            "iteration : 200, loss : 0.2362, accuracy : 93.09\n",
            "iteration : 250, loss : 0.2344, accuracy : 93.11\n",
            "iteration : 300, loss : 0.2352, accuracy : 93.12\n",
            "iteration : 350, loss : 0.2342, accuracy : 93.10\n",
            "Epoch :   9, training loss : 0.2334, training accuracy : 93.14, test loss : 0.2640, test accuracy : 92.39\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2226, accuracy : 93.48\n",
            "iteration : 100, loss : 0.2208, accuracy : 93.37\n",
            "iteration : 150, loss : 0.2205, accuracy : 93.43\n",
            "iteration : 200, loss : 0.2222, accuracy : 93.33\n",
            "iteration : 250, loss : 0.2228, accuracy : 93.27\n",
            "iteration : 300, loss : 0.2225, accuracy : 93.27\n",
            "iteration : 350, loss : 0.2259, accuracy : 93.26\n",
            "Epoch :  10, training loss : 0.2255, training accuracy : 93.28, test loss : 0.2669, test accuracy : 92.24\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.1981, accuracy : 94.23\n",
            "iteration : 100, loss : 0.2104, accuracy : 93.98\n",
            "iteration : 150, loss : 0.2091, accuracy : 93.92\n",
            "iteration : 200, loss : 0.2097, accuracy : 93.92\n",
            "iteration : 250, loss : 0.2141, accuracy : 93.82\n",
            "iteration : 300, loss : 0.2132, accuracy : 93.82\n",
            "iteration : 350, loss : 0.2127, accuracy : 93.84\n",
            "Epoch :  11, training loss : 0.2125, training accuracy : 93.85, test loss : 0.2610, test accuracy : 92.37\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.1958, accuracy : 94.09\n",
            "iteration : 100, loss : 0.2004, accuracy : 93.98\n",
            "iteration : 150, loss : 0.2103, accuracy : 93.72\n",
            "iteration : 200, loss : 0.2090, accuracy : 93.89\n",
            "iteration : 250, loss : 0.2064, accuracy : 93.95\n",
            "iteration : 300, loss : 0.2051, accuracy : 93.98\n",
            "iteration : 350, loss : 0.2031, accuracy : 94.03\n",
            "Epoch :  12, training loss : 0.2046, training accuracy : 93.98, test loss : 0.2513, test accuracy : 92.98\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.1977, accuracy : 94.33\n",
            "iteration : 100, loss : 0.2034, accuracy : 94.10\n",
            "iteration : 150, loss : 0.2036, accuracy : 94.17\n",
            "iteration : 200, loss : 0.2012, accuracy : 94.22\n",
            "iteration : 250, loss : 0.1981, accuracy : 94.33\n",
            "iteration : 300, loss : 0.1970, accuracy : 94.35\n",
            "iteration : 350, loss : 0.1950, accuracy : 94.38\n",
            "Epoch :  13, training loss : 0.1935, training accuracy : 94.42, test loss : 0.2485, test accuracy : 93.17\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.1900, accuracy : 94.45\n",
            "iteration : 100, loss : 0.1880, accuracy : 94.65\n",
            "iteration : 150, loss : 0.1860, accuracy : 94.70\n",
            "iteration : 200, loss : 0.1861, accuracy : 94.66\n",
            "iteration : 250, loss : 0.1871, accuracy : 94.62\n",
            "iteration : 300, loss : 0.1882, accuracy : 94.57\n",
            "iteration : 350, loss : 0.1883, accuracy : 94.57\n",
            "Epoch :  14, training loss : 0.1889, training accuracy : 94.57, test loss : 0.2454, test accuracy : 93.05\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.1651, accuracy : 95.25\n",
            "iteration : 100, loss : 0.1688, accuracy : 95.09\n",
            "iteration : 150, loss : 0.1684, accuracy : 95.13\n",
            "iteration : 200, loss : 0.1760, accuracy : 94.89\n",
            "iteration : 250, loss : 0.1762, accuracy : 94.83\n",
            "iteration : 300, loss : 0.1780, accuracy : 94.83\n",
            "iteration : 350, loss : 0.1785, accuracy : 94.86\n",
            "Epoch :  15, training loss : 0.1790, training accuracy : 94.83, test loss : 0.2352, test accuracy : 93.42\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.1742, accuracy : 95.05\n",
            "iteration : 100, loss : 0.1745, accuracy : 95.08\n",
            "iteration : 150, loss : 0.1727, accuracy : 95.06\n",
            "iteration : 200, loss : 0.1712, accuracy : 95.08\n",
            "iteration : 250, loss : 0.1717, accuracy : 95.09\n",
            "iteration : 300, loss : 0.1716, accuracy : 95.10\n",
            "iteration : 350, loss : 0.1731, accuracy : 95.07\n",
            "Epoch :  16, training loss : 0.1731, training accuracy : 95.06, test loss : 0.2371, test accuracy : 93.38\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.1758, accuracy : 94.78\n",
            "iteration : 100, loss : 0.1689, accuracy : 95.11\n",
            "iteration : 150, loss : 0.1638, accuracy : 95.20\n",
            "iteration : 200, loss : 0.1644, accuracy : 95.19\n",
            "iteration : 250, loss : 0.1640, accuracy : 95.19\n",
            "iteration : 300, loss : 0.1647, accuracy : 95.16\n",
            "iteration : 350, loss : 0.1655, accuracy : 95.15\n",
            "Epoch :  17, training loss : 0.1656, training accuracy : 95.14, test loss : 0.2406, test accuracy : 93.32\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.1523, accuracy : 95.89\n",
            "iteration : 100, loss : 0.1612, accuracy : 95.63\n",
            "iteration : 150, loss : 0.1560, accuracy : 95.66\n",
            "iteration : 200, loss : 0.1548, accuracy : 95.63\n",
            "iteration : 250, loss : 0.1580, accuracy : 95.47\n",
            "iteration : 300, loss : 0.1587, accuracy : 95.42\n",
            "iteration : 350, loss : 0.1587, accuracy : 95.41\n",
            "Epoch :  18, training loss : 0.1594, training accuracy : 95.38, test loss : 0.2463, test accuracy : 93.03\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.1493, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1458, accuracy : 96.01\n",
            "iteration : 150, loss : 0.1473, accuracy : 95.94\n",
            "iteration : 200, loss : 0.1472, accuracy : 95.88\n",
            "iteration : 250, loss : 0.1479, accuracy : 95.84\n",
            "iteration : 300, loss : 0.1505, accuracy : 95.73\n",
            "iteration : 350, loss : 0.1539, accuracy : 95.63\n",
            "Epoch :  19, training loss : 0.1541, training accuracy : 95.61, test loss : 0.2287, test accuracy : 93.71\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.1258, accuracy : 96.44\n",
            "iteration : 100, loss : 0.1357, accuracy : 96.13\n",
            "iteration : 150, loss : 0.1369, accuracy : 96.08\n",
            "iteration : 200, loss : 0.1358, accuracy : 96.09\n",
            "iteration : 250, loss : 0.1382, accuracy : 95.98\n",
            "iteration : 300, loss : 0.1411, accuracy : 95.88\n",
            "iteration : 350, loss : 0.1434, accuracy : 95.88\n",
            "Epoch :  20, training loss : 0.1441, training accuracy : 95.87, test loss : 0.2308, test accuracy : 93.73\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.1387, accuracy : 96.17\n",
            "iteration : 100, loss : 0.1377, accuracy : 96.09\n",
            "iteration : 150, loss : 0.1353, accuracy : 96.23\n",
            "iteration : 200, loss : 0.1371, accuracy : 96.07\n",
            "iteration : 250, loss : 0.1362, accuracy : 96.08\n",
            "iteration : 300, loss : 0.1386, accuracy : 96.01\n",
            "iteration : 350, loss : 0.1385, accuracy : 95.96\n",
            "Epoch :  21, training loss : 0.1398, training accuracy : 95.94, test loss : 0.2321, test accuracy : 93.69\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.1249, accuracy : 96.19\n",
            "iteration : 100, loss : 0.1293, accuracy : 96.19\n",
            "iteration : 150, loss : 0.1315, accuracy : 96.14\n",
            "iteration : 200, loss : 0.1328, accuracy : 96.07\n",
            "iteration : 250, loss : 0.1312, accuracy : 96.15\n",
            "iteration : 300, loss : 0.1325, accuracy : 96.16\n",
            "iteration : 350, loss : 0.1319, accuracy : 96.24\n",
            "Epoch :  22, training loss : 0.1325, training accuracy : 96.24, test loss : 0.2338, test accuracy : 93.78\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.1333, accuracy : 96.16\n",
            "iteration : 100, loss : 0.1238, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1231, accuracy : 96.46\n",
            "iteration : 200, loss : 0.1237, accuracy : 96.43\n",
            "iteration : 250, loss : 0.1237, accuracy : 96.42\n",
            "iteration : 300, loss : 0.1265, accuracy : 96.35\n",
            "iteration : 350, loss : 0.1267, accuracy : 96.34\n",
            "Epoch :  23, training loss : 0.1264, training accuracy : 96.33, test loss : 0.2326, test accuracy : 93.74\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.1231, accuracy : 96.86\n",
            "iteration : 100, loss : 0.1211, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1171, accuracy : 96.68\n",
            "iteration : 200, loss : 0.1165, accuracy : 96.66\n",
            "iteration : 250, loss : 0.1177, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1158, accuracy : 96.66\n",
            "iteration : 350, loss : 0.1184, accuracy : 96.58\n",
            "Epoch :  24, training loss : 0.1213, training accuracy : 96.52, test loss : 0.2492, test accuracy : 93.26\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.0996, accuracy : 96.95\n",
            "iteration : 100, loss : 0.1057, accuracy : 96.87\n",
            "iteration : 150, loss : 0.1097, accuracy : 96.75\n",
            "iteration : 200, loss : 0.1135, accuracy : 96.65\n",
            "iteration : 250, loss : 0.1128, accuracy : 96.65\n",
            "iteration : 300, loss : 0.1167, accuracy : 96.57\n",
            "iteration : 350, loss : 0.1170, accuracy : 96.61\n",
            "Epoch :  25, training loss : 0.1179, training accuracy : 96.58, test loss : 0.2467, test accuracy : 93.30\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.1028, accuracy : 97.14\n",
            "iteration : 100, loss : 0.1014, accuracy : 97.01\n",
            "iteration : 150, loss : 0.1055, accuracy : 96.86\n",
            "iteration : 200, loss : 0.1068, accuracy : 96.89\n",
            "iteration : 250, loss : 0.1080, accuracy : 96.86\n",
            "iteration : 300, loss : 0.1098, accuracy : 96.85\n",
            "iteration : 350, loss : 0.1123, accuracy : 96.79\n",
            "Epoch :  26, training loss : 0.1115, training accuracy : 96.80, test loss : 0.2332, test accuracy : 93.93\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.0842, accuracy : 97.56\n",
            "iteration : 100, loss : 0.0989, accuracy : 97.06\n",
            "iteration : 150, loss : 0.1015, accuracy : 97.05\n",
            "iteration : 200, loss : 0.1054, accuracy : 96.96\n",
            "iteration : 250, loss : 0.1056, accuracy : 96.98\n",
            "iteration : 300, loss : 0.1046, accuracy : 97.04\n",
            "iteration : 350, loss : 0.1046, accuracy : 96.99\n",
            "Epoch :  27, training loss : 0.1049, training accuracy : 96.98, test loss : 0.2333, test accuracy : 94.07\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.0942, accuracy : 97.11\n",
            "iteration : 100, loss : 0.0935, accuracy : 97.27\n",
            "iteration : 150, loss : 0.0975, accuracy : 97.27\n",
            "iteration : 200, loss : 0.0970, accuracy : 97.20\n",
            "iteration : 250, loss : 0.0996, accuracy : 97.17\n",
            "iteration : 300, loss : 0.1006, accuracy : 97.15\n",
            "iteration : 350, loss : 0.1012, accuracy : 97.12\n",
            "Epoch :  28, training loss : 0.1011, training accuracy : 97.12, test loss : 0.2326, test accuracy : 94.12\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.0853, accuracy : 97.73\n",
            "iteration : 100, loss : 0.0902, accuracy : 97.48\n",
            "iteration : 150, loss : 0.0979, accuracy : 97.27\n",
            "iteration : 200, loss : 0.0998, accuracy : 97.22\n",
            "iteration : 250, loss : 0.0997, accuracy : 97.19\n",
            "iteration : 300, loss : 0.0997, accuracy : 97.17\n",
            "iteration : 350, loss : 0.0991, accuracy : 97.17\n",
            "Epoch :  29, training loss : 0.0986, training accuracy : 97.20, test loss : 0.2575, test accuracy : 93.60\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.0840, accuracy : 97.42\n",
            "iteration : 100, loss : 0.0861, accuracy : 97.48\n",
            "iteration : 150, loss : 0.0847, accuracy : 97.51\n",
            "iteration : 200, loss : 0.0882, accuracy : 97.42\n",
            "iteration : 250, loss : 0.0904, accuracy : 97.32\n",
            "iteration : 300, loss : 0.0917, accuracy : 97.28\n",
            "iteration : 350, loss : 0.0942, accuracy : 97.21\n",
            "Epoch :  30, training loss : 0.0933, training accuracy : 97.23, test loss : 0.2394, test accuracy : 94.04\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.0767, accuracy : 97.81\n",
            "iteration : 100, loss : 0.0716, accuracy : 98.03\n",
            "iteration : 150, loss : 0.0690, accuracy : 98.08\n",
            "iteration : 200, loss : 0.0639, accuracy : 98.23\n",
            "iteration : 250, loss : 0.0639, accuracy : 98.24\n",
            "iteration : 300, loss : 0.0632, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0621, accuracy : 98.33\n",
            "Epoch :  31, training loss : 0.0616, training accuracy : 98.33, test loss : 0.2186, test accuracy : 94.88\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.0562, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0527, accuracy : 98.61\n",
            "iteration : 150, loss : 0.0493, accuracy : 98.71\n",
            "iteration : 200, loss : 0.0481, accuracy : 98.74\n",
            "iteration : 250, loss : 0.0509, accuracy : 98.69\n",
            "iteration : 300, loss : 0.0493, accuracy : 98.71\n",
            "iteration : 350, loss : 0.0501, accuracy : 98.71\n",
            "Epoch :  32, training loss : 0.0501, training accuracy : 98.71, test loss : 0.2249, test accuracy : 94.84\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.0467, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0429, accuracy : 98.95\n",
            "iteration : 150, loss : 0.0418, accuracy : 98.92\n",
            "iteration : 200, loss : 0.0439, accuracy : 98.89\n",
            "iteration : 250, loss : 0.0452, accuracy : 98.83\n",
            "iteration : 300, loss : 0.0456, accuracy : 98.82\n",
            "iteration : 350, loss : 0.0455, accuracy : 98.82\n",
            "Epoch :  33, training loss : 0.0457, training accuracy : 98.82, test loss : 0.2260, test accuracy : 94.81\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.0430, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0437, accuracy : 98.83\n",
            "iteration : 150, loss : 0.0432, accuracy : 98.89\n",
            "iteration : 200, loss : 0.0432, accuracy : 98.92\n",
            "iteration : 250, loss : 0.0413, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0418, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0421, accuracy : 98.94\n",
            "Epoch :  34, training loss : 0.0421, training accuracy : 98.95, test loss : 0.2331, test accuracy : 94.87\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.0370, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0370, accuracy : 98.98\n",
            "iteration : 150, loss : 0.0367, accuracy : 99.06\n",
            "iteration : 200, loss : 0.0364, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0372, accuracy : 99.04\n",
            "iteration : 300, loss : 0.0371, accuracy : 99.05\n",
            "iteration : 350, loss : 0.0379, accuracy : 99.03\n",
            "Epoch :  35, training loss : 0.0376, training accuracy : 99.04, test loss : 0.2381, test accuracy : 94.92\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.0359, accuracy : 98.98\n",
            "iteration : 100, loss : 0.0329, accuracy : 99.10\n",
            "iteration : 150, loss : 0.0346, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0344, accuracy : 99.11\n",
            "iteration : 250, loss : 0.0350, accuracy : 99.09\n",
            "iteration : 300, loss : 0.0355, accuracy : 99.07\n",
            "iteration : 350, loss : 0.0357, accuracy : 99.05\n",
            "Epoch :  36, training loss : 0.0353, training accuracy : 99.06, test loss : 0.2410, test accuracy : 94.89\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.0412, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0417, accuracy : 98.98\n",
            "iteration : 150, loss : 0.0394, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0368, accuracy : 99.10\n",
            "iteration : 250, loss : 0.0353, accuracy : 99.11\n",
            "iteration : 300, loss : 0.0341, accuracy : 99.11\n",
            "iteration : 350, loss : 0.0336, accuracy : 99.14\n",
            "Epoch :  37, training loss : 0.0340, training accuracy : 99.13, test loss : 0.2527, test accuracy : 94.87\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.0296, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0287, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0284, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0306, accuracy : 99.22\n",
            "iteration : 250, loss : 0.0297, accuracy : 99.24\n",
            "iteration : 300, loss : 0.0306, accuracy : 99.22\n",
            "iteration : 350, loss : 0.0314, accuracy : 99.21\n",
            "Epoch :  38, training loss : 0.0312, training accuracy : 99.22, test loss : 0.2539, test accuracy : 94.84\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.0309, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0294, accuracy : 99.22\n",
            "iteration : 150, loss : 0.0305, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0323, accuracy : 99.15\n",
            "iteration : 250, loss : 0.0323, accuracy : 99.15\n",
            "iteration : 300, loss : 0.0325, accuracy : 99.15\n",
            "iteration : 350, loss : 0.0312, accuracy : 99.17\n",
            "Epoch :  39, training loss : 0.0309, training accuracy : 99.17, test loss : 0.2590, test accuracy : 94.71\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.0318, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0288, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0265, accuracy : 99.28\n",
            "iteration : 200, loss : 0.0280, accuracy : 99.26\n",
            "iteration : 250, loss : 0.0291, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0287, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0292, accuracy : 99.25\n",
            "Epoch :  40, training loss : 0.0291, training accuracy : 99.25, test loss : 0.2654, test accuracy : 94.56\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.0263, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0286, accuracy : 99.32\n",
            "iteration : 150, loss : 0.0265, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0268, accuracy : 99.31\n",
            "iteration : 250, loss : 0.0268, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0274, accuracy : 99.31\n",
            "iteration : 350, loss : 0.0270, accuracy : 99.33\n",
            "Epoch :  41, training loss : 0.0269, training accuracy : 99.32, test loss : 0.2675, test accuracy : 94.76\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.0239, accuracy : 99.28\n",
            "iteration : 100, loss : 0.0259, accuracy : 99.24\n",
            "iteration : 150, loss : 0.0249, accuracy : 99.28\n",
            "iteration : 200, loss : 0.0254, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0248, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0252, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0246, accuracy : 99.35\n",
            "Epoch :  42, training loss : 0.0248, training accuracy : 99.34, test loss : 0.2758, test accuracy : 94.74\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.0228, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0241, accuracy : 99.40\n",
            "iteration : 150, loss : 0.0239, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0220, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0219, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0217, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0228, accuracy : 99.40\n",
            "Epoch :  43, training loss : 0.0233, training accuracy : 99.38, test loss : 0.2835, test accuracy : 94.65\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.0177, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0201, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0196, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0210, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0220, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0223, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0223, accuracy : 99.40\n",
            "Epoch :  44, training loss : 0.0220, training accuracy : 99.40, test loss : 0.2891, test accuracy : 94.65\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.0210, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0232, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0219, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0206, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0202, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0201, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0210, accuracy : 99.45\n",
            "Epoch :  45, training loss : 0.0211, training accuracy : 99.45, test loss : 0.2938, test accuracy : 94.69\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.0247, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0201, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0197, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0199, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0199, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0201, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0202, accuracy : 99.46\n",
            "Epoch :  46, training loss : 0.0202, training accuracy : 99.46, test loss : 0.3016, test accuracy : 94.73\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.0194, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0190, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0193, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0201, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0207, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0204, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0205, accuracy : 99.44\n",
            "Epoch :  47, training loss : 0.0204, training accuracy : 99.44, test loss : 0.2968, test accuracy : 94.68\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.0151, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0159, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0175, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0175, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0174, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0178, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0180, accuracy : 99.50\n",
            "Epoch :  48, training loss : 0.0181, training accuracy : 99.50, test loss : 0.3060, test accuracy : 94.74\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.0142, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0149, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0161, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0166, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0170, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0173, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0170, accuracy : 99.52\n",
            "Epoch :  49, training loss : 0.0169, training accuracy : 99.52, test loss : 0.3124, test accuracy : 94.62\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.0170, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0183, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0169, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0174, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0174, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0168, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0165, accuracy : 99.55\n",
            "Epoch :  50, training loss : 0.0163, training accuracy : 99.55, test loss : 0.3139, test accuracy : 94.65\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.0180, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0168, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0159, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0162, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0166, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0160, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0156, accuracy : 99.58\n",
            "Epoch :  51, training loss : 0.0157, training accuracy : 99.58, test loss : 0.3220, test accuracy : 94.55\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.0178, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0170, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0167, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0162, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0160, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0163, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0158, accuracy : 99.57\n",
            "Epoch :  52, training loss : 0.0158, training accuracy : 99.56, test loss : 0.3318, test accuracy : 94.54\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0129, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0128, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0136, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0134, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0144, accuracy : 99.57\n",
            "Epoch :  53, training loss : 0.0150, training accuracy : 99.56, test loss : 0.3355, test accuracy : 94.63\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.0136, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0157, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0147, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0141, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0142, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0141, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0140, accuracy : 99.62\n",
            "Epoch :  54, training loss : 0.0139, training accuracy : 99.62, test loss : 0.3384, test accuracy : 94.62\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.0148, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0145, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0135, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0130, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0125, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0128, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0125, accuracy : 99.66\n",
            "Epoch :  55, training loss : 0.0125, training accuracy : 99.66, test loss : 0.3436, test accuracy : 94.61\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.0133, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0120, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0103, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0097, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0110, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0111, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0116, accuracy : 99.68\n",
            "Epoch :  56, training loss : 0.0115, training accuracy : 99.68, test loss : 0.3455, test accuracy : 94.52\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0111, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0120, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0121, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0121, accuracy : 99.62\n",
            "Epoch :  57, training loss : 0.0121, training accuracy : 99.62, test loss : 0.3456, test accuracy : 94.64\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0118, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0113, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0111, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0115, accuracy : 99.64\n",
            "Epoch :  58, training loss : 0.0117, training accuracy : 99.63, test loss : 0.3532, test accuracy : 94.56\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.0098, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0105, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0108, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0103, accuracy : 99.71\n",
            "Epoch :  59, training loss : 0.0106, training accuracy : 99.70, test loss : 0.3543, test accuracy : 94.74\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.0109, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0104, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0091, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0097, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0104, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0105, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0105, accuracy : 99.73\n",
            "Epoch :  60, training loss : 0.0106, training accuracy : 99.72, test loss : 0.3555, test accuracy : 94.61\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.0111, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0092, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0093, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0093, accuracy : 99.73\n",
            "Epoch :  61, training loss : 0.0091, training accuracy : 99.74, test loss : 0.3601, test accuracy : 94.59\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0068, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0081, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0078, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0080, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0084, accuracy : 99.75\n",
            "Epoch :  62, training loss : 0.0082, training accuracy : 99.76, test loss : 0.3536, test accuracy : 94.76\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0087, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0084, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0081, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0080, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.77\n",
            "Epoch :  63, training loss : 0.0079, training accuracy : 99.77, test loss : 0.3636, test accuracy : 94.76\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.0106, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0084, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0084, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0078, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0076, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.80\n",
            "Epoch :  64, training loss : 0.0074, training accuracy : 99.80, test loss : 0.3636, test accuracy : 94.66\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0072, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0079, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0080, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0083, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0079, accuracy : 99.77\n",
            "Epoch :  65, training loss : 0.0080, training accuracy : 99.76, test loss : 0.3567, test accuracy : 94.73\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0068, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0068, accuracy : 99.84\n",
            "Epoch :  66, training loss : 0.0068, training accuracy : 99.84, test loss : 0.3549, test accuracy : 94.68\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.0077, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0069, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0071, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0072, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0073, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0070, accuracy : 99.80\n",
            "Epoch :  67, training loss : 0.0070, training accuracy : 99.80, test loss : 0.3703, test accuracy : 94.53\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0072, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0071, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0069, accuracy : 99.82\n",
            "Epoch :  68, training loss : 0.0070, training accuracy : 99.81, test loss : 0.3622, test accuracy : 94.65\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0071, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0073, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0070, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0068, accuracy : 99.81\n",
            "Epoch :  69, training loss : 0.0069, training accuracy : 99.81, test loss : 0.3605, test accuracy : 94.66\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.0077, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0070, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0063, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0065, accuracy : 99.83\n",
            "Epoch :  70, training loss : 0.0064, training accuracy : 99.83, test loss : 0.3578, test accuracy : 94.70\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0067, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0067, accuracy : 99.79\n",
            "Epoch :  71, training loss : 0.0066, training accuracy : 99.80, test loss : 0.3677, test accuracy : 94.63\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.0062, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0067, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0064, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0063, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.83\n",
            "Epoch :  72, training loss : 0.0067, training accuracy : 99.82, test loss : 0.3634, test accuracy : 94.76\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0061, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.83\n",
            "Epoch :  73, training loss : 0.0062, training accuracy : 99.84, test loss : 0.3653, test accuracy : 94.80\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0060, accuracy : 99.83\n",
            "Epoch :  74, training loss : 0.0061, training accuracy : 99.82, test loss : 0.3691, test accuracy : 94.68\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0061, accuracy : 99.83\n",
            "Epoch :  75, training loss : 0.0060, training accuracy : 99.84, test loss : 0.3650, test accuracy : 94.58\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.83\n",
            "Epoch :  76, training loss : 0.0061, training accuracy : 99.83, test loss : 0.3716, test accuracy : 94.55\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.93\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.84\n",
            "Epoch :  77, training loss : 0.0063, training accuracy : 99.84, test loss : 0.3773, test accuracy : 94.63\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0070, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0059, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0061, accuracy : 99.85\n",
            "Epoch :  78, training loss : 0.0060, training accuracy : 99.85, test loss : 0.3706, test accuracy : 94.72\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0081, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0072, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0064, accuracy : 99.82\n",
            "Epoch :  79, training loss : 0.0062, training accuracy : 99.83, test loss : 0.3703, test accuracy : 94.66\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0058, accuracy : 99.83\n",
            "Epoch :  80, training loss : 0.0058, training accuracy : 99.83, test loss : 0.3735, test accuracy : 94.67\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0057, accuracy : 99.84\n",
            "Epoch :  81, training loss : 0.0057, training accuracy : 99.84, test loss : 0.3768, test accuracy : 94.61\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0070, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0075, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0066, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0060, accuracy : 99.83\n",
            "Epoch :  82, training loss : 0.0059, training accuracy : 99.84, test loss : 0.3716, test accuracy : 94.79\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.88\n",
            "Epoch :  83, training loss : 0.0054, training accuracy : 99.86, test loss : 0.3709, test accuracy : 94.76\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0062, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0061, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0059, accuracy : 99.85\n",
            "Epoch :  84, training loss : 0.0058, training accuracy : 99.86, test loss : 0.3722, test accuracy : 94.69\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.87\n",
            "Epoch :  85, training loss : 0.0052, training accuracy : 99.86, test loss : 0.3763, test accuracy : 94.64\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.88\n",
            "Epoch :  86, training loss : 0.0050, training accuracy : 99.87, test loss : 0.3800, test accuracy : 94.63\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.84\n",
            "Epoch :  87, training loss : 0.0053, training accuracy : 99.84, test loss : 0.3768, test accuracy : 94.69\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0058, accuracy : 99.83\n",
            "Epoch :  88, training loss : 0.0058, training accuracy : 99.83, test loss : 0.3772, test accuracy : 94.59\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch :  89, training loss : 0.0051, training accuracy : 99.85, test loss : 0.3848, test accuracy : 94.58\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.85\n",
            "Epoch :  90, training loss : 0.0055, training accuracy : 99.86, test loss : 0.3798, test accuracy : 94.65\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch :  91, training loss : 0.0052, training accuracy : 99.85, test loss : 0.3773, test accuracy : 94.69\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch :  92, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3820, test accuracy : 94.71\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch :  93, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3786, test accuracy : 94.70\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.86\n",
            "Epoch :  94, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3740, test accuracy : 94.66\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.89\n",
            "Epoch :  95, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3735, test accuracy : 94.70\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.86\n",
            "Epoch :  96, training loss : 0.0052, training accuracy : 99.86, test loss : 0.3842, test accuracy : 94.61\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.87\n",
            "Epoch :  97, training loss : 0.0049, training accuracy : 99.88, test loss : 0.3775, test accuracy : 94.71\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch :  98, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3699, test accuracy : 94.83\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.0080, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0069, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.85\n",
            "Epoch :  99, training loss : 0.0054, training accuracy : 99.86, test loss : 0.3731, test accuracy : 94.70\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.0033, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.85\n",
            "Epoch : 100, training loss : 0.0050, training accuracy : 99.85, test loss : 0.3813, test accuracy : 94.73\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.85\n",
            "Epoch : 101, training loss : 0.0052, training accuracy : 99.86, test loss : 0.3835, test accuracy : 94.50\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 102, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3829, test accuracy : 94.75\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 103, training loss : 0.0047, training accuracy : 99.86, test loss : 0.3859, test accuracy : 94.61\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 104, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3800, test accuracy : 94.65\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.86\n",
            "Epoch : 105, training loss : 0.0049, training accuracy : 99.85, test loss : 0.3770, test accuracy : 94.65\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.0082, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.86\n",
            "Epoch : 106, training loss : 0.0052, training accuracy : 99.86, test loss : 0.3852, test accuracy : 94.66\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch : 107, training loss : 0.0050, training accuracy : 99.85, test loss : 0.3824, test accuracy : 94.70\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 108, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3821, test accuracy : 94.56\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.85\n",
            "Epoch : 109, training loss : 0.0053, training accuracy : 99.84, test loss : 0.3803, test accuracy : 94.66\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 110, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3844, test accuracy : 94.67\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 111, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3836, test accuracy : 94.78\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 112, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3811, test accuracy : 94.86\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 113, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3795, test accuracy : 94.66\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0065, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.84\n",
            "Epoch : 114, training loss : 0.0053, training accuracy : 99.85, test loss : 0.3757, test accuracy : 94.78\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 115, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3803, test accuracy : 94.79\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.88\n",
            "Epoch : 116, training loss : 0.0045, training accuracy : 99.87, test loss : 0.3816, test accuracy : 94.63\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 117, training loss : 0.0051, training accuracy : 99.85, test loss : 0.3811, test accuracy : 94.79\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.0033, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0036, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.89\n",
            "Epoch : 118, training loss : 0.0044, training accuracy : 99.89, test loss : 0.3829, test accuracy : 94.65\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 119, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3883, test accuracy : 94.56\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 120, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3822, test accuracy : 94.73\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 121, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3927, test accuracy : 94.61\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.85\n",
            "Epoch : 122, training loss : 0.0052, training accuracy : 99.84, test loss : 0.3814, test accuracy : 94.69\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.87\n",
            "Epoch : 123, training loss : 0.0050, training accuracy : 99.87, test loss : 0.3878, test accuracy : 94.70\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 124, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3845, test accuracy : 94.61\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.90\n",
            "Epoch : 125, training loss : 0.0046, training accuracy : 99.89, test loss : 0.3777, test accuracy : 94.67\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.89\n",
            "Epoch : 126, training loss : 0.0045, training accuracy : 99.89, test loss : 0.3835, test accuracy : 94.77\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch : 127, training loss : 0.0052, training accuracy : 99.85, test loss : 0.3735, test accuracy : 94.74\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 128, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3764, test accuracy : 94.65\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.86\n",
            "Epoch : 129, training loss : 0.0045, training accuracy : 99.87, test loss : 0.3809, test accuracy : 94.69\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.85\n",
            "Epoch : 130, training loss : 0.0048, training accuracy : 99.85, test loss : 0.3816, test accuracy : 94.65\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0056, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.86\n",
            "Epoch : 131, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3842, test accuracy : 94.70\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0057, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.86\n",
            "Epoch : 132, training loss : 0.0053, training accuracy : 99.86, test loss : 0.3851, test accuracy : 94.68\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.0028, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0026, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0031, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0037, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0041, accuracy : 99.89\n",
            "Epoch : 133, training loss : 0.0043, training accuracy : 99.88, test loss : 0.3825, test accuracy : 94.71\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.86\n",
            "Epoch : 134, training loss : 0.0045, training accuracy : 99.86, test loss : 0.3749, test accuracy : 94.70\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 135, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3819, test accuracy : 94.67\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.86\n",
            "Epoch : 136, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3786, test accuracy : 94.66\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 137, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3842, test accuracy : 94.68\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 138, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3766, test accuracy : 94.76\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 139, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3753, test accuracy : 94.76\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 140, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3799, test accuracy : 94.70\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 141, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3830, test accuracy : 94.65\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 142, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3795, test accuracy : 94.74\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0033, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.86\n",
            "Epoch : 143, training loss : 0.0046, training accuracy : 99.86, test loss : 0.3835, test accuracy : 94.54\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 144, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3926, test accuracy : 94.55\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.88\n",
            "Epoch : 145, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3754, test accuracy : 94.75\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0072, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.85\n",
            "Epoch : 146, training loss : 0.0053, training accuracy : 99.86, test loss : 0.3836, test accuracy : 94.68\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 147, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3833, test accuracy : 94.67\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.83\n",
            "Epoch : 148, training loss : 0.0051, training accuracy : 99.83, test loss : 0.3721, test accuracy : 94.67\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 149, training loss : 0.0047, training accuracy : 99.86, test loss : 0.3871, test accuracy : 94.70\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0035, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0037, accuracy : 99.92\n",
            "iteration : 250, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.90\n",
            "Epoch : 150, training loss : 0.0045, training accuracy : 99.89, test loss : 0.3719, test accuracy : 94.81\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 151, training loss : 0.0049, training accuracy : 99.87, test loss : 0.3774, test accuracy : 94.64\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 152, training loss : 0.0047, training accuracy : 99.85, test loss : 0.3764, test accuracy : 94.72\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.85\n",
            "Epoch : 153, training loss : 0.0050, training accuracy : 99.85, test loss : 0.3727, test accuracy : 94.74\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 154, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3794, test accuracy : 94.78\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.85\n",
            "Epoch : 155, training loss : 0.0050, training accuracy : 99.85, test loss : 0.3804, test accuracy : 94.65\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.88\n",
            "Epoch : 156, training loss : 0.0049, training accuracy : 99.88, test loss : 0.3719, test accuracy : 94.58\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch : 157, training loss : 0.0051, training accuracy : 99.85, test loss : 0.3837, test accuracy : 94.62\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 158, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3782, test accuracy : 94.61\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.86\n",
            "Epoch : 159, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3733, test accuracy : 94.76\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.85\n",
            "Epoch : 160, training loss : 0.0047, training accuracy : 99.86, test loss : 0.3825, test accuracy : 94.80\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.86\n",
            "Epoch : 161, training loss : 0.0052, training accuracy : 99.86, test loss : 0.3792, test accuracy : 94.68\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.84\n",
            "Epoch : 162, training loss : 0.0051, training accuracy : 99.84, test loss : 0.3819, test accuracy : 94.69\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 163, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3812, test accuracy : 94.73\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0033, accuracy : 99.93\n",
            "iteration : 150, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 164, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3840, test accuracy : 94.61\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 165, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3847, test accuracy : 94.57\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.87\n",
            "Epoch : 166, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3755, test accuracy : 94.75\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.85\n",
            "Epoch : 167, training loss : 0.0052, training accuracy : 99.85, test loss : 0.3818, test accuracy : 94.80\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 168, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3782, test accuracy : 94.70\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.89\n",
            "Epoch : 169, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3840, test accuracy : 94.66\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch : 170, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3786, test accuracy : 94.66\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 171, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3753, test accuracy : 94.69\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 172, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3859, test accuracy : 94.72\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.88\n",
            "Epoch : 173, training loss : 0.0044, training accuracy : 99.88, test loss : 0.3799, test accuracy : 94.69\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0057, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.88\n",
            "Epoch : 174, training loss : 0.0053, training accuracy : 99.88, test loss : 0.3782, test accuracy : 94.55\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0034, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.89\n",
            "Epoch : 175, training loss : 0.0043, training accuracy : 99.89, test loss : 0.3853, test accuracy : 94.76\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch : 176, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3867, test accuracy : 94.65\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch : 177, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3780, test accuracy : 94.72\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.93\n",
            "iteration : 200, loss : 0.0038, accuracy : 99.93\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.91\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.89\n",
            "Epoch : 178, training loss : 0.0042, training accuracy : 99.89, test loss : 0.3838, test accuracy : 94.54\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.87\n",
            "Epoch : 179, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3809, test accuracy : 94.73\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.89\n",
            "Epoch : 180, training loss : 0.0049, training accuracy : 99.88, test loss : 0.3892, test accuracy : 94.66\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 181, training loss : 0.0049, training accuracy : 99.87, test loss : 0.3867, test accuracy : 94.63\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.87\n",
            "Epoch : 182, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3798, test accuracy : 94.61\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.92\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0040, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.89\n",
            "Epoch : 183, training loss : 0.0048, training accuracy : 99.88, test loss : 0.3798, test accuracy : 94.72\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0068, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0056, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.86\n",
            "Epoch : 184, training loss : 0.0053, training accuracy : 99.85, test loss : 0.3741, test accuracy : 94.78\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 185, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3815, test accuracy : 94.63\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.88\n",
            "Epoch : 186, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3820, test accuracy : 94.66\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 187, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3875, test accuracy : 94.55\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.89\n",
            "Epoch : 188, training loss : 0.0050, training accuracy : 99.88, test loss : 0.3828, test accuracy : 94.68\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 189, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3870, test accuracy : 94.66\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch : 190, training loss : 0.0052, training accuracy : 99.85, test loss : 0.3808, test accuracy : 94.56\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.84\n",
            "Epoch : 191, training loss : 0.0054, training accuracy : 99.84, test loss : 0.3865, test accuracy : 94.77\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 192, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3831, test accuracy : 94.75\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 193, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3805, test accuracy : 94.68\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 194, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3839, test accuracy : 94.63\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0057, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0057, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.82\n",
            "Epoch : 195, training loss : 0.0055, training accuracy : 99.83, test loss : 0.3849, test accuracy : 94.58\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 196, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3808, test accuracy : 94.61\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 197, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3849, test accuracy : 94.66\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.88\n",
            "Epoch : 198, training loss : 0.0049, training accuracy : 99.88, test loss : 0.3855, test accuracy : 94.71\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 199, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3836, test accuracy : 94.60\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.90\n",
            "Epoch : 200, training loss : 0.0042, training accuracy : 99.90, test loss : 0.3802, test accuracy : 94.62\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.88\n",
            "Epoch : 201, training loss : 0.0044, training accuracy : 99.88, test loss : 0.3815, test accuracy : 94.73\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 202, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3820, test accuracy : 94.72\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 203, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3853, test accuracy : 94.67\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.85\n",
            "Epoch : 204, training loss : 0.0055, training accuracy : 99.85, test loss : 0.3775, test accuracy : 94.68\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 205, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3871, test accuracy : 94.59\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.88\n",
            "Epoch : 206, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3774, test accuracy : 94.76\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 207, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3905, test accuracy : 94.69\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0040, accuracy : 99.92\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.90\n",
            "Epoch : 208, training loss : 0.0042, training accuracy : 99.89, test loss : 0.3807, test accuracy : 94.66\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.88\n",
            "Epoch : 209, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3815, test accuracy : 94.67\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.86\n",
            "Epoch : 210, training loss : 0.0054, training accuracy : 99.85, test loss : 0.3839, test accuracy : 94.69\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch : 211, training loss : 0.0047, training accuracy : 99.85, test loss : 0.3910, test accuracy : 94.65\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.88\n",
            "Epoch : 212, training loss : 0.0048, training accuracy : 99.88, test loss : 0.3865, test accuracy : 94.68\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.89\n",
            "Epoch : 213, training loss : 0.0046, training accuracy : 99.89, test loss : 0.3797, test accuracy : 94.79\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0040, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0035, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.88\n",
            "Epoch : 214, training loss : 0.0043, training accuracy : 99.88, test loss : 0.3823, test accuracy : 94.68\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 215, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3815, test accuracy : 94.78\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.87\n",
            "Epoch : 216, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3788, test accuracy : 94.68\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 217, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3761, test accuracy : 94.74\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 218, training loss : 0.0049, training accuracy : 99.87, test loss : 0.3740, test accuracy : 94.84\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0040, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0039, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0039, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0039, accuracy : 99.90\n",
            "Epoch : 219, training loss : 0.0040, training accuracy : 99.90, test loss : 0.3861, test accuracy : 94.73\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.84\n",
            "Epoch : 220, training loss : 0.0053, training accuracy : 99.84, test loss : 0.3735, test accuracy : 94.72\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.85\n",
            "Epoch : 221, training loss : 0.0054, training accuracy : 99.85, test loss : 0.3757, test accuracy : 94.70\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 222, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3836, test accuracy : 94.61\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 223, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3818, test accuracy : 94.57\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0030, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 224, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3755, test accuracy : 94.73\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 225, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3817, test accuracy : 94.71\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.87\n",
            "Epoch : 226, training loss : 0.0052, training accuracy : 99.87, test loss : 0.3789, test accuracy : 94.67\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.88\n",
            "Epoch : 227, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3852, test accuracy : 94.63\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 228, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3789, test accuracy : 94.64\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 229, training loss : 0.0044, training accuracy : 99.87, test loss : 0.3835, test accuracy : 94.66\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.84\n",
            "Epoch : 230, training loss : 0.0055, training accuracy : 99.84, test loss : 0.3935, test accuracy : 94.48\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 231, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3787, test accuracy : 94.68\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.85\n",
            "Epoch : 232, training loss : 0.0052, training accuracy : 99.85, test loss : 0.3792, test accuracy : 94.74\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 233, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3838, test accuracy : 94.60\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.87\n",
            "Epoch : 234, training loss : 0.0046, training accuracy : 99.86, test loss : 0.3794, test accuracy : 94.71\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.90\n",
            "Epoch : 235, training loss : 0.0045, training accuracy : 99.89, test loss : 0.3768, test accuracy : 94.76\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.84\n",
            "Epoch : 236, training loss : 0.0051, training accuracy : 99.85, test loss : 0.3790, test accuracy : 94.76\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 237, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3791, test accuracy : 94.76\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 238, training loss : 0.0044, training accuracy : 99.88, test loss : 0.3889, test accuracy : 94.62\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0040, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.89\n",
            "Epoch : 239, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3743, test accuracy : 94.72\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.88\n",
            "Epoch : 240, training loss : 0.0045, training accuracy : 99.87, test loss : 0.3834, test accuracy : 94.72\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 241, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3849, test accuracy : 94.66\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.86\n",
            "Epoch : 242, training loss : 0.0049, training accuracy : 99.87, test loss : 0.3746, test accuracy : 94.67\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 243, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3824, test accuracy : 94.63\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.87\n",
            "Epoch : 244, training loss : 0.0044, training accuracy : 99.87, test loss : 0.3821, test accuracy : 94.71\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0028, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.91\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.89\n",
            "Epoch : 245, training loss : 0.0048, training accuracy : 99.89, test loss : 0.3865, test accuracy : 94.68\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.88\n",
            "Epoch : 246, training loss : 0.0049, training accuracy : 99.87, test loss : 0.3774, test accuracy : 94.70\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0064, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.85\n",
            "Epoch : 247, training loss : 0.0054, training accuracy : 99.85, test loss : 0.3737, test accuracy : 94.69\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.89\n",
            "Epoch : 248, training loss : 0.0048, training accuracy : 99.89, test loss : 0.3788, test accuracy : 94.70\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 249, training loss : 0.0048, training accuracy : 99.88, test loss : 0.3864, test accuracy : 94.61\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.89\n",
            "Epoch : 250, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3840, test accuracy : 94.67\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 251, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3823, test accuracy : 94.55\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.87\n",
            "Epoch : 252, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3869, test accuracy : 94.60\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.88\n",
            "Epoch : 253, training loss : 0.0050, training accuracy : 99.88, test loss : 0.3875, test accuracy : 94.66\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0030, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 254, training loss : 0.0045, training accuracy : 99.89, test loss : 0.3786, test accuracy : 94.61\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0072, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.85\n",
            "Epoch : 255, training loss : 0.0049, training accuracy : 99.85, test loss : 0.3735, test accuracy : 94.60\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 256, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3730, test accuracy : 94.74\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 257, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3816, test accuracy : 94.63\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0034, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.85\n",
            "Epoch : 258, training loss : 0.0049, training accuracy : 99.86, test loss : 0.3767, test accuracy : 94.74\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0074, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 259, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3841, test accuracy : 94.78\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.87\n",
            "Epoch : 260, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3875, test accuracy : 94.60\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 261, training loss : 0.0047, training accuracy : 99.86, test loss : 0.3692, test accuracy : 94.76\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.87\n",
            "Epoch : 262, training loss : 0.0051, training accuracy : 99.88, test loss : 0.3754, test accuracy : 94.74\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.86\n",
            "Epoch : 263, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3828, test accuracy : 94.65\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0057, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.86\n",
            "Epoch : 264, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3801, test accuracy : 94.59\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.88\n",
            "Epoch : 265, training loss : 0.0048, training accuracy : 99.88, test loss : 0.3871, test accuracy : 94.60\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch : 266, training loss : 0.0052, training accuracy : 99.85, test loss : 0.3755, test accuracy : 94.80\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.85\n",
            "Epoch : 267, training loss : 0.0050, training accuracy : 99.86, test loss : 0.3841, test accuracy : 94.61\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.85\n",
            "Epoch : 268, training loss : 0.0050, training accuracy : 99.85, test loss : 0.3726, test accuracy : 94.71\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.84\n",
            "Epoch : 269, training loss : 0.0052, training accuracy : 99.84, test loss : 0.3794, test accuracy : 94.65\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 270, training loss : 0.0049, training accuracy : 99.87, test loss : 0.3838, test accuracy : 94.67\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 271, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3798, test accuracy : 94.75\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.86\n",
            "Epoch : 272, training loss : 0.0051, training accuracy : 99.86, test loss : 0.3778, test accuracy : 94.72\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.89\n",
            "Epoch : 273, training loss : 0.0046, training accuracy : 99.89, test loss : 0.3821, test accuracy : 94.62\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 274, training loss : 0.0048, training accuracy : 99.86, test loss : 0.3801, test accuracy : 94.66\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 275, training loss : 0.0046, training accuracy : 99.88, test loss : 0.3791, test accuracy : 94.80\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 276, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3813, test accuracy : 94.64\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.85\n",
            "Epoch : 277, training loss : 0.0051, training accuracy : 99.85, test loss : 0.3761, test accuracy : 94.66\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0064, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.88\n",
            "Epoch : 278, training loss : 0.0051, training accuracy : 99.87, test loss : 0.3808, test accuracy : 94.70\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.87\n",
            "Epoch : 279, training loss : 0.0049, training accuracy : 99.88, test loss : 0.3796, test accuracy : 94.65\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 280, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3693, test accuracy : 94.84\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 281, training loss : 0.0050, training accuracy : 99.88, test loss : 0.3727, test accuracy : 94.70\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 282, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3811, test accuracy : 94.71\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.85\n",
            "Epoch : 283, training loss : 0.0053, training accuracy : 99.86, test loss : 0.3710, test accuracy : 94.77\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.89\n",
            "Epoch : 284, training loss : 0.0045, training accuracy : 99.89, test loss : 0.3803, test accuracy : 94.63\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.89\n",
            "Epoch : 285, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3810, test accuracy : 94.74\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.90\n",
            "Epoch : 286, training loss : 0.0043, training accuracy : 99.89, test loss : 0.3768, test accuracy : 94.70\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0036, accuracy : 99.92\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 287, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3750, test accuracy : 94.62\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.86\n",
            "Epoch : 288, training loss : 0.0045, training accuracy : 99.87, test loss : 0.3817, test accuracy : 94.73\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 289, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3736, test accuracy : 94.74\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0036, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.89\n",
            "Epoch : 290, training loss : 0.0043, training accuracy : 99.89, test loss : 0.3825, test accuracy : 94.67\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.88\n",
            "Epoch : 291, training loss : 0.0047, training accuracy : 99.87, test loss : 0.3820, test accuracy : 94.73\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 292, training loss : 0.0045, training accuracy : 99.88, test loss : 0.3842, test accuracy : 94.69\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 293, training loss : 0.0048, training accuracy : 99.87, test loss : 0.3735, test accuracy : 94.77\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.88\n",
            "Epoch : 294, training loss : 0.0047, training accuracy : 99.88, test loss : 0.3787, test accuracy : 94.66\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.88\n",
            "Epoch : 295, training loss : 0.0044, training accuracy : 99.88, test loss : 0.3772, test accuracy : 94.68\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0027, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0030, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 296, training loss : 0.0046, training accuracy : 99.87, test loss : 0.3868, test accuracy : 94.61\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.84\n",
            "Epoch : 297, training loss : 0.0050, training accuracy : 99.84, test loss : 0.3816, test accuracy : 94.65\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0064, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.88\n",
            "Epoch : 298, training loss : 0.0048, training accuracy : 99.88, test loss : 0.3815, test accuracy : 94.72\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.87\n",
            "Epoch : 299, training loss : 0.0050, training accuracy : 99.87, test loss : 0.3853, test accuracy : 94.71\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0041, accuracy : 99.88\n",
            "Epoch : 300, training loss : 0.0041, training accuracy : 99.89, test loss : 0.3878, test accuracy : 94.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "id": "iUQVIKR-X3v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52d374a-405e-4a97-b3e1-f58dfb17ec63"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.33536864510353875, 95.21358328211431)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : Step\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7CNz1iabSB21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "a2138b7f-80aa-4807-8f25-512df20c1cf2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : Step\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c505b0ec-1101-4a02-eb63-9b72a48093bc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_step = {train_loss_list}\") \n",
        "print(f\"train_acc_list_step = {train_acc_list}\")\n",
        "print(f\"test_loss_list_step = {test_loss_list}\")\n",
        "print(f\"test_acc_list_step = {test_acc_list}\")"
      ],
      "metadata": {
        "id": "3eiY3bTlWipW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c6c9bc-136a-4856-9617-5c8c026a01e2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_step = [1.5044204033810271, 0.49208608810817644, 0.37830603914209177, 0.33356322168415475, 0.30051283241529775, 0.2731129997548695, 0.25931339448464275, 0.24751879757416603, 0.23341218238762076, 0.22551919120114025, 0.21245744068246225, 0.20461816869696303, 0.19351729513385754, 0.18888680500591673, 0.1790498158513369, 0.17313903582794718, 0.16563977062863708, 0.15938490460599017, 0.1541340289319434, 0.14407484958246147, 0.13983443950815252, 0.13245242257630277, 0.12638695052443805, 0.12125968426304458, 0.11788717211605249, 0.11147532753424269, 0.10493720971996869, 0.10114793143257862, 0.0986312177905505, 0.09325535373060603, 0.06156983597173802, 0.050082290781091464, 0.04566037622696864, 0.042090283896077454, 0.037587769078418896, 0.03525592588446025, 0.034040165755286976, 0.031216950466235478, 0.030938032096463855, 0.029107897011765093, 0.02691679777953582, 0.024817724369887797, 0.02327188092970408, 0.022008026162917072, 0.021083637177603455, 0.020199724932410693, 0.020445302296316296, 0.01811823503557229, 0.016885934063002497, 0.016305384605991728, 0.015734232263797315, 0.01582022506202922, 0.01498541162761943, 0.013855328222454038, 0.012501976389670114, 0.011526370763357194, 0.012135067156394237, 0.011748958516110058, 0.01063293406628148, 0.010577014861956521, 0.009060478683824204, 0.00822005160518879, 0.007903504668233972, 0.0073660259777244665, 0.007953700653634155, 0.006759841606805863, 0.0069558188274406844, 0.007026840372753726, 0.006894602971896024, 0.0063807274687209935, 0.006584282455781946, 0.006663436767551165, 0.006154599042466102, 0.006103539849245962, 0.006048463034234704, 0.00614482418276391, 0.006322457585395453, 0.006028014029933677, 0.006245754332408476, 0.00581059281254909, 0.005709030747089944, 0.005923201890116833, 0.00537758718227695, 0.005807922808844647, 0.005243748521360034, 0.004974289045318631, 0.005323306708443634, 0.005775554073696229, 0.005093053230784864, 0.005536427160324011, 0.0052108629591284105, 0.004749226128792285, 0.005127630246161776, 0.005065120122888249, 0.004685268356975264, 0.005243820296808683, 0.0049121043785080205, 0.005093204319377071, 0.005370734221114888, 0.005008859195898547, 0.005249464887712999, 0.004638611593474268, 0.004728934781005616, 0.005035177127017733, 0.004871168427265863, 0.005197190184529169, 0.004969428170602996, 0.004836069228517858, 0.005308753678475464, 0.004877718586694237, 0.0046394178739258, 0.004715173941094246, 0.005071522070231865, 0.005326596872854554, 0.004782932753934018, 0.00452435254143787, 0.005128751716483132, 0.004405792564614916, 0.005072576416472445, 0.004683375086275378, 0.004745950062742184, 0.005199869099014116, 0.004980171865863766, 0.004840858398257126, 0.0046297258561474545, 0.004515150684752434, 0.005211585615480793, 0.0046955638231178315, 0.004485690257187711, 0.004824536544108968, 0.004908301922912914, 0.005288558260384356, 0.004324457162535601, 0.004497020012851448, 0.004832310812420976, 0.004629586771150122, 0.00469785524222389, 0.0051049072852895816, 0.004760202388700317, 0.00467975922823293, 0.004756802660750659, 0.004577458516745399, 0.0046352100029977686, 0.0045855785586362005, 0.0045302773612948275, 0.00527981271794404, 0.004842230399004512, 0.005128007588957292, 0.0047336897144657575, 0.00452429031405685, 0.004871112167589785, 0.004707888855899686, 0.004987144301444176, 0.004576647475924656, 0.0050210763291275344, 0.004869554478370365, 0.005145938421950075, 0.0049324849194394005, 0.005007630027365255, 0.004694439974090723, 0.005161294266647788, 0.005086089323139447, 0.0045723088694465674, 0.0049315163145030455, 0.005043683056833177, 0.004782498019489051, 0.005208640262013842, 0.004906343821498318, 0.004504839811582297, 0.004765471186647242, 0.004636792729359163, 0.0048650642755604795, 0.00440384292277286, 0.005331020985203442, 0.004309070902481564, 0.005132160800841049, 0.0048202084057818, 0.0042395592619208135, 0.004579358937877841, 0.004854094481356265, 0.004861267090400903, 0.004641431140822869, 0.004752275634260134, 0.005280723348089562, 0.0049866622946490744, 0.004713922302353166, 0.0049851976106240516, 0.0049522482596579456, 0.0047414097434517405, 0.005198805679831853, 0.005367297160049249, 0.004801289511232152, 0.004936103088396973, 0.004513620775553264, 0.005461517685133368, 0.005038318795404084, 0.004621399451298942, 0.004861979541443869, 0.004728190580562456, 0.00424972596940481, 0.004389661508080736, 0.004675524760544651, 0.004678315578823907, 0.005468495250993464, 0.004904287783485815, 0.004628283689890138, 0.004957924133196958, 0.004207717516179686, 0.004497190112673026, 0.0054004669855449725, 0.004738672002041044, 0.004832978636525801, 0.004593934473861766, 0.00428500973297249, 0.004800252157330026, 0.004579809123577946, 0.0047766751335504105, 0.004927973183699149, 0.003962443585616099, 0.005313499298157754, 0.005448946752569696, 0.004626204171514537, 0.00486521731347835, 0.00464669697920995, 0.00473364783223967, 0.0051715146905747635, 0.004559630797677459, 0.004807513805560901, 0.004414798407140251, 0.0055058369074104515, 0.004735360263094689, 0.00523965430700253, 0.004729446927025576, 0.0046202187999946675, 0.004473174700221971, 0.005083680975845281, 0.00473606128646736, 0.004358955291625135, 0.004462744230992882, 0.00445035025452959, 0.004661599569843557, 0.004881452044679059, 0.004753526896741532, 0.004433994836329803, 0.004789544894031266, 0.004920090539939323, 0.005374149948520697, 0.004787621462456112, 0.004773729829575745, 0.0044936602159881545, 0.004565336937149469, 0.004793149001421886, 0.004954056282359589, 0.00447947610881707, 0.004911790105014495, 0.004962967739427566, 0.004856595992693377, 0.0049429370942105965, 0.004495315723300048, 0.005116432885005868, 0.004663471084030195, 0.005105866048162296, 0.0048063792890729005, 0.004996042520567828, 0.0048235019935209174, 0.0052012274302681025, 0.004979272351725164, 0.004979745390991236, 0.005170751338667516, 0.004858217863982788, 0.004663840651548111, 0.005053441917670759, 0.004584236903877065, 0.004780888221675258, 0.004612757356412744, 0.004703525326821731, 0.005128545914503725, 0.005097627184756964, 0.004888794860495936, 0.004546376856262884, 0.0049637655482588335, 0.004738951851973477, 0.005256142429007358, 0.004507868475276601, 0.004676521270572799, 0.0042853716939203765, 0.004519075942008067, 0.004505532296253924, 0.004830542344546013, 0.004257976696643534, 0.004718432969060035, 0.004490681710655531, 0.004793198622190861, 0.004690485046958793, 0.004381172440937899, 0.004597538895479108, 0.0050219146489527905, 0.004799934776746264, 0.004984567811975109, 0.004093347564194513]\n",
            "train_acc_list_step = [47.394388565378506, 84.38962413975648, 88.32609846479619, 89.65802011646373, 90.93912122816305, 91.76071995764956, 92.30492323980943, 92.69878242456326, 93.1371095817893, 93.283218634198, 93.84859714134463, 93.97988353626258, 94.4203282159873, 94.56643726839597, 94.83324510322922, 95.06405505558496, 95.14452091053468, 95.38168343038645, 95.61461090524087, 95.86871360508205, 95.94282689253573, 96.23928004235044, 96.33456855479089, 96.51667548967708, 96.58231868713605, 96.79618845950239, 96.97829539438857, 97.11805187930122, 97.2006352567496, 97.23451561672843, 98.32927474854421, 98.71254632080466, 98.81630492323981, 98.94970884065643, 99.04499735309687, 99.05558496559026, 99.12969825304394, 99.22075172048703, 99.16993118051879, 99.24616199047115, 99.32027527792482, 99.33721545791424, 99.38168343038645, 99.40285865537321, 99.45156167284277, 99.46426680783483, 99.43885653785071, 99.49814716781366, 99.51932239280042, 99.55320275277924, 99.58073054526204, 99.56379036527264, 99.56379036527264, 99.6209634727369, 99.65907887771307, 99.68237162519851, 99.61884595023822, 99.62731604023293, 99.69507676019057, 99.72260455267337, 99.73742721016411, 99.7564849126522, 99.76919004764426, 99.79883536262572, 99.7649550026469, 99.83906829010058, 99.80307040762308, 99.81154049761778, 99.81154049761778, 99.83271572260455, 99.8009528851244, 99.82424563260984, 99.8369507676019, 99.82424563260984, 99.83906829010058, 99.83483324510323, 99.8369507676019, 99.84965590259397, 99.82636315510852, 99.83059820010588, 99.83906829010058, 99.84118581259926, 99.86236103758603, 99.85600847008999, 99.86236103758603, 99.87083112758073, 99.8369507676019, 99.8284806776072, 99.84753838009529, 99.85600847008999, 99.85389094759132, 99.87083112758073, 99.85812599258867, 99.86024351508735, 99.88353626257279, 99.85600847008999, 99.87506617257809, 99.85600847008999, 99.86236103758603, 99.84753838009529, 99.85600847008999, 99.87930121757543, 99.86236103758603, 99.8644785600847, 99.85177342509265, 99.86024351508735, 99.85389094759132, 99.85812599258867, 99.84330333509793, 99.86024351508735, 99.87718369507677, 99.86871360508205, 99.85812599258867, 99.84753838009529, 99.86659608258337, 99.87083112758073, 99.85389094759132, 99.88988883006881, 99.86024351508735, 99.87718369507677, 99.86659608258337, 99.84118581259926, 99.87083112758073, 99.85812599258867, 99.88777130757015, 99.88988883006881, 99.84542085759661, 99.87294865007941, 99.87083112758073, 99.85177342509265, 99.86024351508735, 99.86236103758603, 99.87506617257809, 99.8644785600847, 99.86024351508735, 99.86659608258337, 99.86871360508205, 99.85600847008999, 99.87083112758073, 99.87294865007941, 99.87294865007941, 99.88353626257279, 99.86236103758603, 99.87930121757543, 99.87930121757543, 99.86024351508735, 99.86024351508735, 99.83483324510323, 99.8644785600847, 99.89200635256749, 99.87083112758073, 99.84965590259397, 99.84965590259397, 99.88141874007411, 99.84753838009529, 99.87506617257809, 99.85389094759132, 99.85812599258867, 99.86236103758603, 99.85600847008999, 99.85600847008999, 99.84118581259926, 99.87930121757543, 99.86024351508735, 99.85812599258867, 99.86871360508205, 99.85389094759132, 99.86236103758603, 99.87718369507677, 99.85812599258867, 99.87083112758073, 99.85600847008999, 99.87506617257809, 99.87506617257809, 99.88777130757015, 99.85812599258867, 99.85600847008999, 99.88777130757015, 99.86659608258337, 99.87718369507677, 99.86659608258337, 99.86659608258337, 99.88141874007411, 99.85177342509265, 99.85600847008999, 99.87718369507677, 99.86236103758603, 99.87930121757543, 99.87506617257809, 99.85177342509265, 99.84118581259926, 99.86236103758603, 99.85812599258867, 99.87506617257809, 99.82636315510852, 99.85600847008999, 99.87718369507677, 99.88353626257279, 99.87294865007941, 99.89624139756485, 99.88353626257279, 99.87718369507677, 99.87083112758073, 99.84542085759661, 99.8644785600847, 99.88141874007411, 99.85600847008999, 99.89412387506617, 99.88141874007411, 99.85177342509265, 99.85389094759132, 99.87930121757543, 99.88565378507147, 99.88353626257279, 99.86871360508205, 99.86871360508205, 99.8644785600847, 99.86871360508205, 99.89624139756485, 99.84330333509793, 99.84542085759661, 99.88353626257279, 99.86236103758603, 99.87930121757543, 99.87506617257809, 99.87083112758073, 99.87294865007941, 99.87294865007941, 99.86871360508205, 99.84330333509793, 99.86659608258337, 99.84542085759661, 99.87506617257809, 99.8644785600847, 99.89200635256749, 99.84542085759661, 99.86871360508205, 99.87506617257809, 99.88141874007411, 99.87083112758073, 99.87083112758073, 99.86659608258337, 99.86659608258337, 99.86871360508205, 99.88565378507147, 99.87083112758073, 99.84753838009529, 99.88777130757015, 99.87718369507677, 99.88353626257279, 99.87083112758073, 99.86236103758603, 99.87718369507677, 99.88777130757015, 99.84965590259397, 99.85600847008999, 99.86236103758603, 99.85812599258867, 99.87718369507677, 99.8644785600847, 99.86024351508735, 99.87506617257809, 99.8644785600847, 99.8644785600847, 99.87930121757543, 99.85177342509265, 99.86024351508735, 99.85177342509265, 99.84118581259926, 99.86871360508205, 99.87718369507677, 99.86024351508735, 99.88988883006881, 99.8644785600847, 99.88141874007411, 99.87506617257809, 99.85177342509265, 99.86871360508205, 99.87506617257809, 99.87506617257809, 99.87930121757543, 99.86871360508205, 99.85812599258867, 99.88988883006881, 99.88353626257279, 99.89412387506617, 99.87930121757543, 99.86659608258337, 99.86659608258337, 99.88988883006881, 99.87083112758073, 99.87506617257809, 99.87083112758073, 99.87718369507677, 99.87930121757543, 99.87083112758073, 99.84330333509793, 99.87718369507677, 99.87294865007941, 99.88565378507147]\n",
            "test_loss_list_step = [0.7521707053278007, 0.41922322909037274, 0.3967930445191907, 0.35680990534670215, 0.3250766685049908, 0.30643289908766747, 0.27803952721696273, 0.26461896110399097, 0.26403681638047977, 0.2668783114309989, 0.2609864034708224, 0.25131638298797254, 0.24851202544774495, 0.24535505795011334, 0.23515790532909187, 0.23713360856488055, 0.2406249167215006, 0.24633904404061682, 0.22870721204169825, 0.23081522924350759, 0.23210598513776182, 0.23380596216256713, 0.23264140811036615, 0.2492445185597913, 0.24672274596477842, 0.23322659500819795, 0.2332557491848574, 0.2325650899324055, 0.2574915187433362, 0.23942345477567584, 0.21860766188953729, 0.22490006466122234, 0.22599033247588166, 0.2330661828027052, 0.23813083913980745, 0.2410290090677639, 0.25265564182408007, 0.25388093285408675, 0.25897152009694013, 0.2653597001095905, 0.267472947769634, 0.2757880237756991, 0.28352711534164116, 0.2891207381023788, 0.2937827925471699, 0.3016136303891008, 0.29683313752506296, 0.3059604737939605, 0.31242154799766986, 0.313900034299449, 0.3219801964347853, 0.33177046875889393, 0.3355082166837711, 0.33843776536192377, 0.34364481102309974, 0.3455076480613035, 0.34563379169569586, 0.35321818167051555, 0.35429473890576, 0.35551488774317297, 0.36010582940470354, 0.35364405492631096, 0.3636273516743791, 0.36364757276012327, 0.35665451670887277, 0.3549169595261999, 0.3703063836905594, 0.36219835961146246, 0.3605355029998749, 0.3578162646861564, 0.36774474406140106, 0.3633771698702784, 0.3653042366746448, 0.3691165424956411, 0.36504066177625577, 0.3715900047225695, 0.377250660302154, 0.3705682765610297, 0.3703488810170515, 0.37351591648606985, 0.37677567382343113, 0.37160208523638694, 0.3709423184650494, 0.3721590631769276, 0.3763345616011351, 0.3800408639488559, 0.37680745827874135, 0.3771674718214747, 0.3848353975142042, 0.3798498407590623, 0.3773468048494382, 0.3819653425073507, 0.3785710880149375, 0.37396970703654614, 0.37354924148131236, 0.38423583746029466, 0.3775230319622685, 0.36990976165614875, 0.3731101816017911, 0.3812584620443921, 0.3834761079017292, 0.38286398570327196, 0.3858671422372116, 0.3799955192816389, 0.3770175949378195, 0.3851937071338077, 0.38235137271968755, 0.38205398502303106, 0.38025479872400564, 0.3843826920378442, 0.38355313671533675, 0.3811237645043316, 0.37951354816665545, 0.3756716687428564, 0.38028262046110983, 0.38163883488296585, 0.38113248269712807, 0.38293040138395396, 0.38826611508414444, 0.3821931903378344, 0.3927168071534777, 0.3814124613087259, 0.38783683029788674, 0.384523262598497, 0.3777104146395098, 0.3834854347001323, 0.37351655306331083, 0.37643494215958256, 0.38091917734081837, 0.3816218973667014, 0.38416977602915436, 0.3851156278796421, 0.3824646536297366, 0.37491797317988146, 0.3818769982979432, 0.37861419655382633, 0.3842346754938583, 0.37655236199498177, 0.37527537754024654, 0.37987153332077844, 0.3830455071018899, 0.3794875234803733, 0.38347619624041457, 0.39262797689868834, 0.37537328259763764, 0.3836479137067263, 0.3833252173236699, 0.37210422399563386, 0.387080483238998, 0.3719306717816211, 0.37743640282446994, 0.3764378684846794, 0.37271541687568616, 0.37936363588361177, 0.3804472682075392, 0.371879458409168, 0.38374935142586336, 0.3781576618488294, 0.3732976428844838, 0.3825277503210065, 0.37921793430167083, 0.38190348801550034, 0.3811836604795912, 0.3839620574760963, 0.3847266497711341, 0.3755092674263698, 0.38182204500680755, 0.3782161524446279, 0.383978412504874, 0.3785792852560168, 0.37529283202728075, 0.38586872027200814, 0.37986113718144743, 0.3781509461410928, 0.385330743640296, 0.3866525995362477, 0.3780062812672672, 0.38378071400574315, 0.38093069549102115, 0.38920295537065935, 0.38671026251041424, 0.3797818386251582, 0.3798417299438049, 0.3740828680422376, 0.38145504804218516, 0.38200287843196123, 0.3875016655645096, 0.38280217152308016, 0.38698648213974984, 0.3808485120260978, 0.38649215569317924, 0.3831135995238654, 0.3805198727321683, 0.38389426964682105, 0.38486584148132336, 0.3807648518761876, 0.3849117392856701, 0.38554816227406263, 0.38357163151251333, 0.3802410184734446, 0.3815109624947403, 0.382021243195506, 0.3853015545938237, 0.377494075719048, 0.38711871966427447, 0.3774435311857173, 0.3905071147407095, 0.3806642469603057, 0.38146250298721535, 0.3838621310104488, 0.39097126777849944, 0.38649382303450625, 0.3796885114930132, 0.38229777593183895, 0.3814927838633166, 0.37877432031410874, 0.37605211639082897, 0.37402715621625676, 0.3860787514177169, 0.3734693876285033, 0.3757060868665576, 0.3836308258823028, 0.3818031575019453, 0.3754675344144012, 0.3817046451035376, 0.3788763102984019, 0.38524902579100695, 0.3789480691410455, 0.383543092720941, 0.39350814419780294, 0.3787268260402568, 0.3791520138837251, 0.3838299135951435, 0.3793616016559741, 0.3767985271874304, 0.3790097143866268, 0.37912912847583785, 0.38894346707007466, 0.3742955804382469, 0.38342078903909116, 0.3848988146238102, 0.37459168269061577, 0.38242888645561157, 0.3820874952601598, 0.38645922241951614, 0.37736037103276626, 0.37371054134450343, 0.3787742013148233, 0.3864014833873394, 0.38398269388605566, 0.3823276355716528, 0.38692381239368345, 0.38753888685731036, 0.37855833295878827, 0.37346514290673477, 0.37298866356814314, 0.3816173101830132, 0.3767242920311058, 0.3841047031056209, 0.38750297252965327, 0.369205586767445, 0.3753658247490724, 0.38275528518373475, 0.3800859667141648, 0.3870932775020015, 0.37551862878414494, 0.3840912418421723, 0.3726249620291021, 0.37941730108486454, 0.38384662088298915, 0.37976392629720707, 0.37775822057772207, 0.38208118103006306, 0.3800909294399853, 0.37907081163104844, 0.3812881442672555, 0.3761177863743083, 0.38076628101350485, 0.37963548230518607, 0.3693312196718419, 0.37269937157557875, 0.3810739266987452, 0.37098004634254705, 0.38030854867332997, 0.3809964715083148, 0.37681868622152537, 0.3750186215574835, 0.3816620926098788, 0.3736031973351012, 0.38245252669588026, 0.3819549302798787, 0.38419706475756626, 0.3735217913966991, 0.3787470100191878, 0.37715579814040195, 0.3867923977976555, 0.3815506379960068, 0.3814866198135503, 0.3853403181933305, 0.3877864530602214]\n",
            "test_acc_list_step = [75.81054087277197, 86.9660417947142, 87.58451137062077, 89.28242163491088, 90.1160110633067, 91.05331899200984, 91.73325138291334, 92.37092808850646, 92.390135218193, 92.24031960663798, 92.37092808850646, 92.9778733866011, 93.1737861094038, 93.04701905347265, 93.41963736939152, 93.37738168408113, 93.3159188690842, 93.0278119237861, 93.7077443146896, 93.72695144437616, 93.68853718500307, 93.78073140749846, 93.73847572218807, 93.26213890596189, 93.30055316533497, 93.93438844499079, 94.06883835279656, 94.11877688998156, 93.6040258143823, 94.0419483712354, 94.87553779963122, 94.83712354025815, 94.81023355869699, 94.87169637369392, 94.91779348494161, 94.89474492931777, 94.86785494775661, 94.84096496619546, 94.71419791026429, 94.56054087277197, 94.75645359557468, 94.73724646588813, 94.64889366933005, 94.64889366933005, 94.69499078057775, 94.72956361401353, 94.6757836508912, 94.74492931776275, 94.6220036877689, 94.65273509526736, 94.54901659496005, 94.53749231714812, 94.63352796558083, 94.61816226183159, 94.61432083589429, 94.5221266133989, 94.64121081745544, 94.56054087277197, 94.74492931776275, 94.61047940995698, 94.59127228027043, 94.75645359557468, 94.75645359557468, 94.66425937307929, 94.73340503995082, 94.6757836508912, 94.53365089121081, 94.64505224339274, 94.66041794714198, 94.70267363245236, 94.6258451137062, 94.75645359557468, 94.80255070682237, 94.67962507682851, 94.58358942839583, 94.54517516902274, 94.6258451137062, 94.7180393362016, 94.66425937307929, 94.6681007990166, 94.61432083589429, 94.78718500307313, 94.76029502151198, 94.68730792870313, 94.64121081745544, 94.62968653964352, 94.69114935464044, 94.58743085433314, 94.58358942839583, 94.64889366933005, 94.69114935464044, 94.70651505838967, 94.70267363245236, 94.66041794714198, 94.69883220651506, 94.61432083589429, 94.71419791026429, 94.82559926244622, 94.69883220651506, 94.72572218807622, 94.49523663183774, 94.75261216963737, 94.61047940995698, 94.64505224339274, 94.64889366933005, 94.66041794714198, 94.70267363245236, 94.55669944683467, 94.65657652120467, 94.6681007990166, 94.77566072526122, 94.85633066994468, 94.66041794714198, 94.78334357713584, 94.79102642901044, 94.63352796558083, 94.78718500307313, 94.64505224339274, 94.56438229870928, 94.73340503995082, 94.61047940995698, 94.68730792870313, 94.70267363245236, 94.61432083589429, 94.67194222495391, 94.77181929932391, 94.73724646588813, 94.64505224339274, 94.69499078057775, 94.65273509526736, 94.69883220651506, 94.6757836508912, 94.71035648432698, 94.69883220651506, 94.67194222495391, 94.66425937307929, 94.6757836508912, 94.76029502151198, 94.75645359557468, 94.70267363245236, 94.64505224339274, 94.74108789182544, 94.54133374308543, 94.54901659496005, 94.74877074370006, 94.67962507682851, 94.67194222495391, 94.67194222495391, 94.69883220651506, 94.8140749846343, 94.64121081745544, 94.7180393362016, 94.74492931776275, 94.78334357713584, 94.65273509526736, 94.5759065765212, 94.61816226183159, 94.61047940995698, 94.76413644744929, 94.79870928088506, 94.67962507682851, 94.69114935464044, 94.72572218807622, 94.60663798401967, 94.5720651505839, 94.75261216963737, 94.80255070682237, 94.70267363245236, 94.66041794714198, 94.66425937307929, 94.69114935464044, 94.72188076213891, 94.68730792870313, 94.55285802089736, 94.75645359557468, 94.64505224339274, 94.7180393362016, 94.53749231714812, 94.72572218807622, 94.65657652120467, 94.6258451137062, 94.61432083589429, 94.72188076213891, 94.77566072526122, 94.62968653964352, 94.66041794714198, 94.54901659496005, 94.6757836508912, 94.66425937307929, 94.55669944683467, 94.7679778733866, 94.74877074370006, 94.68346650276582, 94.63352796558083, 94.5759065765212, 94.61047940995698, 94.66425937307929, 94.71419791026429, 94.59895513214505, 94.61816226183159, 94.72956361401353, 94.7180393362016, 94.67194222495391, 94.68346650276582, 94.59127228027043, 94.76029502151198, 94.69499078057775, 94.66041794714198, 94.6681007990166, 94.69499078057775, 94.64889366933005, 94.67962507682851, 94.78718500307313, 94.67962507682851, 94.77950215119853, 94.6757836508912, 94.74492931776275, 94.84096496619546, 94.73340503995082, 94.72188076213891, 94.69883220651506, 94.61047940995698, 94.5720651505839, 94.72572218807622, 94.70651505838967, 94.6681007990166, 94.6258451137062, 94.64121081745544, 94.65657652120467, 94.48371235402581, 94.6757836508912, 94.73724646588813, 94.59895513214505, 94.70651505838967, 94.76029502151198, 94.75645359557468, 94.76413644744929, 94.6220036877689, 94.72188076213891, 94.7180393362016, 94.66425937307929, 94.6681007990166, 94.62968653964352, 94.71419791026429, 94.68346650276582, 94.69883220651506, 94.69114935464044, 94.70267363245236, 94.60663798401967, 94.67194222495391, 94.55285802089736, 94.59895513214505, 94.66041794714198, 94.61432083589429, 94.59895513214505, 94.73724646588813, 94.63352796558083, 94.73724646588813, 94.77566072526122, 94.60279655808236, 94.76413644744929, 94.74108789182544, 94.65273509526736, 94.58743085433314, 94.59511370620774, 94.80255070682237, 94.61047940995698, 94.71035648432698, 94.65273509526736, 94.6681007990166, 94.75261216963737, 94.72188076213891, 94.61816226183159, 94.65657652120467, 94.79870928088506, 94.64121081745544, 94.66425937307929, 94.69883220651506, 94.64889366933005, 94.83712354025815, 94.69883220651506, 94.71419791026429, 94.77181929932391, 94.63352796558083, 94.74492931776275, 94.69883220651506, 94.61816226183159, 94.73340503995082, 94.73724646588813, 94.67194222495391, 94.72956361401353, 94.69114935464044, 94.77181929932391, 94.65657652120467, 94.68346650276582, 94.60663798401967, 94.64889366933005, 94.72188076213891, 94.70651505838967, 94.70267363245236]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list_01 = [2.3849518770770977, 2.2417600981911345, 2.2415069635644516, 2.2409557133186153, 2.2419579268147953, 2.240125378942102, 2.240931454066662, 2.2417839348800785, 2.242252948807507, 2.241471426273749, 2.241231945472035, 2.2413479971691843, 2.241036834432504, 2.2407813569717616, 2.2415969093963706]\n",
        "train_acc_list_01 = [18.56855479089465, 18.617257808364215, 18.746426680783483, 18.60243515087348, 18.61937533086289, 18.886183165696135, 18.60243515087348, 18.598200105876124, 18.604552673372154, 18.740074113287452, 18.731604023292746, 18.814187400741133, 18.848067760719957, 18.82901005823187, 18.752779248279513]\n",
        "test_loss_list_01 = [2.2387831538331273, 2.241503697984359, 2.241926829020182, 2.240557459055209, 2.2406100852816713, 2.252836311564726, 2.2468298743752873, 2.2446437150824305, 2.2425780202828203, 2.240177970306546, 2.243702617346072, 2.2441832843948815, 2.253918958645241, 2.245230858232461, 2.2435202879064224]\n",
        "test_acc_list_01 = [18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 14.27858020897357, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463]\n",
        "train_loss_list_001 = [2.2888378896687414, 2.232192633274771, 1.4893088792236193, 0.5001831288098643, 0.3830500937251218, 0.32867970416539405, 0.29770232389774426, 0.27895135900919354, 0.2627035691970732, 0.24814978033950336, 0.23438045485475198, 0.2243682525668364, 0.21301924496848731, 0.20818865677811266, 0.1958482328166322]\n",
        "train_acc_list_001 = [18.50926416093171, 18.968766543144522, 47.80307040762308, 84.23292747485442, 88.15881418740074, 89.9142403388036, 91.04923239809423, 91.63790365272631, 92.08893594494441, 92.58020116463737, 93.03758602435151, 93.36791953414505, 93.715193223928, 93.84012705134992, 94.35044997353097]\n",
        "test_loss_list_001 = [2.2451091665847627, 2.2302937355695986, 0.684039752711268, 0.4374444575286379, 0.39873581839834943, 0.36398082489476485, 0.3313831433507742, 0.3210300371854329, 0.2847569804346445, 0.29315854806233854, 0.27853756525791157, 0.26896954926789973, 0.2596830692069203, 0.26028463620619446, 0.2491153629460171]\n",
        "test_acc_list_001 = [18.88060848186847, 19.053472649047325, 78.20759065765212, 86.33988936693301, 87.90334972341734, 88.90596189305471, 90.08143822987093, 90.81515058389674, 91.70636140135218, 91.54118008604794, 91.8638598647818, 92.2480024585126, 92.43239090350338, 92.50537799631223, 92.82037492317149]\n",
        "train_loss_list_0001 = [1.8565612323885041, 0.5532636212785715, 0.4003713336094285, 0.3402645644860539, 0.309145948165639, 0.2848975209968523, 0.262982070708501, 0.24855145600026216, 0.2386487888167221, 0.2278864942390098, 0.21327269485164788, 0.20556094100683686, 0.19517452138549268, 0.18913837452608395, 0.18232752032436653]\n",
        "train_acc_list_0001 = [34.7993647432504, 82.35468501852831, 87.65272631021705, 89.51614610905241, 90.6723133933298, 91.51932239280042, 92.26892535733192, 92.59925886712546, 92.97617787188989, 93.34674430915828, 93.69401799894123, 93.97353096876654, 94.31868713605083, 94.53255690841715, 94.71254632080466]\n",
        "test_loss_list_0001 = [1.0861167063315709, 0.46979708385233787, 0.41340532643245714, 0.3364271931350231, 0.3262409484561752, 0.34040282589986043, 0.28803076817854945, 0.2942232322678262, 0.2779932311169949, 0.27514038454083833, 0.2478603608906269, 0.2512748020463714, 0.26197264781769586, 0.2462065773194327, 0.24985540344142446]\n",
        "test_acc_list_0001 = [64.00583896742471, 85.58312845728334, 87.41933005531654, 89.81637984019667, 89.970036877689, 89.99308543331284, 91.59111862323294, 91.35295021511985, 91.82928703134604, 92.03672403196066, 92.76275353411187, 92.83574062692071, 92.90488629379226, 92.98555623847572, 92.87031346035648]\n",
        "train_loss_list_00001 = [1.2440541174192092, 0.4583125376927497, 0.36052036624613815, 0.3151768069603256, 0.2875107263081119, 0.26835051384883196, 0.25187577066948097, 0.23390740957767336, 0.22118305101950317, 0.21190600004299545, 0.20555683115350845, 0.1952596850249018, 0.18612936185546683, 0.18178928815090883, 0.17480877608181986]\n",
        "train_acc_list_00001 = [57.18581259925887, 85.53943885653786, 88.79618845950239, 90.25304393859184, 91.214399152991, 92.01058761249338, 92.36421386977237, 93.09899417681312, 93.43991529910005, 93.80412916887242, 93.86765484383271, 94.25092641609317, 94.500794070937, 94.64478560084702, 94.82477501323451]\n",
        "test_loss_list_00001 = [0.5935118007017117, 0.43328142947718207, 0.3668157114994292, 0.34575528556517526, 0.3590214093964474, 0.3134572241893586, 0.2903973020467104, 0.28062032824199573, 0.26905518266208034, 0.26447016406146917, 0.27132851287138227, 0.26421160440818936, 0.25491809147391836, 0.24584030433028353, 0.2630316608895858]\n",
        "test_acc_list_00001 = [80.93500307314075, 86.63567916410571, 88.97510755992624, 89.3438844499078, 88.99431468961278, 90.58466502765826, 91.54502151198525, 91.56422864167179, 92.02135832821143, 92.25184388444991, 91.97910264290104, 92.37861094038107, 92.72433927473878, 92.96250768285188, 92.59757221880763]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# train_loss_list_001 = [1.5218167792493924, 1.0340943300305083, 0.7934107637633911, 0.6561045421959874, 0.5800367868936862, 0.5100019117132925, 0.4574989944982072, 0.4208847186245476, 0.3881891872079228, 0.35861467820006054, 0.3339165490084944, 0.3111862796849717, 0.2948872684575498, 0.27018505394363557, 0.25883166084940823]\n",
        "# train_acc_list_001 = [44.0975, 63.055, 72.2075, 77.0575, 79.93, 82.43, 84.0575, 85.47, 86.5625, 87.675, 88.48, 89.16, 89.645, 90.5075, 90.9225]\n",
        "# test_loss_list_001 = [1.3665137306044373, 1.0809951907471766, 0.8369579171832604, 0.7580914105041118, 0.6663558483123779, 0.7236429437806334, 0.5709026199352892, 0.5137608224832559, 0.5140901245648348, 0.48385591937016836, 0.47491426897954336, 0.5022718974306614, 0.5243912806993798, 0.42129093069064466, 0.4074777747634091]\n",
        "# test_acc_list_001 = [52.13, 63.83, 71.47, 73.82, 77.34, 76.49, 80.87, 82.32, 82.77, 83.6, 84.17, 83.3, 83.55, 86.28, 86.86]\n",
        "# train_loss_list_01 = [1.8962965864723864, 1.472022865146113, 1.2269341696184664, 1.0348994015885618, 0.9050399440165144, 0.7892040218027255, 0.6947498847120486, 0.6181783355272616, 0.5774767158892208, 0.5502305545936377, 0.5206893583456167, 0.5026034010104097, 0.4774538204311944, 0.4683271567471111, 0.4492297477710742]\n",
        "# train_acc_list_01 = [31.045, 45.415, 55.22, 62.9525, 67.66, 72.095, 75.7825, 78.6325, 80.1775, 80.8325, 82.04, 82.7775, 83.69, 83.865, 84.6225]\n",
        "# test_loss_list_01 = [1.6557437422909314, 1.5662637073782426, 1.1888765285286722, 1.1435910184172136, 0.9932384845576708, 0.7845515208908275, 0.7405012536652481, 0.7023888849004915, 0.6914112718799447, 0.8937227891970284, 0.6744754122027868, 0.7125071339969393, 0.595223272148567, 0.6645651912387414, 0.5624623864511901]\n",
        "# test_acc_list_01 = [38.54, 45.39, 57.61, 59.19, 65.26, 72.6, 74.56, 76.24, 76.7, 71.16, 77.34, 75.59, 80.06, 77.85, 81.28]\n",
        "# train_loss_list_0001 = [1.6901012103016766, 1.282657652045972, 1.0717586383652002, 0.940055356619838, 0.8364242675205389, 0.7547609810821545, 0.6785155514749094, 0.6320573160061821, 0.5852954303875518, 0.5420240767466755, 0.506621429809747, 0.47925190327647393, 0.4483506758563435, 0.4286465794800189, 0.40272510780122717]\n",
        "# train_acc_list_0001 = [37.2, 53.36, 61.4225, 66.4725, 70.2, 73.2275, 76.08, 77.8025, 79.53, 80.905, 82.42, 83.1425, 84.5575, 85.195, 86.005]\n",
        "# test_loss_list_0001 = [1.4665760239468346, 1.2375031430510026, 1.0847761902628066, 1.0535234567485279, 0.8635394460038294, 0.757294207434111, 0.7295623666877988, 0.7312412850464447, 0.7336276015148887, 0.6307676260984396, 0.6266382736495778, 0.6370392079594769, 0.5392829055273081, 0.5410988666588747, 0.5530912065053288]\n",
        "# test_acc_list_0001 = [46.98, 55.76, 61.4, 63.25, 69.13, 73.23, 74.32, 74.64, 73.96, 78.38, 78.25, 78.58, 81.0, 81.49, 81.32]\n",
        "\n",
        "train_loss_list_cut = [1.613359474907287, 1.186605121571416, 0.9613291156558564, 0.8175660327981455, 0.7329503829105974, 0.6675834229198127, 0.6155491586500844, 0.5636878716298186, 0.5260912411319562, 0.5027581219094249, 0.4726218120834698, 0.44723697268543916, 0.43208082353535554, 0.41164722000828946, 0.3974420052652542, 0.3826391176103403, 0.35894755928661115, 0.3507439031863746, 0.34107909901454425, 0.32801356564124173, 0.3124787288542373, 0.31465199137457645, 0.298767308029123, 0.28487236778766584, 0.27646335530966615, 0.27320462827103587, 0.26612033452184053, 0.26308306258993025, 0.2525483318411123, 0.24124891794146822, 0.24217433363389665, 0.23168825182004477, 0.23012469901730076, 0.21693348256162942, 0.21930161923074876, 0.21653421889669217, 0.21434410071125426, 0.20718723166579256, 0.2052558541250305, 0.19716152494041303, 0.1941497164983719, 0.19097941378339792, 0.18447992457939794, 0.18228636479701477, 0.17657887242948667, 0.17477260330043282, 0.1733962548212312, 0.17061091698825168, 0.16732201631219623, 0.1637549052556483, 0.1613706673700779, 0.16051728827075457, 0.15993363755389142, 0.15627589414770993, 0.15532852677158274, 0.15280420149858007, 0.15101519260353174, 0.14880275238341037, 0.13978494027742563, 0.14830744789002803, 0.14331960949463585, 0.13924681502409256, 0.13707038418601114, 0.1308897960062225, 0.13411890785581768, 0.13409768981627002, 0.13344513029217148, 0.1273199183205827, 0.1283481349103367, 0.12471981016924968, 0.1280518789260913, 0.12418704933394639, 0.12167530884139073, 0.12545658500430684, 0.11795977773234105, 0.11789289145423962, 0.12059933257202943, 0.11666790398355491, 0.11807365684558789, 0.11512304616931338, 0.11561917723082125, 0.11491954412323217, 0.11419005636661388, 0.10912806965624944, 0.11002509410198504, 0.11631911551466766, 0.10705401468129394, 0.1092875181331326, 0.10784903160942058, 0.10965043486069186, 0.10153051761511606, 0.1083350477579493, 0.10791226423467501, 0.10326221440665828, 0.10488084647149895, 0.100699621469925, 0.1017464189983595, 0.1026811115919782, 0.10191941613587327, 0.10146693645503384, 0.0997033108370944, 0.09700418662386961, 0.09839744385653221, 0.09619725261109706, 0.0990427272757307, 0.0953601595407096, 0.09479633424157342, 0.09373613773062588, 0.09360281742228486, 0.09450248972224161, 0.0905749333885531, 0.09483718122251499, 0.094004499824188, 0.09207623065731967, 0.0874123571696468, 0.08727556101073282, 0.09273509363444468, 0.08398670141106121, 0.08792692410964935, 0.08606394986732128, 0.08853992108648387, 0.08270674364302105, 0.08553103019539922, 0.08325290489799013, 0.08070171735109613, 0.08357380274028633, 0.0855067997409132, 0.07974059663760585, 0.07902837820208301, 0.07707989068862539, 0.08178071627101768, 0.07777025827025168, 0.0837722305065622, 0.07823536036232599, 0.07849429675731986, 0.08163950307229266, 0.0772132684176151, 0.0742708618630664, 0.07653025378839086, 0.07370652056361635, 0.07428970992767488, 0.066969591103637, 0.07278624822656377, 0.0711755252714022, 0.07434361697005007, 0.0714579665521606, 0.06873778960468194, 0.06797652607099317, 0.06636746319385763, 0.06754642966004035, 0.07100995908232448, 0.07219889727584757, 0.06691185421884631, 0.06648011294047768, 0.06790807842018125, 0.06884961091755583, 0.06649817791454994, 0.06520170254853015, 0.06067689375196116, 0.06284649541583685, 0.058577289441808726, 0.06126952098277859, 0.05914597977369357, 0.06332467258952487, 0.057384233588513474, 0.055878468872473455, 0.0626550735429691, 0.05836306477245241, 0.058223563505294985, 0.05526084442453358, 0.05660795328168633, 0.0533848489447238, 0.05570016410677863, 0.052253410154685806, 0.053411352781418224, 0.05460695667971913, 0.051183629400147417, 0.053861447934287425, 0.05485730031029152, 0.05130477134769146, 0.04791999825934014, 0.0483061204941128, 0.04967576256259895, 0.045770330684848676, 0.04519799896501028, 0.04884168341720161, 0.04494558678963742, 0.04718005882141689, 0.044031581079688506, 0.04476618649699865, 0.043659136812098494, 0.04084714540554145, 0.04083325983599399, 0.04264434854375026, 0.04191593292166297, 0.03981593097694004, 0.043085993813594785, 0.040142026240416705, 0.03974097740119353, 0.037398707843162474, 0.03773854231051268, 0.035977833013172256, 0.037191637455464936, 0.03843252851401631, 0.03596398131787991, 0.03615279636307123, 0.03480513299854038, 0.034845688106557623, 0.03309281391743273, 0.03143419835645075, 0.030938323252438643, 0.031188237175833397, 0.032819104747632485, 0.03191146987844437, 0.03148931583624702, 0.030030576116479815, 0.03224378252050843, 0.03121553240239787, 0.02776832960601063, 0.02880164314531528, 0.029192425741311222, 0.030026258470062107, 0.029695636324269085, 0.027793155418494687, 0.026486221334551828, 0.026275856554293976, 0.025177531725408646, 0.025483246410664278, 0.023590462400574986, 0.025644099438033356, 0.027068594341187146, 0.02301408553766771, 0.02522345676426047, 0.023035302803220865, 0.021253975071444418, 0.02299421675913869, 0.022151207279476424, 0.02280317421388119, 0.022020146825734655, 0.019744288622618865, 0.021754962591507946, 0.02058648679832431, 0.019481258098625193, 0.018746506298904102, 0.02082187346794223, 0.019484504032284973, 0.018540539403487676, 0.018026220258518744, 0.018277281947350635, 0.019090669363150937, 0.018498932709917426, 0.016855266360646357, 0.017087985687076854, 0.01755107378292555, 0.0162497674808287, 0.016687223960023624, 0.01735119057555918, 0.016141063884457057, 0.015963702109648276, 0.01607858874766638, 0.015593683267140184, 0.017362511261907843, 0.015086121411217502, 0.014080422949388076, 0.01572396905130198, 0.01647015061964409, 0.015150130807850569, 0.014522404045821688, 0.015314155997271045, 0.014406249093742798, 0.014474769565128028, 0.014898723138633151, 0.014346211947822056, 0.013767596740370836, 0.012941224452929376, 0.01388948843457376, 0.013728754877591856, 0.012941669198511817, 0.013923830428598122, 0.01301820403204475, 0.013493596776626622, 0.014167216677105608, 0.012224345361983505, 0.013615870727791479, 0.014401413207968916, 0.013055011124228137, 0.013154573026321495, 0.013493608308438295, 0.012507462098944587, 0.012766335765792492, 0.013181844661488367, 0.013105310207699982, 0.014206890060682409, 0.01374277858861005, 0.012588148265660475, 0.013434472534368141, 0.013608296886800576, 0.012433568900153517, 0.012731193249246266, 0.012850580789437429]\n",
        "train_acc_list_cut = [40.9, 57.2025, 65.81, 71.35, 74.33, 76.565, 78.5275, 80.4, 81.7075, 82.5125, 83.585, 84.51, 85.1225, 85.4725, 85.9375, 86.5425, 87.3725, 87.685, 88.0775, 88.5075, 89.195, 88.7475, 89.465, 89.89, 90.23, 90.4575, 90.6025, 90.64, 91.0475, 91.385, 91.485, 91.8375, 91.855, 92.455, 92.2575, 92.455, 92.4025, 92.6625, 92.92, 93.085, 93.19, 93.35, 93.635, 93.515, 93.77, 93.805, 93.8825, 93.9425, 94.15, 94.27, 94.3925, 94.3325, 94.4625, 94.545, 94.6175, 94.6675, 94.66, 94.7825, 95.1875, 94.845, 94.9125, 95.1, 95.2425, 95.45, 95.28, 95.2525, 95.3375, 95.6875, 95.58, 95.74, 95.56, 95.7275, 95.7375, 95.7325, 95.96, 96.0225, 95.8975, 95.9725, 95.9975, 96.035, 95.935, 96.0, 96.1025, 96.3225, 96.3075, 96.0575, 96.32, 96.3125, 96.3075, 96.26, 96.575, 96.3125, 96.3875, 96.495, 96.4125, 96.585, 96.49, 96.455, 96.4925, 96.4725, 96.6375, 96.6125, 96.715, 96.775, 96.675, 96.7725, 96.7825, 96.7825, 96.775, 96.835, 96.9425, 96.72, 96.7375, 96.8475, 97.035, 97.0625, 96.845, 97.2175, 97.06, 97.1575, 97.055, 97.2325, 97.125, 97.2375, 97.315, 97.2125, 97.1925, 97.2775, 97.3325, 97.45, 97.25, 97.395, 97.23, 97.3725, 97.355, 97.2325, 97.4625, 97.4875, 97.4075, 97.545, 97.5525, 97.7425, 97.545, 97.6125, 97.505, 97.635, 97.7125, 97.72, 97.8225, 97.78, 97.6175, 97.5975, 97.86, 97.7825, 97.7725, 97.7525, 97.8075, 97.7875, 97.9775, 97.96, 98.1425, 97.9675, 98.0225, 97.88, 98.1, 98.2375, 97.9275, 97.995, 98.0375, 98.2175, 98.12, 98.26, 98.185, 98.2625, 98.275, 98.23, 98.285, 98.2475, 98.195, 98.36, 98.48, 98.4025, 98.315, 98.4725, 98.5175, 98.4075, 98.5175, 98.46, 98.535, 98.48, 98.6225, 98.6675, 98.72, 98.5975, 98.6575, 98.68, 98.525, 98.7025, 98.715, 98.78, 98.8, 98.845, 98.7975, 98.75, 98.835, 98.8225, 98.8975, 98.9, 98.925, 99.045, 98.995, 99.0175, 99.0025, 98.97, 98.9425, 99.075, 98.945, 98.9975, 99.07, 99.0825, 99.0525, 99.0625, 99.0075, 99.085, 99.2075, 99.19, 99.2475, 99.1775, 99.265, 99.1775, 99.1425, 99.255, 99.21, 99.215, 99.34, 99.245, 99.29, 99.28, 99.3, 99.3725, 99.3175, 99.3475, 99.4, 99.41, 99.2975, 99.4, 99.44, 99.45, 99.4325, 99.425, 99.4025, 99.4775, 99.44, 99.4275, 99.495, 99.4925, 99.4575, 99.485, 99.5075, 99.5, 99.5075, 99.425, 99.535, 99.5825, 99.4725, 99.5025, 99.5375, 99.545, 99.53, 99.58, 99.5525, 99.5425, 99.5775, 99.58, 99.635, 99.605, 99.59, 99.605, 99.6075, 99.65, 99.59, 99.58, 99.6125, 99.575, 99.5775, 99.59, 99.59, 99.585, 99.6075, 99.5975, 99.61, 99.635, 99.5625, 99.6, 99.6175, 99.5925, 99.6075, 99.6225, 99.5925, 99.615]\n",
        "test_loss_list_cut = [1.4791154242769073, 1.1658613644068754, 0.9872214477273482, 0.8314349047745331, 0.9944802085055581, 0.7577941908112055, 0.7259182379215579, 0.6789853301229356, 0.7040932129455518, 0.6710618870167793, 0.5932643009891992, 0.7190747709968422, 0.5915243693544895, 0.5677886009216309, 0.5548223072214972, 0.48938161738311187, 0.5485320487354375, 0.5049594323846358, 0.5024378439293632, 0.5253199633163742, 0.545006987037538, 0.49844539429568036, 0.4618456284456615, 0.47183522431156305, 0.45087659849396233, 0.5046738835075234, 0.4737275925618184, 0.517040255326259, 0.47144785334792316, 0.43287935023066365, 0.4623851370585116, 0.4191177538301371, 0.40598977762687055, 0.39758752437332007, 0.429098657414883, 0.49839673049842254, 0.4335803312214115, 0.4391304107406471, 0.4258851580604722, 0.4272765200349349, 0.41979367031326775, 0.3996517728023891, 0.39644326382799994, 0.4050762924966933, 0.37220431883123856, 0.41266086810751806, 0.4058349334363696, 0.40598829731911046, 0.4262330528301529, 0.4142030260608166, 0.35964680000951016, 0.3874821408262736, 0.3994222947313816, 0.4099999762411359, 0.48654232843767237, 0.40172827545600603, 0.3953292958344085, 0.4276507317642622, 0.37284702835958217, 0.3759660202113888, 0.39387373701681067, 0.3743562951118131, 0.3970871818593786, 0.41597020795828177, 0.3505105532800095, 0.3577760239190693, 0.3745738009486017, 0.36575221789034107, 0.3922628566056867, 0.3804579684628716, 0.3428110468991195, 0.38194986197012887, 0.35429555196550827, 0.361780740603616, 0.3707258914467655, 0.3518540302786646, 0.3650943885875654, 0.3918004062356828, 0.3596177604756778, 0.37847324338140365, 0.40541056722779817, 0.39005918272688417, 0.3564605245107337, 0.36085661019705517, 0.41434526443481445, 0.3446986880860751, 0.3807661754043796, 0.3560769367444364, 0.3401542149389846, 0.33212360739707947, 0.34799107928064804, 0.40249927549422543, 0.3623131317428396, 0.38307136523572705, 0.3425910604905479, 0.3611237068153635, 0.37625472587120684, 0.3761880048845388, 0.3633357567500465, 0.32559320141997516, 0.3548705181743525, 0.37774981407425073, 0.33313915548445305, 0.3662335353938839, 0.34161262233046036, 0.33970868606356125, 0.33961755310810066, 0.35591573598264137, 0.36459438993206505, 0.34952369747282586, 0.3646538981908484, 0.3669853182155875, 0.346222849586342, 0.3648104143293598, 0.35329970227012153, 0.3857896882521955, 0.3397268560491031, 0.3996561256390584, 0.344681443670128, 0.35413239726537393, 0.3316430355174632, 0.38716104249410993, 0.32942197028594683, 0.35217226400405544, 0.37594665285152723, 0.34076649818239335, 0.3449836048898818, 0.3587528810470919, 0.3401296163284326, 0.3270592946983591, 0.32152744599535493, 0.3485121142260636, 0.33158980649483355, 0.35384547361467455, 0.3337541961971718, 0.31837486522861674, 0.32609030859002586, 0.35300223284129856, 0.31153832630643363, 0.331745189390605, 0.31228505301324627, 0.32915962798685966, 0.307383563322357, 0.3250211654584619, 0.32271280979053885, 0.3312963891444327, 0.3233849070494688, 0.3239633294789097, 0.3277184535996823, 0.3104742332538472, 0.32017407990709135, 0.32736679232573207, 0.31899695456782473, 0.29834681513566, 0.3152913172033769, 0.3275818183452268, 0.33088456924203075, 0.3209677238630343, 0.32485942636864096, 0.3357626995708369, 0.3363142457755306, 0.3152533290506918, 0.3161260941171948, 0.3332944193595572, 0.3038376487697227, 0.34032498092591007, 0.31091189195838154, 0.31926076189626623, 0.31000442501110365, 0.3010354260855083, 0.30018055627617657, 0.3127445231510114, 0.30238260064698474, 0.3182603884724122, 0.31473101213385785, 0.29884613908921615, 0.31703367735011667, 0.2913209448886823, 0.2881702622280845, 0.31681924888604807, 0.29637292385855807, 0.2983416555614411, 0.29558402111258686, 0.29196765764227395, 0.3100640736048735, 0.31831720773177813, 0.29677640637264974, 0.30746520772764957, 0.2865563590503946, 0.30253688547807406, 0.28961686959749533, 0.29738174973032144, 0.29680030708071553, 0.29409151684634294, 0.2892140953601161, 0.3005977722851536, 0.2880074861872045, 0.27800113717211955, 0.2859659475993507, 0.2899065754270252, 0.28989569729642023, 0.29461942848902717, 0.27184448862754845, 0.2677759771482854, 0.2699777761214896, 0.3000830202540265, 0.2926526506301723, 0.2815253124395503, 0.2683991437094121, 0.2708052956982504, 0.2659032362737233, 0.29111536092396023, 0.27172746575331386, 0.2721299055633666, 0.27334422420097304, 0.27823255380874945, 0.2846702391022368, 0.26914313680763485, 0.2824036598299878, 0.26155152982926066, 0.28833567049307157, 0.2757935132595557, 0.28530820940114276, 0.26061007040965406, 0.2689024790932861, 0.2690693513502049, 0.2623352914859977, 0.26749783254499676, 0.2726406627629377, 0.2707079539570627, 0.2750299688947352, 0.2752766144237941, 0.26105580568502224, 0.2643242926348614, 0.2672247627302061, 0.27170711321921287, 0.28075962617427486, 0.25020905593528026, 0.24921881190583675, 0.26484523855055436, 0.2614167183637619, 0.27086676525164255, 0.2653467998474459, 0.25267240150442605, 0.2534334840653818, 0.26007933452536786, 0.2619169997640803, 0.2546714186857018, 0.2537678420732293, 0.27184398217669015, 0.24711663049610355, 0.26054487415129624, 0.2526520066246202, 0.2480248349565494, 0.24464330218638045, 0.2543100225208681, 0.2467851509587674, 0.2543815068806274, 0.2550846805112271, 0.24686935845809646, 0.25780645163753363, 0.24647484842357756, 0.2498779943849467, 0.24236657725104802, 0.25234464791756644, 0.23720906674861908, 0.2444623167378993, 0.24947976245533063, 0.24974906840656377, 0.24106483623574052, 0.24051216914306714, 0.24444433122496062, 0.240749250955974, 0.24741494844231424, 0.24492908486082585, 0.24239608881217015, 0.24886162715810764, 0.24737394035239763, 0.24470583080679556, 0.23975468323200563, 0.25973454749659647, 0.24880992423129988, 0.25671024592239644, 0.2394170629072793, 0.2508298989526833, 0.25579182813061946, 0.23148325047915494, 0.2473917234736153, 0.23667787901962858, 0.24764571838741062, 0.23395111053427564, 0.23682133947746664, 0.2453216223777095, 0.2395489558200293, 0.24663213140602352, 0.2379017510934721, 0.24195825148232375, 0.23623901610321638, 0.24765730121090443, 0.24364126761314236]\n",
        "test_acc_list_cut = [47.45, 60.0, 65.86, 71.15, 68.02, 74.04, 74.75, 76.5, 76.38, 77.35, 79.87, 75.79, 80.02, 80.57, 81.27, 83.4, 81.97, 82.87, 83.07, 82.72, 81.44, 83.43, 84.51, 84.41, 84.85, 83.96, 84.21, 82.61, 85.09, 85.42, 85.35, 86.23, 86.96, 86.92, 85.98, 84.21, 85.86, 85.85, 85.91, 86.93, 86.74, 86.97, 87.32, 87.19, 87.67, 86.55, 86.81, 87.37, 86.84, 87.3, 88.46, 87.92, 87.66, 87.19, 85.59, 87.67, 87.69, 86.81, 88.24, 88.24, 87.26, 88.01, 87.61, 87.88, 88.96, 88.76, 88.97, 88.92, 87.96, 88.36, 88.67, 87.88, 88.96, 88.67, 88.46, 88.99, 88.87, 88.21, 88.92, 88.53, 88.17, 88.26, 89.01, 88.57, 87.5, 89.68, 88.57, 88.93, 89.45, 89.58, 89.15, 88.23, 88.64, 88.35, 89.49, 89.34, 89.23, 88.47, 88.78, 89.82, 89.47, 89.04, 89.77, 88.77, 89.85, 89.69, 89.79, 89.18, 89.03, 89.57, 89.04, 89.3, 89.43, 89.17, 89.25, 88.69, 89.73, 88.08, 89.49, 89.59, 90.02, 88.41, 89.94, 89.77, 88.87, 89.91, 89.6, 89.29, 90.01, 90.18, 90.39, 89.52, 90.2, 89.82, 89.98, 90.55, 90.27, 89.84, 90.81, 90.16, 90.58, 90.17, 91.06, 90.49, 90.02, 90.36, 90.15, 90.41, 90.29, 91.05, 90.46, 90.37, 90.3, 91.15, 90.77, 90.49, 90.34, 90.44, 90.56, 90.45, 90.12, 90.36, 91.18, 90.31, 91.04, 90.24, 90.89, 90.56, 91.21, 91.14, 90.88, 90.8, 90.98, 90.69, 90.8, 91.24, 90.76, 91.31, 91.56, 90.68, 90.91, 91.29, 91.61, 91.43, 91.23, 90.98, 91.61, 91.17, 91.7, 91.2, 92.06, 91.39, 91.26, 91.56, 91.42, 91.42, 91.78, 91.94, 91.69, 91.44, 92.0, 91.36, 91.88, 92.07, 92.17, 91.33, 91.74, 91.8, 92.13, 92.09, 92.31, 91.55, 92.01, 92.26, 92.4, 91.8, 91.83, 92.3, 92.02, 92.56, 92.16, 92.24, 91.79, 92.59, 92.35, 92.7, 92.71, 92.45, 92.55, 92.34, 92.02, 92.56, 92.65, 92.35, 92.53, 92.23, 92.47, 92.65, 92.84, 92.75, 92.85, 92.58, 92.62, 92.95, 92.94, 92.61, 92.87, 92.93, 92.82, 92.37, 92.94, 92.82, 92.81, 93.03, 93.0, 92.85, 93.13, 92.88, 92.75, 93.02, 92.82, 93.14, 92.91, 93.07, 92.55, 93.2, 93.19, 93.02, 93.01, 93.03, 93.35, 93.0, 93.14, 92.89, 93.14, 93.2, 93.12, 93.15, 93.17, 93.22, 93.14, 93.1, 93.0, 93.72, 92.96, 92.94, 93.39, 92.96, 93.36, 93.22, 93.51, 93.28, 93.27, 93.3, 93.29, 93.18, 93.24, 93.64, 93.27, 93.29]\n",
        "train_loss_list_wd5e4 = [1.5027774480965952, 1.0276084239490497, 0.7975675012356938, 0.6686166104988549, 0.5772798815474343, 0.5116038408142309, 0.461431055594557, 0.4285996074493701, 0.3872021212459753, 0.36954540337998265, 0.33349453384122146, 0.3191225348760526, 0.29570154136362164, 0.2778870724736692, 0.26335706169041584, 0.2490621807809455, 0.23208605314786443, 0.22031858939522753, 0.20280856089279675, 0.1948807320465295, 0.18225760984058959, 0.17901248411058238, 0.16944760515000493, 0.15725475339034495, 0.15281064094255525, 0.14829571347552747, 0.1351967308396539, 0.1339439639982324, 0.12818257354747373, 0.1161808082328056, 0.11971007128612111, 0.11157218916728474, 0.11184166256969158, 0.1071621371736637, 0.09870970028396041, 0.09892762057221355, 0.0908012348063552, 0.08706318784933596, 0.08129636609492401, 0.0844175164608624, 0.07711375228608378, 0.0727171496318552, 0.07881306375439365, 0.06988748621207456, 0.06977271015187517, 0.07213704411785443, 0.06465452981178467, 0.059860886116259206, 0.06565443985164165, 0.055741984914904966, 0.05937281503273656, 0.054797398726851604, 0.04783525239057339, 0.057667528927778475, 0.0519976342906253, 0.05075258484479195, 0.0479780873997857, 0.051437813953047216, 0.044275228567897514, 0.05218542266202668, 0.047849885717677045, 0.04727344876809861, 0.04353566757275369, 0.05274154159564751, 0.03900954714669778, 0.038132579750217756, 0.0438798694252468, 0.04052391117319655, 0.0444194049285814, 0.03921739171041896, 0.04322019030456059, 0.03981477512677495, 0.04147126957232627, 0.03451747147622295, 0.03873391680645581, 0.03558630044438159, 0.03961444919863448, 0.03973888016243379, 0.03753592373844915, 0.03485113727642943, 0.036234460864811184, 0.03558692057875875, 0.03258687717030747, 0.03053688010224662, 0.03501797043614256, 0.02845712277083137, 0.03135093330991821, 0.03356501266074638, 0.03350056209170019, 0.03376623985473626, 0.0308699009333032, 0.02893257925968272, 0.027391545217150984, 0.0286959023021471, 0.030725791665781942, 0.026020749267517997, 0.03082717279829204, 0.02599965874775173, 0.03394242989184995, 0.03584211132750986, 0.032864981473753815, 0.03261832850271116, 0.03152383655345383, 0.028458409716550725, 0.0237263294655425, 0.02717581400335335, 0.028603503928362084, 0.02477863742285572, 0.026328867512603348, 0.02738554204796283, 0.024270188131818946, 0.02639033293170027, 0.028431153394275915, 0.0267944870994221, 0.022552848333112014, 0.027474652415665147, 0.022996818839450376, 0.02170721744931044, 0.024480172923168005, 0.025365150559651918, 0.02576148252154644, 0.024583259466499946, 0.026209578144188506, 0.022519710091368172, 0.022190634743682446, 0.017703767737207082, 0.019398964044396966, 0.02643031852926833, 0.02380849324953787, 0.02303797679733092, 0.015461873901173318, 0.016369462731820994, 0.023921750444984333, 0.024145581647188375, 0.024439221988923062, 0.021081448698151536, 0.019269814025595806, 0.01770727836546271, 0.015750449192457307, 0.018357921256341586, 0.016851160442084668, 0.016779405207331545, 0.01922887265860749, 0.01491048913590384, 0.017494613310685175, 0.020450935306655357, 0.019253915843045036, 0.01861391122789143, 0.017561238870908397, 0.01683490170342937, 0.012796001498417827, 0.016780197644567552, 0.014076976357370067, 0.0157260496032457, 0.016004126569547784, 0.017361198594353307, 0.01750588047760018, 0.014100992143118439, 0.012314881729229857, 0.008250951137283621, 0.013473853790245878, 0.010151084203766985, 0.009842516266005918, 0.010620841719269657, 0.009969146477769667, 0.011355855877097613, 0.007406337388096372, 0.008986727931512931, 0.009986997959159387, 0.009125781858848712, 0.007472109663621567, 0.008835015864224551, 0.004292508458737998, 0.005812553818874692, 0.00677099364538924, 0.0067322269050826946, 0.004625315853274061, 0.005215383676132158, 0.006344635733596671, 0.005187604376992669, 0.003839952308653345, 0.003965519353213271, 0.002669242659610467, 0.002117635275329502, 0.002775355486739308, 0.00265736014845546, 0.0022718249013076145, 0.0020042899910497447, 0.0016117119838441976, 0.0014573458997613063, 0.001228732236187917, 0.0012267742422409356, 0.0014500122589130586, 0.0014915024765362493, 0.0012808567260394986, 0.0011165388441382767, 0.0012906825464864531, 0.001357362313477245, 0.0012554813951131897, 0.0011106275753623928, 0.0011766462907799112, 0.0011652772541354281, 0.0014127067940387006, 0.0012243740158691145, 0.0011723753179798421, 0.0012344509843341149, 0.0012428820435963451, 0.0011997493186641259, 0.001336276902967451, 0.0011452813391029217, 0.0011089205116661378, 0.001165861148571673, 0.0012621701129313452, 0.0011624022613475902, 0.0012330106737020725, 0.0011809163343030425, 0.001101714661181235, 0.0011684441624340205, 0.0011685377142329615, 0.0012322912058129478, 0.0011872104088293787, 0.001143343054307119, 0.0011744529526597394, 0.001203257890125088, 0.00115107328663714, 0.0011390011661653273, 0.0012292457904782706, 0.0011242386569215443, 0.0011883258320072802, 0.0011454876748467097, 0.001156560716423066, 0.0011528850767014627, 0.001165562277152456, 0.0011236844305395365, 0.0011447015948914967, 0.0012182490931773268, 0.0011218772364628559, 0.001162500955319157, 0.0011594401151403047, 0.001198594366161587, 0.0011824657069370388, 0.001172395464897263, 0.0012272381267998333, 0.001153445542647173, 0.0012023703809961462, 0.0011459320135334262, 0.0012018852982485589, 0.0011949562754023809, 0.0011886567682230149, 0.0011700176551697043, 0.0011815206049539792, 0.0011973182879798947, 0.0012122210003242206, 0.0011632622707971392, 0.0011874910569734894, 0.0011854940763534828, 0.0011964635884451765, 0.0011862511147771734, 0.001194577806401617, 0.0012562638589407142, 0.0012219592303293534, 0.0011857493995159221, 0.0011942158280217204, 0.0011750017043601828, 0.001190489213080547, 0.0011951297786780082, 0.0011906062601321635, 0.0011730845402048442, 0.0012332784984939206, 0.0012139175384760664, 0.0011773592329021698, 0.0012054313082075394, 0.0011924652807851926, 0.0012579563214404942, 0.001199958441122926, 0.0012173140406632386, 0.0012009810876122083, 0.001210526931255806, 0.0011779946152689143, 0.0011719051533164427, 0.001181355180083432, 0.0011896568855580191, 0.0012324937115670345, 0.0011941396765940534, 0.0011866521274376386, 0.0011868935034494287, 0.0011929624456207093, 0.0012042453618922506, 0.001203572630105665, 0.001169494700063163, 0.0012129276695946893, 0.0012054597970452337, 0.0012071873559750402, 0.0011781047958621797, 0.0011885599540055584, 0.0011778895794518126, 0.0011938168243624079, 0.0011808463227607833, 0.0012280858834437763, 0.001201700613523027]\n",
        "train_acc_list_wd5e4 = [44.9675, 63.1875, 71.8725, 76.5, 79.885, 82.1125, 83.93, 85.2275, 86.5525, 87.015, 88.4975, 88.8775, 89.675, 90.22, 90.8925, 91.28, 91.83, 92.26, 92.8925, 93.18, 93.5725, 93.6975, 94.0725, 94.48, 94.7275, 94.8025, 95.185, 95.255, 95.445, 96.0025, 95.7275, 96.06, 96.1225, 96.2175, 96.5175, 96.4475, 96.815, 96.9275, 97.2, 97.03, 97.3525, 97.5325, 97.2775, 97.6, 97.6325, 97.4725, 97.8175, 97.9575, 97.6775, 98.13, 97.9825, 98.115, 98.465, 98.025, 98.2125, 98.315, 98.395, 98.265, 98.535, 98.2375, 98.3425, 98.4175, 98.59, 98.165, 98.775, 98.7575, 98.4825, 98.6875, 98.425, 98.76, 98.555, 98.6875, 98.565, 98.86, 98.715, 98.8725, 98.665, 98.645, 98.765, 98.8425, 98.845, 98.8475, 98.95, 99.0225, 98.8175, 99.0825, 98.97, 98.91, 98.885, 98.87, 98.975, 99.0475, 99.1425, 99.065, 99.0175, 99.1325, 98.9975, 99.205, 98.9175, 98.85, 98.9625, 98.9, 99.02, 99.0675, 99.295, 99.1675, 99.08, 99.225, 99.1425, 99.1, 99.2525, 99.155, 99.0975, 99.165, 99.3, 99.1275, 99.3225, 99.3125, 99.245, 99.1725, 99.1575, 99.2175, 99.1825, 99.32, 99.29, 99.48, 99.44, 99.17, 99.25, 99.32, 99.5675, 99.5, 99.275, 99.2075, 99.2025, 99.3425, 99.4, 99.5125, 99.5125, 99.495, 99.4975, 99.4925, 99.4125, 99.565, 99.465, 99.365, 99.4025, 99.4375, 99.445, 99.4825, 99.645, 99.4825, 99.62, 99.5275, 99.53, 99.43, 99.485, 99.61, 99.6675, 99.785, 99.5775, 99.705, 99.725, 99.6975, 99.69, 99.705, 99.8125, 99.765, 99.725, 99.77, 99.81, 99.785, 99.92, 99.8575, 99.8125, 99.82, 99.9025, 99.885, 99.84, 99.88, 99.935, 99.93, 99.965, 99.9775, 99.9475, 99.9625, 99.955, 99.98, 99.9875, 99.9925, 100.0, 99.995, 99.985, 99.9925, 99.9975, 100.0, 99.99, 99.9925, 99.9975, 99.9975, 99.995, 99.9925, 99.9925, 99.995, 99.9975, 99.9975, 99.9975, 99.9975, 99.99, 100.0, 100.0, 100.0, 99.9925, 100.0, 99.995, 100.0, 100.0, 99.9975, 99.9975, 99.995, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 99.9975, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 99.9975, 100.0, 99.9975, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
        "test_loss_list_wd5e4 = [1.2136726432208773, 0.9851384208172183, 0.8816822442827346, 0.9342003608051734, 0.7205261165582681, 0.7135021192363545, 0.6295640034766137, 0.6222952057289172, 0.5453219817409033, 0.6333636732041081, 0.5167707955535454, 0.5517654562298255, 0.4426909998247895, 0.41966562520099593, 0.5765619779689403, 0.43953118633620347, 0.3954082934916774, 0.399195172552821, 0.3910487050874324, 0.38879678803908674, 0.4299736600133437, 0.38120186121403415, 0.4164479757788815, 0.3728801941947092, 0.42476696719097184, 0.395566382551495, 0.39108878091166294, 0.390310141670553, 0.4194243029703068, 0.3851487408333187, 0.43473647101015983, 0.3949222198770016, 0.43027733530424817, 0.46044455280032337, 0.4314384918801392, 0.402132544525062, 0.4030761315098292, 0.4495672079958493, 0.39670435857923725, 0.3508086872251728, 0.43815861830982983, 0.4294034951849829, 0.44646918698202204, 0.4012679084192348, 0.3856019262648836, 0.38729274404954306, 0.41598545373240603, 0.4074918993666202, 0.3850251690873617, 0.36644049479237084, 0.4081271881166893, 0.3885994453596163, 0.3874774380952497, 0.38053672147702566, 0.46447082222262515, 0.3927165820628782, 0.38685785233974457, 0.3861157114181337, 0.37443527598169785, 0.3745674545629115, 0.42922606705864774, 0.3947119893906992, 0.4066375641128685, 0.4015842970413498, 0.36048868430566183, 0.35823154091080534, 0.36034491677072983, 0.3813755610321142, 0.35023685667333726, 0.3729325658347033, 0.3793654409767706, 0.3758758772400361, 0.386005234491976, 0.3741716125720664, 0.38768857264820533, 0.4233778007800066, 0.3564016471935224, 0.37897610098500795, 0.40855275943309444, 0.37117944054211244, 0.4877219747138929, 0.32491220892230166, 0.3434305104273784, 0.3840078609653666, 0.3549162511584125, 0.3900517902419537, 0.4008802462227737, 0.37963688882845864, 0.41286554225260697, 0.3738790906116932, 0.39308812431519546, 0.40336327028425434, 0.3903807069681868, 0.3731421724527697, 0.36139429652992683, 0.36114918450011485, 0.3433755166545699, 0.33576442904864684, 0.38036759027951883, 0.389648184368882, 0.3580984584515608, 0.38816530191445653, 0.35682666716696343, 0.3467950366343124, 0.3625389872283875, 0.35996044842125496, 0.3454359777366059, 0.3356767878690852, 0.37700900737243365, 0.35454515490350846, 0.33726465683194656, 0.37725700768111625, 0.37593507747861404, 0.3295244945189621, 0.3762158106399488, 0.38346752543238144, 0.3287717970677569, 0.3532322215128548, 0.3622320907402642, 0.3615872695853439, 0.3733052865047998, 0.33632627308745927, 0.3659961585757099, 0.34503319652020176, 0.3413062731299219, 0.3364775993778736, 0.3670855132462103, 0.34390894257569615, 0.35701036585282675, 0.33068783996225914, 0.3194702649606934, 0.35311298242098166, 0.35698866938488394, 0.35237887407405466, 0.38091013099573834, 0.3747274860551086, 0.3419938689168495, 0.3380954069427297, 0.36431473842527295, 0.37605810872738876, 0.3960524099537089, 0.33081852295730685, 0.36062877370586877, 0.3480613913528527, 0.37270384611962715, 0.34597224412085137, 0.3434583605090274, 0.34766560349660586, 0.3527573428199261, 0.3291342975218085, 0.3357704000759728, 0.3537662517798098, 0.35035382720488534, 0.3609619809291031, 0.36449365525306027, 0.38716287873213806, 0.36449907418293287, 0.3543851946162272, 0.3296925408553474, 0.33976211521444444, 0.36568557687952546, 0.3350432302944268, 0.35040959508358677, 0.33734601575739775, 0.35390251950372625, 0.325316196189651, 0.3150268595995782, 0.34807058489775355, 0.34181052182294147, 0.30128647698254524, 0.3265819955098478, 0.3273799891901922, 0.3255926795964, 0.3264697471374198, 0.31853930699297145, 0.3249078211905081, 0.30419793300613573, 0.31979161029375053, 0.33804947467921653, 0.30469325590360014, 0.32201941085012653, 0.32308190551739707, 0.3179449903531165, 0.29340258193544194, 0.30763082790978347, 0.3099403906660744, 0.3032598841133751, 0.30206969182325316, 0.28878650833157044, 0.298862298172486, 0.2903934564394287, 0.2809895239298857, 0.2981501730936992, 0.2927116846925096, 0.2824138938626157, 0.28358515229406234, 0.2825708462844921, 0.29461616579490374, 0.2836763092988654, 0.28226735923863666, 0.2746177826878391, 0.27155414657502236, 0.2885775946174996, 0.27818745195488387, 0.27747765494675575, 0.28425973069064225, 0.27348717754777474, 0.27310062048933176, 0.2786915644626074, 0.2682647790429713, 0.27626944993492925, 0.28479830703780623, 0.27027927178748046, 0.2733822947344448, 0.2701373880233946, 0.2778020071549506, 0.2666197860542732, 0.26562596329405336, 0.2745143292070944, 0.27233143329997606, 0.26666624231051794, 0.2696480587407758, 0.2745003360736219, 0.27249939272863954, 0.27024260172738307, 0.2686373513144783, 0.27092717569085617, 0.2656354494392872, 0.26311222001721585, 0.26525769873133187, 0.26492436928085134, 0.2699424793731563, 0.2719281587419631, 0.27291557249389115, 0.2715623058095763, 0.26247314769255964, 0.25891310165200054, 0.27840734131728545, 0.27305792734215534, 0.2638986749645275, 0.26656815414375895, 0.2615214324544502, 0.2578285336494446, 0.27850422602665575, 0.2716222282829164, 0.2712610442238518, 0.2711411705594274, 0.26657556308598457, 0.265236472498767, 0.2611617881663238, 0.27041642509306535, 0.25506745703235456, 0.2576812288806408, 0.2628553329577929, 0.2742771508195732, 0.26676151354478883, 0.26692218276896057, 0.27390673850910574, 0.2696215535360801, 0.2623596463022353, 0.2678805201585534, 0.2617303827140905, 0.2673761573254685, 0.2679485789016832, 0.26076228487529335, 0.26153764751138564, 0.2555880120283441, 0.2642273437939113, 0.25984900718248344, 0.26822723297378687, 0.2618812537645992, 0.26794138192376005, 0.2702119716832155, 0.27756816747633717, 0.2612695039450368, 0.2614873691072947, 0.2663590583148636, 0.25799272407459306, 0.2680966036417816, 0.27000968761836425, 0.2693696271015119, 0.25710588410685337, 0.26840231454447855, 0.2647050721924516, 0.2639507615019249, 0.27469288246541085, 0.2502636852898175, 0.26998129741677757, 0.2636978205623506, 0.2510043778279914, 0.26851412955718706, 0.2697306358833102, 0.26649072681423985, 0.2556019111336032, 0.2660581706256806, 0.27122338370809074, 0.2661679719067827, 0.2716638084170939, 0.2666107994280284, 0.25817724770005746]\n",
        "test_acc_list_wd5e4 = [57.49, 65.32, 68.85, 69.25, 75.11, 76.47, 79.04, 78.91, 81.38, 79.78, 83.31, 82.27, 85.39, 86.04, 81.19, 85.57, 86.87, 86.8, 87.39, 87.89, 86.55, 87.98, 87.07, 88.42, 87.53, 87.54, 88.73, 88.01, 87.66, 88.41, 87.62, 88.47, 87.7, 87.31, 87.93, 88.33, 88.35, 87.92, 88.93, 89.82, 88.12, 87.97, 88.15, 89.02, 89.51, 89.51, 88.97, 88.91, 89.73, 90.22, 89.19, 89.69, 89.73, 89.78, 87.74, 89.45, 89.75, 89.88, 89.95, 90.06, 88.65, 89.62, 89.26, 89.31, 90.25, 90.69, 90.2, 90.24, 90.68, 90.65, 90.07, 89.97, 90.12, 90.12, 89.86, 88.49, 90.59, 90.09, 89.83, 90.61, 87.36, 91.49, 90.93, 90.05, 90.69, 89.85, 89.84, 90.3, 89.98, 90.2, 90.1, 89.68, 89.81, 90.13, 90.46, 90.27, 91.08, 91.19, 90.42, 89.93, 90.76, 90.14, 90.89, 90.66, 90.76, 91.03, 91.41, 90.93, 90.67, 90.6, 91.03, 90.24, 90.46, 91.57, 90.73, 90.03, 91.58, 90.88, 90.92, 90.68, 90.39, 91.06, 90.78, 91.26, 91.24, 91.34, 90.73, 90.8, 90.85, 91.48, 91.91, 91.43, 90.71, 90.95, 90.31, 90.52, 91.39, 91.55, 90.75, 90.81, 90.03, 91.94, 90.96, 91.39, 90.87, 91.48, 91.35, 91.49, 91.32, 91.69, 92.11, 91.03, 91.27, 91.16, 91.05, 90.7, 91.13, 91.46, 91.82, 91.81, 91.49, 91.99, 91.79, 91.73, 91.2, 91.93, 92.24, 91.43, 91.47, 92.96, 92.19, 92.09, 92.48, 92.02, 92.34, 92.2, 92.68, 92.39, 91.92, 92.75, 92.45, 92.44, 92.63, 93.15, 92.81, 92.81, 92.69, 92.93, 93.25, 92.81, 93.06, 93.3, 93.25, 93.18, 93.24, 93.27, 93.09, 92.97, 93.34, 93.48, 93.5, 93.69, 92.96, 93.63, 93.47, 93.2, 93.34, 93.33, 93.45, 93.63, 93.43, 93.45, 93.43, 93.28, 93.34, 93.46, 93.25, 93.5, 93.43, 93.59, 93.52, 93.6, 93.48, 93.48, 93.64, 93.39, 93.43, 93.53, 93.86, 93.52, 93.62, 93.52, 93.53, 93.39, 93.49, 93.6, 93.69, 93.23, 93.55, 93.52, 93.32, 93.61, 93.58, 93.53, 93.55, 93.62, 93.58, 93.59, 93.54, 93.48, 93.56, 93.71, 93.59, 93.5, 93.49, 93.58, 93.58, 93.58, 93.57, 93.58, 93.4, 93.52, 93.66, 93.54, 93.58, 93.84, 93.78, 93.32, 93.76, 93.65, 93.72, 93.33, 93.67, 93.51, 93.87, 93.69, 93.5, 93.81, 93.66, 93.74, 93.39, 93.78, 93.7, 93.6, 93.74, 93.79, 93.85, 93.54, 93.78, 93.83, 93.51, 93.47, 93.65, 93.63, 93.65, 93.52, 93.58, 93.48, 93.61, 93.69]\n",
        "train_loss_list_wd1e2 = [1.5167798984545868, 1.027339467034934, 0.8097865730047987, 0.719615878769384, 0.658255658305872, 0.615522557935014, 0.5982994875207115, 0.5846719085789336, 0.5773727887164289, 0.5627464520664641, 0.5591158762145728, 0.5449530482292175, 0.5340716247550976, 0.5340199959925569, 0.5181504983109788, 0.5176402507498622, 0.5080099205810803, 0.49660853408396055, 0.49530376631992695, 0.4915612660848295, 0.4967052273856946, 0.49324704406741327, 0.49331052558490646, 0.48590693801355817, 0.4841031406443721, 0.48545389605787237, 0.4772332070734554, 0.47386771064406386, 0.47569251241394506, 0.4783006565639386, 0.4702882940967243, 0.47064500009289945, 0.47294196457908555, 0.45738911800110305, 0.4716209914928046, 0.4719133836011917, 0.4696217343068351, 0.45638318650257853, 0.46723246879090136, 0.4585420537870913, 0.45578641851489154, 0.465418814565427, 0.4614397684415689, 0.45631032715590236, 0.4486660618370714, 0.4534833081804525, 0.45102608156280394, 0.4530490816782077, 0.45389505011585957, 0.45014332449093414, 0.4494515047096216, 0.450234164349949, 0.4440193151037533, 0.44662567982658413, 0.44600409516892114, 0.4423091958125178, 0.44665925180950106, 0.4360342098120302, 0.43951448350668715, 0.44023644362394804, 0.4460902175964258, 0.43867638192999475, 0.4361597361465612, 0.433228444367552, 0.43528716051921296, 0.4372344275061696, 0.4323961653838904, 0.43725441934201664, 0.43545952991555675, 0.42910580998792436, 0.43043560903674116, 0.43121091731059286, 0.42305412998024267, 0.42636741055086397, 0.4268225592831834, 0.42100020314748293, 0.41761867097391486, 0.4185335077702428, 0.42368483900452575, 0.41616832524442826, 0.41972924578494536, 0.410687822789049, 0.4157286212562372, 0.4089399065358189, 0.40666280982022085, 0.4132289454198112, 0.40205181121064454, 0.40999404700419395, 0.4086824649819932, 0.40080371218177074, 0.4015931721788626, 0.4063145552580349, 0.4066784158110999, 0.40118428522024674, 0.4009731699483463, 0.3969333416547257, 0.39313930387314133, 0.39997971186432213, 0.3931039036653293, 0.38969586989559685, 0.3916486538351534, 0.38877204070076016, 0.38579398788773595, 0.3810211131557489, 0.3882365185803118, 0.38886873219340756, 0.3846093181984874, 0.38404670505287547, 0.3830401858392234, 0.3767934866701833, 0.37473060204959907, 0.37977902538860187, 0.37377259620843223, 0.38002860750824496, 0.36324887853651383, 0.3618837351235338, 0.36737791051308566, 0.3669434030311176, 0.3633396982099302, 0.36406448107367506, 0.3577735069365547, 0.353955817536805, 0.3599519186888259, 0.35612359438270047, 0.35251178142552175, 0.35027061312343366, 0.3470285452497653, 0.3470985202934034, 0.3467607243468586, 0.33784743396047584, 0.34203178333207823, 0.3446982724312395, 0.3408664211678429, 0.3342146307420426, 0.33859410405920715, 0.3276973366737366, 0.33356578295794537, 0.33072001608415913, 0.32800227998735043, 0.32374877627855675, 0.32380120865643597, 0.323726001829385, 0.3161135405397263, 0.32177544154298193, 0.3120566315639514, 0.31642273844431, 0.308783637401395, 0.32058241168340557, 0.31076529688728505, 0.3038082039489533, 0.2967669633678354, 0.30553271728582654, 0.29508178397870294, 0.2989721709070876, 0.2943779766654816, 0.29553699583862536, 0.28911615298769344, 0.29165190915330147, 0.29362286717746966, 0.28382395979124136, 0.2868227225999101, 0.2803722090138414, 0.27424085373505236, 0.28322451868758036, 0.2771561364777172, 0.2667812052816629, 0.2687817446578044, 0.26123926229179856, 0.2610385168474703, 0.2666228299799819, 0.2595565085784315, 0.25669008474380445, 0.2462938559798006, 0.2528521096982514, 0.24889454671654837, 0.2559791601504, 0.24216721735347194, 0.2481234513056545, 0.2393635288356973, 0.23972950325891995, 0.23211169509461133, 0.23631990731905061, 0.2304189305621595, 0.22999336732366976, 0.2226063944994451, 0.22054641979475753, 0.21703065643771388, 0.21686043295140464, 0.21233341681024137, 0.21059229460577614, 0.20723726490911204, 0.20685803997345245, 0.20887376510868438, 0.1978470858531638, 0.2038871747569535, 0.19181662209974693, 0.19145933929057166, 0.1935519773167924, 0.19051632768334673, 0.18543817138614746, 0.17970925828995415, 0.18026368427105224, 0.17701446067410917, 0.17569915164773836, 0.16774458888049323, 0.17213442190862693, 0.1631373402100211, 0.16456650209407836, 0.16205769175062545, 0.1562980950569002, 0.1515140765891098, 0.15663365386545466, 0.15088544290857955, 0.14032981417382867, 0.14692522343783715, 0.14212213777981628, 0.1392525550894463, 0.13808176975947217, 0.12566476896071968, 0.13361202100642955, 0.1256047902753749, 0.1272383151462855, 0.11795103549957275, 0.11710383291966237, 0.11266100632782561, 0.11401164303191554, 0.106656764119197, 0.10556819721961173, 0.10439886295566925, 0.10726791070387386, 0.09460471522884246, 0.0962810536590628, 0.09549028507341592, 0.09219917137259112, 0.0865226152379768, 0.08575040581651008, 0.08219950793745419, 0.07716210633992387, 0.07188480715163219, 0.08065827630650692, 0.06764008765355847, 0.06986326664186324, 0.07430443038551, 0.06353453783609997, 0.06618601354523398, 0.057043209338721375, 0.051298089039782745, 0.054104091200870445, 0.04759680214352882, 0.053709484345187394, 0.046893905104396824, 0.043303748533224905, 0.04337597922228586, 0.0364828174844527, 0.035764870218956434, 0.03372384082918731, 0.03319300545718723, 0.03330101266193885, 0.03186584125978116, 0.027693336871199715, 0.026877068286648573, 0.026169767811561166, 0.0256406356053897, 0.025631253985456005, 0.024362976106401448, 0.023579948727553264, 0.02450076937556457, 0.023537636362611296, 0.02355496631786465, 0.023737981790504136, 0.022986546093330215, 0.023408689955695748, 0.023427624023332, 0.02295914088813261, 0.023163542187156768, 0.023356065636102003, 0.023212027953217584, 0.02309029637434231, 0.022946245367296586, 0.022692412935411587, 0.02339219701842378, 0.023122945366004784, 0.023124087244843522, 0.02343619195893169, 0.022982527528660365, 0.023213101919895163, 0.02317301971248735, 0.022974888207956245, 0.023228626388806503, 0.02310882277263049, 0.023166305644396014, 0.022929255026407518, 0.02320044601377778, 0.02337465347787633, 0.02341003445224069, 0.022785558921698566, 0.023203995150213423, 0.023422795285384494, 0.023028960588355414, 0.023342699217148863]\n",
        "train_acc_list_wd1e2 = [44.33, 63.2525, 71.6725, 75.2775, 77.445, 79.1175, 80.16, 80.8725, 80.74, 81.4475, 81.53, 82.1375, 82.65, 82.6, 82.96, 83.1775, 83.415, 83.7225, 83.915, 84.0275, 83.7325, 83.8675, 83.855, 84.1725, 84.09, 84.1675, 84.56, 84.5075, 84.6425, 84.4775, 84.74, 84.7025, 84.6975, 85.115, 84.7575, 84.705, 84.82, 85.3025, 84.7525, 85.0175, 85.3575, 84.9425, 85.02, 85.27, 85.5525, 85.28, 85.3825, 85.275, 85.2875, 85.52, 85.4475, 85.18, 85.4925, 85.525, 85.6125, 85.75, 85.4725, 85.7925, 85.8875, 85.5825, 85.7075, 85.8275, 86.0, 86.11, 85.95, 85.9425, 85.93, 85.89, 85.92, 86.29, 86.0525, 86.085, 86.41, 86.3275, 86.2675, 86.5075, 86.5475, 86.4525, 86.245, 86.705, 86.37, 86.7625, 86.705, 86.8875, 86.9225, 86.7075, 87.0625, 86.7475, 87.0175, 87.1175, 87.105, 86.8725, 87.09, 87.08, 87.1425, 87.405, 87.4725, 87.09, 87.4975, 87.53, 87.6025, 87.6175, 87.61, 87.685, 87.5425, 87.6175, 87.635, 87.8375, 87.78, 87.9375, 88.0275, 87.7925, 88.2425, 87.6675, 88.5325, 88.5275, 88.3, 88.4075, 88.42, 88.435, 88.6525, 88.795, 88.54, 88.7475, 88.7525, 88.77, 88.8375, 88.9225, 88.9325, 89.41, 89.1775, 89.045, 89.1575, 89.53, 89.3225, 89.855, 89.5225, 89.5275, 89.67, 89.66, 89.7775, 89.81, 90.0825, 89.9125, 90.125, 90.0025, 90.1125, 89.94, 90.265, 90.4675, 90.795, 90.4, 90.8425, 90.7075, 90.79, 90.6825, 91.02, 91.01, 90.935, 91.225, 90.995, 91.265, 91.47, 91.1175, 91.3675, 91.805, 91.6375, 91.945, 92.08, 91.76, 91.995, 92.1175, 92.48, 92.1875, 92.3625, 92.13, 92.595, 92.445, 92.71, 92.67, 93.005, 92.7375, 92.9675, 93.0725, 93.2, 93.335, 93.5175, 93.475, 93.62, 93.7325, 93.7275, 93.7925, 93.66, 94.175, 93.9525, 94.44, 94.3725, 94.22, 94.4575, 94.51, 94.8425, 94.695, 94.77, 94.87, 95.23, 94.9475, 95.2325, 95.23, 95.28, 95.5575, 95.725, 95.595, 95.7125, 96.0975, 95.9275, 95.985, 96.155, 96.23, 96.7125, 96.4, 96.5725, 96.5825, 96.8275, 96.9275, 97.0525, 96.9475, 97.2575, 97.295, 97.4575, 97.2975, 97.66, 97.6175, 97.5925, 97.81, 98.01, 97.9825, 98.08, 98.2975, 98.5225, 98.105, 98.595, 98.47, 98.3475, 98.6925, 98.59, 99.0025, 99.1525, 99.095, 99.27, 99.0025, 99.2475, 99.4225, 99.43, 99.6275, 99.635, 99.6975, 99.715, 99.7175, 99.7575, 99.885, 99.895, 99.9175, 99.925, 99.9225, 99.9675, 99.99, 99.955, 99.9825, 99.985, 99.98, 99.9875, 99.985, 99.9875, 99.995, 99.9925, 99.995, 99.9975, 99.9975, 99.9975, 99.9975, 99.995, 99.9975, 99.9975, 99.9975, 99.9925, 99.995, 99.995, 99.9975, 99.9925, 99.9975, 99.9975, 99.9925, 99.9925, 99.9975, 100.0, 99.99, 99.9975, 99.9975, 99.9925, 99.995]\n",
        "test_loss_list_wd1e2 = [1.3725743022146104, 1.1929742784439763, 0.9703254548809196, 0.8823626479016075, 0.7912342133401316, 0.7744240081762965, 0.9433422028263913, 0.698423402218879, 0.6534588306765013, 0.8720321919344649, 0.9694693926014478, 0.7136559109144573, 1.0214628554597687, 0.8677132740805421, 0.7085553053059156, 0.6858387778076944, 0.8705045671402654, 0.7853590740433222, 0.6123643494859526, 0.8236432565918451, 0.639409672987612, 0.7271271928956237, 0.6982333724257312, 0.8511822268932681, 0.8309068830707406, 0.8227003050755851, 0.7763343317599236, 1.2531145765811582, 0.7305624854715564, 0.7311744048625608, 0.9035976242415512, 0.6800222517568854, 0.5853997695295117, 0.7430345084093795, 0.8785871493665478, 1.108451329454591, 0.6588784128050261, 0.7044716858411137, 0.6049494954604137, 0.6143702955185613, 0.7461452868920339, 0.6528882425797137, 0.8694826870024959, 0.5893921456005, 0.6651792013192479, 0.6350515420678295, 0.6344684543488901, 0.6994746138777914, 0.5725251994555509, 0.6624086785920059, 0.9637637417527694, 0.8048486015464686, 0.6040883234030083, 0.7363305929340894, 0.6205344230313844, 0.6153643851793265, 0.7306718335875982, 0.6487590738489658, 0.7805856805813464, 0.6067438793333271, 0.6581050104732755, 0.6229219368741482, 0.5560055741026432, 0.6660232819333861, 0.7725009058095231, 0.6505244955231871, 0.5968554544297955, 0.7541794965538797, 0.6514349609990663, 0.7917481374137009, 0.6845550778545911, 0.7952898657774623, 0.6756763978849484, 0.7763081943687005, 0.8057050523878653, 0.7995553084566623, 0.6522585726991484, 0.5840945632397374, 0.6991102891632274, 0.6438415571104122, 0.5860964950126938, 0.635802041126203, 0.9848449245283876, 0.7002730075317093, 0.7162983300565164, 0.637317259100419, 1.0616072612472727, 0.5819362533997886, 0.6480541802659819, 0.7091145688974405, 0.532154082497464, 0.7218736040441296, 1.2941748115080822, 0.5242264851739135, 0.5975038005581385, 0.5866859227041655, 0.7909847196144394, 0.5758454048935371, 0.613415573971181, 0.6326792579662951, 0.6269164990775192, 0.5572914639605752, 0.5726913866362994, 0.6187993967080418, 0.6928256652023219, 0.5965810301183145, 0.6376756267457069, 0.5003311121011083, 0.5492256042323534, 0.601341659509683, 0.68496092286291, 0.5872152938118463, 0.5926056911673727, 0.5051948205579685, 0.5469717281528667, 0.7418177361729779, 0.7266407209106639, 0.7612239318557933, 0.4831244179719611, 0.6568866824801964, 0.7060362685330307, 0.693530101564866, 0.5567308860489085, 0.7726584718197207, 0.7781753985187675, 0.761501886422121, 0.6479162256928939, 0.7326241222363484, 0.5910413627383075, 0.7026853682119635, 0.7172112155564224, 0.6026144582259504, 0.5826918271523488, 0.5626278546037553, 0.49854521132722684, 0.6062672617314737, 0.5234693847125089, 0.5272691702540917, 0.6541247133967243, 0.42634944221641446, 0.638887640041641, 0.49163430740561664, 0.8650434718856329, 0.6591806264617776, 0.5614948665039449, 0.5188067898720126, 0.583303042982198, 0.5776442757135705, 0.7945110775247405, 0.4383143160162093, 0.5127309507961515, 0.48725567663772196, 0.5552749682830859, 0.5127951763098753, 0.5552909845792795, 0.6396092316017875, 0.6915487266039546, 0.5918025759202016, 0.6109406363360489, 0.49616090745865543, 0.4317665135935892, 0.5233679349663891, 0.46711384986020343, 0.5448861080634443, 0.705641579401644, 0.48266834171512457, 0.4491133007067668, 0.6736038417755803, 0.41231273925757106, 0.4804556973372834, 0.49244492793384986, 0.5156307778780973, 0.5698083575013317, 0.4579203664502011, 0.5245823471606532, 0.4540955895864511, 0.4462763463394551, 0.47661840538435346, 0.3877083959081505, 0.5359865875938271, 0.47837658622596835, 0.5976610659044, 0.4057789820281765, 0.4568231581132623, 0.5033613733852966, 0.4880643072007578, 0.421935570013674, 0.3864406771674941, 0.3924215535951566, 0.4215288765822785, 0.3856588805778117, 0.39591050393219235, 0.3960021594657174, 0.4162447214881076, 0.4317766793921024, 0.43716210089152374, 0.3752851269290417, 0.3684254226428044, 0.4046018751738947, 0.4570601512736912, 0.42681945275656785, 0.3557315610254867, 0.38338644142392314, 0.45626604632486273, 0.48571689672107937, 0.4053226277420792, 0.38042039244989806, 0.40194732573213454, 0.3663549528846258, 0.40040300088592723, 0.33492799480504626, 0.34661694832994966, 0.3733026383421089, 0.31432308200039444, 0.35983454105974755, 0.3449418092075783, 0.38464838134337076, 0.34258481332018403, 0.3416274746384802, 0.3719708046203927, 0.33954131207134153, 0.3303550594969641, 0.3256681533176688, 0.3663260734911206, 0.305223887479758, 0.3600897785229019, 0.359975767286518, 0.3134645165144643, 0.31159543180013005, 0.2859196728920635, 0.3370714645031132, 0.30697164241271685, 0.3175534585231467, 0.31721039870871776, 0.3941425471743451, 0.34323022165630435, 0.2986761056169679, 0.2926558808812612, 0.298173848963991, 0.29151669446426104, 0.3160338721509221, 0.2907146306920655, 0.29263262648748445, 0.27918298995193047, 0.2787177439166021, 0.27893129094869273, 0.27565936725350876, 0.2982580420337146, 0.32113587752550465, 0.28794968552604505, 0.2873940949764433, 0.2720843562031094, 0.255550967647305, 0.24216599573817435, 0.2502805740584301, 0.2626812101542195, 0.2440197715842271, 0.24417917901956582, 0.2448149986470802, 0.2319961440148233, 0.22655500405574147, 0.2284358123058005, 0.2258316146422036, 0.22443584173540526, 0.21244820348824126, 0.21419980961687957, 0.20566311539917054, 0.20899182690095297, 0.20759060931733891, 0.20997956381002558, 0.2037774644131902, 0.20164045950845827, 0.20583347683843178, 0.209160113353518, 0.2061407360000701, 0.1988746996827518, 0.2016011231307742, 0.20003076869098446, 0.20136920108070858, 0.20208889264849167, 0.19937377427763578, 0.20227546367464186, 0.19644954659139055, 0.2079336524386949, 0.19830586704649503, 0.2019951339763931, 0.2011260839202736, 0.1998056554341618, 0.20407455923813808, 0.20218830621695216, 0.20711066649307178, 0.20228053129549267, 0.1982952310126039, 0.1989742641018916, 0.20098775002775313, 0.19563924860727938, 0.20572516359860385, 0.19656268205446534, 0.19923276595677, 0.1936172448287282]\n",
        "test_acc_list_wd1e2 = [52.13, 58.0, 66.47, 68.48, 71.93, 73.38, 68.66, 76.35, 78.12, 70.27, 67.31, 76.04, 66.26, 71.09, 76.74, 77.08, 70.56, 74.21, 79.8, 73.72, 78.4, 76.32, 76.58, 71.8, 72.87, 73.02, 74.38, 58.91, 74.56, 75.75, 69.39, 78.21, 80.51, 75.74, 71.26, 65.74, 77.96, 76.81, 80.1, 79.77, 74.87, 77.76, 72.43, 80.66, 78.24, 79.29, 79.0, 77.18, 81.23, 77.41, 70.82, 74.14, 79.96, 75.72, 79.75, 80.63, 76.93, 78.66, 73.71, 79.9, 78.5, 79.77, 82.0, 78.63, 74.67, 77.94, 80.15, 75.57, 78.33, 74.1, 77.43, 74.34, 78.18, 75.35, 74.2, 73.88, 78.85, 80.44, 76.92, 78.07, 80.31, 78.92, 68.42, 77.39, 76.56, 79.35, 67.33, 80.82, 78.55, 77.35, 82.3, 77.25, 61.02, 82.77, 79.87, 81.04, 73.94, 81.59, 79.69, 78.21, 79.9, 81.84, 81.13, 80.0, 77.16, 80.57, 79.35, 83.74, 81.48, 80.41, 78.15, 80.5, 80.9, 83.64, 82.31, 75.42, 76.71, 75.99, 84.67, 79.01, 76.87, 77.9, 81.65, 74.49, 73.92, 75.98, 78.22, 76.52, 80.6, 77.42, 76.86, 80.4, 80.66, 81.63, 84.08, 80.38, 83.18, 82.73, 78.42, 86.07, 78.92, 84.8, 74.23, 78.45, 82.33, 83.06, 80.78, 81.68, 75.94, 85.72, 83.08, 84.02, 81.86, 83.38, 81.98, 79.64, 79.1, 80.78, 80.36, 83.85, 86.0, 82.75, 85.08, 82.98, 77.55, 84.19, 85.7, 77.95, 86.42, 84.35, 83.96, 82.9, 81.53, 85.4, 83.22, 85.75, 85.89, 84.67, 87.57, 83.16, 85.08, 81.54, 87.43, 85.54, 83.96, 84.48, 86.54, 87.69, 87.78, 86.26, 87.73, 87.31, 87.28, 86.94, 86.55, 86.06, 88.29, 88.32, 87.19, 85.53, 86.85, 89.14, 88.15, 85.86, 84.75, 87.47, 88.24, 87.6, 88.28, 87.78, 89.53, 89.36, 88.74, 90.36, 88.87, 89.5, 87.99, 89.19, 89.71, 88.6, 89.51, 90.06, 90.36, 88.83, 90.54, 89.04, 89.2, 90.63, 90.62, 91.59, 90.18, 90.87, 90.09, 90.93, 88.63, 89.71, 91.42, 91.72, 91.15, 91.61, 90.94, 91.63, 91.49, 92.14, 92.34, 92.5, 92.22, 91.41, 90.91, 92.27, 92.05, 92.77, 93.17, 93.48, 93.25, 92.79, 93.44, 93.49, 93.54, 93.83, 93.98, 93.84, 94.0, 94.24, 94.31, 94.42, 94.67, 94.34, 94.28, 94.61, 94.58, 94.66, 94.53, 94.5, 94.65, 94.62, 94.71, 94.73, 94.69, 94.61, 94.78, 94.59, 94.83, 94.68, 94.68, 94.76, 94.76, 94.77, 94.61, 94.75, 94.64, 94.51, 94.78, 94.81, 94.72, 94.81, 94.72, 94.92, 94.66, 94.96]\n",
        "train_loss_list_300const = [1.5072595094339536, 1.0231038059670323, 0.7837633473423723, 0.6642771163306678, 0.5742667439265754, 0.5122711343315843, 0.4693711296723673, 0.42363618176204326, 0.38468483099922207, 0.3563146945386649, 0.32989333312922775, 0.3088601354402475, 0.28732080238695723, 0.2773427833050204, 0.255741254994854, 0.2355493075502947, 0.22032112675829055, 0.20569888635660513, 0.19303267375348857, 0.18455393645710078, 0.1760997775430306, 0.1610161360388937, 0.15303532645915643, 0.14379756043132502, 0.13295495837998275, 0.12522198601414603, 0.12198948780425821, 0.10830174501949606, 0.1100872532187845, 0.1019453627054398, 0.10001775390876178, 0.0945176584318804, 0.09056858165552631, 0.08527834246905086, 0.07428395165243563, 0.073358194616894, 0.0680886913840763, 0.06761494047416094, 0.06709703737006972, 0.05811852629716023, 0.05694502567985473, 0.054410113292499285, 0.048847372924831635, 0.05006321483610061, 0.04818616545470521, 0.04477997207657074, 0.04385583504540518, 0.044935471408021524, 0.04291497548661245, 0.03747522448019955, 0.03720155920232304, 0.03781872331246603, 0.03907512170084106, 0.03808571144822426, 0.0337266662997155, 0.03255701906303652, 0.02515740589887356, 0.024542150663780255, 0.02793147737166276, 0.02593546934959928, 0.02561265908809492, 0.027039424086304994, 0.02195272859767639, 0.022481467933937037, 0.023805265842651288, 0.019805631663177174, 0.020343415882660094, 0.022202910110800743, 0.021919588551681024, 0.025648440866278835, 0.024350768256822405, 0.02123939903258099, 0.01514708600285948, 0.013058545996253482, 0.016306638918867626, 0.01685327520953289, 0.016574702121484014, 0.016640731977996603, 0.01489181877258933, 0.015860306949298176, 0.014099853219945703, 0.013995581963445438, 0.016321878165777286, 0.013422961495113753, 0.010429043487289373, 0.015911166273230783, 0.011775680992100685, 0.014917363881686935, 0.018574517294552517, 0.012787643851754746, 0.014939473080523192, 0.01176600590211186, 0.012982209259662748, 0.008858771788844534, 0.009820060386200072, 0.009505460298391053, 0.011240429890194399, 0.010675670454989353, 0.010158317344160542, 0.006973291840534449, 0.006880928409012844, 0.005811905768775168, 0.007408292713742807, 0.007451252999362914, 0.010354434164527717, 0.01092628369954178, 0.012062771040645908, 0.007832180905609038, 0.010386954512029251, 0.010358608426182522, 0.00795716502255174, 0.0063932851670936985, 0.006447436372195698, 0.004587642372207018, 0.007233876957458231, 0.007980881170325053, 0.010550779263485601, 0.012090449684582942, 0.009726078081678914, 0.00813099715300611, 0.006632155526443176, 0.00597371292983393, 0.004589174477389631, 0.003584162614903578, 0.005510094879151764, 0.007690454193111211, 0.006682628281595362, 0.005014813251454181, 0.005320725576812532, 0.007166650853292368, 0.007646939404855167, 0.0068286928395223144, 0.008263683283372191, 0.007336255545465137, 0.007655256824561777, 0.007321817508736698, 0.00431125137086996, 0.003226296991330126, 0.004760576777890367, 0.0063299430897483565, 0.004795740673675748, 0.0033784159969536777, 0.0035024787487254625, 0.005299788615376221, 0.003612780912293712, 0.0032062702902157835, 0.004884716233539565, 0.005524865538374664, 0.0067742976795348445, 0.00882896750121604, 0.005893147790118473, 0.00848282681003614, 0.007084418307778205, 0.0041311039470466075, 0.003654746470935981, 0.0033575089104424215, 0.0021545471597365076, 0.003389105463073925, 0.004264044310640164, 0.004854566317398292, 0.0023904619371291056, 0.002829549484107492, 0.002053513441469564, 0.002324710492194019, 0.0021624633618179965, 0.00212872851480438, 0.0023564651491906493, 0.003220773860256611, 0.002329243483711235, 0.0029083483295660978, 0.001973968918878853, 0.00209470545457433, 0.0020779970437868503, 0.0028365065618229167, 0.003357957487961407, 0.0030591108087931595, 0.002803605341979417, 0.0023873494445828206, 0.0023525519992746557, 0.002546498180350455, 0.0016468311933363184, 0.004213925738022168, 0.003313322557223379, 0.003231858396061877, 0.002629125782985243, 0.0029141006389791914, 0.003536610023096286, 0.004142602702752726, 0.005321672680347896, 0.005746499752380254, 0.0031256999963618884, 0.0032810929011951122, 0.0043974452997825285, 0.005239838635538117, 0.004797415686162696, 0.0032062243664227707, 0.003533768229311691, 0.0029152913773755726, 0.0041634554793835415, 0.0021060470147562338, 0.005407320675629354, 0.006459823155465985, 0.004450791821244248, 0.004911506875814143, 0.003201270101376409, 0.0062131848153570805, 0.003254528715726523, 0.003693553237162894, 0.00231178058878833, 0.002128929863538825, 0.0034310214809528308, 0.0021012531412071865, 0.0037686275719023775, 0.0026402074050687757, 0.0015023963813803039, 0.0016959904317550368, 0.0023601827229728445, 0.0016248808494453031, 0.001744074698825616, 0.0017098960009427912, 0.001745167300260726, 0.001709676564420346, 0.0014354304178802375, 0.0017766182493312826, 0.0017726632421619605, 0.005174195695061486, 0.00404562306744882, 0.0019462628384505156, 0.0021156957895648327, 0.003270384036677395, 0.004819947180601103, 0.004801801557994203, 0.003892041585816488, 0.003026418740098223, 0.0016867717583173738, 0.00243665445885507, 0.001924227454584904, 0.0032565365431517818, 0.0025015898762403913, 0.004080375838660543, 0.0034671720829839744, 0.002242059777764631, 0.00174158359747937, 0.0023047459024574768, 0.0018280270336568435, 0.003222634576515772, 0.002665815578360469, 0.002171202788719592, 0.004139460606425585, 0.005407126804135614, 0.003192231660575842, 0.0023893227233684547, 0.0017126825603354112, 0.0012934664354602478, 0.0024260497488599003, 0.0017768104415345862, 0.001707909157712362, 0.0025417737575718987, 0.0026618135311662788, 0.0013963878702470945, 0.001963627433890095, 0.0015268292372885813, 0.0010931521974983068, 0.0014422832873635296, 0.0014909466068760181, 0.0013153712134022825, 0.0010111237624123271, 0.0009827128673119666, 0.0005205708235784471, 0.0008319928409817137, 0.0007095568490024797, 0.00037128912167129697, 0.0006035467810239169, 0.0011062782132314122, 0.000675376708713361, 0.0006885260622773773, 0.00101789951790438, 0.0006217252514125575, 0.0007492425983336682, 0.0021772955004840527, 0.0016886424110349782, 0.0013806177742215066, 0.001364799225246333, 0.0013202676723717748, 0.0013149851532095538, 0.0010957021063876863, 0.0007910217270622839, 0.0011311705605573214, 0.000875898118397272, 0.0015793223269400525, 0.0013224604617411992, 0.0022564116427801696, 0.0026849516309372387, 0.0020301068258375223, 0.0006494859589330384, 0.0005210057817731313, 0.000972889763034617, 0.0010060088542230168, 0.0009481032836110347, 0.001107927744007216]\n",
        "train_acc_list_300const = [44.7525, 63.5, 72.3975, 76.605, 79.975, 82.355, 83.73, 85.3525, 86.61, 87.5825, 88.51, 89.1375, 90.01, 90.3875, 91.105, 91.8175, 92.1325, 92.8725, 93.125, 93.5025, 93.6875, 94.285, 94.705, 94.91, 95.275, 95.5425, 95.6975, 96.1225, 96.08, 96.3625, 96.4425, 96.6725, 96.75, 96.98, 97.385, 97.405, 97.635, 97.55, 97.615, 97.935, 97.9225, 98.115, 98.2575, 98.245, 98.31, 98.4675, 98.44, 98.46, 98.4925, 98.7, 98.6225, 98.6575, 98.6, 98.63, 98.765, 98.835, 99.135, 99.13, 99.0825, 99.0975, 99.1425, 99.085, 99.25, 99.21, 99.1575, 99.3575, 99.285, 99.2325, 99.23, 99.12, 99.155, 99.3075, 99.505, 99.57, 99.4, 99.43, 99.425, 99.4425, 99.495, 99.4975, 99.4875, 99.5125, 99.4175, 99.5525, 99.6225, 99.455, 99.59, 99.475, 99.355, 99.5525, 99.4975, 99.6075, 99.545, 99.7325, 99.655, 99.6675, 99.655, 99.63, 99.65, 99.7875, 99.8125, 99.82, 99.7425, 99.7475, 99.66, 99.6325, 99.55, 99.74, 99.685, 99.6475, 99.7425, 99.7775, 99.77, 99.86, 99.7625, 99.7475, 99.6, 99.5925, 99.68, 99.7275, 99.775, 99.8, 99.8475, 99.895, 99.82, 99.7325, 99.7725, 99.8375, 99.8025, 99.7625, 99.725, 99.7525, 99.705, 99.7675, 99.735, 99.72, 99.8675, 99.8975, 99.845, 99.7875, 99.8475, 99.8725, 99.8825, 99.8425, 99.865, 99.905, 99.8525, 99.805, 99.7725, 99.695, 99.8, 99.7325, 99.76, 99.8675, 99.8725, 99.9125, 99.9425, 99.9, 99.86, 99.83, 99.935, 99.8975, 99.9425, 99.9225, 99.9325, 99.9325, 99.9275, 99.8825, 99.9275, 99.8975, 99.9425, 99.92, 99.94, 99.8975, 99.895, 99.89, 99.9075, 99.9225, 99.9175, 99.91, 99.9525, 99.855, 99.885, 99.88, 99.915, 99.9, 99.8775, 99.8475, 99.8175, 99.7875, 99.905, 99.8825, 99.8775, 99.8225, 99.8675, 99.875, 99.89, 99.9025, 99.8725, 99.9275, 99.825, 99.77, 99.86, 99.815, 99.905, 99.78, 99.8975, 99.8875, 99.925, 99.9225, 99.8875, 99.9375, 99.8775, 99.91, 99.96, 99.9425, 99.9175, 99.9525, 99.945, 99.9425, 99.945, 99.935, 99.96, 99.9375, 99.9275, 99.8525, 99.8525, 99.945, 99.935, 99.8825, 99.8275, 99.825, 99.8725, 99.8875, 99.9425, 99.9125, 99.925, 99.89, 99.9175, 99.8725, 99.8975, 99.925, 99.9475, 99.9325, 99.93, 99.8975, 99.88, 99.925, 99.8575, 99.8025, 99.895, 99.9175, 99.9475, 99.955, 99.9225, 99.94, 99.9325, 99.92, 99.8875, 99.9625, 99.925, 99.95, 99.965, 99.955, 99.9625, 99.9675, 99.9725, 99.97, 99.9925, 99.9725, 99.9825, 99.9925, 99.9825, 99.9625, 99.9875, 99.9725, 99.9625, 99.985, 99.975, 99.9375, 99.9475, 99.9525, 99.9625, 99.965, 99.97, 99.97, 99.9775, 99.9675, 99.9775, 99.9525, 99.95, 99.9225, 99.905, 99.945, 99.985, 99.985, 99.9675, 99.98, 99.9725, 99.95]\n",
        "test_loss_list_300const = [1.3109248876571655, 0.9655934632578983, 0.8187996443313889, 0.689740173047102, 0.6772998514809186, 0.6437552631655826, 0.5553377157525171, 0.5520454826234262, 0.5120236824584913, 0.5249090968053552, 0.47987785124326054, 0.45728976138030425, 0.4652932938895648, 0.49931400976603546, 0.5139825019655349, 0.46358482577378235, 0.5389019496078733, 0.4064606526229955, 0.4374082180895383, 0.41569147004356866, 0.42417793334284914, 0.43346760355973546, 0.4166975364654879, 0.3910007622045807, 0.4190592248983021, 0.4890744652174696, 0.4130742766811878, 0.44733505535729323, 0.3960334837813921, 0.4205396575263784, 0.4506959509623202, 0.45510448968108697, 0.4337734672464902, 0.4140701989961576, 0.44726073873948446, 0.43756397483469567, 0.4081488196985631, 0.4404116733164727, 0.4069605218836024, 0.4365550541802298, 0.4524993513581119, 0.4810156839180596, 0.4475790313150309, 0.43388396841061266, 0.4388974361781833, 0.4250442619565167, 0.4161135846678215, 0.4599215022370785, 0.4951787300502198, 0.5083512184740622, 0.4711726530443264, 0.5019661293754095, 0.42351165593047685, 0.4634768115946009, 0.472742423415184, 0.4849290226267863, 0.4477674431061443, 0.48274370330043986, 0.4825481849757931, 0.4942950615777245, 0.5367305686202231, 0.4768067407834379, 0.4528346563441844, 0.49772302598892887, 0.43751826391944404, 0.4484408743019345, 0.5320610732217378, 0.47107771444547025, 0.4548007876058168, 0.48090932169292544, 0.47415826705437675, 0.44294909170911284, 0.408969871039632, 0.4602267291349701, 0.4937338570627985, 0.5272085793033431, 0.5100355125680754, 0.46345121894456165, 0.4728180213442332, 0.48767814587188674, 0.4734298460468461, 0.5140062019794802, 0.48423475234568875, 0.4522600043800813, 0.4885755015324943, 0.4898544026515152, 0.48496218667000157, 0.5125900219324269, 0.4700437818146959, 0.45802032928678055, 0.49178935455370554, 0.48068239813364005, 0.4772832633196553, 0.45595825010839897, 0.4741470496865767, 0.508940930796575, 0.47970542651188525, 0.5067461744139466, 0.47261701335635364, 0.4504903811442701, 0.4822925106634068, 0.4916953529360928, 0.4976545244078093, 0.5076064607010612, 0.512105936019481, 0.5002134619634363, 0.4895322826466983, 0.46702025679847864, 0.5068643896640102, 0.5748100429773331, 0.4924963390148139, 0.49676062770282164, 0.5234567566385752, 0.49199416045146654, 0.5275697438400003, 0.5010442611160157, 0.49138097276416004, 0.5251847050989731, 0.5231706929169123, 0.4994612190919586, 0.475140321481077, 0.4973598703553405, 0.46391360763507555, 0.4987307276718224, 0.5342934448507768, 0.490154218258737, 0.5101715593209749, 0.5079800128182278, 0.4876681245580504, 0.5047554036107245, 0.4933491258681575, 0.5017587872622888, 0.5059225696928894, 0.4965471655507631, 0.5347162768999233, 0.5293986288995682, 0.48946061645504796, 0.4993787820957884, 0.5046113696468028, 0.5203786311270315, 0.5163537305367144, 0.4821115889692608, 0.554601405617557, 0.5177212712130969, 0.5428616695011719, 0.5283017215094988, 0.5123567226566846, 0.5303934999281847, 0.5816303280335439, 0.5290483341941351, 0.5672795996069908, 0.579266353309909, 0.5199603319545335, 0.4895725855721703, 0.5205931299472157, 0.5044250222323816, 0.5172668089029155, 0.48294846157107174, 0.5366536256255983, 0.5250458570220803, 0.5189918728568886, 0.5009268411918532, 0.5045282197526738, 0.5237355313346356, 0.511520393098457, 0.525018301002587, 0.510031249515618, 0.5295086546411997, 0.4966339695302746, 0.52833208498321, 0.49447107315063477, 0.5136264913444277, 0.5089094453031504, 0.5464415133376664, 0.5187408701528476, 0.5190243262656128, 0.5516869642689258, 0.5397132114519047, 0.5154607352576678, 0.5146083532820774, 0.5073600661339639, 0.5701480022148241, 0.5735466538728038, 0.5491804146691214, 0.5422320174454134, 0.5446482123453406, 0.5426935783669918, 0.5710887145015258, 0.5220382987887044, 0.5417104106915148, 0.5754493603223487, 0.533619965744924, 0.5762778842185117, 0.5338592627380467, 0.5573781338296359, 0.5375887101775483, 0.5312822029183183, 0.5451974290647085, 0.5270344963933848, 0.5519347873669637, 0.5583887022884586, 0.5747697632524031, 0.53732075910025, 0.544713698799097, 0.54281858609447, 0.5719384611407413, 0.5202449362111997, 0.5084290615742719, 0.5141259058366848, 0.5029577503475962, 0.5330851511864723, 0.5383445264040669, 0.5445871983147874, 0.5439003353254704, 0.5215786695480347, 0.5109690418349037, 0.530744302310521, 0.49466055395859704, 0.5258592778368841, 0.5579611717522899, 0.5201689512292041, 0.5321517630091196, 0.5553653376011909, 0.5559994182254695, 0.5492764958475209, 0.5437155616811559, 0.5355881104552294, 0.5277321902634222, 0.539707180819934, 0.5771049804325346, 0.5852305162933809, 0.5669170301171798, 0.5253017652261106, 0.5338942474579509, 0.5554900363653521, 0.5405757495119602, 0.5658218766315074, 0.5613152086734772, 0.5465129902468452, 0.5272626559945601, 0.5295005417134189, 0.5047299167777919, 0.49300466383559793, 0.5492192791609825, 0.553901323977905, 0.5375248759607726, 0.5450990347168113, 0.5740641949674751, 0.6136820884067801, 0.549185292064389, 0.5533378690103942, 0.5597253417289709, 0.5323915726776365, 0.5313113266720048, 0.5539356519149828, 0.5392684795056717, 0.5539341314306742, 0.5655378125890901, 0.5571187838346143, 0.5493852450877805, 0.541150254162052, 0.5641609287903279, 0.5385742417619198, 0.5644041978860204, 0.5555187009180649, 0.5374931981669197, 0.5337770737801926, 0.5295022774157645, 0.5276252516085589, 0.526966481646405, 0.5239434323356121, 0.5202406580689587, 0.5424046025057382, 0.5546739329642887, 0.5492268700765658, 0.5330030725726599, 0.5302840090250667, 0.559327697074866, 0.5229975948982601, 0.5579739920323408, 0.5483722508519511, 0.5500919239807732, 0.5255736416276497, 0.5679198548763613, 0.5679364106323146, 0.5342161246115649, 0.5500478484208071, 0.5313761402157289, 0.5704804831479169, 0.5673190015780775, 0.5638997756604907, 0.5896313563932346, 0.5433095578528657, 0.5565140822642967, 0.5584272151883645, 0.5537656983242759, 0.548184348058097, 0.5406601837352861, 0.555926272952104, 0.5514698358653467]\n",
        "test_acc_list_300const = [53.98, 65.75, 72.1, 76.14, 77.31, 77.93, 81.02, 81.31, 82.75, 82.79, 83.69, 84.8, 85.35, 83.73, 83.79, 85.27, 84.08, 86.95, 86.6, 86.76, 87.11, 87.3, 88.07, 88.24, 87.9, 86.96, 88.42, 88.0, 88.7, 88.74, 87.84, 87.77, 88.76, 88.95, 88.48, 88.71, 89.77, 88.58, 89.26, 89.81, 89.22, 88.58, 89.44, 89.89, 89.57, 89.96, 89.93, 89.62, 88.5, 89.02, 89.32, 89.09, 90.25, 89.9, 90.15, 89.83, 90.56, 89.5, 90.0, 89.84, 89.55, 90.01, 90.27, 89.89, 90.63, 90.74, 89.52, 90.6, 90.0, 90.07, 90.5, 91.04, 91.19, 90.84, 90.5, 89.92, 90.31, 90.59, 90.56, 90.44, 90.87, 90.43, 90.47, 91.12, 90.58, 90.7, 90.83, 90.28, 91.18, 91.08, 90.76, 90.93, 91.05, 91.44, 91.19, 90.73, 91.14, 90.84, 91.04, 91.48, 90.9, 91.52, 91.12, 91.39, 90.99, 90.96, 91.09, 91.44, 91.0, 90.32, 91.25, 91.42, 91.16, 91.08, 90.75, 91.24, 91.32, 90.74, 90.9, 91.13, 91.6, 91.41, 91.7, 91.22, 90.61, 91.46, 90.95, 91.5, 91.36, 91.43, 91.4, 90.91, 91.13, 91.28, 91.29, 91.08, 91.73, 91.34, 91.19, 91.15, 91.59, 91.75, 91.13, 91.5, 91.47, 91.33, 91.42, 91.52, 90.83, 91.11, 91.09, 90.95, 91.6, 91.74, 91.37, 91.74, 91.61, 91.83, 90.89, 91.51, 91.79, 91.72, 92.0, 91.68, 91.82, 91.75, 91.83, 91.66, 91.79, 91.57, 91.82, 91.86, 91.98, 91.55, 91.74, 91.64, 91.27, 91.43, 91.88, 91.69, 92.03, 91.23, 91.2, 91.33, 91.81, 91.53, 91.45, 91.15, 91.51, 90.91, 91.23, 91.95, 91.46, 91.29, 91.48, 91.44, 91.69, 91.54, 91.51, 91.46, 91.42, 91.05, 91.18, 91.19, 91.56, 91.29, 91.73, 91.76, 91.56, 91.49, 91.62, 91.53, 91.7, 91.61, 91.97, 91.83, 92.04, 91.91, 91.65, 91.62, 91.6, 91.81, 91.53, 91.51, 91.84, 91.74, 91.75, 91.74, 91.9, 91.56, 91.06, 91.42, 91.51, 91.83, 91.39, 91.63, 91.73, 91.66, 91.55, 91.7, 91.4, 91.93, 92.04, 91.7, 91.47, 91.45, 91.64, 91.32, 90.81, 91.54, 91.31, 91.47, 92.03, 91.83, 91.86, 91.87, 91.91, 91.64, 91.84, 91.63, 91.8, 91.87, 92.0, 91.71, 91.79, 91.94, 92.08, 91.91, 92.31, 92.23, 92.29, 92.38, 92.07, 92.0, 91.81, 92.24, 92.44, 92.16, 92.14, 91.99, 91.69, 91.77, 92.03, 91.96, 91.84, 92.05, 91.96, 91.89, 92.32, 91.83, 91.85, 91.69, 92.02, 91.96, 91.93, 92.11, 91.92, 92.17, 91.77, 92.18]\n",
        "train_loss_list_300cosine = [1.5071465310197287, 1.0219155532864337, 0.7846573638839843, 0.6613810150958479, 0.5711964959153732, 0.5125089824770968, 0.4690145238900718, 0.4279545375142996, 0.38901165142036476, 0.3574826151799089, 0.32995736313323243, 0.3070905926033331, 0.29151374520585177, 0.27276346844415694, 0.2528251954637016, 0.2403833875164818, 0.21865639683251945, 0.21172414217798854, 0.1913816854833795, 0.18111336490692803, 0.1749152649943821, 0.16864872572663875, 0.15266839607645527, 0.1438077868649754, 0.131705386099962, 0.12894651846002086, 0.12342969215097138, 0.11197239956773888, 0.10989574266198915, 0.10079097377058988, 0.08904699315302098, 0.09436483237856684, 0.08817791208898583, 0.08477092711939313, 0.08056267116147393, 0.07277504891490404, 0.07563522219146117, 0.07128996417795222, 0.057986204402324874, 0.06231818245218013, 0.05127317121716591, 0.05075172336099628, 0.05728455614774657, 0.05022484378014414, 0.047201051832007145, 0.04299503102446326, 0.04491299623028396, 0.03698193365499711, 0.03486268078765502, 0.03927968754864539, 0.03724308135345007, 0.03970887712738551, 0.03285741158493292, 0.028173236158518744, 0.028217624198978605, 0.029066849599247234, 0.028810474211156678, 0.026399405389732587, 0.025763797607963174, 0.022101726805115827, 0.0206387186558328, 0.023366123397712605, 0.019345413963012873, 0.024243142143205797, 0.02154286678515214, 0.01816358644301042, 0.021265280448992292, 0.015330405338271595, 0.017995145478354284, 0.014342598256486924, 0.015467149512258362, 0.016130026668673175, 0.013699235769971377, 0.011888031450363573, 0.010789586022736494, 0.010900068600833514, 0.009051817458605739, 0.010178615377595523, 0.011202018590738473, 0.014404841935349116, 0.01132253416424867, 0.010159075095953521, 0.008117529459464283, 0.009489624014945505, 0.010137052083415494, 0.008486186185057242, 0.006968282311358831, 0.007533594105837527, 0.008950318053382235, 0.008889065595822946, 0.007539918537286241, 0.0057594785649967666, 0.006437454009137117, 0.005282934063498991, 0.004347972699934572, 0.005166493265034323, 0.004321007760212431, 0.00471395535047535, 0.0032943137242381028, 0.004311655514717282, 0.003059743971269304, 0.0039670848353404575, 0.0030527470893090434, 0.003868208768115135, 0.0034133147897016363, 0.004668654355293675, 0.0059490043992545235, 0.0041872456292248, 0.0038983581991906308, 0.0037588840379914716, 0.003916245875395213, 0.0034797456036773718, 0.0029193548814658767, 0.0028640743580595858, 0.0023959889678485742, 0.0022046099687237837, 0.00213721157060805, 0.001905863403416865, 0.0018298605383502951, 0.0019732695092532088, 0.0012731390455790247, 0.0014643412322186396, 0.0014584561287045482, 0.0013225552577550437, 0.0012107872296070626, 0.0009283569928718647, 0.0009475817591004316, 0.0009087429049996629, 0.0012826446498267045, 0.0011837309814490989, 0.0007089067522449444, 0.0007332131970385923, 0.0012466143751911653, 0.0009211793556768187, 0.000789917121722373, 0.0007207691697882189, 0.0007975770474887959, 0.0006758498559978029, 0.0007351151072098131, 0.0010837652537124284, 0.000724128287233449, 0.0005810306201093711, 0.00047435187363775613, 0.0004135937659276342, 0.0007063415897789764, 0.000588239943727893, 0.0005826207090225168, 0.0003262833067903336, 0.00038663124462745416, 0.0004899335792227164, 0.00045023311883942547, 0.00032505322259691985, 0.0004240722920772437, 0.000434831311982059, 0.0005759489223388422, 0.0003049708239195287, 0.00034293277186921384, 0.00026971405763318425, 0.00031280760296981017, 0.0007162304362590164, 0.0005976533334854375, 0.00044388456952353377, 0.00032143614894833265, 0.0003041571174924992, 0.00035453209945005774, 0.00023628432779605936, 0.00022958813212190958, 0.00018880277622619602, 0.00021482868581873839, 0.0001753957263578302, 0.00041403284841927493, 0.00021361697475566445, 0.0002520289759267065, 0.00019671708842346002, 0.00019921282705291114, 0.00030435736312682684, 0.00029258786672637244, 0.00027827393721283804, 0.0002795331743087422, 0.00024584021705562536, 0.00020301578391222058, 0.00015932558792188886, 0.00035799621748254593, 0.00028251744383299383, 0.00021141435477205867, 0.00022243324366817674, 0.00020670883468984464, 0.00021545639445687408, 0.00016235478455645368, 0.00022787957680421982, 0.00016665564254608194, 0.0001477420311949244, 0.00018142510538173126, 0.00034612401611176406, 0.00015685088123029036, 0.00019157101443428062, 0.00019864877790692532, 0.00020180520096545342, 0.00012184150966041865, 0.00010955794329265264, 0.0002272417015756974, 0.00012062133251651577, 0.00024990162183020963, 0.00019493304157317115, 0.00011319477053125949, 0.0001146485327846252, 0.0001126736392463569, 0.00013831783792320434, 0.0001353729226037266, 0.0001630674412422994, 0.0001457879433627867, 0.00012657002445250193, 0.00020756496892448712, 0.0001576078423556483, 0.00016432452238864054, 0.00013326239761994157, 0.000126308732199333, 0.00011329932245867901, 9.373737983756487e-05, 0.00012264910064071485, 9.72924513225386e-05, 9.851183427198616e-05, 0.00014160107466290727, 0.00019542356384294292, 0.00010866884136099055, 0.00012103338623344784, 0.00010338596340528902, 0.00012947641171964864, 0.00016585586701756884, 0.00012378471780928916, 0.00012914159678432927, 8.873532887934217e-05, 0.00012277717895865415, 0.00010324342793536724, 0.00011429319050957435, 0.00014108263730566994, 0.00011344413554959498, 0.0001314912391754855, 0.00014366292956832224, 0.00010670398796356165, 9.395798729230545e-05, 9.690826291712445e-05, 8.27918457593515e-05, 7.49523732641198e-05, 0.000116572610751607, 0.00010861793663089236, 0.00010167052199556757, 0.0001307339418356521, 0.00010273090034757616, 0.00012697175096751327, 0.00012768910120719, 7.014640353397078e-05, 9.454710011857551e-05, 0.00011274885802570828, 0.000126904926870751, 9.48965783575255e-05, 9.193459106337249e-05, 8.93238301681306e-05, 0.00010053854063013211, 0.00010620467252830129, 0.00014532459377346714, 8.020320023135105e-05, 7.178885715230178e-05, 0.00011935103254135713, 9.011012685912722e-05, 8.608197714011839e-05, 9.837138899922847e-05, 0.00010366790116276302, 8.538921241570249e-05, 0.0001675107899030607, 7.792623628388836e-05, 6.928247518943407e-05, 8.20774587304151e-05, 0.00011248917587745207, 7.720731971381645e-05, 6.833156445040754e-05, 0.00010231887682267187, 9.609293597030557e-05, 7.292958810775106e-05, 0.00010774314482761675, 9.255914947628301e-05, 0.0001237650751620048, 9.64451171900979e-05, 0.0001169090761506665, 0.0001072614105620238, 9.686175892860217e-05, 0.00011957684801773822, 0.00011583886435356528, 0.0001275175088925282, 6.546866618231805e-05, 9.038513419941541e-05, 0.00010006681310687346, 9.007086580128472e-05, 0.00022282703456296465, 8.881661869528004e-05, 8.626907432722472e-05, 9.961434439616733e-05, 0.0001082291823644854, 0.00010509098167338485, 7.50494904148657e-05]\n",
        "train_acc_list_300cosine = [44.7275, 63.7, 72.3575, 76.9475, 80.11, 82.24, 83.6775, 85.0225, 86.4575, 87.51, 88.3775, 89.1875, 89.8275, 90.4725, 91.27, 91.6125, 92.195, 92.6575, 93.2125, 93.6125, 93.7775, 93.95, 94.65, 94.79, 95.3675, 95.5025, 95.58, 96.0475, 96.075, 96.4175, 96.79, 96.66, 96.8425, 97.0175, 97.1425, 97.4075, 97.345, 97.375, 97.945, 97.73, 98.185, 98.1775, 98.0075, 98.175, 98.36, 98.465, 98.4525, 98.72, 98.815, 98.635, 98.6625, 98.58, 98.825, 99.075, 99.0425, 98.985, 98.965, 99.1025, 99.0875, 99.26, 99.2775, 99.1925, 99.3375, 99.1475, 99.2425, 99.425, 99.2375, 99.475, 99.38, 99.505, 99.465, 99.4675, 99.5625, 99.605, 99.64, 99.6425, 99.71, 99.66, 99.625, 99.4775, 99.625, 99.6725, 99.745, 99.7, 99.67, 99.7325, 99.785, 99.7425, 99.675, 99.6725, 99.7625, 99.83, 99.7925, 99.8025, 99.8675, 99.8425, 99.8625, 99.85, 99.905, 99.8775, 99.9, 99.8825, 99.895, 99.8625, 99.9025, 99.8625, 99.7975, 99.8725, 99.87, 99.89, 99.88, 99.87, 99.9125, 99.915, 99.9375, 99.9475, 99.94, 99.945, 99.94, 99.9425, 99.9775, 99.96, 99.9575, 99.965, 99.9725, 99.975, 99.98, 99.9825, 99.9675, 99.965, 99.9925, 99.985, 99.9675, 99.975, 99.9725, 99.975, 99.98, 99.985, 99.9875, 99.9725, 99.9825, 99.9925, 99.9925, 99.9975, 99.98, 99.99, 99.9875, 99.9975, 99.995, 99.985, 99.99, 100.0, 99.9925, 99.9875, 99.9825, 100.0, 99.9975, 99.9975, 99.9925, 99.9825, 99.9875, 99.99, 99.9975, 99.9975, 99.9925, 100.0, 99.9975, 99.9975, 100.0, 100.0, 99.985, 99.9975, 99.9975, 99.995, 99.9975, 99.9975, 99.9975, 99.9975, 99.9925, 99.995, 99.9975, 100.0, 99.9875, 99.9925, 100.0, 99.995, 100.0, 99.995, 100.0, 99.9925, 100.0, 100.0, 99.9975, 99.99, 100.0, 99.9975, 99.995, 99.995, 100.0, 100.0, 99.9925, 100.0, 99.995, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 99.9975, 99.995, 99.9975, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9925, 100.0, 100.0, 100.0, 99.9975, 99.9975, 100.0, 99.9975, 100.0, 99.9975, 100.0, 99.9975, 100.0, 99.9975, 99.9975, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 99.9975, 100.0, 100.0, 99.9975, 100.0, 99.9925, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
        "test_loss_list_300cosine = [1.3360588429849358, 1.0448881915852994, 0.8597957295707509, 0.6716653414164917, 0.6908234833162042, 0.5932456066336813, 0.5461285729197007, 0.5097410641139066, 0.5401990885221506, 0.5333668193485164, 0.4556268532819386, 0.4484600973657415, 0.4620721623112884, 0.4327629621647581, 0.47597266639335245, 0.48141292925876905, 0.49725772666780255, 0.40901502231253856, 0.4323547872938687, 0.4539363452150852, 0.4590581006641629, 0.40523579641233515, 0.4201337242428261, 0.418421223005162, 0.41465936016432847, 0.43959930557993393, 0.3880773431892636, 0.397792113732688, 0.4114573869524123, 0.3806772181127645, 0.40738119260419775, 0.41724546894996983, 0.3961485907246795, 0.40635761147058463, 0.41718924026700516, 0.4624185173571864, 0.4003025793199298, 0.4194344795202907, 0.43098732470711576, 0.42882050819034817, 0.4735664082101629, 0.4482132949029343, 0.4824381698913212, 0.489483314035814, 0.42275018914590906, 0.46854421854773654, 0.46698710643037966, 0.45736848336609104, 0.48327861801732946, 0.42990076523038406, 0.4146351450228993, 0.4377061692596991, 0.4491257427991191, 0.4464895281987854, 0.5250044391502308, 0.4606155977596211, 0.48777404917946343, 0.46350483313391483, 0.45923378441152696, 0.47999939944925185, 0.4535947281725799, 0.45670295101177844, 0.4861161712604233, 0.5072417436521265, 0.45164442854591563, 0.4723219115145599, 0.465178365948834, 0.48657907800206657, 0.4763214318435403, 0.4628955114491378, 0.4531608254094667, 0.4590076025150999, 0.48202957307236105, 0.47070856392383575, 0.4524376158472858, 0.5099793840435487, 0.4644981126619291, 0.4989790280785742, 0.4910671897704088, 0.5239907608756537, 0.4774412533527688, 0.4662430659879612, 0.47549027996727183, 0.49829778188391577, 0.4727332616531396, 0.48873594582458085, 0.4645166827153556, 0.4946923785949055, 0.5139467286158211, 0.504527775924417, 0.4836999862631665, 0.4878289546770386, 0.49920391819522353, 0.48420851479602767, 0.4741327323113816, 0.4969299920374834, 0.492643963309783, 0.4669509222613105, 0.5004429721002337, 0.5008861348976062, 0.48978376954416686, 0.48975364958183676, 0.4894806569135642, 0.4939129582688778, 0.5057695177914221, 0.483693698161765, 0.49143673158899137, 0.508361778493169, 0.49146506284611136, 0.5179333079464828, 0.4985507955845398, 0.48249870953680596, 0.4882995514552804, 0.48072621575261976, 0.4871305170692975, 0.495542240859587, 0.502074130728275, 0.4848601612486417, 0.46474564952563635, 0.45505768362479876, 0.480818674534182, 0.48904268066339857, 0.48706334348343594, 0.5078222957215731, 0.48777740590180024, 0.48671044654484036, 0.4677445718759223, 0.4843851542925533, 0.47715435280830043, 0.4916676714827743, 0.48873102721534195, 0.4853220989432516, 0.495768607794484, 0.48384790348855755, 0.494273115373865, 0.520792075538937, 0.48109641991838625, 0.4784795025104209, 0.5150823619546769, 0.49928266564501994, 0.508841446871999, 0.4865191377039197, 0.49814116068278685, 0.4979177566268776, 0.509591865388653, 0.5287647111506402, 0.5030483018748367, 0.47968817134446734, 0.5014746417350406, 0.5026743098904815, 0.5065745726416383, 0.4991808756242824, 0.49776733959022956, 0.4859552224980125, 0.505850138354905, 0.507838894294787, 0.4913963109632082, 0.4985096752643585, 0.5035310268779344, 0.49438839943348606, 0.5127879598095447, 0.49104869393985484, 0.4965999275445938, 0.500093090760557, 0.5078368173747123, 0.49182856611058684, 0.4928507318225088, 0.5030448830957654, 0.4847751912436908, 0.5120558350146571, 0.4888622204336939, 0.49651806179103974, 0.49612243831912173, 0.4962354428783248, 0.513516125611112, 0.49529338731795924, 0.4980339683309386, 0.5014159933109826, 0.49898972069915337, 0.507745862573008, 0.52068718268147, 0.5003404364555697, 0.525249100938628, 0.5042423366368571, 0.5168220468714267, 0.5160682231565065, 0.5124318371467953, 0.5218248301291768, 0.4837766302914559, 0.5026627190505402, 0.5024134409201296, 0.512225415887712, 0.5186735810358313, 0.5077592483426951, 0.4826306046187123, 0.512151868848861, 0.5008148086976402, 0.4929636383924303, 0.4970756280648557, 0.5172987021977389, 0.4966934053580972, 0.5119075952451441, 0.4916618470149704, 0.5191661170389079, 0.5202390191298497, 0.5187712847432003, 0.5312336112690877, 0.5149811487409133, 0.5015346613488619, 0.4897740053225167, 0.5123436318922646, 0.4950245019001297, 0.5100116333629512, 0.517671759935874, 0.49631667703012877, 0.5063869094924082, 0.5044590200804457, 0.5116969626161116, 0.5092341914018498, 0.5033747812237921, 0.5193329711880865, 0.5127834038266653, 0.5309269350918033, 0.49558807419070716, 0.513400316049781, 0.5085111194396321, 0.510913455599471, 0.4971718424105946, 0.5032308197851423, 0.5266274438251423, 0.49809465491319005, 0.5013407922432392, 0.4893049780703798, 0.5038147557385361, 0.5071697546334206, 0.5146695088736618, 0.49518201302123976, 0.5041418398105646, 0.5182757000379925, 0.5224867431046087, 0.4978565979230253, 0.507878503939019, 0.49611272823206987, 0.5108789631837531, 0.5073691453737549, 0.5223453206163419, 0.5107548187805128, 0.5007183399004272, 0.5212629056429561, 0.49115426876122437, 0.5297577135925051, 0.5027483824310424, 0.5041316181798524, 0.4906521471618097, 0.493595665768732, 0.502438643687888, 0.4948954885896248, 0.4982064822806588, 0.5073866029328937, 0.5064849340462987, 0.5071891489662702, 0.5056039866390107, 0.5256269764673861, 0.4950309816417815, 0.5119404613594466, 0.4975051161231874, 0.4961673260866841, 0.5196594443125061, 0.4737729119914996, 0.5019824165332166, 0.4959078891367852, 0.5141448318203793, 0.49820214887208575, 0.5068159803182264, 0.4858330869221989, 0.496248337852804, 0.4940428052899204, 0.503951388069346, 0.49354483621029915, 0.4873908001788055, 0.5022429193876967, 0.50107072425794, 0.49141834751714636, 0.50169266326518, 0.4897840660584124, 0.5037467987099781, 0.501852917708928, 0.5113212121061131, 0.5046284511873994, 0.4889931109132646, 0.507928684731073, 0.4926022017681146, 0.5067250309865686, 0.510816790823695, 0.5253809748948375, 0.5128096621247786, 0.4980110616623601, 0.5035155990832969, 0.5060317163603215, 0.49634331974047646]\n",
        "test_acc_list_300cosine = [52.55, 63.48, 70.79, 76.77, 77.07, 79.66, 81.7, 82.61, 82.36, 83.15, 84.72, 85.19, 85.13, 85.7, 85.08, 84.65, 84.64, 87.17, 86.56, 86.12, 86.83, 87.48, 87.37, 87.5, 88.14, 87.33, 89.04, 88.78, 88.32, 89.27, 88.82, 88.61, 89.75, 88.78, 89.11, 88.37, 89.51, 89.38, 88.67, 89.45, 88.48, 89.32, 89.16, 88.72, 89.82, 89.02, 88.77, 89.55, 89.58, 90.05, 90.18, 90.23, 89.78, 90.12, 89.23, 89.82, 89.9, 89.95, 89.87, 90.25, 90.38, 90.39, 90.25, 89.66, 90.88, 90.49, 90.4, 90.26, 90.13, 91.0, 90.89, 90.98, 90.93, 90.94, 91.42, 90.54, 91.11, 90.89, 90.63, 90.02, 91.17, 90.81, 91.24, 90.97, 91.14, 91.35, 91.47, 91.32, 90.83, 90.87, 91.23, 90.95, 91.06, 91.39, 91.41, 90.98, 91.19, 91.65, 91.43, 90.97, 90.93, 91.36, 91.17, 91.47, 91.56, 91.59, 91.49, 90.95, 91.39, 91.46, 91.51, 91.64, 91.6, 91.65, 91.39, 91.51, 91.58, 92.1, 91.79, 91.97, 91.75, 91.74, 91.57, 91.96, 91.91, 92.0, 92.15, 92.02, 91.96, 91.79, 91.77, 92.07, 91.99, 92.41, 92.06, 91.7, 92.06, 92.22, 91.85, 91.93, 92.2, 92.1, 92.04, 91.91, 91.85, 91.94, 91.89, 92.15, 92.18, 91.94, 92.03, 92.19, 92.17, 92.45, 92.43, 91.98, 92.28, 92.25, 92.4, 92.24, 92.02, 92.17, 92.01, 92.24, 92.28, 92.14, 92.37, 92.14, 92.25, 92.3, 92.22, 92.12, 92.18, 92.11, 92.29, 92.04, 92.02, 92.2, 92.37, 92.34, 92.25, 92.27, 91.84, 92.13, 92.18, 91.97, 92.19, 91.85, 91.98, 92.21, 92.11, 92.25, 92.17, 92.13, 92.4, 92.37, 92.21, 92.18, 92.44, 92.29, 92.36, 92.11, 92.35, 92.51, 91.96, 92.18, 92.12, 92.41, 92.3, 92.57, 92.18, 92.45, 92.11, 92.13, 92.52, 92.18, 92.1, 92.1, 92.24, 92.59, 92.23, 92.27, 92.27, 92.48, 92.31, 92.38, 92.04, 92.33, 92.33, 92.15, 92.27, 92.48, 92.52, 92.61, 92.2, 92.3, 92.31, 92.55, 92.25, 92.12, 92.24, 92.12, 92.47, 92.29, 92.22, 92.34, 92.4, 92.34, 92.23, 92.56, 92.08, 92.09, 92.33, 92.22, 92.44, 92.34, 92.41, 92.41, 92.2, 92.35, 92.42, 92.43, 91.96, 92.21, 92.27, 92.41, 92.21, 92.36, 92.66, 92.32, 92.33, 92.37, 92.2, 92.16, 92.31, 92.34, 92.35, 92.43, 92.49, 92.47, 92.15, 92.23, 92.61, 92.26, 92.76, 92.44, 92.32, 92.18, 92.38, 92.38, 92.28, 92.54, 92.5, 92.4, 92.13, 92.37, 92.31, 92.37, 92.12, 92.62]\n"
      ],
      "metadata": {
        "id": "ilmYYfIElcAU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list_01)), train_acc_list_01, 'b')\n",
        "plt.plot(range(len(train_acc_list_001)), train_acc_list_001, 'r')\n",
        "plt.plot(range(len(train_acc_list_0001)), train_acc_list_0001, 'g')\n",
        "\n",
        "plt.plot(range(len(test_acc_list_01)), test_acc_list_01, color='b', linestyle='--')\n",
        "plt.plot(range(len(test_acc_list_001)), test_acc_list_001,color='r', linestyle='--')\n",
        "plt.plot(range(len(test_acc_list_0001)), test_acc_list_0001, color='g', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Combined accuracy\")\n",
        "plt.legend(['train with lr = 1e-1', 'train with lr = 1e-2','train with lr = 1e-3',\n",
        "            'test with lr = 1e-1','test with lr = 1e-2', 'test with lr = 1e-3'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ubm3Y5CSnZ0D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c48c3217-78dd-4f66-fdbd-5a05d5214668"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list_01)), train_loss_list_01, 'b')\n",
        "plt.plot(range(len(train_loss_list_001)), train_loss_list_001, 'r')\n",
        "plt.plot(range(len(train_loss_list_0001)), train_loss_list_0001, 'g')\n",
        "\n",
        "plt.plot(range(len(test_loss_list_01)), test_loss_list_01, color='b', linestyle='--')\n",
        "plt.plot(range(len(test_loss_list_001)), test_loss_list_001,color='r', linestyle='--')\n",
        "plt.plot(range(len(test_loss_list_0001)), test_loss_list_0001, color='g', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Combined loss\")\n",
        "plt.legend(['train with lr = 1e-1', 'train with lr = 1e-2','train with lr = 1e-3',\n",
        "            'test with lr = 1e-1','test with lr = 1e-2', 'test with lr = 1e-3'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QnOTaNoesBEb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "17cfe2c5-361c-479b-f5e8-51a29c8ad25d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}