{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Source code for unpickle function: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "# def unpickle(file):\n",
        "#     import pickle\n",
        "#     with open(file, 'rb') as fo:\n",
        "#         dict = pickle.load(fo, encoding='bytes')\n",
        "#     return dict\n",
        "\n",
        "# import numpy as np\n",
        "# def get_mean_color():\n",
        "#     d=unpickle('./data/cifar-10-batches-py/data_batch_1')\n",
        "#     channels = d[b'data']\n",
        "#     for i in range(2,6):\n",
        "#         d=unpickle('./data/cifar-10-batches-py/data_batch_'+str(i))\n",
        "#         channels=np.concatenate((channels, d[b'data']), axis=0)\n",
        "#     r=np.mean(channels[:,:1024])/255  \n",
        "#     g=np.mean(channels[:,1024:2048])/255\n",
        "#     b=np.mean(channels[:,2048:])/255\n",
        "#     return(r,g,b)\n",
        "# get_mean_color()"
      ],
      "metadata": {
        "id": "8ntKy6oKQGJv"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgAiImV0uURP",
        "outputId": "bef035df-b142-47f4-8a2a-42caf944cfbd"
      },
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    #transforms.RandomErasing(value=get_mean_color()),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./data/train_32x32.mat\n",
            "Using downloaded and verified file: ./data/test_32x32.mat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "862c0e7f-4fc4-4a5b-8785-294161b008e9"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te71lQ17B1L_"
      },
      "source": [
        "#Data Augmentation\n",
        "Data augmentation performs random modifications of the image as a preprocessing step. It serves the following purposes:\n",
        "1. It increases the amount of data for training.\n",
        "2. By deleting features, it prevents the network from relying on a narrow set of features, which may not generalize.\n",
        "3. By changing features while maintaining the same output, it helps the network become tolerant of changes that do not change the image lab. \n",
        "\n",
        "In short, data augmentation desensitivizes the network, so it extracts features that are invariant to changes that should not affect the prediction. \n",
        "\n",
        "We showcase a few random data augmentation provided by PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyG26xoJC0Pa"
      },
      "source": [
        "# import torch.nn as nn\n",
        "# transforms = torch.nn.Sequential(\n",
        "#     T.Resize(256), # resize the short edge to 256.\n",
        "#     T.RandomCrop(224), #randomly crop a 224x224 region from the image\n",
        "#     T.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
        "#     #T.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0.1)\n",
        "#     #T.AutoAugment()\n",
        "# )\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# dog1 = dog1.to(device)\n",
        "# # dog2 = dog2.to(device)\n",
        "\n",
        "# # transformed_dog1 = transforms(dog1)\n",
        "# transformed_dog1 = transforms(dog1)\n",
        "# show([transformed_dog1])"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # This is the \"stem\"\n",
        "        # For CIFAR (32x32 images), it does not perform downsampling\n",
        "        # It should downsample for ImageNet\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "a9df5999-cfc2-4d27-9a16-5ead1b50e71c"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "753c6f3f-7f51-4c70-e00f-1f903bc8c81b"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArgupDVRwB8i",
        "outputId": "e9e4293d-a7c9-43b7-9041-8774a785e490"
      },
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3475, accuracy : 16.53\n",
            "iteration : 100, loss : 2.2805, accuracy : 18.83\n",
            "iteration : 150, loss : 2.2068, accuracy : 21.65\n",
            "iteration : 200, loss : 2.0536, accuracy : 26.98\n",
            "iteration : 250, loss : 1.8730, accuracy : 33.77\n",
            "iteration : 300, loss : 1.7006, accuracy : 40.18\n",
            "iteration : 350, loss : 1.5581, accuracy : 45.48\n",
            "Epoch :   1, training loss : 1.5108, training accuracy : 47.22, test loss : 0.7600, test accuracy : 75.59\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.5415, accuracy : 82.64\n",
            "iteration : 100, loss : 0.5114, accuracy : 83.78\n",
            "iteration : 150, loss : 0.4903, accuracy : 84.47\n",
            "iteration : 200, loss : 0.4795, accuracy : 84.80\n",
            "iteration : 250, loss : 0.4672, accuracy : 85.25\n",
            "iteration : 300, loss : 0.4582, accuracy : 85.60\n",
            "iteration : 350, loss : 0.4577, accuracy : 85.60\n",
            "Epoch :   2, training loss : 0.4557, training accuracy : 85.66, test loss : 0.4139, test accuracy : 87.07\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.3953, accuracy : 87.36\n",
            "iteration : 100, loss : 0.4025, accuracy : 87.20\n",
            "iteration : 150, loss : 0.3912, accuracy : 87.80\n",
            "iteration : 200, loss : 0.3902, accuracy : 87.84\n",
            "iteration : 250, loss : 0.3887, accuracy : 87.92\n",
            "iteration : 300, loss : 0.3904, accuracy : 87.90\n",
            "iteration : 350, loss : 0.3888, accuracy : 87.94\n",
            "Epoch :   3, training loss : 0.3896, training accuracy : 87.93, test loss : 0.3922, test accuracy : 87.53\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3852, accuracy : 87.86\n",
            "iteration : 100, loss : 0.3813, accuracy : 88.00\n",
            "iteration : 150, loss : 0.3784, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3840, accuracy : 88.03\n",
            "iteration : 250, loss : 0.3835, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3826, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3809, accuracy : 88.11\n",
            "Epoch :   4, training loss : 0.3820, training accuracy : 88.12, test loss : 0.3909, test accuracy : 87.89\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.3585, accuracy : 88.67\n",
            "iteration : 100, loss : 0.3656, accuracy : 88.52\n",
            "iteration : 150, loss : 0.3728, accuracy : 88.27\n",
            "iteration : 200, loss : 0.3786, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3820, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3794, accuracy : 88.24\n",
            "Epoch :   5, training loss : 0.3792, training accuracy : 88.25, test loss : 0.3910, test accuracy : 87.77\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.3717, accuracy : 88.55\n",
            "iteration : 100, loss : 0.3755, accuracy : 88.23\n",
            "iteration : 150, loss : 0.3789, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3772, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3802, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.06\n",
            "Epoch :   6, training loss : 0.3785, training accuracy : 88.08, test loss : 0.3915, test accuracy : 87.75\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.3859, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3841, accuracy : 87.97\n",
            "iteration : 150, loss : 0.3816, accuracy : 88.04\n",
            "iteration : 200, loss : 0.3807, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3795, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3790, accuracy : 88.20\n",
            "Epoch :   7, training loss : 0.3779, training accuracy : 88.24, test loss : 0.3894, test accuracy : 87.99\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.3857, accuracy : 87.81\n",
            "iteration : 100, loss : 0.3873, accuracy : 87.88\n",
            "iteration : 150, loss : 0.3855, accuracy : 88.00\n",
            "iteration : 200, loss : 0.3787, accuracy : 88.23\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.19\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.23\n",
            "Epoch :   8, training loss : 0.3783, training accuracy : 88.18, test loss : 0.3928, test accuracy : 87.76\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.3961, accuracy : 87.89\n",
            "iteration : 100, loss : 0.3895, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3880, accuracy : 87.90\n",
            "iteration : 200, loss : 0.3876, accuracy : 87.83\n",
            "iteration : 250, loss : 0.3847, accuracy : 87.92\n",
            "iteration : 300, loss : 0.3825, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.09\n",
            "Epoch :   9, training loss : 0.3789, training accuracy : 88.14, test loss : 0.3913, test accuracy : 87.88\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.3730, accuracy : 88.55\n",
            "iteration : 100, loss : 0.3747, accuracy : 88.17\n",
            "iteration : 150, loss : 0.3775, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3770, accuracy : 88.08\n",
            "iteration : 250, loss : 0.3771, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3770, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.08\n",
            "Epoch :  10, training loss : 0.3799, training accuracy : 88.10, test loss : 0.3898, test accuracy : 87.72\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.3786, accuracy : 88.17\n",
            "iteration : 100, loss : 0.3897, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3826, accuracy : 88.18\n",
            "iteration : 200, loss : 0.3799, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3851, accuracy : 87.99\n",
            "iteration : 300, loss : 0.3816, accuracy : 88.05\n",
            "iteration : 350, loss : 0.3814, accuracy : 88.02\n",
            "Epoch :  11, training loss : 0.3802, training accuracy : 88.07, test loss : 0.3919, test accuracy : 87.75\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.3683, accuracy : 88.72\n",
            "iteration : 100, loss : 0.3739, accuracy : 88.41\n",
            "iteration : 150, loss : 0.3792, accuracy : 88.21\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3799, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3771, accuracy : 88.18\n",
            "Epoch :  12, training loss : 0.3786, training accuracy : 88.14, test loss : 0.3930, test accuracy : 87.80\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.3940, accuracy : 87.80\n",
            "iteration : 100, loss : 0.3949, accuracy : 87.71\n",
            "iteration : 150, loss : 0.3971, accuracy : 87.69\n",
            "iteration : 200, loss : 0.3912, accuracy : 87.81\n",
            "iteration : 250, loss : 0.3849, accuracy : 87.88\n",
            "iteration : 300, loss : 0.3833, accuracy : 87.99\n",
            "iteration : 350, loss : 0.3826, accuracy : 87.96\n",
            "Epoch :  13, training loss : 0.3801, training accuracy : 88.01, test loss : 0.3876, test accuracy : 87.96\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.3895, accuracy : 87.88\n",
            "iteration : 100, loss : 0.3836, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3813, accuracy : 88.17\n",
            "iteration : 200, loss : 0.3832, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3812, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3821, accuracy : 87.97\n",
            "iteration : 350, loss : 0.3821, accuracy : 88.01\n",
            "Epoch :  14, training loss : 0.3819, training accuracy : 87.99, test loss : 0.3887, test accuracy : 87.81\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.3623, accuracy : 88.89\n",
            "iteration : 100, loss : 0.3658, accuracy : 88.84\n",
            "iteration : 150, loss : 0.3617, accuracy : 88.73\n",
            "iteration : 200, loss : 0.3693, accuracy : 88.52\n",
            "iteration : 250, loss : 0.3694, accuracy : 88.49\n",
            "iteration : 300, loss : 0.3732, accuracy : 88.38\n",
            "iteration : 350, loss : 0.3755, accuracy : 88.30\n",
            "Epoch :  15, training loss : 0.3756, training accuracy : 88.28, test loss : 0.3932, test accuracy : 87.81\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.3997, accuracy : 87.48\n",
            "iteration : 100, loss : 0.3872, accuracy : 87.85\n",
            "iteration : 150, loss : 0.3819, accuracy : 87.88\n",
            "iteration : 200, loss : 0.3766, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.01\n",
            "iteration : 300, loss : 0.3769, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3785, accuracy : 88.05\n",
            "Epoch :  16, training loss : 0.3785, training accuracy : 88.07, test loss : 0.3915, test accuracy : 87.81\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.3906, accuracy : 87.83\n",
            "iteration : 100, loss : 0.3891, accuracy : 87.70\n",
            "iteration : 150, loss : 0.3811, accuracy : 88.11\n",
            "iteration : 200, loss : 0.3799, accuracy : 88.08\n",
            "iteration : 250, loss : 0.3796, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.13\n",
            "Epoch :  17, training loss : 0.3810, training accuracy : 88.09, test loss : 0.3903, test accuracy : 87.94\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.3778, accuracy : 88.11\n",
            "iteration : 100, loss : 0.3849, accuracy : 88.03\n",
            "iteration : 150, loss : 0.3807, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3779, accuracy : 88.29\n",
            "iteration : 250, loss : 0.3784, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3785, accuracy : 88.16\n",
            "Epoch :  18, training loss : 0.3791, training accuracy : 88.11, test loss : 0.3896, test accuracy : 87.93\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.3749, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3712, accuracy : 88.31\n",
            "iteration : 150, loss : 0.3807, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3757, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.00\n",
            "iteration : 300, loss : 0.3779, accuracy : 88.00\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.01\n",
            "Epoch :  19, training loss : 0.3803, training accuracy : 87.96, test loss : 0.3941, test accuracy : 87.77\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.3714, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3843, accuracy : 87.91\n",
            "iteration : 150, loss : 0.3791, accuracy : 87.99\n",
            "iteration : 200, loss : 0.3753, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3767, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3747, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3767, accuracy : 88.11\n",
            "Epoch :  20, training loss : 0.3771, training accuracy : 88.11, test loss : 0.3924, test accuracy : 87.78\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.3909, accuracy : 87.88\n",
            "iteration : 100, loss : 0.3919, accuracy : 87.61\n",
            "iteration : 150, loss : 0.3857, accuracy : 87.84\n",
            "iteration : 200, loss : 0.3830, accuracy : 87.92\n",
            "iteration : 250, loss : 0.3811, accuracy : 88.00\n",
            "iteration : 300, loss : 0.3820, accuracy : 88.05\n",
            "iteration : 350, loss : 0.3785, accuracy : 88.15\n",
            "Epoch :  21, training loss : 0.3791, training accuracy : 88.13, test loss : 0.3913, test accuracy : 87.66\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.3730, accuracy : 87.78\n",
            "iteration : 100, loss : 0.3769, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3815, accuracy : 87.97\n",
            "iteration : 200, loss : 0.3845, accuracy : 87.88\n",
            "iteration : 250, loss : 0.3785, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3790, accuracy : 88.07\n",
            "iteration : 350, loss : 0.3790, accuracy : 88.12\n",
            "Epoch :  22, training loss : 0.3796, training accuracy : 88.14, test loss : 0.3941, test accuracy : 87.75\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.3680, accuracy : 88.64\n",
            "iteration : 100, loss : 0.3767, accuracy : 88.39\n",
            "iteration : 150, loss : 0.3791, accuracy : 88.34\n",
            "iteration : 200, loss : 0.3785, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3765, accuracy : 88.34\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3773, accuracy : 88.25\n",
            "Epoch :  23, training loss : 0.3777, training accuracy : 88.24, test loss : 0.3901, test accuracy : 87.75\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.3716, accuracy : 88.41\n",
            "iteration : 100, loss : 0.3788, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3763, accuracy : 88.27\n",
            "iteration : 200, loss : 0.3747, accuracy : 88.31\n",
            "iteration : 250, loss : 0.3761, accuracy : 88.30\n",
            "iteration : 300, loss : 0.3743, accuracy : 88.33\n",
            "iteration : 350, loss : 0.3767, accuracy : 88.21\n",
            "Epoch :  24, training loss : 0.3792, training accuracy : 88.17, test loss : 0.3911, test accuracy : 87.86\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.3696, accuracy : 87.81\n",
            "iteration : 100, loss : 0.3669, accuracy : 88.07\n",
            "iteration : 150, loss : 0.3717, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3739, accuracy : 88.05\n",
            "iteration : 250, loss : 0.3733, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3763, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3755, accuracy : 88.10\n",
            "Epoch :  25, training loss : 0.3764, training accuracy : 88.09, test loss : 0.3896, test accuracy : 87.67\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.3787, accuracy : 88.09\n",
            "iteration : 100, loss : 0.3743, accuracy : 88.25\n",
            "iteration : 150, loss : 0.3713, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3767, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3765, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3771, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3770, accuracy : 88.20\n",
            "Epoch :  26, training loss : 0.3762, training accuracy : 88.20, test loss : 0.3906, test accuracy : 87.71\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.3543, accuracy : 88.75\n",
            "iteration : 100, loss : 0.3740, accuracy : 88.34\n",
            "iteration : 150, loss : 0.3787, accuracy : 88.24\n",
            "iteration : 200, loss : 0.3819, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3805, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.14\n",
            "Epoch :  27, training loss : 0.3767, training accuracy : 88.20, test loss : 0.3924, test accuracy : 87.65\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.3909, accuracy : 87.66\n",
            "iteration : 100, loss : 0.3796, accuracy : 87.94\n",
            "iteration : 150, loss : 0.3814, accuracy : 87.99\n",
            "iteration : 200, loss : 0.3819, accuracy : 87.92\n",
            "iteration : 250, loss : 0.3813, accuracy : 87.91\n",
            "iteration : 300, loss : 0.3793, accuracy : 87.95\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.03\n",
            "Epoch :  28, training loss : 0.3780, training accuracy : 88.01, test loss : 0.3930, test accuracy : 87.66\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.3635, accuracy : 88.75\n",
            "iteration : 100, loss : 0.3803, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3819, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3859, accuracy : 87.80\n",
            "iteration : 250, loss : 0.3837, accuracy : 87.85\n",
            "iteration : 300, loss : 0.3836, accuracy : 87.92\n",
            "iteration : 350, loss : 0.3818, accuracy : 87.95\n",
            "Epoch :  29, training loss : 0.3797, training accuracy : 88.01, test loss : 0.3910, test accuracy : 87.74\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.3611, accuracy : 88.20\n",
            "iteration : 100, loss : 0.3754, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3710, accuracy : 88.08\n",
            "iteration : 200, loss : 0.3742, accuracy : 88.13\n",
            "iteration : 250, loss : 0.3758, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3768, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3792, accuracy : 88.13\n",
            "Epoch :  30, training loss : 0.3769, training accuracy : 88.20, test loss : 0.3926, test accuracy : 87.75\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.3877, accuracy : 87.95\n",
            "iteration : 100, loss : 0.3801, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3752, accuracy : 88.37\n",
            "iteration : 200, loss : 0.3734, accuracy : 88.46\n",
            "iteration : 250, loss : 0.3765, accuracy : 88.30\n",
            "iteration : 300, loss : 0.3779, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.10\n",
            "Epoch :  31, training loss : 0.3782, training accuracy : 88.15, test loss : 0.3910, test accuracy : 87.86\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.3666, accuracy : 88.70\n",
            "iteration : 100, loss : 0.3749, accuracy : 88.34\n",
            "iteration : 150, loss : 0.3747, accuracy : 88.21\n",
            "iteration : 200, loss : 0.3734, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3791, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3765, accuracy : 88.29\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.25\n",
            "Epoch :  32, training loss : 0.3786, training accuracy : 88.22, test loss : 0.3919, test accuracy : 87.90\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.3788, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3787, accuracy : 87.91\n",
            "iteration : 150, loss : 0.3769, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3791, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3821, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3817, accuracy : 88.00\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.09\n",
            "Epoch :  33, training loss : 0.3801, training accuracy : 88.05, test loss : 0.3932, test accuracy : 87.65\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.3795, accuracy : 87.97\n",
            "iteration : 100, loss : 0.3745, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3817, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3819, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.16\n",
            "iteration : 300, loss : 0.3804, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3791, accuracy : 88.09\n",
            "Epoch :  34, training loss : 0.3801, training accuracy : 88.07, test loss : 0.3947, test accuracy : 87.61\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.3894, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3849, accuracy : 88.04\n",
            "iteration : 150, loss : 0.3769, accuracy : 88.30\n",
            "iteration : 200, loss : 0.3767, accuracy : 88.37\n",
            "iteration : 250, loss : 0.3778, accuracy : 88.29\n",
            "iteration : 300, loss : 0.3773, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.28\n",
            "Epoch :  35, training loss : 0.3785, training accuracy : 88.29, test loss : 0.3895, test accuracy : 88.02\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.3783, accuracy : 87.86\n",
            "iteration : 100, loss : 0.3801, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3808, accuracy : 88.01\n",
            "iteration : 200, loss : 0.3809, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3808, accuracy : 88.21\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.14\n",
            "Epoch :  36, training loss : 0.3776, training accuracy : 88.19, test loss : 0.3913, test accuracy : 87.86\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.3953, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3930, accuracy : 88.02\n",
            "iteration : 150, loss : 0.3903, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3842, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3812, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3797, accuracy : 88.09\n",
            "Epoch :  37, training loss : 0.3798, training accuracy : 88.11, test loss : 0.3929, test accuracy : 87.50\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.3854, accuracy : 87.86\n",
            "iteration : 100, loss : 0.3867, accuracy : 87.90\n",
            "iteration : 150, loss : 0.3793, accuracy : 88.20\n",
            "iteration : 200, loss : 0.3809, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3804, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.19\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.16\n",
            "Epoch :  38, training loss : 0.3784, training accuracy : 88.18, test loss : 0.3889, test accuracy : 87.92\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.3852, accuracy : 88.36\n",
            "iteration : 100, loss : 0.3799, accuracy : 88.30\n",
            "iteration : 150, loss : 0.3834, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3778, accuracy : 88.30\n",
            "iteration : 250, loss : 0.3781, accuracy : 88.31\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3790, accuracy : 88.18\n",
            "Epoch :  39, training loss : 0.3778, training accuracy : 88.21, test loss : 0.3907, test accuracy : 87.83\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.3762, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3738, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3707, accuracy : 88.21\n",
            "iteration : 200, loss : 0.3715, accuracy : 88.22\n",
            "iteration : 250, loss : 0.3749, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3725, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.09\n",
            "Epoch :  40, training loss : 0.3784, training accuracy : 88.08, test loss : 0.3918, test accuracy : 87.81\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.3648, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3713, accuracy : 88.23\n",
            "iteration : 150, loss : 0.3734, accuracy : 88.11\n",
            "iteration : 200, loss : 0.3819, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3834, accuracy : 87.98\n",
            "iteration : 300, loss : 0.3829, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3820, accuracy : 88.08\n",
            "Epoch :  41, training loss : 0.3813, training accuracy : 88.07, test loss : 0.3900, test accuracy : 87.83\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.3784, accuracy : 88.28\n",
            "iteration : 100, loss : 0.3796, accuracy : 88.42\n",
            "iteration : 150, loss : 0.3781, accuracy : 88.19\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3778, accuracy : 88.33\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.29\n",
            "iteration : 350, loss : 0.3777, accuracy : 88.27\n",
            "Epoch :  42, training loss : 0.3776, training accuracy : 88.23, test loss : 0.3890, test accuracy : 87.94\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.3738, accuracy : 88.50\n",
            "iteration : 100, loss : 0.3783, accuracy : 88.33\n",
            "iteration : 150, loss : 0.3755, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3747, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3758, accuracy : 88.15\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.18\n",
            "iteration : 350, loss : 0.3768, accuracy : 88.25\n",
            "Epoch :  43, training loss : 0.3778, training accuracy : 88.23, test loss : 0.3931, test accuracy : 87.69\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.3613, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3658, accuracy : 88.24\n",
            "iteration : 150, loss : 0.3712, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3759, accuracy : 88.22\n",
            "iteration : 250, loss : 0.3808, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3809, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3788, accuracy : 88.17\n",
            "Epoch :  44, training loss : 0.3786, training accuracy : 88.16, test loss : 0.3919, test accuracy : 87.72\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.3716, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3847, accuracy : 88.02\n",
            "iteration : 150, loss : 0.3794, accuracy : 88.04\n",
            "iteration : 200, loss : 0.3789, accuracy : 88.16\n",
            "iteration : 250, loss : 0.3756, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3754, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3753, accuracy : 88.33\n",
            "Epoch :  45, training loss : 0.3763, training accuracy : 88.27, test loss : 0.3893, test accuracy : 87.82\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.3974, accuracy : 87.70\n",
            "iteration : 100, loss : 0.3873, accuracy : 87.96\n",
            "iteration : 150, loss : 0.3811, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3795, accuracy : 88.23\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3814, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3777, accuracy : 88.29\n",
            "Epoch :  46, training loss : 0.3766, training accuracy : 88.32, test loss : 0.3919, test accuracy : 87.81\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.3956, accuracy : 87.78\n",
            "iteration : 100, loss : 0.3861, accuracy : 87.87\n",
            "iteration : 150, loss : 0.3829, accuracy : 88.02\n",
            "iteration : 200, loss : 0.3795, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3819, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3810, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3827, accuracy : 88.02\n",
            "Epoch :  47, training loss : 0.3826, training accuracy : 88.06, test loss : 0.3910, test accuracy : 87.88\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.3662, accuracy : 88.22\n",
            "iteration : 100, loss : 0.3708, accuracy : 88.21\n",
            "iteration : 150, loss : 0.3752, accuracy : 88.23\n",
            "iteration : 200, loss : 0.3770, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3806, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3814, accuracy : 88.07\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.17\n",
            "Epoch :  48, training loss : 0.3788, training accuracy : 88.18, test loss : 0.3937, test accuracy : 87.66\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.3816, accuracy : 88.08\n",
            "iteration : 100, loss : 0.3813, accuracy : 87.97\n",
            "iteration : 150, loss : 0.3758, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3756, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3755, accuracy : 88.19\n",
            "iteration : 300, loss : 0.3793, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3783, accuracy : 88.17\n",
            "Epoch :  49, training loss : 0.3783, training accuracy : 88.17, test loss : 0.3921, test accuracy : 87.84\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.3788, accuracy : 88.08\n",
            "iteration : 100, loss : 0.3858, accuracy : 87.91\n",
            "iteration : 150, loss : 0.3864, accuracy : 87.85\n",
            "iteration : 200, loss : 0.3817, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3802, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3798, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3787, accuracy : 88.12\n",
            "Epoch :  50, training loss : 0.3785, training accuracy : 88.16, test loss : 0.3897, test accuracy : 87.77\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.3681, accuracy : 88.69\n",
            "iteration : 100, loss : 0.3776, accuracy : 88.25\n",
            "iteration : 150, loss : 0.3776, accuracy : 88.18\n",
            "iteration : 200, loss : 0.3797, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3819, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3808, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.12\n",
            "Epoch :  51, training loss : 0.3779, training accuracy : 88.14, test loss : 0.3898, test accuracy : 87.88\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.3743, accuracy : 88.22\n",
            "iteration : 100, loss : 0.3766, accuracy : 88.28\n",
            "iteration : 150, loss : 0.3772, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3795, accuracy : 87.96\n",
            "iteration : 250, loss : 0.3781, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3786, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.15\n",
            "Epoch :  52, training loss : 0.3780, training accuracy : 88.15, test loss : 0.3902, test accuracy : 87.78\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.3779, accuracy : 87.88\n",
            "iteration : 100, loss : 0.3827, accuracy : 87.79\n",
            "iteration : 150, loss : 0.3778, accuracy : 87.94\n",
            "iteration : 200, loss : 0.3793, accuracy : 87.91\n",
            "iteration : 250, loss : 0.3811, accuracy : 87.97\n",
            "iteration : 300, loss : 0.3768, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3788, accuracy : 88.12\n",
            "Epoch :  53, training loss : 0.3780, training accuracy : 88.12, test loss : 0.3899, test accuracy : 87.79\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.3670, accuracy : 88.41\n",
            "iteration : 100, loss : 0.3724, accuracy : 88.32\n",
            "iteration : 150, loss : 0.3775, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3818, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3851, accuracy : 87.94\n",
            "iteration : 300, loss : 0.3828, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3818, accuracy : 88.02\n",
            "Epoch :  54, training loss : 0.3814, training accuracy : 88.02, test loss : 0.3912, test accuracy : 87.80\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.3726, accuracy : 88.39\n",
            "iteration : 100, loss : 0.3762, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3689, accuracy : 88.43\n",
            "iteration : 200, loss : 0.3734, accuracy : 88.34\n",
            "iteration : 250, loss : 0.3759, accuracy : 88.33\n",
            "iteration : 300, loss : 0.3799, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3795, accuracy : 88.13\n",
            "Epoch :  55, training loss : 0.3780, training accuracy : 88.19, test loss : 0.3901, test accuracy : 87.82\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.3721, accuracy : 88.48\n",
            "iteration : 100, loss : 0.3691, accuracy : 88.41\n",
            "iteration : 150, loss : 0.3690, accuracy : 88.43\n",
            "iteration : 200, loss : 0.3681, accuracy : 88.39\n",
            "iteration : 250, loss : 0.3758, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3783, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.22\n",
            "Epoch :  56, training loss : 0.3792, training accuracy : 88.19, test loss : 0.3881, test accuracy : 87.89\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.3691, accuracy : 88.75\n",
            "iteration : 100, loss : 0.3718, accuracy : 88.50\n",
            "iteration : 150, loss : 0.3735, accuracy : 88.47\n",
            "iteration : 200, loss : 0.3745, accuracy : 88.49\n",
            "iteration : 250, loss : 0.3794, accuracy : 88.41\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.34\n",
            "iteration : 350, loss : 0.3773, accuracy : 88.32\n",
            "Epoch :  57, training loss : 0.3767, training accuracy : 88.31, test loss : 0.3908, test accuracy : 87.93\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.3958, accuracy : 87.75\n",
            "iteration : 100, loss : 0.3778, accuracy : 88.26\n",
            "iteration : 150, loss : 0.3782, accuracy : 88.27\n",
            "iteration : 200, loss : 0.3768, accuracy : 88.26\n",
            "iteration : 250, loss : 0.3821, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3768, accuracy : 88.23\n",
            "Epoch :  58, training loss : 0.3780, training accuracy : 88.16, test loss : 0.3912, test accuracy : 87.95\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.3775, accuracy : 88.34\n",
            "iteration : 100, loss : 0.3758, accuracy : 88.33\n",
            "iteration : 150, loss : 0.3748, accuracy : 88.40\n",
            "iteration : 200, loss : 0.3809, accuracy : 88.28\n",
            "iteration : 250, loss : 0.3788, accuracy : 88.26\n",
            "iteration : 300, loss : 0.3767, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3760, accuracy : 88.27\n",
            "Epoch :  59, training loss : 0.3787, training accuracy : 88.17, test loss : 0.3905, test accuracy : 87.78\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.3714, accuracy : 88.31\n",
            "iteration : 100, loss : 0.3847, accuracy : 87.80\n",
            "iteration : 150, loss : 0.3778, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3798, accuracy : 87.99\n",
            "iteration : 250, loss : 0.3832, accuracy : 87.89\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3771, accuracy : 88.18\n",
            "Epoch :  60, training loss : 0.3765, training accuracy : 88.19, test loss : 0.3921, test accuracy : 87.89\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.3769, accuracy : 88.67\n",
            "iteration : 100, loss : 0.3713, accuracy : 88.77\n",
            "iteration : 150, loss : 0.3772, accuracy : 88.43\n",
            "iteration : 200, loss : 0.3763, accuracy : 88.46\n",
            "iteration : 250, loss : 0.3774, accuracy : 88.44\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.32\n",
            "iteration : 350, loss : 0.3805, accuracy : 88.23\n",
            "Epoch :  61, training loss : 0.3788, training accuracy : 88.26, test loss : 0.3916, test accuracy : 87.79\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.3594, accuracy : 88.77\n",
            "iteration : 100, loss : 0.3610, accuracy : 88.64\n",
            "iteration : 150, loss : 0.3724, accuracy : 88.35\n",
            "iteration : 200, loss : 0.3790, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3781, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3815, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3815, accuracy : 88.08\n",
            "Epoch :  62, training loss : 0.3799, training accuracy : 88.12, test loss : 0.3869, test accuracy : 88.00\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.3818, accuracy : 88.09\n",
            "iteration : 100, loss : 0.3839, accuracy : 88.06\n",
            "iteration : 150, loss : 0.3840, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3818, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3789, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3767, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3763, accuracy : 88.17\n",
            "Epoch :  63, training loss : 0.3771, training accuracy : 88.14, test loss : 0.3899, test accuracy : 87.88\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.3691, accuracy : 88.36\n",
            "iteration : 100, loss : 0.3749, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3808, accuracy : 87.88\n",
            "iteration : 200, loss : 0.3787, accuracy : 88.01\n",
            "iteration : 250, loss : 0.3802, accuracy : 88.00\n",
            "iteration : 300, loss : 0.3770, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.11\n",
            "Epoch :  64, training loss : 0.3775, training accuracy : 88.16, test loss : 0.3904, test accuracy : 87.79\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.3651, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3648, accuracy : 88.41\n",
            "iteration : 150, loss : 0.3708, accuracy : 88.35\n",
            "iteration : 200, loss : 0.3777, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3806, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3811, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3790, accuracy : 88.10\n",
            "Epoch :  65, training loss : 0.3801, training accuracy : 88.09, test loss : 0.3894, test accuracy : 87.68\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.3869, accuracy : 87.83\n",
            "iteration : 100, loss : 0.3897, accuracy : 87.80\n",
            "iteration : 150, loss : 0.3829, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3868, accuracy : 88.05\n",
            "iteration : 250, loss : 0.3813, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3817, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3795, accuracy : 88.19\n",
            "Epoch :  66, training loss : 0.3791, training accuracy : 88.25, test loss : 0.3909, test accuracy : 87.81\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.3694, accuracy : 88.41\n",
            "iteration : 100, loss : 0.3720, accuracy : 88.33\n",
            "iteration : 150, loss : 0.3783, accuracy : 88.08\n",
            "iteration : 200, loss : 0.3801, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3802, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3800, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3812, accuracy : 88.08\n",
            "Epoch :  67, training loss : 0.3805, training accuracy : 88.12, test loss : 0.3910, test accuracy : 87.72\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.3717, accuracy : 88.47\n",
            "iteration : 100, loss : 0.3709, accuracy : 88.57\n",
            "iteration : 150, loss : 0.3724, accuracy : 88.45\n",
            "iteration : 200, loss : 0.3762, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3732, accuracy : 88.32\n",
            "iteration : 300, loss : 0.3753, accuracy : 88.25\n",
            "iteration : 350, loss : 0.3771, accuracy : 88.14\n",
            "Epoch :  68, training loss : 0.3770, training accuracy : 88.11, test loss : 0.3894, test accuracy : 87.82\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.3806, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3818, accuracy : 88.09\n",
            "iteration : 150, loss : 0.3822, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3808, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3768, accuracy : 88.19\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.14\n",
            "Epoch :  69, training loss : 0.3789, training accuracy : 88.09, test loss : 0.3913, test accuracy : 87.79\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.3798, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3761, accuracy : 88.19\n",
            "iteration : 150, loss : 0.3823, accuracy : 88.08\n",
            "iteration : 200, loss : 0.3790, accuracy : 88.07\n",
            "iteration : 250, loss : 0.3795, accuracy : 87.99\n",
            "iteration : 300, loss : 0.3794, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3779, accuracy : 88.08\n",
            "Epoch :  70, training loss : 0.3784, training accuracy : 88.10, test loss : 0.3887, test accuracy : 87.82\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.3644, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3706, accuracy : 88.07\n",
            "iteration : 150, loss : 0.3779, accuracy : 88.04\n",
            "iteration : 200, loss : 0.3794, accuracy : 87.98\n",
            "iteration : 250, loss : 0.3788, accuracy : 88.04\n",
            "iteration : 300, loss : 0.3818, accuracy : 87.96\n",
            "iteration : 350, loss : 0.3819, accuracy : 88.01\n",
            "Epoch :  71, training loss : 0.3806, training accuracy : 88.04, test loss : 0.3922, test accuracy : 87.83\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.3725, accuracy : 88.45\n",
            "iteration : 100, loss : 0.3775, accuracy : 88.18\n",
            "iteration : 150, loss : 0.3702, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3746, accuracy : 88.25\n",
            "iteration : 250, loss : 0.3755, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.18\n",
            "Epoch :  72, training loss : 0.3778, training accuracy : 88.18, test loss : 0.3925, test accuracy : 87.78\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.3837, accuracy : 87.81\n",
            "iteration : 100, loss : 0.3851, accuracy : 87.84\n",
            "iteration : 150, loss : 0.3818, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3830, accuracy : 87.94\n",
            "iteration : 250, loss : 0.3837, accuracy : 87.95\n",
            "iteration : 300, loss : 0.3821, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3817, accuracy : 88.06\n",
            "Epoch :  73, training loss : 0.3809, training accuracy : 88.10, test loss : 0.3933, test accuracy : 87.78\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.3917, accuracy : 87.80\n",
            "iteration : 100, loss : 0.3790, accuracy : 88.19\n",
            "iteration : 150, loss : 0.3741, accuracy : 88.32\n",
            "iteration : 200, loss : 0.3723, accuracy : 88.34\n",
            "iteration : 250, loss : 0.3743, accuracy : 88.29\n",
            "iteration : 300, loss : 0.3795, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.11\n",
            "Epoch :  74, training loss : 0.3794, training accuracy : 88.14, test loss : 0.3923, test accuracy : 87.72\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.3901, accuracy : 87.77\n",
            "iteration : 100, loss : 0.3873, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3833, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3849, accuracy : 87.91\n",
            "iteration : 250, loss : 0.3840, accuracy : 87.98\n",
            "iteration : 300, loss : 0.3830, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3815, accuracy : 88.06\n",
            "Epoch :  75, training loss : 0.3814, training accuracy : 88.08, test loss : 0.3923, test accuracy : 87.73\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.3858, accuracy : 87.98\n",
            "iteration : 100, loss : 0.3845, accuracy : 88.20\n",
            "iteration : 150, loss : 0.3816, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3776, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3748, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3767, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3790, accuracy : 88.11\n",
            "Epoch :  76, training loss : 0.3796, training accuracy : 88.09, test loss : 0.3902, test accuracy : 87.82\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.3782, accuracy : 88.00\n",
            "iteration : 100, loss : 0.3730, accuracy : 88.37\n",
            "iteration : 150, loss : 0.3755, accuracy : 88.41\n",
            "iteration : 200, loss : 0.3712, accuracy : 88.44\n",
            "iteration : 250, loss : 0.3735, accuracy : 88.35\n",
            "iteration : 300, loss : 0.3767, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3770, accuracy : 88.30\n",
            "Epoch :  77, training loss : 0.3784, training accuracy : 88.24, test loss : 0.3913, test accuracy : 87.81\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.3801, accuracy : 88.70\n",
            "iteration : 100, loss : 0.3870, accuracy : 88.11\n",
            "iteration : 150, loss : 0.3840, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3800, accuracy : 88.19\n",
            "iteration : 250, loss : 0.3807, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3789, accuracy : 88.19\n",
            "iteration : 350, loss : 0.3773, accuracy : 88.21\n",
            "Epoch :  78, training loss : 0.3789, training accuracy : 88.13, test loss : 0.3891, test accuracy : 87.88\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.3686, accuracy : 88.14\n",
            "iteration : 100, loss : 0.3825, accuracy : 87.90\n",
            "iteration : 150, loss : 0.3765, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3789, accuracy : 88.07\n",
            "iteration : 250, loss : 0.3801, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3806, accuracy : 88.08\n",
            "Epoch :  79, training loss : 0.3796, training accuracy : 88.10, test loss : 0.3926, test accuracy : 87.83\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.3781, accuracy : 88.31\n",
            "iteration : 100, loss : 0.3819, accuracy : 88.09\n",
            "iteration : 150, loss : 0.3776, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3744, accuracy : 88.31\n",
            "iteration : 250, loss : 0.3748, accuracy : 88.32\n",
            "iteration : 300, loss : 0.3759, accuracy : 88.33\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.27\n",
            "Epoch :  80, training loss : 0.3785, training accuracy : 88.26, test loss : 0.3907, test accuracy : 87.85\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.3862, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3835, accuracy : 88.02\n",
            "iteration : 150, loss : 0.3826, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3792, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3782, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3766, accuracy : 88.29\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.24\n",
            "Epoch :  81, training loss : 0.3768, training accuracy : 88.25, test loss : 0.3922, test accuracy : 87.91\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.3865, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3859, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3878, accuracy : 88.01\n",
            "iteration : 200, loss : 0.3790, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3757, accuracy : 88.31\n",
            "iteration : 300, loss : 0.3747, accuracy : 88.29\n",
            "iteration : 350, loss : 0.3767, accuracy : 88.17\n",
            "Epoch :  82, training loss : 0.3770, training accuracy : 88.16, test loss : 0.3910, test accuracy : 87.73\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.3755, accuracy : 88.42\n",
            "iteration : 100, loss : 0.3783, accuracy : 88.08\n",
            "iteration : 150, loss : 0.3787, accuracy : 87.97\n",
            "iteration : 200, loss : 0.3776, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3795, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3821, accuracy : 88.00\n",
            "iteration : 350, loss : 0.3812, accuracy : 88.07\n",
            "Epoch :  83, training loss : 0.3815, training accuracy : 88.06, test loss : 0.3908, test accuracy : 87.81\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.3762, accuracy : 87.75\n",
            "iteration : 100, loss : 0.3705, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3699, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3764, accuracy : 88.03\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3766, accuracy : 88.18\n",
            "Epoch :  84, training loss : 0.3775, training accuracy : 88.12, test loss : 0.3899, test accuracy : 87.73\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.3847, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3854, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3785, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3800, accuracy : 88.10\n",
            "iteration : 250, loss : 0.3789, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3807, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.11\n",
            "Epoch :  85, training loss : 0.3793, training accuracy : 88.15, test loss : 0.3929, test accuracy : 87.74\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.3840, accuracy : 87.94\n",
            "iteration : 100, loss : 0.3689, accuracy : 88.33\n",
            "iteration : 150, loss : 0.3719, accuracy : 88.26\n",
            "iteration : 200, loss : 0.3706, accuracy : 88.38\n",
            "iteration : 250, loss : 0.3729, accuracy : 88.35\n",
            "iteration : 300, loss : 0.3752, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3765, accuracy : 88.24\n",
            "Epoch :  86, training loss : 0.3770, training accuracy : 88.25, test loss : 0.3889, test accuracy : 87.73\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.4015, accuracy : 87.61\n",
            "iteration : 100, loss : 0.3788, accuracy : 88.17\n",
            "iteration : 150, loss : 0.3742, accuracy : 88.29\n",
            "iteration : 200, loss : 0.3741, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3777, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3783, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3758, accuracy : 88.13\n",
            "Epoch :  87, training loss : 0.3764, training accuracy : 88.15, test loss : 0.3889, test accuracy : 87.89\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.3805, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3863, accuracy : 88.08\n",
            "iteration : 150, loss : 0.3886, accuracy : 87.99\n",
            "iteration : 200, loss : 0.3801, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3791, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3802, accuracy : 88.00\n",
            "Epoch :  88, training loss : 0.3792, training accuracy : 88.05, test loss : 0.3926, test accuracy : 87.67\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.3534, accuracy : 88.72\n",
            "iteration : 100, loss : 0.3571, accuracy : 88.74\n",
            "iteration : 150, loss : 0.3663, accuracy : 88.55\n",
            "iteration : 200, loss : 0.3709, accuracy : 88.35\n",
            "iteration : 250, loss : 0.3726, accuracy : 88.31\n",
            "iteration : 300, loss : 0.3742, accuracy : 88.27\n",
            "iteration : 350, loss : 0.3775, accuracy : 88.16\n",
            "Epoch :  89, training loss : 0.3787, training accuracy : 88.16, test loss : 0.3897, test accuracy : 87.84\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.3650, accuracy : 88.22\n",
            "iteration : 100, loss : 0.3776, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3821, accuracy : 88.02\n",
            "iteration : 200, loss : 0.3827, accuracy : 88.01\n",
            "iteration : 250, loss : 0.3828, accuracy : 88.01\n",
            "iteration : 300, loss : 0.3784, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3792, accuracy : 88.10\n",
            "Epoch :  90, training loss : 0.3787, training accuracy : 88.12, test loss : 0.3923, test accuracy : 87.79\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.3759, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3818, accuracy : 88.32\n",
            "iteration : 150, loss : 0.3852, accuracy : 88.23\n",
            "iteration : 200, loss : 0.3825, accuracy : 88.19\n",
            "iteration : 250, loss : 0.3801, accuracy : 88.30\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.31\n",
            "Epoch :  91, training loss : 0.3778, training accuracy : 88.33, test loss : 0.3928, test accuracy : 87.70\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.3778, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3835, accuracy : 88.07\n",
            "iteration : 150, loss : 0.3778, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3789, accuracy : 88.17\n",
            "iteration : 250, loss : 0.3760, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.19\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.20\n",
            "Epoch :  92, training loss : 0.3772, training accuracy : 88.21, test loss : 0.3948, test accuracy : 87.80\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.3938, accuracy : 87.94\n",
            "iteration : 100, loss : 0.3857, accuracy : 88.13\n",
            "iteration : 150, loss : 0.3831, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3807, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3795, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.28\n",
            "iteration : 350, loss : 0.3765, accuracy : 88.29\n",
            "Epoch :  93, training loss : 0.3777, training accuracy : 88.24, test loss : 0.3924, test accuracy : 87.78\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.3609, accuracy : 88.88\n",
            "iteration : 100, loss : 0.3639, accuracy : 88.90\n",
            "iteration : 150, loss : 0.3659, accuracy : 88.77\n",
            "iteration : 200, loss : 0.3693, accuracy : 88.75\n",
            "iteration : 250, loss : 0.3731, accuracy : 88.55\n",
            "iteration : 300, loss : 0.3750, accuracy : 88.45\n",
            "iteration : 350, loss : 0.3752, accuracy : 88.36\n",
            "Epoch :  94, training loss : 0.3766, training accuracy : 88.27, test loss : 0.3879, test accuracy : 87.90\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.3646, accuracy : 88.58\n",
            "iteration : 100, loss : 0.3656, accuracy : 88.70\n",
            "iteration : 150, loss : 0.3761, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3783, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3761, accuracy : 88.39\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.39\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.27\n",
            "Epoch :  95, training loss : 0.3802, training accuracy : 88.24, test loss : 0.3911, test accuracy : 87.75\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.3865, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3770, accuracy : 88.01\n",
            "iteration : 150, loss : 0.3777, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3779, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3765, accuracy : 87.97\n",
            "iteration : 300, loss : 0.3770, accuracy : 87.96\n",
            "iteration : 350, loss : 0.3775, accuracy : 87.96\n",
            "Epoch :  96, training loss : 0.3798, training accuracy : 87.92, test loss : 0.3914, test accuracy : 87.83\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.3685, accuracy : 87.97\n",
            "iteration : 100, loss : 0.3699, accuracy : 88.21\n",
            "iteration : 150, loss : 0.3681, accuracy : 88.36\n",
            "iteration : 200, loss : 0.3736, accuracy : 88.30\n",
            "iteration : 250, loss : 0.3764, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3802, accuracy : 88.03\n",
            "Epoch :  97, training loss : 0.3792, training accuracy : 88.05, test loss : 0.3890, test accuracy : 87.87\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.3670, accuracy : 88.48\n",
            "iteration : 100, loss : 0.3735, accuracy : 88.27\n",
            "iteration : 150, loss : 0.3755, accuracy : 88.21\n",
            "iteration : 200, loss : 0.3738, accuracy : 88.28\n",
            "iteration : 250, loss : 0.3744, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3735, accuracy : 88.28\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.16\n",
            "Epoch :  98, training loss : 0.3785, training accuracy : 88.16, test loss : 0.3895, test accuracy : 87.87\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.3815, accuracy : 88.39\n",
            "iteration : 100, loss : 0.3767, accuracy : 88.43\n",
            "iteration : 150, loss : 0.3835, accuracy : 88.30\n",
            "iteration : 200, loss : 0.3802, accuracy : 88.26\n",
            "iteration : 250, loss : 0.3803, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3779, accuracy : 88.27\n",
            "Epoch :  99, training loss : 0.3775, training accuracy : 88.27, test loss : 0.3915, test accuracy : 87.79\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.3651, accuracy : 88.95\n",
            "iteration : 100, loss : 0.3774, accuracy : 88.41\n",
            "iteration : 150, loss : 0.3790, accuracy : 88.27\n",
            "iteration : 200, loss : 0.3828, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3809, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3801, accuracy : 88.18\n",
            "iteration : 350, loss : 0.3806, accuracy : 88.11\n",
            "Epoch : 100, training loss : 0.3807, training accuracy : 88.11, test loss : 0.3905, test accuracy : 87.85\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.3695, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3766, accuracy : 88.29\n",
            "iteration : 150, loss : 0.3773, accuracy : 88.14\n",
            "iteration : 200, loss : 0.3772, accuracy : 88.23\n",
            "iteration : 250, loss : 0.3775, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.15\n",
            "Epoch : 101, training loss : 0.3792, training accuracy : 88.13, test loss : 0.3892, test accuracy : 87.92\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.3714, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3700, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3756, accuracy : 88.20\n",
            "iteration : 200, loss : 0.3770, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.27\n",
            "iteration : 350, loss : 0.3777, accuracy : 88.29\n",
            "Epoch : 102, training loss : 0.3783, training accuracy : 88.28, test loss : 0.3926, test accuracy : 87.85\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.4007, accuracy : 87.42\n",
            "iteration : 100, loss : 0.3921, accuracy : 87.66\n",
            "iteration : 150, loss : 0.3814, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3819, accuracy : 88.07\n",
            "iteration : 250, loss : 0.3816, accuracy : 88.16\n",
            "iteration : 300, loss : 0.3815, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3798, accuracy : 88.21\n",
            "Epoch : 103, training loss : 0.3801, training accuracy : 88.22, test loss : 0.3931, test accuracy : 87.83\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.3685, accuracy : 88.41\n",
            "iteration : 100, loss : 0.3770, accuracy : 88.00\n",
            "iteration : 150, loss : 0.3775, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3778, accuracy : 88.13\n",
            "iteration : 250, loss : 0.3779, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3777, accuracy : 88.20\n",
            "Epoch : 104, training loss : 0.3779, training accuracy : 88.20, test loss : 0.3926, test accuracy : 87.80\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.3715, accuracy : 88.48\n",
            "iteration : 100, loss : 0.3778, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3792, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3843, accuracy : 88.04\n",
            "iteration : 250, loss : 0.3843, accuracy : 88.00\n",
            "iteration : 300, loss : 0.3825, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3815, accuracy : 88.14\n",
            "Epoch : 105, training loss : 0.3803, training accuracy : 88.18, test loss : 0.3940, test accuracy : 87.71\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.3837, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3872, accuracy : 88.19\n",
            "iteration : 150, loss : 0.3746, accuracy : 88.40\n",
            "iteration : 200, loss : 0.3738, accuracy : 88.26\n",
            "iteration : 250, loss : 0.3750, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.11\n",
            "Epoch : 106, training loss : 0.3792, training accuracy : 88.10, test loss : 0.3908, test accuracy : 87.75\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.3928, accuracy : 87.45\n",
            "iteration : 100, loss : 0.3866, accuracy : 87.79\n",
            "iteration : 150, loss : 0.3828, accuracy : 87.93\n",
            "iteration : 200, loss : 0.3814, accuracy : 87.96\n",
            "iteration : 250, loss : 0.3807, accuracy : 87.98\n",
            "iteration : 300, loss : 0.3810, accuracy : 87.99\n",
            "iteration : 350, loss : 0.3820, accuracy : 87.97\n",
            "Epoch : 107, training loss : 0.3811, training accuracy : 88.01, test loss : 0.3892, test accuracy : 87.98\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.3850, accuracy : 87.69\n",
            "iteration : 100, loss : 0.3811, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3794, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3763, accuracy : 88.04\n",
            "iteration : 250, loss : 0.3783, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3761, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.13\n",
            "Epoch : 108, training loss : 0.3782, training accuracy : 88.09, test loss : 0.3915, test accuracy : 87.70\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.3813, accuracy : 88.33\n",
            "iteration : 100, loss : 0.3790, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3745, accuracy : 88.17\n",
            "iteration : 200, loss : 0.3708, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3736, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3759, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3777, accuracy : 88.06\n",
            "Epoch : 109, training loss : 0.3784, training accuracy : 88.10, test loss : 0.3905, test accuracy : 87.85\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.3786, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3820, accuracy : 87.79\n",
            "iteration : 150, loss : 0.3802, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3800, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3790, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3801, accuracy : 88.08\n",
            "Epoch : 110, training loss : 0.3807, training accuracy : 88.10, test loss : 0.3912, test accuracy : 87.88\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.3898, accuracy : 87.66\n",
            "iteration : 100, loss : 0.3875, accuracy : 87.83\n",
            "iteration : 150, loss : 0.3867, accuracy : 87.92\n",
            "iteration : 200, loss : 0.3841, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3815, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3818, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3823, accuracy : 88.03\n",
            "Epoch : 111, training loss : 0.3807, training accuracy : 88.06, test loss : 0.3944, test accuracy : 87.74\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.3835, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3735, accuracy : 88.36\n",
            "iteration : 150, loss : 0.3798, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3782, accuracy : 88.08\n",
            "iteration : 250, loss : 0.3795, accuracy : 87.96\n",
            "iteration : 300, loss : 0.3799, accuracy : 88.02\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.10\n",
            "Epoch : 112, training loss : 0.3776, training accuracy : 88.16, test loss : 0.3913, test accuracy : 87.81\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.3816, accuracy : 87.56\n",
            "iteration : 100, loss : 0.3825, accuracy : 88.08\n",
            "iteration : 150, loss : 0.3823, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3808, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3790, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3795, accuracy : 88.07\n",
            "Epoch : 113, training loss : 0.3794, training accuracy : 88.11, test loss : 0.3938, test accuracy : 87.65\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.3749, accuracy : 88.69\n",
            "iteration : 100, loss : 0.3727, accuracy : 88.48\n",
            "iteration : 150, loss : 0.3734, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3746, accuracy : 88.28\n",
            "iteration : 250, loss : 0.3779, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3791, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.11\n",
            "Epoch : 114, training loss : 0.3799, training accuracy : 88.12, test loss : 0.3891, test accuracy : 87.75\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.3855, accuracy : 87.91\n",
            "iteration : 100, loss : 0.3880, accuracy : 87.93\n",
            "iteration : 150, loss : 0.3837, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3829, accuracy : 88.03\n",
            "iteration : 250, loss : 0.3822, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3814, accuracy : 88.02\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.09\n",
            "Epoch : 115, training loss : 0.3791, training accuracy : 88.09, test loss : 0.3961, test accuracy : 87.52\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.3761, accuracy : 88.61\n",
            "iteration : 100, loss : 0.3757, accuracy : 88.36\n",
            "iteration : 150, loss : 0.3753, accuracy : 88.59\n",
            "iteration : 200, loss : 0.3792, accuracy : 88.40\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.41\n",
            "iteration : 300, loss : 0.3785, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.29\n",
            "Epoch : 116, training loss : 0.3767, training accuracy : 88.29, test loss : 0.3884, test accuracy : 88.06\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.3748, accuracy : 88.11\n",
            "iteration : 100, loss : 0.3828, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3803, accuracy : 88.00\n",
            "iteration : 200, loss : 0.3788, accuracy : 87.99\n",
            "iteration : 250, loss : 0.3770, accuracy : 88.15\n",
            "iteration : 300, loss : 0.3769, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.10\n",
            "Epoch : 117, training loss : 0.3784, training accuracy : 88.08, test loss : 0.3920, test accuracy : 87.80\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.3744, accuracy : 87.97\n",
            "iteration : 100, loss : 0.3840, accuracy : 87.74\n",
            "iteration : 150, loss : 0.3782, accuracy : 88.01\n",
            "iteration : 200, loss : 0.3815, accuracy : 87.99\n",
            "iteration : 250, loss : 0.3777, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3804, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3810, accuracy : 88.00\n",
            "Epoch : 118, training loss : 0.3804, training accuracy : 88.01, test loss : 0.3887, test accuracy : 87.92\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.3663, accuracy : 88.55\n",
            "iteration : 100, loss : 0.3723, accuracy : 88.22\n",
            "iteration : 150, loss : 0.3703, accuracy : 88.30\n",
            "iteration : 200, loss : 0.3721, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3748, accuracy : 88.27\n",
            "iteration : 300, loss : 0.3759, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3764, accuracy : 88.21\n",
            "Epoch : 119, training loss : 0.3770, training accuracy : 88.17, test loss : 0.3946, test accuracy : 87.57\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.3585, accuracy : 88.89\n",
            "iteration : 100, loss : 0.3658, accuracy : 88.52\n",
            "iteration : 150, loss : 0.3716, accuracy : 88.35\n",
            "iteration : 200, loss : 0.3733, accuracy : 88.36\n",
            "iteration : 250, loss : 0.3744, accuracy : 88.38\n",
            "iteration : 300, loss : 0.3751, accuracy : 88.39\n",
            "iteration : 350, loss : 0.3767, accuracy : 88.27\n",
            "Epoch : 120, training loss : 0.3762, training accuracy : 88.28, test loss : 0.3901, test accuracy : 87.82\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.3760, accuracy : 88.02\n",
            "iteration : 100, loss : 0.3765, accuracy : 88.15\n",
            "iteration : 150, loss : 0.3783, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3775, accuracy : 88.23\n",
            "iteration : 250, loss : 0.3800, accuracy : 88.21\n",
            "iteration : 300, loss : 0.3825, accuracy : 88.07\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.16\n",
            "Epoch : 121, training loss : 0.3803, training accuracy : 88.11, test loss : 0.3927, test accuracy : 87.73\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.3518, accuracy : 88.91\n",
            "iteration : 100, loss : 0.3683, accuracy : 88.55\n",
            "iteration : 150, loss : 0.3657, accuracy : 88.56\n",
            "iteration : 200, loss : 0.3709, accuracy : 88.27\n",
            "iteration : 250, loss : 0.3725, accuracy : 88.23\n",
            "iteration : 300, loss : 0.3727, accuracy : 88.22\n",
            "iteration : 350, loss : 0.3753, accuracy : 88.16\n",
            "Epoch : 122, training loss : 0.3769, training accuracy : 88.12, test loss : 0.3912, test accuracy : 87.76\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.3710, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3734, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3772, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3776, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3810, accuracy : 87.96\n",
            "iteration : 300, loss : 0.3811, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3816, accuracy : 87.95\n",
            "Epoch : 123, training loss : 0.3809, training accuracy : 87.99, test loss : 0.3919, test accuracy : 87.81\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.3758, accuracy : 88.58\n",
            "iteration : 100, loss : 0.3836, accuracy : 88.23\n",
            "iteration : 150, loss : 0.3807, accuracy : 88.22\n",
            "iteration : 200, loss : 0.3754, accuracy : 88.30\n",
            "iteration : 250, loss : 0.3770, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3798, accuracy : 88.18\n",
            "Epoch : 124, training loss : 0.3785, training accuracy : 88.22, test loss : 0.3916, test accuracy : 87.85\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.3841, accuracy : 87.50\n",
            "iteration : 100, loss : 0.3825, accuracy : 87.84\n",
            "iteration : 150, loss : 0.3759, accuracy : 87.99\n",
            "iteration : 200, loss : 0.3790, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3769, accuracy : 88.04\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3775, accuracy : 88.17\n",
            "Epoch : 125, training loss : 0.3802, training accuracy : 88.13, test loss : 0.3960, test accuracy : 87.75\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.3530, accuracy : 89.12\n",
            "iteration : 100, loss : 0.3740, accuracy : 88.57\n",
            "iteration : 150, loss : 0.3739, accuracy : 88.46\n",
            "iteration : 200, loss : 0.3765, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3778, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3785, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3815, accuracy : 88.11\n",
            "Epoch : 126, training loss : 0.3799, training accuracy : 88.12, test loss : 0.3925, test accuracy : 87.68\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.3789, accuracy : 87.88\n",
            "iteration : 100, loss : 0.3812, accuracy : 88.01\n",
            "iteration : 150, loss : 0.3823, accuracy : 87.85\n",
            "iteration : 200, loss : 0.3798, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3789, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3774, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3780, accuracy : 88.09\n",
            "Epoch : 127, training loss : 0.3770, training accuracy : 88.14, test loss : 0.3923, test accuracy : 87.63\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.3789, accuracy : 87.98\n",
            "iteration : 100, loss : 0.3808, accuracy : 88.06\n",
            "iteration : 150, loss : 0.3744, accuracy : 88.17\n",
            "iteration : 200, loss : 0.3735, accuracy : 88.29\n",
            "iteration : 250, loss : 0.3737, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3756, accuracy : 88.22\n",
            "iteration : 350, loss : 0.3785, accuracy : 88.16\n",
            "Epoch : 128, training loss : 0.3781, training accuracy : 88.17, test loss : 0.3896, test accuracy : 87.83\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.3648, accuracy : 88.33\n",
            "iteration : 100, loss : 0.3807, accuracy : 88.07\n",
            "iteration : 150, loss : 0.3771, accuracy : 88.20\n",
            "iteration : 200, loss : 0.3796, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.15\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3772, accuracy : 88.16\n",
            "Epoch : 129, training loss : 0.3764, training accuracy : 88.14, test loss : 0.3903, test accuracy : 87.91\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.3841, accuracy : 87.69\n",
            "iteration : 100, loss : 0.3817, accuracy : 87.94\n",
            "iteration : 150, loss : 0.3803, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3773, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3798, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.06\n",
            "Epoch : 130, training loss : 0.3790, training accuracy : 88.10, test loss : 0.3892, test accuracy : 87.77\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.3555, accuracy : 89.00\n",
            "iteration : 100, loss : 0.3727, accuracy : 88.55\n",
            "iteration : 150, loss : 0.3787, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3782, accuracy : 88.10\n",
            "iteration : 250, loss : 0.3764, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.22\n",
            "Epoch : 131, training loss : 0.3790, training accuracy : 88.19, test loss : 0.3902, test accuracy : 87.81\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.3829, accuracy : 87.83\n",
            "iteration : 100, loss : 0.3895, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3814, accuracy : 88.24\n",
            "iteration : 200, loss : 0.3827, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.31\n",
            "iteration : 300, loss : 0.3782, accuracy : 88.27\n",
            "iteration : 350, loss : 0.3809, accuracy : 88.15\n",
            "Epoch : 132, training loss : 0.3789, training accuracy : 88.20, test loss : 0.3889, test accuracy : 88.05\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.3788, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3785, accuracy : 88.25\n",
            "iteration : 150, loss : 0.3747, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.01\n",
            "iteration : 250, loss : 0.3816, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3802, accuracy : 88.12\n",
            "Epoch : 133, training loss : 0.3787, training accuracy : 88.15, test loss : 0.3914, test accuracy : 87.82\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.3640, accuracy : 88.47\n",
            "iteration : 100, loss : 0.3768, accuracy : 88.33\n",
            "iteration : 150, loss : 0.3778, accuracy : 88.34\n",
            "iteration : 200, loss : 0.3795, accuracy : 88.30\n",
            "iteration : 250, loss : 0.3813, accuracy : 88.15\n",
            "iteration : 300, loss : 0.3821, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3808, accuracy : 88.03\n",
            "Epoch : 134, training loss : 0.3802, training accuracy : 88.07, test loss : 0.3924, test accuracy : 87.83\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.3873, accuracy : 88.56\n",
            "iteration : 100, loss : 0.3874, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3823, accuracy : 87.95\n",
            "iteration : 200, loss : 0.3825, accuracy : 87.99\n",
            "iteration : 250, loss : 0.3823, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3808, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3802, accuracy : 88.12\n",
            "Epoch : 135, training loss : 0.3800, training accuracy : 88.12, test loss : 0.3911, test accuracy : 87.69\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.3916, accuracy : 88.17\n",
            "iteration : 100, loss : 0.3784, accuracy : 88.17\n",
            "iteration : 150, loss : 0.3826, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3830, accuracy : 88.16\n",
            "iteration : 250, loss : 0.3798, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3785, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.24\n",
            "Epoch : 136, training loss : 0.3767, training accuracy : 88.26, test loss : 0.3887, test accuracy : 87.76\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.3809, accuracy : 88.16\n",
            "iteration : 100, loss : 0.3865, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3867, accuracy : 87.95\n",
            "iteration : 200, loss : 0.3821, accuracy : 88.03\n",
            "iteration : 250, loss : 0.3803, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.14\n",
            "Epoch : 137, training loss : 0.3791, training accuracy : 88.15, test loss : 0.3895, test accuracy : 87.74\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.3939, accuracy : 87.67\n",
            "iteration : 100, loss : 0.3837, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3810, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3802, accuracy : 88.17\n",
            "iteration : 250, loss : 0.3801, accuracy : 88.15\n",
            "iteration : 300, loss : 0.3786, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3763, accuracy : 88.27\n",
            "Epoch : 138, training loss : 0.3769, training accuracy : 88.26, test loss : 0.3938, test accuracy : 87.80\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.3847, accuracy : 87.67\n",
            "iteration : 100, loss : 0.3784, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3847, accuracy : 87.81\n",
            "iteration : 200, loss : 0.3878, accuracy : 87.82\n",
            "iteration : 250, loss : 0.3849, accuracy : 87.93\n",
            "iteration : 300, loss : 0.3829, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3801, accuracy : 88.06\n",
            "Epoch : 139, training loss : 0.3782, training accuracy : 88.12, test loss : 0.3892, test accuracy : 87.96\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.3603, accuracy : 88.64\n",
            "iteration : 100, loss : 0.3671, accuracy : 88.59\n",
            "iteration : 150, loss : 0.3709, accuracy : 88.28\n",
            "iteration : 200, loss : 0.3705, accuracy : 88.33\n",
            "iteration : 250, loss : 0.3750, accuracy : 88.26\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3801, accuracy : 88.09\n",
            "Epoch : 140, training loss : 0.3796, training accuracy : 88.10, test loss : 0.3928, test accuracy : 87.84\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.3682, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3776, accuracy : 88.09\n",
            "iteration : 150, loss : 0.3830, accuracy : 87.95\n",
            "iteration : 200, loss : 0.3790, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3781, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3790, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3792, accuracy : 88.10\n",
            "Epoch : 141, training loss : 0.3791, training accuracy : 88.11, test loss : 0.3930, test accuracy : 87.65\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.3613, accuracy : 88.47\n",
            "iteration : 100, loss : 0.3813, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3766, accuracy : 88.25\n",
            "iteration : 200, loss : 0.3749, accuracy : 88.25\n",
            "iteration : 250, loss : 0.3743, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3762, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3756, accuracy : 88.25\n",
            "Epoch : 142, training loss : 0.3767, training accuracy : 88.18, test loss : 0.3918, test accuracy : 87.85\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.3654, accuracy : 88.42\n",
            "iteration : 100, loss : 0.3724, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3765, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3819, accuracy : 87.96\n",
            "iteration : 250, loss : 0.3800, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3785, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.13\n",
            "Epoch : 143, training loss : 0.3786, training accuracy : 88.15, test loss : 0.3902, test accuracy : 87.92\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.3721, accuracy : 88.70\n",
            "iteration : 100, loss : 0.3774, accuracy : 88.56\n",
            "iteration : 150, loss : 0.3865, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3781, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3785, accuracy : 88.26\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.35\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.27\n",
            "Epoch : 144, training loss : 0.3785, training accuracy : 88.27, test loss : 0.3916, test accuracy : 87.68\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.3632, accuracy : 88.48\n",
            "iteration : 100, loss : 0.3689, accuracy : 88.38\n",
            "iteration : 150, loss : 0.3780, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3749, accuracy : 88.26\n",
            "iteration : 250, loss : 0.3761, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.20\n",
            "Epoch : 145, training loss : 0.3786, training accuracy : 88.16, test loss : 0.3908, test accuracy : 87.75\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.3886, accuracy : 87.94\n",
            "iteration : 100, loss : 0.3782, accuracy : 88.36\n",
            "iteration : 150, loss : 0.3834, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3838, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3866, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3849, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3803, accuracy : 88.17\n",
            "Epoch : 146, training loss : 0.3798, training accuracy : 88.17, test loss : 0.3900, test accuracy : 87.78\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.3734, accuracy : 88.23\n",
            "iteration : 100, loss : 0.3694, accuracy : 88.13\n",
            "iteration : 150, loss : 0.3788, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3810, accuracy : 88.05\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.17\n",
            "Epoch : 147, training loss : 0.3790, training accuracy : 88.14, test loss : 0.3919, test accuracy : 87.82\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.3834, accuracy : 88.16\n",
            "iteration : 100, loss : 0.3799, accuracy : 88.36\n",
            "iteration : 150, loss : 0.3798, accuracy : 88.17\n",
            "iteration : 200, loss : 0.3822, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3819, accuracy : 87.96\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.08\n",
            "Epoch : 148, training loss : 0.3773, training accuracy : 88.09, test loss : 0.3893, test accuracy : 87.98\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.3938, accuracy : 87.70\n",
            "iteration : 100, loss : 0.3970, accuracy : 87.55\n",
            "iteration : 150, loss : 0.3887, accuracy : 87.69\n",
            "iteration : 200, loss : 0.3874, accuracy : 87.84\n",
            "iteration : 250, loss : 0.3847, accuracy : 87.95\n",
            "iteration : 300, loss : 0.3824, accuracy : 87.98\n",
            "iteration : 350, loss : 0.3831, accuracy : 88.00\n",
            "Epoch : 149, training loss : 0.3811, training accuracy : 88.04, test loss : 0.3916, test accuracy : 87.88\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.3798, accuracy : 88.12\n",
            "iteration : 100, loss : 0.3854, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3752, accuracy : 88.21\n",
            "iteration : 200, loss : 0.3742, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3732, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3732, accuracy : 88.27\n",
            "iteration : 350, loss : 0.3759, accuracy : 88.20\n",
            "Epoch : 150, training loss : 0.3777, training accuracy : 88.15, test loss : 0.3920, test accuracy : 87.64\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.3773, accuracy : 88.20\n",
            "iteration : 100, loss : 0.3812, accuracy : 88.04\n",
            "iteration : 150, loss : 0.3861, accuracy : 87.99\n",
            "iteration : 200, loss : 0.3854, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3850, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.18\n",
            "Epoch : 151, training loss : 0.3799, training accuracy : 88.15, test loss : 0.3898, test accuracy : 87.93\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.3519, accuracy : 89.11\n",
            "iteration : 100, loss : 0.3704, accuracy : 88.53\n",
            "iteration : 150, loss : 0.3720, accuracy : 88.36\n",
            "iteration : 200, loss : 0.3798, accuracy : 88.17\n",
            "iteration : 250, loss : 0.3805, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3774, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3768, accuracy : 88.34\n",
            "Epoch : 152, training loss : 0.3776, training accuracy : 88.29, test loss : 0.3942, test accuracy : 87.65\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.3980, accuracy : 88.20\n",
            "iteration : 100, loss : 0.3851, accuracy : 88.20\n",
            "iteration : 150, loss : 0.3808, accuracy : 88.28\n",
            "iteration : 200, loss : 0.3867, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3862, accuracy : 87.95\n",
            "iteration : 300, loss : 0.3840, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.16\n",
            "Epoch : 153, training loss : 0.3794, training accuracy : 88.14, test loss : 0.3912, test accuracy : 87.68\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.3973, accuracy : 87.20\n",
            "iteration : 100, loss : 0.3892, accuracy : 87.77\n",
            "iteration : 150, loss : 0.3812, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3794, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3785, accuracy : 88.26\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.27\n",
            "Epoch : 154, training loss : 0.3774, training accuracy : 88.29, test loss : 0.3932, test accuracy : 87.60\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.3782, accuracy : 88.44\n",
            "iteration : 100, loss : 0.3855, accuracy : 88.08\n",
            "iteration : 150, loss : 0.3824, accuracy : 88.34\n",
            "iteration : 200, loss : 0.3841, accuracy : 88.27\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.24\n",
            "Epoch : 155, training loss : 0.3787, training accuracy : 88.26, test loss : 0.3956, test accuracy : 87.66\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.3889, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3773, accuracy : 88.47\n",
            "iteration : 150, loss : 0.3751, accuracy : 88.54\n",
            "iteration : 200, loss : 0.3780, accuracy : 88.49\n",
            "iteration : 250, loss : 0.3807, accuracy : 88.31\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.22\n",
            "iteration : 350, loss : 0.3788, accuracy : 88.24\n",
            "Epoch : 156, training loss : 0.3796, training accuracy : 88.22, test loss : 0.3907, test accuracy : 87.65\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.3738, accuracy : 88.28\n",
            "iteration : 100, loss : 0.3811, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3871, accuracy : 87.95\n",
            "iteration : 200, loss : 0.3811, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3805, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3794, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3765, accuracy : 88.26\n",
            "Epoch : 157, training loss : 0.3777, training accuracy : 88.22, test loss : 0.3902, test accuracy : 87.78\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.3813, accuracy : 88.52\n",
            "iteration : 100, loss : 0.3694, accuracy : 88.52\n",
            "iteration : 150, loss : 0.3798, accuracy : 88.32\n",
            "iteration : 200, loss : 0.3773, accuracy : 88.29\n",
            "iteration : 250, loss : 0.3785, accuracy : 88.26\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.25\n",
            "Epoch : 158, training loss : 0.3766, training accuracy : 88.27, test loss : 0.3919, test accuracy : 87.78\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.3729, accuracy : 87.97\n",
            "iteration : 100, loss : 0.3725, accuracy : 88.19\n",
            "iteration : 150, loss : 0.3726, accuracy : 88.20\n",
            "iteration : 200, loss : 0.3736, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3746, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3761, accuracy : 88.19\n",
            "iteration : 350, loss : 0.3745, accuracy : 88.26\n",
            "Epoch : 159, training loss : 0.3749, training accuracy : 88.27, test loss : 0.3922, test accuracy : 87.58\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.3976, accuracy : 87.48\n",
            "iteration : 100, loss : 0.3923, accuracy : 87.69\n",
            "iteration : 150, loss : 0.3846, accuracy : 87.95\n",
            "iteration : 200, loss : 0.3812, accuracy : 88.13\n",
            "iteration : 250, loss : 0.3826, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3793, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3797, accuracy : 88.12\n",
            "Epoch : 160, training loss : 0.3787, training accuracy : 88.11, test loss : 0.3897, test accuracy : 87.98\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.3833, accuracy : 88.08\n",
            "iteration : 100, loss : 0.3785, accuracy : 87.90\n",
            "iteration : 150, loss : 0.3745, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3731, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3766, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3800, accuracy : 87.97\n",
            "Epoch : 161, training loss : 0.3791, training accuracy : 88.03, test loss : 0.3905, test accuracy : 87.83\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.3705, accuracy : 88.33\n",
            "iteration : 100, loss : 0.3766, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3769, accuracy : 88.02\n",
            "iteration : 200, loss : 0.3757, accuracy : 88.13\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3795, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3809, accuracy : 87.99\n",
            "Epoch : 162, training loss : 0.3798, training accuracy : 88.03, test loss : 0.3903, test accuracy : 87.73\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.3881, accuracy : 87.78\n",
            "iteration : 100, loss : 0.3877, accuracy : 87.92\n",
            "iteration : 150, loss : 0.3883, accuracy : 87.88\n",
            "iteration : 200, loss : 0.3852, accuracy : 87.96\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.15\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3803, accuracy : 88.11\n",
            "Epoch : 163, training loss : 0.3802, training accuracy : 88.08, test loss : 0.3909, test accuracy : 87.85\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.3878, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3830, accuracy : 87.99\n",
            "iteration : 150, loss : 0.3794, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3798, accuracy : 88.16\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.13\n",
            "Epoch : 164, training loss : 0.3778, training accuracy : 88.15, test loss : 0.3925, test accuracy : 87.76\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.3804, accuracy : 87.50\n",
            "iteration : 100, loss : 0.3767, accuracy : 88.01\n",
            "iteration : 150, loss : 0.3764, accuracy : 88.00\n",
            "iteration : 200, loss : 0.3790, accuracy : 87.92\n",
            "iteration : 250, loss : 0.3771, accuracy : 88.04\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3783, accuracy : 88.10\n",
            "Epoch : 165, training loss : 0.3792, training accuracy : 88.10, test loss : 0.3903, test accuracy : 87.76\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.3686, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3738, accuracy : 88.21\n",
            "iteration : 150, loss : 0.3790, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3785, accuracy : 88.17\n",
            "iteration : 250, loss : 0.3760, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3762, accuracy : 88.28\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.21\n",
            "Epoch : 166, training loss : 0.3789, training accuracy : 88.19, test loss : 0.3945, test accuracy : 87.64\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.3904, accuracy : 87.69\n",
            "iteration : 100, loss : 0.3834, accuracy : 87.73\n",
            "iteration : 150, loss : 0.3792, accuracy : 88.04\n",
            "iteration : 200, loss : 0.3781, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3789, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3801, accuracy : 88.07\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.16\n",
            "Epoch : 167, training loss : 0.3776, training accuracy : 88.19, test loss : 0.3892, test accuracy : 87.91\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.3814, accuracy : 87.94\n",
            "iteration : 100, loss : 0.3766, accuracy : 88.18\n",
            "iteration : 150, loss : 0.3814, accuracy : 88.03\n",
            "iteration : 200, loss : 0.3785, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3764, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3786, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3795, accuracy : 88.18\n",
            "Epoch : 168, training loss : 0.3797, training accuracy : 88.18, test loss : 0.3900, test accuracy : 87.75\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.3881, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3833, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3842, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3809, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3828, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3772, accuracy : 88.14\n",
            "Epoch : 169, training loss : 0.3785, training accuracy : 88.12, test loss : 0.3926, test accuracy : 87.57\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.3738, accuracy : 88.42\n",
            "iteration : 100, loss : 0.3724, accuracy : 88.30\n",
            "iteration : 150, loss : 0.3786, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3784, accuracy : 88.05\n",
            "iteration : 250, loss : 0.3803, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.11\n",
            "Epoch : 170, training loss : 0.3821, training accuracy : 88.06, test loss : 0.3920, test accuracy : 87.71\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.3859, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3770, accuracy : 88.01\n",
            "iteration : 150, loss : 0.3754, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3774, accuracy : 87.95\n",
            "iteration : 250, loss : 0.3778, accuracy : 87.95\n",
            "iteration : 300, loss : 0.3751, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3759, accuracy : 88.08\n",
            "Epoch : 171, training loss : 0.3769, training accuracy : 88.05, test loss : 0.3911, test accuracy : 87.73\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.3660, accuracy : 88.23\n",
            "iteration : 100, loss : 0.3794, accuracy : 88.03\n",
            "iteration : 150, loss : 0.3770, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3715, accuracy : 88.33\n",
            "iteration : 250, loss : 0.3726, accuracy : 88.36\n",
            "iteration : 300, loss : 0.3761, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3762, accuracy : 88.25\n",
            "Epoch : 172, training loss : 0.3763, training accuracy : 88.27, test loss : 0.3924, test accuracy : 87.78\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.3753, accuracy : 88.11\n",
            "iteration : 100, loss : 0.3787, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3775, accuracy : 88.01\n",
            "iteration : 200, loss : 0.3753, accuracy : 88.07\n",
            "iteration : 250, loss : 0.3740, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3753, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.07\n",
            "Epoch : 173, training loss : 0.3791, training accuracy : 88.07, test loss : 0.3939, test accuracy : 87.77\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.3781, accuracy : 87.95\n",
            "iteration : 100, loss : 0.3813, accuracy : 88.19\n",
            "iteration : 150, loss : 0.3826, accuracy : 88.26\n",
            "iteration : 200, loss : 0.3798, accuracy : 88.33\n",
            "iteration : 250, loss : 0.3793, accuracy : 88.26\n",
            "iteration : 300, loss : 0.3795, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.24\n",
            "Epoch : 174, training loss : 0.3801, training accuracy : 88.22, test loss : 0.3900, test accuracy : 88.02\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.3676, accuracy : 88.38\n",
            "iteration : 100, loss : 0.3719, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3725, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3744, accuracy : 88.13\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3794, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3803, accuracy : 88.09\n",
            "Epoch : 175, training loss : 0.3801, training accuracy : 88.09, test loss : 0.3922, test accuracy : 87.85\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.3944, accuracy : 88.08\n",
            "iteration : 100, loss : 0.3917, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3860, accuracy : 87.92\n",
            "iteration : 200, loss : 0.3852, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3835, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3815, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3806, accuracy : 88.13\n",
            "Epoch : 176, training loss : 0.3797, training accuracy : 88.12, test loss : 0.3933, test accuracy : 87.73\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.3879, accuracy : 88.33\n",
            "iteration : 100, loss : 0.3841, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3813, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3822, accuracy : 88.27\n",
            "iteration : 250, loss : 0.3812, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3823, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3805, accuracy : 88.16\n",
            "Epoch : 177, training loss : 0.3787, training accuracy : 88.19, test loss : 0.3925, test accuracy : 87.88\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.3845, accuracy : 88.00\n",
            "iteration : 100, loss : 0.3779, accuracy : 88.02\n",
            "iteration : 150, loss : 0.3771, accuracy : 88.00\n",
            "iteration : 200, loss : 0.3780, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3771, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3757, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3768, accuracy : 88.13\n",
            "Epoch : 178, training loss : 0.3778, training accuracy : 88.11, test loss : 0.3926, test accuracy : 87.70\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.3640, accuracy : 88.73\n",
            "iteration : 100, loss : 0.3621, accuracy : 88.53\n",
            "iteration : 150, loss : 0.3712, accuracy : 88.34\n",
            "iteration : 200, loss : 0.3745, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3770, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3776, accuracy : 88.27\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.21\n",
            "Epoch : 179, training loss : 0.3780, training accuracy : 88.22, test loss : 0.3874, test accuracy : 87.88\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.3669, accuracy : 88.02\n",
            "iteration : 100, loss : 0.3732, accuracy : 87.94\n",
            "iteration : 150, loss : 0.3741, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3779, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3808, accuracy : 87.88\n",
            "iteration : 300, loss : 0.3785, accuracy : 87.95\n",
            "iteration : 350, loss : 0.3795, accuracy : 87.99\n",
            "Epoch : 180, training loss : 0.3796, training accuracy : 88.00, test loss : 0.3920, test accuracy : 87.88\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.3725, accuracy : 88.53\n",
            "iteration : 100, loss : 0.3704, accuracy : 88.53\n",
            "iteration : 150, loss : 0.3718, accuracy : 88.34\n",
            "iteration : 200, loss : 0.3725, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3712, accuracy : 88.38\n",
            "iteration : 300, loss : 0.3757, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3772, accuracy : 88.13\n",
            "Epoch : 181, training loss : 0.3773, training accuracy : 88.12, test loss : 0.3949, test accuracy : 87.68\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.3913, accuracy : 88.00\n",
            "iteration : 100, loss : 0.3794, accuracy : 88.30\n",
            "iteration : 150, loss : 0.3856, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.25\n",
            "iteration : 250, loss : 0.3816, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3783, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3780, accuracy : 88.25\n",
            "Epoch : 182, training loss : 0.3774, training accuracy : 88.28, test loss : 0.3899, test accuracy : 87.93\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.3650, accuracy : 88.70\n",
            "iteration : 100, loss : 0.3740, accuracy : 88.48\n",
            "iteration : 150, loss : 0.3781, accuracy : 88.38\n",
            "iteration : 200, loss : 0.3751, accuracy : 88.44\n",
            "iteration : 250, loss : 0.3762, accuracy : 88.45\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.35\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.31\n",
            "Epoch : 183, training loss : 0.3775, training accuracy : 88.30, test loss : 0.3894, test accuracy : 87.96\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.3920, accuracy : 88.08\n",
            "iteration : 100, loss : 0.3768, accuracy : 88.09\n",
            "iteration : 150, loss : 0.3773, accuracy : 88.27\n",
            "iteration : 200, loss : 0.3767, accuracy : 88.27\n",
            "iteration : 250, loss : 0.3776, accuracy : 88.24\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3780, accuracy : 88.24\n",
            "Epoch : 184, training loss : 0.3774, training accuracy : 88.25, test loss : 0.3864, test accuracy : 87.91\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.3770, accuracy : 87.95\n",
            "iteration : 100, loss : 0.3766, accuracy : 88.15\n",
            "iteration : 150, loss : 0.3740, accuracy : 88.26\n",
            "iteration : 200, loss : 0.3778, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3791, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3795, accuracy : 88.13\n",
            "Epoch : 185, training loss : 0.3787, training accuracy : 88.15, test loss : 0.3910, test accuracy : 87.68\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.3861, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3768, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3730, accuracy : 88.18\n",
            "iteration : 200, loss : 0.3762, accuracy : 88.14\n",
            "iteration : 250, loss : 0.3807, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3812, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3790, accuracy : 88.13\n",
            "Epoch : 186, training loss : 0.3800, training accuracy : 88.12, test loss : 0.3909, test accuracy : 87.76\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.3842, accuracy : 87.61\n",
            "iteration : 100, loss : 0.3907, accuracy : 87.52\n",
            "iteration : 150, loss : 0.3830, accuracy : 87.89\n",
            "iteration : 200, loss : 0.3844, accuracy : 87.95\n",
            "iteration : 250, loss : 0.3839, accuracy : 87.92\n",
            "iteration : 300, loss : 0.3832, accuracy : 87.96\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.13\n",
            "Epoch : 187, training loss : 0.3793, training accuracy : 88.11, test loss : 0.3916, test accuracy : 87.78\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.3756, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3847, accuracy : 87.88\n",
            "iteration : 150, loss : 0.3811, accuracy : 87.94\n",
            "iteration : 200, loss : 0.3764, accuracy : 88.08\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3765, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.04\n",
            "Epoch : 188, training loss : 0.3793, training accuracy : 88.02, test loss : 0.3938, test accuracy : 87.67\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.3796, accuracy : 88.22\n",
            "iteration : 100, loss : 0.3827, accuracy : 87.91\n",
            "iteration : 150, loss : 0.3818, accuracy : 88.01\n",
            "iteration : 200, loss : 0.3791, accuracy : 88.22\n",
            "iteration : 250, loss : 0.3751, accuracy : 88.42\n",
            "iteration : 300, loss : 0.3760, accuracy : 88.36\n",
            "iteration : 350, loss : 0.3785, accuracy : 88.28\n",
            "Epoch : 189, training loss : 0.3787, training accuracy : 88.25, test loss : 0.3898, test accuracy : 87.68\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.3841, accuracy : 88.02\n",
            "iteration : 100, loss : 0.3765, accuracy : 88.00\n",
            "iteration : 150, loss : 0.3809, accuracy : 87.85\n",
            "iteration : 200, loss : 0.3814, accuracy : 87.88\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.00\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3813, accuracy : 88.07\n",
            "Epoch : 190, training loss : 0.3805, training accuracy : 88.07, test loss : 0.3912, test accuracy : 87.88\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.3778, accuracy : 88.55\n",
            "iteration : 100, loss : 0.3704, accuracy : 88.59\n",
            "iteration : 150, loss : 0.3775, accuracy : 88.40\n",
            "iteration : 200, loss : 0.3765, accuracy : 88.44\n",
            "iteration : 250, loss : 0.3758, accuracy : 88.35\n",
            "iteration : 300, loss : 0.3758, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3783, accuracy : 88.20\n",
            "Epoch : 191, training loss : 0.3785, training accuracy : 88.24, test loss : 0.3916, test accuracy : 87.80\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.3802, accuracy : 87.95\n",
            "iteration : 100, loss : 0.3786, accuracy : 87.93\n",
            "iteration : 150, loss : 0.3776, accuracy : 88.04\n",
            "iteration : 200, loss : 0.3766, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3793, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3767, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.04\n",
            "Epoch : 192, training loss : 0.3791, training accuracy : 88.06, test loss : 0.3923, test accuracy : 87.66\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.3749, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3815, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3846, accuracy : 87.92\n",
            "iteration : 200, loss : 0.3820, accuracy : 87.92\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3783, accuracy : 88.12\n",
            "Epoch : 193, training loss : 0.3786, training accuracy : 88.14, test loss : 0.3911, test accuracy : 87.83\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.3755, accuracy : 88.25\n",
            "iteration : 100, loss : 0.3766, accuracy : 88.31\n",
            "iteration : 150, loss : 0.3791, accuracy : 88.29\n",
            "iteration : 200, loss : 0.3801, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3798, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3762, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3758, accuracy : 88.25\n",
            "Epoch : 194, training loss : 0.3773, training accuracy : 88.22, test loss : 0.3902, test accuracy : 87.86\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.3961, accuracy : 88.09\n",
            "iteration : 100, loss : 0.3798, accuracy : 88.42\n",
            "iteration : 150, loss : 0.3786, accuracy : 88.41\n",
            "iteration : 200, loss : 0.3825, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3811, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3766, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3769, accuracy : 88.24\n",
            "Epoch : 195, training loss : 0.3779, training accuracy : 88.19, test loss : 0.3944, test accuracy : 87.78\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.3532, accuracy : 89.20\n",
            "iteration : 100, loss : 0.3701, accuracy : 88.67\n",
            "iteration : 150, loss : 0.3782, accuracy : 88.32\n",
            "iteration : 200, loss : 0.3772, accuracy : 88.32\n",
            "iteration : 250, loss : 0.3781, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3779, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.26\n",
            "Epoch : 196, training loss : 0.3778, training accuracy : 88.26, test loss : 0.3916, test accuracy : 87.67\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.3806, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3774, accuracy : 88.20\n",
            "iteration : 150, loss : 0.3806, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3790, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3791, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3816, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.16\n",
            "Epoch : 197, training loss : 0.3789, training accuracy : 88.16, test loss : 0.3920, test accuracy : 87.81\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.3699, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3716, accuracy : 88.68\n",
            "iteration : 150, loss : 0.3750, accuracy : 88.47\n",
            "iteration : 200, loss : 0.3759, accuracy : 88.40\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3783, accuracy : 88.21\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.17\n",
            "Epoch : 198, training loss : 0.3769, training accuracy : 88.20, test loss : 0.3936, test accuracy : 87.64\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.3790, accuracy : 87.67\n",
            "iteration : 100, loss : 0.3607, accuracy : 88.55\n",
            "iteration : 150, loss : 0.3690, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3680, accuracy : 88.46\n",
            "iteration : 250, loss : 0.3700, accuracy : 88.44\n",
            "iteration : 300, loss : 0.3724, accuracy : 88.28\n",
            "iteration : 350, loss : 0.3751, accuracy : 88.19\n",
            "Epoch : 199, training loss : 0.3765, training accuracy : 88.14, test loss : 0.3911, test accuracy : 87.70\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.3621, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3664, accuracy : 88.47\n",
            "iteration : 150, loss : 0.3741, accuracy : 88.38\n",
            "iteration : 200, loss : 0.3778, accuracy : 88.23\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.23\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.32\n",
            "iteration : 350, loss : 0.3773, accuracy : 88.28\n",
            "Epoch : 200, training loss : 0.3774, training accuracy : 88.28, test loss : 0.3919, test accuracy : 87.71\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.3811, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3779, accuracy : 88.24\n",
            "iteration : 150, loss : 0.3751, accuracy : 88.34\n",
            "iteration : 200, loss : 0.3686, accuracy : 88.50\n",
            "iteration : 250, loss : 0.3727, accuracy : 88.33\n",
            "iteration : 300, loss : 0.3745, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3754, accuracy : 88.27\n",
            "Epoch : 201, training loss : 0.3762, training accuracy : 88.25, test loss : 0.3912, test accuracy : 87.85\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.3768, accuracy : 88.39\n",
            "iteration : 100, loss : 0.3737, accuracy : 88.32\n",
            "iteration : 150, loss : 0.3755, accuracy : 88.22\n",
            "iteration : 200, loss : 0.3774, accuracy : 88.26\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3767, accuracy : 88.34\n",
            "iteration : 350, loss : 0.3757, accuracy : 88.34\n",
            "Epoch : 202, training loss : 0.3759, training accuracy : 88.33, test loss : 0.3898, test accuracy : 87.82\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.3842, accuracy : 87.66\n",
            "iteration : 100, loss : 0.3750, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3825, accuracy : 87.82\n",
            "iteration : 200, loss : 0.3799, accuracy : 87.98\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3779, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.04\n",
            "Epoch : 203, training loss : 0.3795, training accuracy : 88.02, test loss : 0.3926, test accuracy : 87.79\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.3898, accuracy : 87.66\n",
            "iteration : 100, loss : 0.3831, accuracy : 87.87\n",
            "iteration : 150, loss : 0.3797, accuracy : 88.17\n",
            "iteration : 200, loss : 0.3771, accuracy : 88.30\n",
            "iteration : 250, loss : 0.3772, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3810, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.21\n",
            "Epoch : 204, training loss : 0.3793, training accuracy : 88.16, test loss : 0.3901, test accuracy : 87.71\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.3835, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3814, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3812, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3765, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3760, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3797, accuracy : 88.05\n",
            "iteration : 350, loss : 0.3801, accuracy : 88.08\n",
            "Epoch : 205, training loss : 0.3799, training accuracy : 88.07, test loss : 0.3890, test accuracy : 87.83\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.3946, accuracy : 87.66\n",
            "iteration : 100, loss : 0.3803, accuracy : 88.06\n",
            "iteration : 150, loss : 0.3809, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3806, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3811, accuracy : 88.04\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.14\n",
            "Epoch : 206, training loss : 0.3774, training accuracy : 88.17, test loss : 0.3910, test accuracy : 87.89\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.4017, accuracy : 87.48\n",
            "iteration : 100, loss : 0.3782, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3818, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3811, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3825, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3838, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3805, accuracy : 88.19\n",
            "Epoch : 207, training loss : 0.3801, training accuracy : 88.15, test loss : 0.3908, test accuracy : 87.76\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.3706, accuracy : 88.97\n",
            "iteration : 100, loss : 0.3669, accuracy : 88.75\n",
            "iteration : 150, loss : 0.3710, accuracy : 88.45\n",
            "iteration : 200, loss : 0.3771, accuracy : 88.24\n",
            "iteration : 250, loss : 0.3778, accuracy : 88.25\n",
            "iteration : 300, loss : 0.3802, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3805, accuracy : 88.12\n",
            "Epoch : 208, training loss : 0.3804, training accuracy : 88.12, test loss : 0.3921, test accuracy : 87.73\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.3705, accuracy : 88.45\n",
            "iteration : 100, loss : 0.3662, accuracy : 88.34\n",
            "iteration : 150, loss : 0.3703, accuracy : 88.28\n",
            "iteration : 200, loss : 0.3711, accuracy : 88.33\n",
            "iteration : 250, loss : 0.3753, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.17\n",
            "Epoch : 209, training loss : 0.3790, training accuracy : 88.16, test loss : 0.3912, test accuracy : 87.76\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.3840, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3778, accuracy : 88.03\n",
            "iteration : 150, loss : 0.3789, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3779, accuracy : 88.11\n",
            "iteration : 250, loss : 0.3766, accuracy : 88.19\n",
            "iteration : 300, loss : 0.3746, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3765, accuracy : 88.22\n",
            "Epoch : 210, training loss : 0.3781, training accuracy : 88.18, test loss : 0.3915, test accuracy : 87.84\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.3781, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3798, accuracy : 88.15\n",
            "iteration : 150, loss : 0.3863, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3838, accuracy : 87.96\n",
            "iteration : 250, loss : 0.3813, accuracy : 88.04\n",
            "iteration : 300, loss : 0.3802, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3813, accuracy : 88.03\n",
            "Epoch : 211, training loss : 0.3802, training accuracy : 88.07, test loss : 0.3884, test accuracy : 87.75\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.3685, accuracy : 88.34\n",
            "iteration : 100, loss : 0.3784, accuracy : 88.11\n",
            "iteration : 150, loss : 0.3860, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3806, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3802, accuracy : 88.05\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.06\n",
            "Epoch : 212, training loss : 0.3795, training accuracy : 88.08, test loss : 0.3938, test accuracy : 87.72\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.3849, accuracy : 88.14\n",
            "iteration : 100, loss : 0.3842, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3761, accuracy : 88.14\n",
            "iteration : 200, loss : 0.3762, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3777, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3771, accuracy : 88.11\n",
            "iteration : 350, loss : 0.3783, accuracy : 88.07\n",
            "Epoch : 213, training loss : 0.3772, training accuracy : 88.12, test loss : 0.3903, test accuracy : 87.85\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.3728, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3678, accuracy : 88.46\n",
            "iteration : 150, loss : 0.3679, accuracy : 88.53\n",
            "iteration : 200, loss : 0.3746, accuracy : 88.25\n",
            "iteration : 250, loss : 0.3758, accuracy : 88.29\n",
            "iteration : 300, loss : 0.3815, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3819, accuracy : 88.06\n",
            "Epoch : 214, training loss : 0.3793, training accuracy : 88.14, test loss : 0.3955, test accuracy : 87.60\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.3789, accuracy : 88.70\n",
            "iteration : 100, loss : 0.3711, accuracy : 88.67\n",
            "iteration : 150, loss : 0.3706, accuracy : 88.51\n",
            "iteration : 200, loss : 0.3701, accuracy : 88.52\n",
            "iteration : 250, loss : 0.3750, accuracy : 88.35\n",
            "iteration : 300, loss : 0.3782, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3812, accuracy : 88.12\n",
            "Epoch : 215, training loss : 0.3812, training accuracy : 88.14, test loss : 0.3932, test accuracy : 87.70\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.3898, accuracy : 87.66\n",
            "iteration : 100, loss : 0.3789, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3769, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3779, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.06\n",
            "iteration : 300, loss : 0.3804, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3798, accuracy : 88.12\n",
            "Epoch : 216, training loss : 0.3786, training accuracy : 88.19, test loss : 0.3933, test accuracy : 87.62\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.3793, accuracy : 88.66\n",
            "iteration : 100, loss : 0.3788, accuracy : 88.60\n",
            "iteration : 150, loss : 0.3789, accuracy : 88.32\n",
            "iteration : 200, loss : 0.3717, accuracy : 88.55\n",
            "iteration : 250, loss : 0.3755, accuracy : 88.42\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.32\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.23\n",
            "Epoch : 217, training loss : 0.3783, training accuracy : 88.19, test loss : 0.3866, test accuracy : 88.04\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.3813, accuracy : 87.56\n",
            "iteration : 100, loss : 0.3846, accuracy : 87.90\n",
            "iteration : 150, loss : 0.3897, accuracy : 87.81\n",
            "iteration : 200, loss : 0.3891, accuracy : 87.88\n",
            "iteration : 250, loss : 0.3840, accuracy : 88.01\n",
            "iteration : 300, loss : 0.3802, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3802, accuracy : 88.10\n",
            "Epoch : 218, training loss : 0.3792, training accuracy : 88.14, test loss : 0.3902, test accuracy : 87.78\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.3788, accuracy : 88.02\n",
            "iteration : 100, loss : 0.3754, accuracy : 88.23\n",
            "iteration : 150, loss : 0.3774, accuracy : 88.23\n",
            "iteration : 200, loss : 0.3773, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3737, accuracy : 88.33\n",
            "iteration : 300, loss : 0.3745, accuracy : 88.33\n",
            "iteration : 350, loss : 0.3765, accuracy : 88.30\n",
            "Epoch : 219, training loss : 0.3775, training accuracy : 88.25, test loss : 0.3952, test accuracy : 87.74\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.3878, accuracy : 87.80\n",
            "iteration : 100, loss : 0.3809, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3801, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3740, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3767, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3766, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3773, accuracy : 88.20\n",
            "Epoch : 220, training loss : 0.3756, training accuracy : 88.22, test loss : 0.3926, test accuracy : 87.81\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.3744, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3693, accuracy : 88.41\n",
            "iteration : 150, loss : 0.3754, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3744, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3776, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3779, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.17\n",
            "Epoch : 221, training loss : 0.3771, training accuracy : 88.18, test loss : 0.3933, test accuracy : 87.57\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.3701, accuracy : 88.50\n",
            "iteration : 100, loss : 0.3839, accuracy : 88.05\n",
            "iteration : 150, loss : 0.3817, accuracy : 88.02\n",
            "iteration : 200, loss : 0.3787, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3799, accuracy : 88.07\n",
            "iteration : 350, loss : 0.3792, accuracy : 88.10\n",
            "Epoch : 222, training loss : 0.3798, training accuracy : 88.08, test loss : 0.3950, test accuracy : 87.63\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.3914, accuracy : 87.70\n",
            "iteration : 100, loss : 0.3872, accuracy : 87.89\n",
            "iteration : 150, loss : 0.3848, accuracy : 88.04\n",
            "iteration : 200, loss : 0.3835, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3847, accuracy : 88.05\n",
            "iteration : 300, loss : 0.3836, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3807, accuracy : 88.09\n",
            "Epoch : 223, training loss : 0.3790, training accuracy : 88.14, test loss : 0.3911, test accuracy : 87.70\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.3730, accuracy : 88.50\n",
            "iteration : 100, loss : 0.3685, accuracy : 88.38\n",
            "iteration : 150, loss : 0.3683, accuracy : 88.60\n",
            "iteration : 200, loss : 0.3743, accuracy : 88.37\n",
            "iteration : 250, loss : 0.3778, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.15\n",
            "Epoch : 224, training loss : 0.3787, training accuracy : 88.10, test loss : 0.3922, test accuracy : 87.72\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.3590, accuracy : 88.77\n",
            "iteration : 100, loss : 0.3700, accuracy : 88.42\n",
            "iteration : 150, loss : 0.3798, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3799, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.19\n",
            "iteration : 300, loss : 0.3805, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.30\n",
            "Epoch : 225, training loss : 0.3768, training accuracy : 88.31, test loss : 0.3913, test accuracy : 87.90\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.3623, accuracy : 88.52\n",
            "iteration : 100, loss : 0.3655, accuracy : 88.75\n",
            "iteration : 150, loss : 0.3694, accuracy : 88.60\n",
            "iteration : 200, loss : 0.3719, accuracy : 88.43\n",
            "iteration : 250, loss : 0.3733, accuracy : 88.45\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.20\n",
            "Epoch : 226, training loss : 0.3783, training accuracy : 88.19, test loss : 0.3906, test accuracy : 87.83\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.3757, accuracy : 88.42\n",
            "iteration : 100, loss : 0.3726, accuracy : 88.46\n",
            "iteration : 150, loss : 0.3716, accuracy : 88.40\n",
            "iteration : 200, loss : 0.3734, accuracy : 88.38\n",
            "iteration : 250, loss : 0.3746, accuracy : 88.36\n",
            "iteration : 300, loss : 0.3778, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.23\n",
            "Epoch : 227, training loss : 0.3774, training accuracy : 88.23, test loss : 0.3907, test accuracy : 87.76\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.3690, accuracy : 88.34\n",
            "iteration : 100, loss : 0.3800, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3761, accuracy : 88.26\n",
            "iteration : 200, loss : 0.3819, accuracy : 88.04\n",
            "iteration : 250, loss : 0.3795, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3811, accuracy : 88.07\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.16\n",
            "Epoch : 228, training loss : 0.3790, training accuracy : 88.10, test loss : 0.3923, test accuracy : 87.63\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.3482, accuracy : 89.11\n",
            "iteration : 100, loss : 0.3591, accuracy : 88.67\n",
            "iteration : 150, loss : 0.3680, accuracy : 88.35\n",
            "iteration : 200, loss : 0.3740, accuracy : 88.25\n",
            "iteration : 250, loss : 0.3756, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.11\n",
            "Epoch : 229, training loss : 0.3793, training accuracy : 88.13, test loss : 0.3917, test accuracy : 87.80\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.3848, accuracy : 87.80\n",
            "iteration : 100, loss : 0.3736, accuracy : 88.24\n",
            "iteration : 150, loss : 0.3756, accuracy : 88.18\n",
            "iteration : 200, loss : 0.3786, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3799, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3794, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3779, accuracy : 88.20\n",
            "Epoch : 230, training loss : 0.3788, training accuracy : 88.19, test loss : 0.3913, test accuracy : 87.65\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.3694, accuracy : 88.25\n",
            "iteration : 100, loss : 0.3718, accuracy : 88.27\n",
            "iteration : 150, loss : 0.3758, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3786, accuracy : 88.14\n",
            "iteration : 250, loss : 0.3787, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3772, accuracy : 88.13\n",
            "Epoch : 231, training loss : 0.3773, training accuracy : 88.11, test loss : 0.3890, test accuracy : 87.82\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.3734, accuracy : 88.80\n",
            "iteration : 100, loss : 0.3806, accuracy : 88.02\n",
            "iteration : 150, loss : 0.3806, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3787, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3808, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3782, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.11\n",
            "Epoch : 232, training loss : 0.3800, training accuracy : 88.13, test loss : 0.3918, test accuracy : 87.85\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.3900, accuracy : 87.83\n",
            "iteration : 100, loss : 0.3787, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3757, accuracy : 88.16\n",
            "iteration : 200, loss : 0.3785, accuracy : 88.19\n",
            "iteration : 250, loss : 0.3786, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.22\n",
            "iteration : 350, loss : 0.3760, accuracy : 88.26\n",
            "Epoch : 233, training loss : 0.3775, training accuracy : 88.22, test loss : 0.3901, test accuracy : 87.85\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.3891, accuracy : 88.03\n",
            "iteration : 100, loss : 0.3829, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3782, accuracy : 88.31\n",
            "iteration : 200, loss : 0.3795, accuracy : 88.24\n",
            "iteration : 250, loss : 0.3769, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3777, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.23\n",
            "Epoch : 234, training loss : 0.3784, training accuracy : 88.18, test loss : 0.3931, test accuracy : 87.80\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.3694, accuracy : 88.28\n",
            "iteration : 100, loss : 0.3766, accuracy : 88.07\n",
            "iteration : 150, loss : 0.3779, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3777, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3813, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3804, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3808, accuracy : 88.14\n",
            "Epoch : 235, training loss : 0.3801, training accuracy : 88.13, test loss : 0.3900, test accuracy : 87.88\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.3946, accuracy : 87.86\n",
            "iteration : 100, loss : 0.3881, accuracy : 87.97\n",
            "iteration : 150, loss : 0.3867, accuracy : 87.87\n",
            "iteration : 200, loss : 0.3833, accuracy : 87.98\n",
            "iteration : 250, loss : 0.3813, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3826, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3809, accuracy : 88.11\n",
            "Epoch : 236, training loss : 0.3803, training accuracy : 88.13, test loss : 0.3931, test accuracy : 87.85\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.3767, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3876, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3810, accuracy : 88.31\n",
            "iteration : 200, loss : 0.3806, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3831, accuracy : 88.19\n",
            "iteration : 300, loss : 0.3790, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3779, accuracy : 88.27\n",
            "Epoch : 237, training loss : 0.3782, training accuracy : 88.25, test loss : 0.3888, test accuracy : 87.96\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.3787, accuracy : 88.34\n",
            "iteration : 100, loss : 0.3820, accuracy : 88.29\n",
            "iteration : 150, loss : 0.3848, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3803, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3818, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3806, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.12\n",
            "Epoch : 238, training loss : 0.3790, training accuracy : 88.17, test loss : 0.3910, test accuracy : 87.85\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.3621, accuracy : 88.36\n",
            "iteration : 100, loss : 0.3733, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3756, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3805, accuracy : 87.92\n",
            "iteration : 250, loss : 0.3770, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3781, accuracy : 87.97\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.08\n",
            "Epoch : 239, training loss : 0.3783, training accuracy : 88.07, test loss : 0.3900, test accuracy : 87.72\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.3587, accuracy : 88.81\n",
            "iteration : 100, loss : 0.3714, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3733, accuracy : 88.11\n",
            "iteration : 200, loss : 0.3731, accuracy : 88.22\n",
            "iteration : 250, loss : 0.3745, accuracy : 88.32\n",
            "iteration : 300, loss : 0.3761, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3767, accuracy : 88.27\n",
            "Epoch : 240, training loss : 0.3765, training accuracy : 88.25, test loss : 0.3897, test accuracy : 87.86\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.3881, accuracy : 87.92\n",
            "iteration : 100, loss : 0.3799, accuracy : 88.08\n",
            "iteration : 150, loss : 0.3836, accuracy : 87.89\n",
            "iteration : 200, loss : 0.3866, accuracy : 87.91\n",
            "iteration : 250, loss : 0.3785, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3793, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3787, accuracy : 88.06\n",
            "Epoch : 241, training loss : 0.3781, training accuracy : 88.12, test loss : 0.3927, test accuracy : 87.78\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.3880, accuracy : 87.80\n",
            "iteration : 100, loss : 0.3837, accuracy : 88.04\n",
            "iteration : 150, loss : 0.3878, accuracy : 87.91\n",
            "iteration : 200, loss : 0.3907, accuracy : 87.76\n",
            "iteration : 250, loss : 0.3859, accuracy : 87.90\n",
            "iteration : 300, loss : 0.3842, accuracy : 87.98\n",
            "iteration : 350, loss : 0.3806, accuracy : 88.12\n",
            "Epoch : 242, training loss : 0.3794, training accuracy : 88.15, test loss : 0.3901, test accuracy : 87.78\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.3711, accuracy : 88.52\n",
            "iteration : 100, loss : 0.3752, accuracy : 88.36\n",
            "iteration : 150, loss : 0.3808, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3800, accuracy : 88.14\n",
            "iteration : 250, loss : 0.3763, accuracy : 88.27\n",
            "iteration : 300, loss : 0.3794, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3768, accuracy : 88.25\n",
            "Epoch : 243, training loss : 0.3768, training accuracy : 88.23, test loss : 0.3926, test accuracy : 87.62\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.3560, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3687, accuracy : 88.13\n",
            "iteration : 150, loss : 0.3744, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3776, accuracy : 88.08\n",
            "iteration : 250, loss : 0.3771, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3787, accuracy : 88.06\n",
            "Epoch : 244, training loss : 0.3785, training accuracy : 88.07, test loss : 0.3906, test accuracy : 87.86\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.3695, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3668, accuracy : 88.33\n",
            "iteration : 150, loss : 0.3784, accuracy : 87.95\n",
            "iteration : 200, loss : 0.3770, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3779, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3785, accuracy : 88.05\n",
            "iteration : 350, loss : 0.3797, accuracy : 88.04\n",
            "Epoch : 245, training loss : 0.3789, training accuracy : 88.08, test loss : 0.3895, test accuracy : 87.96\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.3645, accuracy : 88.78\n",
            "iteration : 100, loss : 0.3727, accuracy : 88.26\n",
            "iteration : 150, loss : 0.3758, accuracy : 88.17\n",
            "iteration : 200, loss : 0.3770, accuracy : 88.15\n",
            "iteration : 250, loss : 0.3782, accuracy : 88.17\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.17\n",
            "iteration : 350, loss : 0.3754, accuracy : 88.25\n",
            "Epoch : 246, training loss : 0.3758, training accuracy : 88.24, test loss : 0.3907, test accuracy : 87.86\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.3812, accuracy : 88.19\n",
            "iteration : 100, loss : 0.3812, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3819, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3781, accuracy : 88.16\n",
            "iteration : 250, loss : 0.3772, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3803, accuracy : 87.97\n",
            "iteration : 350, loss : 0.3798, accuracy : 88.02\n",
            "Epoch : 247, training loss : 0.3790, training accuracy : 88.07, test loss : 0.3908, test accuracy : 87.76\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.3722, accuracy : 88.47\n",
            "iteration : 100, loss : 0.3686, accuracy : 88.48\n",
            "iteration : 150, loss : 0.3750, accuracy : 88.30\n",
            "iteration : 200, loss : 0.3775, accuracy : 88.24\n",
            "iteration : 250, loss : 0.3763, accuracy : 88.33\n",
            "iteration : 300, loss : 0.3776, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3759, accuracy : 88.22\n",
            "Epoch : 248, training loss : 0.3772, training accuracy : 88.22, test loss : 0.3907, test accuracy : 87.85\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.3782, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3840, accuracy : 87.98\n",
            "iteration : 150, loss : 0.3842, accuracy : 88.01\n",
            "iteration : 200, loss : 0.3807, accuracy : 88.08\n",
            "iteration : 250, loss : 0.3817, accuracy : 88.04\n",
            "iteration : 300, loss : 0.3803, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3798, accuracy : 88.19\n",
            "Epoch : 249, training loss : 0.3796, training accuracy : 88.16, test loss : 0.3914, test accuracy : 87.79\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.3760, accuracy : 88.41\n",
            "iteration : 100, loss : 0.3755, accuracy : 88.21\n",
            "iteration : 150, loss : 0.3793, accuracy : 88.18\n",
            "iteration : 200, loss : 0.3804, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3791, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3793, accuracy : 88.18\n",
            "iteration : 350, loss : 0.3809, accuracy : 88.13\n",
            "Epoch : 250, training loss : 0.3802, training accuracy : 88.15, test loss : 0.3907, test accuracy : 87.85\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.3890, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3790, accuracy : 88.18\n",
            "iteration : 150, loss : 0.3741, accuracy : 88.19\n",
            "iteration : 200, loss : 0.3720, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3718, accuracy : 88.21\n",
            "iteration : 300, loss : 0.3724, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3778, accuracy : 88.10\n",
            "Epoch : 251, training loss : 0.3773, training accuracy : 88.12, test loss : 0.3904, test accuracy : 87.70\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.3919, accuracy : 87.41\n",
            "iteration : 100, loss : 0.3858, accuracy : 87.63\n",
            "iteration : 150, loss : 0.3839, accuracy : 87.93\n",
            "iteration : 200, loss : 0.3841, accuracy : 87.88\n",
            "iteration : 250, loss : 0.3828, accuracy : 88.01\n",
            "iteration : 300, loss : 0.3814, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.09\n",
            "Epoch : 252, training loss : 0.3791, training accuracy : 88.11, test loss : 0.3949, test accuracy : 87.71\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.3689, accuracy : 88.12\n",
            "iteration : 100, loss : 0.3697, accuracy : 88.62\n",
            "iteration : 150, loss : 0.3756, accuracy : 88.38\n",
            "iteration : 200, loss : 0.3764, accuracy : 88.37\n",
            "iteration : 250, loss : 0.3778, accuracy : 88.32\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.31\n",
            "iteration : 350, loss : 0.3784, accuracy : 88.28\n",
            "Epoch : 253, training loss : 0.3785, training accuracy : 88.28, test loss : 0.3924, test accuracy : 87.66\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.3937, accuracy : 87.36\n",
            "iteration : 100, loss : 0.3841, accuracy : 87.84\n",
            "iteration : 150, loss : 0.3879, accuracy : 87.70\n",
            "iteration : 200, loss : 0.3800, accuracy : 87.95\n",
            "iteration : 250, loss : 0.3794, accuracy : 87.99\n",
            "iteration : 300, loss : 0.3815, accuracy : 87.97\n",
            "iteration : 350, loss : 0.3791, accuracy : 88.00\n",
            "Epoch : 254, training loss : 0.3790, training accuracy : 88.00, test loss : 0.3934, test accuracy : 87.62\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.3798, accuracy : 88.12\n",
            "iteration : 100, loss : 0.3796, accuracy : 88.00\n",
            "iteration : 150, loss : 0.3841, accuracy : 87.76\n",
            "iteration : 200, loss : 0.3805, accuracy : 88.01\n",
            "iteration : 250, loss : 0.3754, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3744, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3754, accuracy : 88.25\n",
            "Epoch : 255, training loss : 0.3779, training accuracy : 88.17, test loss : 0.3916, test accuracy : 87.72\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.3986, accuracy : 87.73\n",
            "iteration : 100, loss : 0.3830, accuracy : 88.14\n",
            "iteration : 150, loss : 0.3868, accuracy : 88.05\n",
            "iteration : 200, loss : 0.3844, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3826, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3794, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.15\n",
            "Epoch : 256, training loss : 0.3796, training accuracy : 88.18, test loss : 0.3898, test accuracy : 87.97\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.3833, accuracy : 88.20\n",
            "iteration : 100, loss : 0.3715, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3740, accuracy : 88.20\n",
            "iteration : 200, loss : 0.3766, accuracy : 88.19\n",
            "iteration : 250, loss : 0.3836, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3820, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3804, accuracy : 88.16\n",
            "Epoch : 257, training loss : 0.3805, training accuracy : 88.14, test loss : 0.3907, test accuracy : 87.98\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.3650, accuracy : 88.61\n",
            "iteration : 100, loss : 0.3708, accuracy : 88.39\n",
            "iteration : 150, loss : 0.3701, accuracy : 88.38\n",
            "iteration : 200, loss : 0.3725, accuracy : 88.28\n",
            "iteration : 250, loss : 0.3766, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3779, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3766, accuracy : 88.23\n",
            "Epoch : 258, training loss : 0.3778, training accuracy : 88.19, test loss : 0.3934, test accuracy : 87.76\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.4101, accuracy : 87.38\n",
            "iteration : 100, loss : 0.3912, accuracy : 87.84\n",
            "iteration : 150, loss : 0.3833, accuracy : 87.83\n",
            "iteration : 200, loss : 0.3813, accuracy : 87.98\n",
            "iteration : 250, loss : 0.3782, accuracy : 88.09\n",
            "iteration : 300, loss : 0.3812, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3809, accuracy : 88.06\n",
            "Epoch : 259, training loss : 0.3802, training accuracy : 88.09, test loss : 0.3920, test accuracy : 87.75\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.3698, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3773, accuracy : 88.26\n",
            "iteration : 150, loss : 0.3801, accuracy : 88.08\n",
            "iteration : 200, loss : 0.3749, accuracy : 88.19\n",
            "iteration : 250, loss : 0.3781, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3782, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3796, accuracy : 88.10\n",
            "Epoch : 260, training loss : 0.3788, training accuracy : 88.11, test loss : 0.3918, test accuracy : 87.81\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.3808, accuracy : 87.81\n",
            "iteration : 100, loss : 0.3794, accuracy : 87.76\n",
            "iteration : 150, loss : 0.3857, accuracy : 87.66\n",
            "iteration : 200, loss : 0.3867, accuracy : 87.79\n",
            "iteration : 250, loss : 0.3804, accuracy : 88.03\n",
            "iteration : 300, loss : 0.3807, accuracy : 88.08\n",
            "iteration : 350, loss : 0.3783, accuracy : 88.10\n",
            "Epoch : 261, training loss : 0.3779, training accuracy : 88.09, test loss : 0.3897, test accuracy : 87.89\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.3733, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3780, accuracy : 88.02\n",
            "iteration : 150, loss : 0.3723, accuracy : 88.36\n",
            "iteration : 200, loss : 0.3759, accuracy : 88.19\n",
            "iteration : 250, loss : 0.3765, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3769, accuracy : 88.25\n",
            "iteration : 350, loss : 0.3757, accuracy : 88.29\n",
            "Epoch : 262, training loss : 0.3769, training accuracy : 88.23, test loss : 0.3894, test accuracy : 87.83\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.3865, accuracy : 88.25\n",
            "iteration : 100, loss : 0.3903, accuracy : 88.17\n",
            "iteration : 150, loss : 0.3855, accuracy : 88.12\n",
            "iteration : 200, loss : 0.3850, accuracy : 88.13\n",
            "iteration : 250, loss : 0.3782, accuracy : 88.31\n",
            "iteration : 300, loss : 0.3770, accuracy : 88.28\n",
            "iteration : 350, loss : 0.3770, accuracy : 88.24\n",
            "Epoch : 263, training loss : 0.3782, training accuracy : 88.21, test loss : 0.3899, test accuracy : 87.88\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.3904, accuracy : 87.97\n",
            "iteration : 100, loss : 0.3818, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3837, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3822, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3831, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3794, accuracy : 88.16\n",
            "Epoch : 264, training loss : 0.3794, training accuracy : 88.14, test loss : 0.3917, test accuracy : 87.68\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.3798, accuracy : 87.91\n",
            "iteration : 100, loss : 0.3747, accuracy : 88.00\n",
            "iteration : 150, loss : 0.3812, accuracy : 87.82\n",
            "iteration : 200, loss : 0.3789, accuracy : 88.06\n",
            "iteration : 250, loss : 0.3821, accuracy : 87.97\n",
            "iteration : 300, loss : 0.3831, accuracy : 88.02\n",
            "iteration : 350, loss : 0.3808, accuracy : 88.12\n",
            "Epoch : 265, training loss : 0.3815, training accuracy : 88.12, test loss : 0.3905, test accuracy : 87.89\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.3794, accuracy : 88.16\n",
            "iteration : 100, loss : 0.3814, accuracy : 88.15\n",
            "iteration : 150, loss : 0.3793, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3817, accuracy : 88.20\n",
            "iteration : 250, loss : 0.3796, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.23\n",
            "iteration : 350, loss : 0.3757, accuracy : 88.29\n",
            "Epoch : 266, training loss : 0.3768, training accuracy : 88.29, test loss : 0.3918, test accuracy : 87.63\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.3646, accuracy : 88.41\n",
            "iteration : 100, loss : 0.3639, accuracy : 88.41\n",
            "iteration : 150, loss : 0.3701, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3754, accuracy : 88.27\n",
            "iteration : 250, loss : 0.3773, accuracy : 88.20\n",
            "iteration : 300, loss : 0.3793, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3763, accuracy : 88.20\n",
            "Epoch : 267, training loss : 0.3760, training accuracy : 88.20, test loss : 0.3922, test accuracy : 87.77\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.3543, accuracy : 88.62\n",
            "iteration : 100, loss : 0.3618, accuracy : 88.52\n",
            "iteration : 150, loss : 0.3656, accuracy : 88.42\n",
            "iteration : 200, loss : 0.3707, accuracy : 88.35\n",
            "iteration : 250, loss : 0.3713, accuracy : 88.36\n",
            "iteration : 300, loss : 0.3750, accuracy : 88.29\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.22\n",
            "Epoch : 268, training loss : 0.3787, training accuracy : 88.19, test loss : 0.3944, test accuracy : 87.60\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.3622, accuracy : 88.61\n",
            "iteration : 100, loss : 0.3670, accuracy : 88.35\n",
            "iteration : 150, loss : 0.3669, accuracy : 88.44\n",
            "iteration : 200, loss : 0.3731, accuracy : 88.24\n",
            "iteration : 250, loss : 0.3710, accuracy : 88.28\n",
            "iteration : 300, loss : 0.3731, accuracy : 88.27\n",
            "iteration : 350, loss : 0.3764, accuracy : 88.17\n",
            "Epoch : 269, training loss : 0.3778, training accuracy : 88.14, test loss : 0.3945, test accuracy : 87.66\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.3639, accuracy : 88.53\n",
            "iteration : 100, loss : 0.3780, accuracy : 88.17\n",
            "iteration : 150, loss : 0.3848, accuracy : 87.87\n",
            "iteration : 200, loss : 0.3853, accuracy : 87.83\n",
            "iteration : 250, loss : 0.3834, accuracy : 87.89\n",
            "iteration : 300, loss : 0.3809, accuracy : 87.94\n",
            "iteration : 350, loss : 0.3791, accuracy : 88.02\n",
            "Epoch : 270, training loss : 0.3781, training accuracy : 88.04, test loss : 0.3913, test accuracy : 87.98\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.3701, accuracy : 88.59\n",
            "iteration : 100, loss : 0.3713, accuracy : 88.39\n",
            "iteration : 150, loss : 0.3749, accuracy : 88.22\n",
            "iteration : 200, loss : 0.3764, accuracy : 88.28\n",
            "iteration : 250, loss : 0.3792, accuracy : 88.14\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.18\n",
            "iteration : 350, loss : 0.3786, accuracy : 88.17\n",
            "Epoch : 271, training loss : 0.3789, training accuracy : 88.15, test loss : 0.3867, test accuracy : 87.88\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.3671, accuracy : 88.70\n",
            "iteration : 100, loss : 0.3694, accuracy : 88.55\n",
            "iteration : 150, loss : 0.3738, accuracy : 88.47\n",
            "iteration : 200, loss : 0.3729, accuracy : 88.46\n",
            "iteration : 250, loss : 0.3767, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3792, accuracy : 88.09\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.15\n",
            "Epoch : 272, training loss : 0.3792, training accuracy : 88.14, test loss : 0.3895, test accuracy : 87.87\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.3766, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3735, accuracy : 88.03\n",
            "iteration : 150, loss : 0.3756, accuracy : 87.97\n",
            "iteration : 200, loss : 0.3775, accuracy : 88.07\n",
            "iteration : 250, loss : 0.3788, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3813, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.03\n",
            "Epoch : 273, training loss : 0.3796, training accuracy : 88.08, test loss : 0.3912, test accuracy : 87.77\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.3635, accuracy : 89.12\n",
            "iteration : 100, loss : 0.3689, accuracy : 88.89\n",
            "iteration : 150, loss : 0.3748, accuracy : 88.74\n",
            "iteration : 200, loss : 0.3764, accuracy : 88.62\n",
            "iteration : 250, loss : 0.3752, accuracy : 88.61\n",
            "iteration : 300, loss : 0.3786, accuracy : 88.44\n",
            "iteration : 350, loss : 0.3814, accuracy : 88.28\n",
            "Epoch : 274, training loss : 0.3803, training accuracy : 88.30, test loss : 0.3884, test accuracy : 87.94\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.3550, accuracy : 89.05\n",
            "iteration : 100, loss : 0.3644, accuracy : 88.48\n",
            "iteration : 150, loss : 0.3707, accuracy : 88.38\n",
            "iteration : 200, loss : 0.3735, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3724, accuracy : 88.30\n",
            "iteration : 300, loss : 0.3759, accuracy : 88.22\n",
            "iteration : 350, loss : 0.3760, accuracy : 88.24\n",
            "Epoch : 275, training loss : 0.3771, training accuracy : 88.21, test loss : 0.3898, test accuracy : 87.91\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.3890, accuracy : 88.09\n",
            "iteration : 100, loss : 0.3816, accuracy : 88.25\n",
            "iteration : 150, loss : 0.3821, accuracy : 88.09\n",
            "iteration : 200, loss : 0.3803, accuracy : 88.17\n",
            "iteration : 250, loss : 0.3794, accuracy : 88.16\n",
            "iteration : 300, loss : 0.3768, accuracy : 88.15\n",
            "iteration : 350, loss : 0.3776, accuracy : 88.16\n",
            "Epoch : 276, training loss : 0.3779, training accuracy : 88.17, test loss : 0.3917, test accuracy : 87.79\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.3916, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3805, accuracy : 88.32\n",
            "iteration : 150, loss : 0.3780, accuracy : 88.33\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.27\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.22\n",
            "iteration : 300, loss : 0.3808, accuracy : 88.20\n",
            "iteration : 350, loss : 0.3761, accuracy : 88.36\n",
            "Epoch : 277, training loss : 0.3758, training accuracy : 88.35, test loss : 0.3907, test accuracy : 87.70\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.3748, accuracy : 88.31\n",
            "iteration : 100, loss : 0.3787, accuracy : 88.12\n",
            "iteration : 150, loss : 0.3799, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3824, accuracy : 87.82\n",
            "iteration : 250, loss : 0.3805, accuracy : 87.94\n",
            "iteration : 300, loss : 0.3785, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3789, accuracy : 88.02\n",
            "Epoch : 278, training loss : 0.3796, training accuracy : 88.00, test loss : 0.3934, test accuracy : 87.78\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.3707, accuracy : 88.30\n",
            "iteration : 100, loss : 0.3815, accuracy : 87.94\n",
            "iteration : 150, loss : 0.3852, accuracy : 87.92\n",
            "iteration : 200, loss : 0.3867, accuracy : 87.71\n",
            "iteration : 250, loss : 0.3868, accuracy : 87.74\n",
            "iteration : 300, loss : 0.3847, accuracy : 87.83\n",
            "iteration : 350, loss : 0.3809, accuracy : 87.89\n",
            "Epoch : 279, training loss : 0.3801, training accuracy : 87.91, test loss : 0.3894, test accuracy : 87.85\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.3930, accuracy : 87.78\n",
            "iteration : 100, loss : 0.3873, accuracy : 88.06\n",
            "iteration : 150, loss : 0.3849, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3796, accuracy : 88.16\n",
            "iteration : 250, loss : 0.3766, accuracy : 88.21\n",
            "iteration : 300, loss : 0.3760, accuracy : 88.13\n",
            "iteration : 350, loss : 0.3757, accuracy : 88.16\n",
            "Epoch : 280, training loss : 0.3757, training accuracy : 88.15, test loss : 0.3914, test accuracy : 87.67\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.3808, accuracy : 87.95\n",
            "iteration : 100, loss : 0.3825, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3768, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3787, accuracy : 88.01\n",
            "iteration : 250, loss : 0.3776, accuracy : 88.08\n",
            "iteration : 300, loss : 0.3762, accuracy : 88.14\n",
            "iteration : 350, loss : 0.3769, accuracy : 88.12\n",
            "Epoch : 281, training loss : 0.3770, training accuracy : 88.16, test loss : 0.3898, test accuracy : 87.81\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.3665, accuracy : 87.94\n",
            "iteration : 100, loss : 0.3730, accuracy : 87.99\n",
            "iteration : 150, loss : 0.3739, accuracy : 88.13\n",
            "iteration : 200, loss : 0.3755, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3784, accuracy : 88.10\n",
            "iteration : 300, loss : 0.3781, accuracy : 88.10\n",
            "iteration : 350, loss : 0.3780, accuracy : 88.13\n",
            "Epoch : 282, training loss : 0.3781, training accuracy : 88.16, test loss : 0.3938, test accuracy : 87.74\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.3669, accuracy : 88.48\n",
            "iteration : 100, loss : 0.3767, accuracy : 88.11\n",
            "iteration : 150, loss : 0.3828, accuracy : 87.98\n",
            "iteration : 200, loss : 0.3841, accuracy : 88.00\n",
            "iteration : 250, loss : 0.3801, accuracy : 88.18\n",
            "iteration : 300, loss : 0.3812, accuracy : 88.16\n",
            "iteration : 350, loss : 0.3799, accuracy : 88.10\n",
            "Epoch : 283, training loss : 0.3787, training accuracy : 88.14, test loss : 0.3907, test accuracy : 87.72\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.3709, accuracy : 87.92\n",
            "iteration : 100, loss : 0.3737, accuracy : 88.30\n",
            "iteration : 150, loss : 0.3785, accuracy : 88.10\n",
            "iteration : 200, loss : 0.3788, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3775, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3788, accuracy : 88.06\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.05\n",
            "Epoch : 284, training loss : 0.3799, training accuracy : 88.08, test loss : 0.3921, test accuracy : 87.65\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.3824, accuracy : 87.56\n",
            "iteration : 100, loss : 0.3684, accuracy : 88.40\n",
            "iteration : 150, loss : 0.3739, accuracy : 88.51\n",
            "iteration : 200, loss : 0.3763, accuracy : 88.34\n",
            "iteration : 250, loss : 0.3762, accuracy : 88.33\n",
            "iteration : 300, loss : 0.3773, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3761, accuracy : 88.35\n",
            "Epoch : 285, training loss : 0.3762, training accuracy : 88.37, test loss : 0.3921, test accuracy : 87.68\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.3645, accuracy : 88.88\n",
            "iteration : 100, loss : 0.3699, accuracy : 88.65\n",
            "iteration : 150, loss : 0.3678, accuracy : 88.46\n",
            "iteration : 200, loss : 0.3740, accuracy : 88.40\n",
            "iteration : 250, loss : 0.3735, accuracy : 88.42\n",
            "iteration : 300, loss : 0.3772, accuracy : 88.18\n",
            "iteration : 350, loss : 0.3787, accuracy : 88.19\n",
            "Epoch : 286, training loss : 0.3776, training accuracy : 88.23, test loss : 0.3916, test accuracy : 87.87\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.3825, accuracy : 87.47\n",
            "iteration : 100, loss : 0.3793, accuracy : 87.67\n",
            "iteration : 150, loss : 0.3786, accuracy : 87.93\n",
            "iteration : 200, loss : 0.3760, accuracy : 88.04\n",
            "iteration : 250, loss : 0.3751, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3796, accuracy : 88.04\n",
            "iteration : 350, loss : 0.3794, accuracy : 88.08\n",
            "Epoch : 287, training loss : 0.3777, training accuracy : 88.14, test loss : 0.3911, test accuracy : 87.82\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.3753, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3758, accuracy : 88.16\n",
            "iteration : 150, loss : 0.3791, accuracy : 88.07\n",
            "iteration : 200, loss : 0.3810, accuracy : 88.02\n",
            "iteration : 250, loss : 0.3774, accuracy : 88.11\n",
            "iteration : 300, loss : 0.3771, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.16\n",
            "Epoch : 288, training loss : 0.3768, training accuracy : 88.18, test loss : 0.3954, test accuracy : 87.78\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.3803, accuracy : 88.25\n",
            "iteration : 100, loss : 0.3767, accuracy : 88.23\n",
            "iteration : 150, loss : 0.3751, accuracy : 88.18\n",
            "iteration : 200, loss : 0.3760, accuracy : 88.25\n",
            "iteration : 250, loss : 0.3737, accuracy : 88.40\n",
            "iteration : 300, loss : 0.3753, accuracy : 88.26\n",
            "iteration : 350, loss : 0.3774, accuracy : 88.18\n",
            "Epoch : 289, training loss : 0.3778, training accuracy : 88.15, test loss : 0.3894, test accuracy : 87.73\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.3787, accuracy : 88.27\n",
            "iteration : 100, loss : 0.3795, accuracy : 88.19\n",
            "iteration : 150, loss : 0.3782, accuracy : 88.28\n",
            "iteration : 200, loss : 0.3801, accuracy : 88.12\n",
            "iteration : 250, loss : 0.3823, accuracy : 88.02\n",
            "iteration : 300, loss : 0.3811, accuracy : 88.05\n",
            "iteration : 350, loss : 0.3804, accuracy : 88.15\n",
            "Epoch : 290, training loss : 0.3788, training accuracy : 88.18, test loss : 0.3904, test accuracy : 87.98\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.3813, accuracy : 88.05\n",
            "iteration : 100, loss : 0.3727, accuracy : 88.53\n",
            "iteration : 150, loss : 0.3780, accuracy : 88.29\n",
            "iteration : 200, loss : 0.3774, accuracy : 88.26\n",
            "iteration : 250, loss : 0.3744, accuracy : 88.40\n",
            "iteration : 300, loss : 0.3756, accuracy : 88.39\n",
            "iteration : 350, loss : 0.3761, accuracy : 88.30\n",
            "Epoch : 291, training loss : 0.3775, training accuracy : 88.28, test loss : 0.3921, test accuracy : 87.67\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.3923, accuracy : 87.91\n",
            "iteration : 100, loss : 0.3836, accuracy : 87.95\n",
            "iteration : 150, loss : 0.3762, accuracy : 88.32\n",
            "iteration : 200, loss : 0.3707, accuracy : 88.45\n",
            "iteration : 250, loss : 0.3721, accuracy : 88.35\n",
            "iteration : 300, loss : 0.3749, accuracy : 88.24\n",
            "iteration : 350, loss : 0.3782, accuracy : 88.16\n",
            "Epoch : 292, training loss : 0.3782, training accuracy : 88.17, test loss : 0.3941, test accuracy : 87.70\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.3771, accuracy : 87.77\n",
            "iteration : 100, loss : 0.3772, accuracy : 88.01\n",
            "iteration : 150, loss : 0.3735, accuracy : 88.28\n",
            "iteration : 200, loss : 0.3746, accuracy : 88.21\n",
            "iteration : 250, loss : 0.3766, accuracy : 88.13\n",
            "iteration : 300, loss : 0.3775, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3794, accuracy : 88.07\n",
            "Epoch : 293, training loss : 0.3770, training accuracy : 88.15, test loss : 0.3913, test accuracy : 87.75\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.3752, accuracy : 88.25\n",
            "iteration : 100, loss : 0.3741, accuracy : 88.48\n",
            "iteration : 150, loss : 0.3758, accuracy : 88.35\n",
            "iteration : 200, loss : 0.3810, accuracy : 88.29\n",
            "iteration : 250, loss : 0.3797, accuracy : 88.29\n",
            "iteration : 300, loss : 0.3808, accuracy : 88.19\n",
            "iteration : 350, loss : 0.3781, accuracy : 88.28\n",
            "Epoch : 294, training loss : 0.3788, training accuracy : 88.26, test loss : 0.3903, test accuracy : 87.91\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.3926, accuracy : 87.56\n",
            "iteration : 100, loss : 0.3836, accuracy : 87.88\n",
            "iteration : 150, loss : 0.3805, accuracy : 88.11\n",
            "iteration : 200, loss : 0.3770, accuracy : 88.24\n",
            "iteration : 250, loss : 0.3798, accuracy : 88.12\n",
            "iteration : 300, loss : 0.3762, accuracy : 88.29\n",
            "iteration : 350, loss : 0.3779, accuracy : 88.17\n",
            "Epoch : 295, training loss : 0.3795, training accuracy : 88.14, test loss : 0.3932, test accuracy : 87.90\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.3729, accuracy : 88.06\n",
            "iteration : 100, loss : 0.3786, accuracy : 88.09\n",
            "iteration : 150, loss : 0.3727, accuracy : 88.40\n",
            "iteration : 200, loss : 0.3739, accuracy : 88.38\n",
            "iteration : 250, loss : 0.3766, accuracy : 88.30\n",
            "iteration : 300, loss : 0.3780, accuracy : 88.30\n",
            "iteration : 350, loss : 0.3768, accuracy : 88.29\n",
            "Epoch : 296, training loss : 0.3774, training accuracy : 88.26, test loss : 0.3924, test accuracy : 87.72\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.3572, accuracy : 88.89\n",
            "iteration : 100, loss : 0.3768, accuracy : 88.21\n",
            "iteration : 150, loss : 0.3841, accuracy : 88.15\n",
            "iteration : 200, loss : 0.3849, accuracy : 88.10\n",
            "iteration : 250, loss : 0.3833, accuracy : 88.07\n",
            "iteration : 300, loss : 0.3841, accuracy : 88.01\n",
            "iteration : 350, loss : 0.3825, accuracy : 88.03\n",
            "Epoch : 297, training loss : 0.3810, training accuracy : 88.09, test loss : 0.3888, test accuracy : 87.91\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.3649, accuracy : 88.53\n",
            "iteration : 100, loss : 0.3704, accuracy : 88.39\n",
            "iteration : 150, loss : 0.3706, accuracy : 88.38\n",
            "iteration : 200, loss : 0.3786, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3761, accuracy : 88.16\n",
            "iteration : 300, loss : 0.3793, accuracy : 88.12\n",
            "iteration : 350, loss : 0.3800, accuracy : 88.10\n",
            "Epoch : 298, training loss : 0.3796, training accuracy : 88.10, test loss : 0.3888, test accuracy : 87.88\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.3799, accuracy : 87.84\n",
            "iteration : 100, loss : 0.3813, accuracy : 87.83\n",
            "iteration : 150, loss : 0.3846, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3847, accuracy : 87.97\n",
            "iteration : 250, loss : 0.3853, accuracy : 87.95\n",
            "iteration : 300, loss : 0.3833, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3793, accuracy : 88.19\n",
            "Epoch : 299, training loss : 0.3808, training accuracy : 88.17, test loss : 0.3895, test accuracy : 87.91\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.3877, accuracy : 87.47\n",
            "iteration : 100, loss : 0.3806, accuracy : 87.82\n",
            "iteration : 150, loss : 0.3795, accuracy : 88.06\n",
            "iteration : 200, loss : 0.3785, accuracy : 87.98\n",
            "iteration : 250, loss : 0.3797, accuracy : 87.94\n",
            "iteration : 300, loss : 0.3776, accuracy : 88.03\n",
            "iteration : 350, loss : 0.3760, accuracy : 88.15\n",
            "Epoch : 300, training loss : 0.3770, training accuracy : 88.13, test loss : 0.3917, test accuracy : 87.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUQVIKR-X3v6",
        "outputId": "2ecdd7cb-c1b4-4d99-d2e6-353a610a6d76"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3929436405499776, 87.68822987092808)"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : Exponential\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7CNz1iabSB21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "98e71224-b023-492a-e3d9-57d7a35d085c"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : Exponential\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ed74fb9a-e437-4a18-c35a-5b46cdbc5f80"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_exp = {train_loss_list}\") \n",
        "print(f\"train_acc_list_exp = {train_acc_list}\")\n",
        "print(f\"test_loss_list_exp = {test_loss_list}\")\n",
        "print(f\"test_acc_list_exp = {test_acc_list}\")"
      ],
      "metadata": {
        "id": "3eiY3bTlWipW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "957c16f6-1c6a-4cc8-dccb-aaf64760be57"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_exp = [1.5108453227575556, 0.45569133027620756, 0.38955149030297753, 0.38201055714108434, 0.3791554563736851, 0.37846633355791975, 0.37789585104156637, 0.37833547551780533, 0.37885424543202406, 0.3798585463830126, 0.3801606303146538, 0.3785690202864851, 0.380058984244419, 0.38191306041831247, 0.3755665107309657, 0.37845069464790787, 0.3810173268240642, 0.37906573632060675, 0.38025176412044825, 0.37710853672124506, 0.37910957913088605, 0.3795646985129612, 0.37768419939004955, 0.37915716253645054, 0.3763544963304266, 0.37615088787343764, 0.3767496614356028, 0.3780435439209305, 0.37970296291477956, 0.3768931365190806, 0.3782231926433439, 0.37863624968179843, 0.3801447201146666, 0.38008787327504095, 0.3784985868184547, 0.3775513489152681, 0.3797982176387213, 0.378436125835106, 0.37775709227656284, 0.3784049479618951, 0.3812676381047179, 0.377607957215167, 0.37778531927564923, 0.3785970114110931, 0.3763440459320539, 0.37659880587563604, 0.38260392961786366, 0.3788121801404772, 0.3782783241937477, 0.37851579224837184, 0.37789375260270386, 0.37802613358995135, 0.3780238080800064, 0.38144587775879113, 0.3779912972595634, 0.3791698661600025, 0.3766854087996289, 0.37804330437163997, 0.3786587099718854, 0.37654282059772876, 0.3787634488609102, 0.3799068733524824, 0.37713607115958764, 0.37751451530430696, 0.3801226066299247, 0.37911422368956776, 0.3805197463610631, 0.37700172774190827, 0.3788724349847008, 0.3784166157326401, 0.38057625192775313, 0.37780569122251134, 0.38091700400924944, 0.3794297932528545, 0.3814112742338077, 0.37960589101644066, 0.37835461256626823, 0.37888884483798735, 0.3795582245439695, 0.37849634607148364, 0.37682519744082194, 0.37698636250444223, 0.3814956169464401, 0.3775139360570003, 0.379286357057773, 0.37696852730864755, 0.3763741816123973, 0.37921798265561824, 0.3787178158921601, 0.3787029756682352, 0.37783306570557074, 0.3772085670452454, 0.377733004771597, 0.3765660253401371, 0.3802021978912638, 0.37975252381346736, 0.3792028047528047, 0.3785102287039847, 0.3774594428739574, 0.3806928960773034, 0.3792232088441771, 0.37827566011649805, 0.38008303063994825, 0.3778988100325835, 0.3803016372974003, 0.3791544923614357, 0.38108806501882186, 0.37818956795100594, 0.37843327103106955, 0.38068289166382013, 0.38067130910025704, 0.37755737811084683, 0.37938234137325755, 0.3798758971497295, 0.37914535011540906, 0.37674440685811084, 0.37839110369281714, 0.3803737539505248, 0.3769939240966709, 0.37618244623104086, 0.3802966764625818, 0.376914295521855, 0.38093100847590583, 0.3785027291797364, 0.38015424458153524, 0.3799492670753138, 0.3770360491140102, 0.3781222916311688, 0.37643939103214397, 0.37902817707559283, 0.37899113222350916, 0.3788869072428241, 0.37866583002130516, 0.38024979207896925, 0.37998249028433306, 0.37665241209633626, 0.3791340853221371, 0.3768656629776244, 0.3781943121738227, 0.37956079872966136, 0.3790555945660687, 0.3767165967280949, 0.3785593254860178, 0.3785436793071468, 0.3786095209157241, 0.37977585394369556, 0.37895426606421223, 0.37726105127715803, 0.3811360982456181, 0.3777372095239195, 0.37985739951857384, 0.3775850888190231, 0.37942486108964696, 0.3774096647575296, 0.37866538746893247, 0.37956950698441605, 0.37765448986676325, 0.3765703885535884, 0.37493878506063444, 0.3787174464322041, 0.3790939331539278, 0.3798395115024029, 0.3802033216972661, 0.3778447469237051, 0.37922620373528176, 0.3788863822696655, 0.3776389633010073, 0.37965354569720705, 0.37847416466329153, 0.38210083584636856, 0.3769269231858292, 0.376283608316406, 0.3791458204105941, 0.3800827085891067, 0.3800524467940576, 0.379729083156198, 0.3786869500145357, 0.37784998191566, 0.37798632447150987, 0.37959555187199495, 0.3772686913326827, 0.37737370587299834, 0.3774894487647829, 0.3773925192149351, 0.37866038096144916, 0.3799699583673865, 0.37930802181161194, 0.37929151229419034, 0.37872534558217374, 0.38051237880699035, 0.3785012537224829, 0.37907243408969427, 0.3786347356188265, 0.3772817448306536, 0.37789186423386983, 0.377803510807071, 0.37891541296227516, 0.3768928705596019, 0.3764786839808229, 0.37735731827049723, 0.37620092403436417, 0.37594923223583354, 0.37946950848186567, 0.37934725721515616, 0.3799315985383057, 0.37742675118006985, 0.3800657158137014, 0.38043200723362486, 0.3790072623468673, 0.37812678907621844, 0.380248889528962, 0.3795400029679301, 0.37716066687895355, 0.37926767851279036, 0.3811982564002195, 0.3785968935344277, 0.37829239452434427, 0.3791591095003655, 0.37750050801087204, 0.3755794913788152, 0.37707439347657407, 0.3798365030023787, 0.37901620768757693, 0.3787338199815776, 0.3768027005237616, 0.378330920527621, 0.3773879486774688, 0.37904529988281127, 0.3793277707364824, 0.37877471587522243, 0.3773436505135482, 0.38000925620235404, 0.37747299065434836, 0.3783611831707037, 0.38008849536823386, 0.3802644786069064, 0.37820831300604957, 0.37898807813158525, 0.37832132050500006, 0.37651883493755567, 0.3780955279745707, 0.3794448867157546, 0.37681427719147226, 0.37846876878725483, 0.37892088152690306, 0.3757547806433546, 0.3790287462272618, 0.3771775669764051, 0.37960463952081314, 0.38023487892415786, 0.3773157599818739, 0.3790636116734688, 0.37847773605568946, 0.3790109474969104, 0.3778621018094422, 0.3795850692483468, 0.38052020777208695, 0.3777503734681664, 0.38023589062819957, 0.37878056504539037, 0.3778544519813403, 0.3769152445117956, 0.37817029428837423, 0.3794189135879682, 0.3814815942268708, 0.3767595124357761, 0.376037456559618, 0.3787438214067521, 0.37781988387185383, 0.3780568046621514, 0.3789186481295562, 0.3792363849031893, 0.3795603767723895, 0.38027571985715125, 0.37710057130350977, 0.37790428061633896, 0.3758348726887044, 0.37957349484205893, 0.3801340398184329, 0.37570041014250055, 0.37696901714898706, 0.3780564808748602, 0.37865450675409984, 0.37989892315896867, 0.37623435913062675, 0.3776249932402841, 0.3777484252120098, 0.37681428954853274, 0.3777670302203677, 0.37880285431537525, 0.3775242522641572, 0.37820892516513505, 0.37701137581976446, 0.378751233704691, 0.37953954490865793, 0.3773617081364319, 0.3809620241324107, 0.37960273697770386, 0.3808248356428896, 0.37698043551709914]\n",
            "train_acc_list_exp = [47.21651667548968, 85.66437268395977, 87.928004235045, 88.11858125992589, 88.25410269984118, 88.07623080995235, 88.23928004235044, 88.18422445738486, 88.14187400741133, 88.09740603493913, 88.06564319745897, 88.14399152991001, 88.0084700899947, 87.9915299100053, 88.28163049232398, 88.06776071995765, 88.09105346744309, 88.11434621492853, 87.96188459502382, 88.11222869242985, 88.12916887241927, 88.13975648491265, 88.2435150873478, 88.16728427739545, 88.09105346744309, 88.20116463737428, 88.20328215987296, 88.01482265749074, 88.0084700899947, 88.20116463737428, 88.15034409740603, 88.22445738485972, 88.04870301746956, 88.06776071995765, 88.28586553732133, 88.19269454737956, 88.11011116993119, 88.18422445738486, 88.20539968237162, 88.08046585494971, 88.071995764955, 88.23292747485442, 88.23292747485442, 88.15881418740074, 88.26680783483324, 88.32398094229751, 88.06140815246162, 88.18210693488618, 88.17363684489148, 88.15881418740074, 88.13975648491265, 88.15034409740603, 88.12281630492323, 88.01694017998942, 88.19481206987824, 88.18634197988354, 88.3070407623081, 88.16093170989942, 88.16516675489677, 88.19481206987824, 88.25622022233986, 88.12493382742191, 88.1355214399153, 88.16093170989942, 88.09105346744309, 88.24563260984648, 88.11858125992589, 88.10799364743251, 88.09317098994177, 88.09528851244045, 88.0359978824775, 88.17787188988883, 88.10375860243515, 88.1355214399153, 88.08470089994707, 88.09105346744309, 88.23928004235044, 88.12916887241927, 88.10375860243515, 88.26045526733722, 88.24775013234516, 88.16093170989942, 88.05717310746427, 88.11858125992589, 88.15034409740603, 88.24563260984648, 88.14822657490735, 88.05293806246691, 88.1630492323981, 88.12281630492323, 88.32821598729487, 88.21386977236634, 88.23928004235044, 88.26892535733192, 88.23928004235044, 87.91529910005293, 88.05293806246691, 88.16093170989942, 88.26892535733192, 88.10587612493383, 88.13128639491795, 88.28374801482266, 88.21810481736368, 88.20328215987296, 88.18422445738486, 88.09740603493913, 88.00635256749602, 88.08681842244575, 88.10375860243515, 88.09528851244045, 88.06140815246162, 88.16093170989942, 88.10799364743251, 88.12493382742191, 88.08681842244575, 88.29433562731604, 88.08258337744839, 88.0084700899947, 88.17363684489148, 88.27527792482795, 88.10587612493383, 88.12069878242457, 87.98941238750662, 88.22022233986236, 88.12916887241927, 88.12281630492323, 88.14399152991001, 88.16516675489677, 88.14399152991001, 88.09528851244045, 88.18845950238222, 88.19692959237692, 88.15246161990471, 88.06564319745897, 88.12069878242457, 88.25833774483854, 88.15246161990471, 88.25622022233986, 88.11646373742721, 88.10375860243515, 88.11434621492853, 88.18422445738486, 88.14610905240868, 88.27316040232928, 88.15881418740074, 88.16728427739545, 88.13763896241397, 88.08681842244575, 88.03811540497618, 88.15457914240339, 88.15034409740603, 88.28586553732133, 88.13975648491265, 88.29010058231869, 88.25622022233986, 88.21810481736368, 88.22445738485972, 88.27316040232928, 88.2710428798306, 88.11222869242985, 88.02541026998412, 88.02541026998412, 88.08258337744839, 88.14610905240868, 88.09740603493913, 88.18634197988354, 88.18634197988354, 88.17998941238751, 88.11646373742721, 88.06140815246162, 88.05293806246691, 88.2710428798306, 88.071995764955, 88.22233986236104, 88.09317098994177, 88.12493382742191, 88.18634197988354, 88.11434621492853, 88.21810481736368, 88.00211752249868, 88.11858125992589, 88.27527792482795, 88.30280571731075, 88.24563260984648, 88.15246161990471, 88.12493382742191, 88.11434621492853, 88.02117522498676, 88.24563260984648, 88.06987824245633, 88.23716251985178, 88.0635256749603, 88.14399152991001, 88.22445738485972, 88.19269454737956, 88.26045526733722, 88.1630492323981, 88.1990471148756, 88.14399152991001, 88.27527792482795, 88.24563260984648, 88.32609846479619, 88.02117522498676, 88.15669666490207, 88.071995764955, 88.16728427739545, 88.15034409740603, 88.12281630492323, 88.15881418740074, 88.17787188988883, 88.06987824245633, 88.08470089994707, 88.11646373742721, 88.13763896241397, 88.1355214399153, 88.18845950238222, 88.18845950238222, 88.14399152991001, 88.25410269984118, 88.22445738485972, 88.17998941238751, 88.07623080995235, 88.14187400741133, 88.09952355743779, 88.31127580730545, 88.19481206987824, 88.2265749073584, 88.10164107993647, 88.12705134992059, 88.1905770248809, 88.11434621492853, 88.13128639491795, 88.22233986236104, 88.17998941238751, 88.13128639491795, 88.12705134992059, 88.24563260984648, 88.16940179989412, 88.06987824245633, 88.24986765484384, 88.12493382742191, 88.15457914240339, 88.22869242985706, 88.06987824245633, 88.08470089994707, 88.23928004235044, 88.07411328745368, 88.215987294865, 88.15881418740074, 88.14822657490735, 88.12069878242457, 88.10799364743251, 88.27527792482795, 88.00423504499736, 88.17363684489148, 88.18422445738486, 88.1355214399153, 88.18634197988354, 88.08893594494441, 88.11011116993119, 88.09105346744309, 88.2265749073584, 88.20539968237162, 88.13763896241397, 88.11646373742721, 88.29221810481737, 88.20116463737428, 88.18845950238222, 88.14399152991001, 88.0359978824775, 88.15246161990471, 88.14399152991001, 88.07834833245103, 88.30068819481207, 88.21386977236634, 88.1715193223928, 88.34515616728427, 87.99788247750132, 87.91106405505559, 88.15034409740603, 88.15669666490207, 88.1630492323981, 88.13975648491265, 88.08470089994707, 88.3705664372684, 88.22869242985706, 88.1355214399153, 88.17787188988883, 88.14610905240868, 88.18210693488618, 88.27527792482795, 88.17363684489148, 88.15034409740603, 88.26469031233457, 88.14187400741133, 88.26045526733722, 88.08681842244575, 88.09952355743779, 88.16728427739545, 88.12916887241927]\n",
            "test_loss_list_exp = [0.7599668520338395, 0.41391250622623105, 0.392150557216476, 0.3908533724207504, 0.3910430484980929, 0.3915113131059151, 0.3893689420439449, 0.39278720669886646, 0.3913472319642703, 0.3898209324654411, 0.3919214537622882, 0.3930370972729197, 0.3875842901567618, 0.3887199363579937, 0.3931986349178295, 0.3915082841527228, 0.39034265929869577, 0.38962281656031517, 0.39410267273585003, 0.3923792628680958, 0.3913208600498882, 0.3940731910806076, 0.3901035269978, 0.3910912872091228, 0.38958175787154364, 0.3905664257266942, 0.3923723243323027, 0.3930160653795682, 0.3910279740013328, 0.39261947053612445, 0.39096589437594603, 0.391885179076709, 0.3932027472730945, 0.39465218753206965, 0.38952960351518556, 0.3913427036182553, 0.39291733357251857, 0.38893387598149914, 0.3907097995865579, 0.39178018352272465, 0.38997077613192443, 0.38897235888768644, 0.39309656094102297, 0.3918713939686616, 0.38930126476813764, 0.3919200319431576, 0.3909892667742336, 0.3936515018782195, 0.3921337866900014, 0.3897011807444049, 0.389769053050116, 0.3902200594106141, 0.38991651070468564, 0.3911624582228707, 0.3900841569491461, 0.38808364508783116, 0.39078373330480914, 0.3911517692693308, 0.3905084923494096, 0.392090796986047, 0.3915708720245782, 0.3868667685664168, 0.3898552243469977, 0.3904008638186782, 0.38936665559224054, 0.3908520120323873, 0.3909977381574173, 0.3894340494538055, 0.3913114227938886, 0.38867319550584345, 0.3922208594340904, 0.39250611046365663, 0.3933124074748918, 0.3923281282916957, 0.3923263843445217, 0.3901771641537255, 0.39134884019400556, 0.38913627334085166, 0.39259269424513277, 0.39072176375809836, 0.3921530346806143, 0.39100976697370116, 0.39079241250075547, 0.3899397780643959, 0.3929170014373228, 0.3889102427398457, 0.38894158966985404, 0.39256049473496046, 0.3897372295020842, 0.39233241295989824, 0.39279353041567056, 0.394793822411813, 0.3923672336865874, 0.38789207575952306, 0.391120845853698, 0.3914312515042576, 0.3890334714715387, 0.38947177671042144, 0.3914659348334752, 0.39052898429480254, 0.38922284959870224, 0.3926088816541083, 0.3930806327684253, 0.392626012584158, 0.3940140974580073, 0.3908073946687521, 0.3892480945762466, 0.3914746798428835, 0.3904998555925547, 0.39119182118013796, 0.39440940546931, 0.39126060596283746, 0.39380357363352586, 0.3891267492344566, 0.3960636570027061, 0.3884448780879086, 0.39201007885675804, 0.3887008518418845, 0.3946489935704306, 0.39012787235425966, 0.3926868131201641, 0.391174876587648, 0.39185114438627283, 0.3916316426121721, 0.39595334516728625, 0.3925237079315326, 0.3923376457510041, 0.3896145831574412, 0.3903454670719072, 0.38917045835770814, 0.3901947463552157, 0.38894958538459795, 0.39138510881685745, 0.3923698270729944, 0.391075956543871, 0.3886808121905607, 0.3895190705855687, 0.3937717227666986, 0.3892171628334943, 0.3928420579462659, 0.3929640487128613, 0.39179296280239145, 0.39022867213569434, 0.3915944700585861, 0.3908092556338684, 0.38998721832153843, 0.3919305083360158, 0.38930609106433156, 0.3915616778620318, 0.39198073642510994, 0.3898476078083702, 0.3942314390750492, 0.39122209778311207, 0.39321639089315547, 0.39556199356037025, 0.39069073004465477, 0.3901906652631713, 0.3918502590089452, 0.3921655998656563, 0.3897442833027419, 0.39050330440787706, 0.39026205139417275, 0.3908781868716081, 0.39245978443353785, 0.3903481831007144, 0.39445868105280635, 0.3892170712351799, 0.390042986282531, 0.3925706706941128, 0.39200084298556925, 0.3911351076528138, 0.39240403268851487, 0.3938527383348521, 0.3900003031480546, 0.3922225654709573, 0.3932921053001694, 0.39253465655971975, 0.3926194997540876, 0.3874349548097919, 0.39200115612908903, 0.3948955665908608, 0.3898700127998988, 0.38938687602971117, 0.3864065995257275, 0.39098570473930416, 0.3909385864641152, 0.3916171494067884, 0.39381099733359676, 0.3898175518740626, 0.3911994169740116, 0.39155133265782804, 0.39230194549058, 0.39114902234252763, 0.39017920955723406, 0.3944379236622184, 0.3916288679283039, 0.39200210498244153, 0.39363550146420795, 0.3911365672361617, 0.39186333141782703, 0.39117470471297994, 0.38984077394593, 0.39264219977399883, 0.3900822729018389, 0.38898184559508864, 0.3910406724202867, 0.3907987319809549, 0.39212526841198697, 0.39124271427007284, 0.3914998725202738, 0.38839635442869336, 0.3937654222781752, 0.3902626017875531, 0.3955241435883092, 0.39323169840317146, 0.39330630157800284, 0.38655056892072454, 0.39017754106544983, 0.3952399448436849, 0.39261257144458156, 0.3932858498073092, 0.39499423879326556, 0.3910665978111473, 0.3922448316041161, 0.39132243789294185, 0.3905644398547855, 0.39066209877822916, 0.39234474681171716, 0.3916584701806891, 0.39128884439374884, 0.38904376978091165, 0.39183107588221044, 0.3900741520611679, 0.3930635173969409, 0.38995324082526506, 0.39311547854951784, 0.3887742017121876, 0.3909773443113355, 0.38996322007448064, 0.38965144855718986, 0.39268765284442436, 0.390111534545819, 0.3925885803559247, 0.39064669915858435, 0.3895316495030534, 0.3907163414154567, 0.3907886814399093, 0.39067917981860684, 0.3914143468673323, 0.39072976911477014, 0.39038341521632436, 0.39487345085716713, 0.39242782728636966, 0.39340144921751585, 0.391638440317383, 0.38982952050134245, 0.39072682591629965, 0.39341319571523103, 0.39195014886996327, 0.3917578032203749, 0.38969211536003096, 0.3894471714601797, 0.3898716885961738, 0.39170538400318106, 0.39051353675769823, 0.39179608466870647, 0.3921622397998969, 0.39439454747765673, 0.39445579307628614, 0.39132673534400325, 0.3867330071972866, 0.3894542139388767, 0.39122700530524346, 0.3883669243431559, 0.38984382444737004, 0.3917064682817927, 0.3907441727670969, 0.39336200226463525, 0.3893578349083078, 0.3913763885696729, 0.3897980788320887, 0.3937839002293699, 0.39068301957027585, 0.3921215317997278, 0.39208485865417647, 0.3916279115513259, 0.3911278141918136, 0.39544760121726524, 0.38938475297946556, 0.39043475139667005, 0.39209672136634005, 0.39414026142627584, 0.3913420901871195, 0.39028101735839654, 0.3931575333516972, 0.39237413212072614, 0.38880399651094977, 0.3887612786801422, 0.3894933131979961, 0.3917112931901333]\n",
            "test_acc_list_exp = [75.58773816840811, 87.06976029502151, 87.53073140749846, 87.8918254456054, 87.76889981561156, 87.74969268592501, 87.98786109403811, 87.76121696373694, 87.87645974185618, 87.71896127842655, 87.74969268592501, 87.79578979717272, 87.96481253841426, 87.81115550092194, 87.81499692685925, 87.80731407498463, 87.9417639827904, 87.93023970497849, 87.76505838967425, 87.78042409342348, 87.66133988936693, 87.7458512599877, 87.7458512599877, 87.85725261216963, 87.66518131530424, 87.71127842655194, 87.65365703749232, 87.66133988936693, 87.74200983405039, 87.7458512599877, 87.85725261216963, 87.89566687154272, 87.64981561155501, 87.61140135218193, 88.02243392747388, 87.85725261216963, 87.50384142593731, 87.91871542716656, 87.82652120467118, 87.80731407498463, 87.82652120467118, 87.93792255685311, 87.6920712968654, 87.72280270436386, 87.82267977873387, 87.81499692685925, 87.87645974185618, 87.66133988936693, 87.8418869084204, 87.76889981561156, 87.88030116779349, 87.78042409342348, 87.7919483712354, 87.79963122311001, 87.81883835279656, 87.8918254456054, 87.92639827904118, 87.94944683466503, 87.78426551936079, 87.8918254456054, 87.7881069452981, 88.00322679778733, 87.87645974185618, 87.7881069452981, 87.68054701905348, 87.81499692685925, 87.71896127842655, 87.82267977873387, 87.7919483712354, 87.81883835279656, 87.83036263060848, 87.78042409342348, 87.78042409342348, 87.71511985248924, 87.72664413030117, 87.82267977873387, 87.80731407498463, 87.88030116779349, 87.83036263060848, 87.84956976029503, 87.91487400122925, 87.73048555623848, 87.81115550092194, 87.73048555623848, 87.7381684081131, 87.72664413030117, 87.8918254456054, 87.66518131530424, 87.8418869084204, 87.7919483712354, 87.69975414874001, 87.79578979717272, 87.77658266748617, 87.90334972341734, 87.75353411186232, 87.83036263060848, 87.86877688998156, 87.86877688998156, 87.7919483712354, 87.84572833435772, 87.91871542716656, 87.84956976029503, 87.83036263060848, 87.80347264904732, 87.70743700061463, 87.75353411186232, 87.9840196681008, 87.70359557467732, 87.84956976029503, 87.87645974185618, 87.7381684081131, 87.81115550092194, 87.6459741856177, 87.7458512599877, 87.51920712968654, 88.05700676090964, 87.79578979717272, 87.91871542716656, 87.56914566687155, 87.82267977873387, 87.73432698217579, 87.75737553779963, 87.81115550092194, 87.85341118623234, 87.74969268592501, 87.68054701905348, 87.63444990780577, 87.83420405654579, 87.91103257529196, 87.76505838967425, 87.80731407498463, 88.04548248309773, 87.81883835279656, 87.82652120467118, 87.6920712968654, 87.76121696373694, 87.74200983405039, 87.80347264904732, 87.96481253841426, 87.8380454824831, 87.6459741856177, 87.85341118623234, 87.92255685310387, 87.68438844499079, 87.7458512599877, 87.78426551936079, 87.81883835279656, 87.98017824216349, 87.88030116779349, 87.6421327596804, 87.9340811309158, 87.65365703749232, 87.68438844499079, 87.60371850030731, 87.66133988936693, 87.6459741856177, 87.78042409342348, 87.77658266748617, 87.58451137062077, 87.97633681622618, 87.83420405654579, 87.73048555623848, 87.84572833435772, 87.75737553779963, 87.76121696373694, 87.63829133374308, 87.91103257529196, 87.75353411186232, 87.57298709280884, 87.70743700061463, 87.73048555623848, 87.77658266748617, 87.76505838967425, 88.02243392747388, 87.84572833435772, 87.73048555623848, 87.88414259373079, 87.69975414874001, 87.88030116779349, 87.88030116779349, 87.68054701905348, 87.92639827904118, 87.95712968653964, 87.91103257529196, 87.68054701905348, 87.76121696373694, 87.78426551936079, 87.67286416717886, 87.67670559311617, 87.88414259373079, 87.79578979717272, 87.66133988936693, 87.83036263060848, 87.86493546404425, 87.78426551936079, 87.67286416717886, 87.81115550092194, 87.6421327596804, 87.70359557467732, 87.71127842655194, 87.84956976029503, 87.82267977873387, 87.7919483712354, 87.71127842655194, 87.83036263060848, 87.8879840196681, 87.76121696373694, 87.73048555623848, 87.76121696373694, 87.8380454824831, 87.75353411186232, 87.72280270436386, 87.84956976029503, 87.59987707437, 87.6959127228027, 87.62292562999386, 88.03779963122311, 87.77658266748617, 87.7381684081131, 87.80731407498463, 87.56530424093424, 87.63060848186846, 87.69975414874001, 87.71896127842655, 87.89566687154272, 87.83420405654579, 87.76121696373694, 87.63060848186846, 87.79578979717272, 87.65365703749232, 87.82267977873387, 87.85341118623234, 87.85341118623234, 87.80347264904732, 87.87645974185618, 87.84572833435772, 87.96097111247695, 87.84572833435772, 87.71511985248924, 87.85725261216963, 87.78426551936079, 87.78426551936079, 87.61908420405655, 87.86493546404425, 87.96097111247695, 87.86109403810694, 87.75737553779963, 87.84956976029503, 87.7881069452981, 87.84572833435772, 87.70359557467732, 87.71127842655194, 87.65749846342962, 87.62292562999386, 87.71896127842655, 87.96865396435157, 87.97633681622618, 87.76121696373694, 87.75353411186232, 87.81115550092194, 87.8879840196681, 87.82652120467118, 87.87645974185618, 87.68438844499079, 87.8918254456054, 87.63060848186846, 87.76505838967425, 87.5960356484327, 87.66133988936693, 87.9840196681008, 87.88030116779349, 87.86877688998156, 87.77274124154886, 87.9417639827904, 87.90719114935465, 87.7881069452981, 87.69975414874001, 87.77658266748617, 87.84572833435772, 87.67286416717886, 87.80731407498463, 87.74200983405039, 87.71896127842655, 87.6459741856177, 87.68438844499079, 87.86877688998156, 87.82267977873387, 87.78426551936079, 87.72664413030117, 87.98017824216349, 87.66518131530424, 87.6959127228027, 87.75353411186232, 87.91487400122925, 87.89566687154272, 87.72280270436386, 87.91487400122925, 87.88030116779349, 87.91103257529196, 87.75353411186232]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list_01 = [2.3849518770770977, 2.2417600981911345, 2.2415069635644516, 2.2409557133186153, 2.2419579268147953, 2.240125378942102, 2.240931454066662, 2.2417839348800785, 2.242252948807507, 2.241471426273749, 2.241231945472035, 2.2413479971691843, 2.241036834432504, 2.2407813569717616, 2.2415969093963706]\n",
        "train_acc_list_01 = [18.56855479089465, 18.617257808364215, 18.746426680783483, 18.60243515087348, 18.61937533086289, 18.886183165696135, 18.60243515087348, 18.598200105876124, 18.604552673372154, 18.740074113287452, 18.731604023292746, 18.814187400741133, 18.848067760719957, 18.82901005823187, 18.752779248279513]\n",
        "test_loss_list_01 = [2.2387831538331273, 2.241503697984359, 2.241926829020182, 2.240557459055209, 2.2406100852816713, 2.252836311564726, 2.2468298743752873, 2.2446437150824305, 2.2425780202828203, 2.240177970306546, 2.243702617346072, 2.2441832843948815, 2.253918958645241, 2.245230858232461, 2.2435202879064224]\n",
        "test_acc_list_01 = [18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 14.27858020897357, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463]\n",
        "train_loss_list_001 = [2.2888378896687414, 2.232192633274771, 1.4893088792236193, 0.5001831288098643, 0.3830500937251218, 0.32867970416539405, 0.29770232389774426, 0.27895135900919354, 0.2627035691970732, 0.24814978033950336, 0.23438045485475198, 0.2243682525668364, 0.21301924496848731, 0.20818865677811266, 0.1958482328166322]\n",
        "train_acc_list_001 = [18.50926416093171, 18.968766543144522, 47.80307040762308, 84.23292747485442, 88.15881418740074, 89.9142403388036, 91.04923239809423, 91.63790365272631, 92.08893594494441, 92.58020116463737, 93.03758602435151, 93.36791953414505, 93.715193223928, 93.84012705134992, 94.35044997353097]\n",
        "test_loss_list_001 = [2.2451091665847627, 2.2302937355695986, 0.684039752711268, 0.4374444575286379, 0.39873581839834943, 0.36398082489476485, 0.3313831433507742, 0.3210300371854329, 0.2847569804346445, 0.29315854806233854, 0.27853756525791157, 0.26896954926789973, 0.2596830692069203, 0.26028463620619446, 0.2491153629460171]\n",
        "test_acc_list_001 = [18.88060848186847, 19.053472649047325, 78.20759065765212, 86.33988936693301, 87.90334972341734, 88.90596189305471, 90.08143822987093, 90.81515058389674, 91.70636140135218, 91.54118008604794, 91.8638598647818, 92.2480024585126, 92.43239090350338, 92.50537799631223, 92.82037492317149]\n",
        "train_loss_list_0001 = [1.8565612323885041, 0.5532636212785715, 0.4003713336094285, 0.3402645644860539, 0.309145948165639, 0.2848975209968523, 0.262982070708501, 0.24855145600026216, 0.2386487888167221, 0.2278864942390098, 0.21327269485164788, 0.20556094100683686, 0.19517452138549268, 0.18913837452608395, 0.18232752032436653]\n",
        "train_acc_list_0001 = [34.7993647432504, 82.35468501852831, 87.65272631021705, 89.51614610905241, 90.6723133933298, 91.51932239280042, 92.26892535733192, 92.59925886712546, 92.97617787188989, 93.34674430915828, 93.69401799894123, 93.97353096876654, 94.31868713605083, 94.53255690841715, 94.71254632080466]\n",
        "test_loss_list_0001 = [1.0861167063315709, 0.46979708385233787, 0.41340532643245714, 0.3364271931350231, 0.3262409484561752, 0.34040282589986043, 0.28803076817854945, 0.2942232322678262, 0.2779932311169949, 0.27514038454083833, 0.2478603608906269, 0.2512748020463714, 0.26197264781769586, 0.2462065773194327, 0.24985540344142446]\n",
        "test_acc_list_0001 = [64.00583896742471, 85.58312845728334, 87.41933005531654, 89.81637984019667, 89.970036877689, 89.99308543331284, 91.59111862323294, 91.35295021511985, 91.82928703134604, 92.03672403196066, 92.76275353411187, 92.83574062692071, 92.90488629379226, 92.98555623847572, 92.87031346035648]\n",
        "train_loss_list_00001 = [1.2440541174192092, 0.4583125376927497, 0.36052036624613815, 0.3151768069603256, 0.2875107263081119, 0.26835051384883196, 0.25187577066948097, 0.23390740957767336, 0.22118305101950317, 0.21190600004299545, 0.20555683115350845, 0.1952596850249018, 0.18612936185546683, 0.18178928815090883, 0.17480877608181986]\n",
        "train_acc_list_00001 = [57.18581259925887, 85.53943885653786, 88.79618845950239, 90.25304393859184, 91.214399152991, 92.01058761249338, 92.36421386977237, 93.09899417681312, 93.43991529910005, 93.80412916887242, 93.86765484383271, 94.25092641609317, 94.500794070937, 94.64478560084702, 94.82477501323451]\n",
        "test_loss_list_00001 = [0.5935118007017117, 0.43328142947718207, 0.3668157114994292, 0.34575528556517526, 0.3590214093964474, 0.3134572241893586, 0.2903973020467104, 0.28062032824199573, 0.26905518266208034, 0.26447016406146917, 0.27132851287138227, 0.26421160440818936, 0.25491809147391836, 0.24584030433028353, 0.2630316608895858]\n",
        "test_acc_list_00001 = [80.93500307314075, 86.63567916410571, 88.97510755992624, 89.3438844499078, 88.99431468961278, 90.58466502765826, 91.54502151198525, 91.56422864167179, 92.02135832821143, 92.25184388444991, 91.97910264290104, 92.37861094038107, 92.72433927473878, 92.96250768285188, 92.59757221880763]\n",
        "\n",
        "\n",
        "train_loss_list_const = [1.5944857293674293, 0.4869092239677745, 0.3710624326454592, 0.3296906750215101, 0.29884549410039496, 0.27146996971633697, 0.2604337949053382, 0.24417246496532022, 0.23419785655045575, 0.22457996957751147, 0.21173857996457315, 0.20202396321425917, 0.1943116241051414, 0.18745123495052501, 0.1774632519257424, 0.17214223601650902, 0.16552220173886797, 0.15473239550866733, 0.15404348591276948, 0.14213740165398372, 0.13690419029192066, 0.13265049481779578, 0.12643728150322348, 0.1223953123176647, 0.11572830584960256, 0.10872174589453028, 0.10607895330301306, 0.10143619665729645, 0.09846852844464908, 0.09155762590831373, 0.09176684167932689, 0.08320119946962853, 0.07973214024854547, 0.07931727142424037, 0.06983993944351063, 0.07091535234995448, 0.0671700671211713, 0.06470882359558974, 0.061619590856841586, 0.061459206172131346, 0.057121708876198427, 0.053374216585170206, 0.0498615506466044, 0.05073856983490308, 0.04987424655961312, 0.046790412364042994, 0.04711244383266544, 0.04263703334889801, 0.043034394194061555, 0.04126133985860339, 0.03625233271121373, 0.03783284285014904, 0.0379838087573284, 0.03416189216178284, 0.03383261713717981, 0.03370409071676403, 0.033968668481206325, 0.03165312097212274, 0.03038872497441464, 0.02841417055683044, 0.029356409022635036, 0.02889881345480024, 0.028939039615515447, 0.02568263415692672, 0.027656220918051838, 0.026301205131101617, 0.023402816848356156, 0.024891200900357974, 0.022950206206843942, 0.022271349775869285, 0.0250707899006071, 0.022344395426495713, 0.019562920480307198, 0.022191016989943715, 0.02281398802152065, 0.0203801996838415, 0.020550807893653537, 0.02117716848076041, 0.019771566791103416, 0.022764958889409142, 0.017198871523580392, 0.02078044665365311, 0.020109122048629333, 0.01588759385191313, 0.01774437841210012, 0.020397339600326397, 0.01534856222032823, 0.016435430918308794, 0.01711455919842304, 0.01770936636729633, 0.018040290616609177, 0.01564757464108749, 0.015251886521385311, 0.01534154344099917, 0.015427147975282334, 0.01419120683843651, 0.01811075882562171, 0.016642653122089984, 0.01305790530750528, 0.013949767823940267, 0.014656124457392462, 0.013457846312966957, 0.014343252129156173, 0.013850259383991146, 0.012460459422866397, 0.012413865320129272, 0.013889214684304953, 0.01221698466477806, 0.015144580592092777, 0.010421825547786688, 0.01315658220278792, 0.014559990600058128, 0.012665790851758906, 0.01143317199561781, 0.011717887349175301, 0.011158559258571676, 0.012634062059679534, 0.011741106513152792, 0.012515452659958159, 0.011751638104695717, 0.011457235534427129, 0.011438692509360596, 0.012512708038015468, 0.008602326916451698, 0.010682771495822786, 0.011196336384384224, 0.009268063090769713, 0.009998142540840234, 0.012729091897495166, 0.010267185127698763, 0.010566324319321028, 0.010713477439735606, 0.010119398856430022, 0.012654226244056406, 0.009628543746825273, 0.0102562899387879, 0.010169452542220153, 0.011113815507671713, 0.01246779066452764, 0.007518461719419533, 0.007389902458939484, 0.011104748547043264, 0.01135712010794771, 0.008941290669380526, 0.009924649224834431, 0.009838336234147059, 0.007612355080635734, 0.009282812758648675, 0.008423602285464519, 0.009065308554876006, 0.010365516194014629, 0.006115070432431249, 0.006490228478146479, 0.011122480412586441, 0.00872685812082587, 0.008791860558152852, 0.009787340952780856, 0.008418811355189983, 0.007423892029066022, 0.009898100130695444, 0.008828991230531168, 0.007722080036888794, 0.009378696144602659, 0.00829515335842403, 0.006554383687879633, 0.006126663065066873, 0.010910632581322311, 0.007716069375264186, 0.006358072765228986, 0.007697185452836498, 0.008877029283653514, 0.008559577807589543, 0.00707696825589556, 0.00850764569837624, 0.0062317561151193695, 0.0066259507287960455, 0.008618629247509826, 0.007878521594295732, 0.007498281048782551, 0.006598806819186967, 0.006230976371933824, 0.006141664580167378, 0.007923200858393221, 0.008045455975570557, 0.00792612456981016, 0.007601726206049413, 0.007419711852838272, 0.004584900550783639, 0.00685006011318293, 0.008050930358640324, 0.008370204849807444, 0.005507358266772813, 0.006857401374734981, 0.008172955556791895, 0.004244687181667273, 0.008633549227632837, 0.008705449396538356, 0.0067560189635043925, 0.006710546794999935, 0.00550614756518684, 0.0069180619830691474, 0.00835907400430254, 0.005712247417545953, 0.006098340349643973, 0.007154704712266156, 0.0061693518397019, 0.005750073719051822, 0.005536363746094307, 0.006909997050809464, 0.006392437509654407, 0.004530645426771153, 0.00935107147158372, 0.005991038718238898, 0.004413133404240297, 0.007152540924743997, 0.006123794079590131, 0.004265024197029019, 0.007488072243164834, 0.006849384789138319, 0.006199823599835201, 0.004939858833603685, 0.006542191565922548, 0.006134184821023406, 0.006127372573951168, 0.006120713498504846, 0.005705225819374859, 0.0054501006014126396, 0.00476737067947397, 0.005204442173390228, 0.006888031104115721, 0.005438639611901732, 0.005198591940753804, 0.006226397409455738, 0.00398913007948585, 0.006725981141281091, 0.0057350191533619786, 0.004547678196042624, 0.003760278201966155, 0.004862335098237806, 0.0053206211415940214, 0.004511028374038128, 0.006714381331292853, 0.006507630908786355, 0.004150821972775074, 0.005154698122662017, 0.005662354828146746, 0.004606740834664833, 0.004731273036621596, 0.004916682220897782, 0.0048261530498232945, 0.006001339212270287, 0.007263619651073496, 0.0038986454153652154, 0.005036929771996826, 0.005049693082181207, 0.00644224065052205, 0.004578751418381182, 0.004686958738437559, 0.003585059260631736, 0.003101582141557616, 0.007221638428928184, 0.005000114876925785, 0.0051700827908760325, 0.005254589704386773, 0.00406367993565528, 0.0036232792127412505, 0.006171727926178867, 0.005875237415135579, 0.00392443083374783, 0.006561909474143628, 0.0037487958143322068, 0.004443046452993911, 0.005447318903908573, 0.004017829913798502, 0.00472008602112813, 0.0055074588290326305, 0.005279580161203713, 0.0036886348879267314, 0.0035813810519778975, 0.004883374933941502, 0.004846986711028166, 0.004054715485154408, 0.004949977601104352, 0.0046826732979178415, 0.004384392334953691, 0.004187341836835158, 0.004243570542040143, 0.005868256121943751, 0.00557879406272601, 0.0037290168971954394, 0.0024883511193469366, 0.004939023949055762, 0.004748034648033541, 0.0038421762496379727, 0.003486280636854575, 0.0036575751641310756, 0.00670746078328728, 0.004645855993902198, 0.004636629221823842, 0.004005611049817734]\n",
        "train_acc_list_const = [44.03176283748015, 84.49761778718899, 88.61196400211752, 89.87400741132875, 91.01958708311275, 91.79460031762838, 92.2350449973531, 92.80889359449444, 93.03546850185283, 93.37427210164108, 93.80412916887242, 94.0052938062467, 94.38009528851244, 94.6278454208576, 94.88618316569614, 95.01323451561673, 95.1868713605082, 95.66543144520911, 95.57226045526734, 95.91953414505029, 96.01905770248808, 96.16093170989942, 96.35150873478031, 96.54632080465855, 96.74113287453679, 96.86183165696136, 96.96347273689783, 97.10746426680784, 97.21757543673901, 97.32133403917416, 97.3276866066702, 97.63896241397565, 97.72154579142403, 97.7342509264161, 97.88883006881949, 97.87400741132875, 97.96506087877184, 98.09211222869243, 98.24245632609846, 98.17257808364214, 98.28692429857067, 98.34833245103229, 98.44997353096876, 98.49444150344097, 98.49655902593965, 98.54102699841185, 98.55796717840127, 98.67443091582848, 98.63419798835362, 98.74219163578613, 98.87559555320276, 98.77607199576495, 98.78030704076231, 98.9793541556379, 98.94970884065643, 98.9433562731604, 98.90947591318158, 98.98782424563261, 98.96453149814717, 99.08099523557438, 99.05770248808894, 99.05134992059291, 99.0428798305982, 99.1784012705135, 99.13393329804128, 99.1148755955532, 99.20381154049761, 99.18051879301217, 99.27580730545262, 99.23980942297511, 99.1784012705135, 99.25674960296453, 99.3499205929063, 99.26310217046056, 99.2503970354685, 99.32874536791954, 99.28215987294865, 99.27157226045527, 99.29486500794071, 99.24404446797247, 99.42615140285865, 99.2779248279513, 99.31180518793012, 99.46214928533615, 99.40921122286925, 99.36262572789836, 99.49602964531498, 99.43673901535203, 99.42615140285865, 99.40921122286925, 99.41132874536792, 99.47697194282689, 99.49602964531498, 99.49814716781366, 99.47273689782953, 99.5044997353097, 99.39015352038115, 99.45156167284277, 99.57649550026468, 99.55320275277924, 99.5404976177872, 99.5659078877713, 99.53414505029116, 99.54685018528322, 99.58073054526204, 99.5849655902594, 99.49391212281631, 99.58920063525674, 99.51720487030175, 99.66543144520911, 99.5574377977766, 99.56167284277396, 99.56802541026998, 99.60402329274748, 99.62519851773425, 99.6019057702488, 99.5934356802541, 99.62519851773425, 99.60402329274748, 99.61461090524087, 99.61461090524087, 99.63790365272631, 99.6019057702488, 99.70566437268396, 99.66119640021175, 99.64002117522499, 99.69507676019057, 99.68025410269983, 99.58920063525674, 99.6569613552144, 99.62519851773425, 99.6569613552144, 99.66754896770779, 99.54261514028586, 99.69719428268925, 99.63366860772896, 99.67178401270513, 99.60825833774484, 99.6019057702488, 99.75436739015352, 99.75436739015352, 99.59555320275278, 99.62731604023293, 99.6929592376919, 99.65484383271573, 99.66966649020645, 99.7289571201694, 99.71836950767602, 99.69084171519323, 99.67601905770249, 99.66119640021175, 99.79671784012704, 99.78824775013234, 99.62519851773425, 99.71836950767602, 99.72683959767072, 99.68872419269455, 99.73107464266808, 99.73107464266808, 99.67601905770249, 99.70989941768131, 99.73319216516676, 99.68025410269983, 99.7204870301747, 99.78824775013234, 99.80518793012176, 99.64849126521969, 99.73107464266808, 99.79883536262572, 99.74377977766014, 99.69719428268925, 99.69719428268925, 99.77130757014294, 99.71625198517734, 99.7924827951297, 99.784012705135, 99.70354685018528, 99.75860243515088, 99.73319216516676, 99.79036527263102, 99.78189518263632, 99.79036527263102, 99.73530968766543, 99.69719428268925, 99.75013234515616, 99.72472207517205, 99.76071995764956, 99.84330333509793, 99.784012705135, 99.73530968766543, 99.69084171519323, 99.82212811011117, 99.76919004764426, 99.73319216516676, 99.85812599258867, 99.70989941768131, 99.70142932768661, 99.76071995764956, 99.78824775013234, 99.8284806776072, 99.7924827951297, 99.7204870301747, 99.8009528851244, 99.81577554261514, 99.75436739015352, 99.7924827951297, 99.81365802011646, 99.8009528851244, 99.77554261514028, 99.77977766013764, 99.85389094759132, 99.67601905770249, 99.81789306511382, 99.86871360508205, 99.74377977766014, 99.78189518263632, 99.84965590259397, 99.77130757014294, 99.78824775013234, 99.82001058761249, 99.81365802011646, 99.77130757014294, 99.78189518263632, 99.79883536262572, 99.80730545262044, 99.82636315510852, 99.79036527263102, 99.83059820010588, 99.81154049761778, 99.7564849126522, 99.8094229751191, 99.8284806776072, 99.81154049761778, 99.88353626257279, 99.75224986765484, 99.79671784012704, 99.85389094759132, 99.88565378507147, 99.83059820010588, 99.80730545262044, 99.84118581259926, 99.77130757014294, 99.81154049761778, 99.85177342509265, 99.83271572260455, 99.8094229751191, 99.8284806776072, 99.86236103758603, 99.8369507676019, 99.82424563260984, 99.8009528851244, 99.78824775013234, 99.84330333509793, 99.82636315510852, 99.8284806776072, 99.78189518263632, 99.85812599258867, 99.85177342509265, 99.89624139756485, 99.89835892006353, 99.77130757014294, 99.84118581259926, 99.81577554261514, 99.8369507676019, 99.85600847008999, 99.86659608258337, 99.81154049761778, 99.79671784012704, 99.87718369507677, 99.79883536262572, 99.85812599258867, 99.85389094759132, 99.83906829010058, 99.88353626257279, 99.84753838009529, 99.8369507676019, 99.83906829010058, 99.88565378507147, 99.87506617257809, 99.83271572260455, 99.84330333509793, 99.85812599258867, 99.86659608258337, 99.8369507676019, 99.8284806776072, 99.84753838009529, 99.85177342509265, 99.7924827951297, 99.79671784012704, 99.88141874007411, 99.92376919004765, 99.86024351508735, 99.84753838009529, 99.85600847008999, 99.87506617257809, 99.87718369507677, 99.79671784012704, 99.86024351508735, 99.85177342509265, 99.87083112758073]\n",
        "test_loss_list_const = [0.9403148495099124, 0.46971940738605517, 0.34306909633325594, 0.3384028169892582, 0.3164549315823059, 0.31764499748162195, 0.282772644879479, 0.27239831069520876, 0.2788072020618939, 0.2605946859454407, 0.2569786167758353, 0.25151387514436946, 0.23040011815507622, 0.23389688687508597, 0.23323347488893012, 0.22432381940969065, 0.23885389055837603, 0.22965874502837075, 0.23512107754747072, 0.23369459731175618, 0.22183155821745887, 0.2491116562639089, 0.23399925456546686, 0.25359821337841304, 0.23499618255186314, 0.22481991891183106, 0.2324665375966944, 0.23745987874766192, 0.24356824685545528, 0.24952035153503804, 0.23930090873082185, 0.25436064657554325, 0.25367995643732594, 0.26609678615761156, 0.2672480665622096, 0.2746706861893044, 0.25824960023529975, 0.26025373894063863, 0.2791316908072023, 0.26918739248432366, 0.2763673086997633, 0.2997446244436444, 0.2916372886438872, 0.3003798500981693, 0.2806282534501424, 0.2931029381979184, 0.29610180335265457, 0.3087221756942716, 0.32797835285172744, 0.30948590080929445, 0.31205729929292025, 0.3264701991711798, 0.31612680869761345, 0.3249107254029927, 0.3218845636081681, 0.3128845189679779, 0.3158182442261308, 0.3337518404621412, 0.34533743259003935, 0.32409434777447116, 0.3321887091642209, 0.3324712138604738, 0.33405348602864965, 0.3346349073199592, 0.3316200801226146, 0.35435040097446274, 0.3272651647738017, 0.3605334343612377, 0.3315225986252521, 0.36446072681642633, 0.37520757036320135, 0.3792235794586732, 0.3674935271379118, 0.3611337201596767, 0.35125305392213313, 0.3664919300844856, 0.3579704279956572, 0.358177753384499, 0.37499817195074525, 0.3516562903372973, 0.36848902222061275, 0.37098927200570997, 0.37174441270968495, 0.3886868230103716, 0.36849470644751015, 0.3531273581982389, 0.3660184144471571, 0.37421249364520986, 0.3748011891273599, 0.3667490633350669, 0.3643053767691348, 0.37450089440772344, 0.37444961906465535, 0.3669995936729452, 0.37039543743080955, 0.38952702733085437, 0.39265704210208474, 0.3686957350274658, 0.3655954713695774, 0.39540933398529887, 0.37815322328870205, 0.3858970995974161, 0.37387897242682383, 0.37425000457020074, 0.38926156392941874, 0.3786647673787586, 0.37843504400156874, 0.4000659274078869, 0.38383779517721894, 0.38883355182513374, 0.38116612786189746, 0.39886108679952575, 0.4039541971048011, 0.4085461224575399, 0.4040325807553588, 0.40590017215878355, 0.38623136059180196, 0.38693335685221586, 0.41144327609343273, 0.3980357802459313, 0.4007313655798926, 0.40366758222636934, 0.38523547610669745, 0.42097171660804866, 0.4246416387294291, 0.40921079142786126, 0.4052840040850581, 0.4231068941344525, 0.38739459854824576, 0.4194799537094785, 0.41747657055327414, 0.4164134435739149, 0.3939945743985328, 0.393845445202554, 0.4172966992665155, 0.4208380446276244, 0.408174136686641, 0.43686796033608855, 0.3842315211042981, 0.4022488394277353, 0.41533097460427704, 0.4123304054533661, 0.3947430606748836, 0.4344836208585869, 0.42245670334509045, 0.4120090134880122, 0.4084896664774301, 0.4208405370477076, 0.4342218170389898, 0.4342225538966173, 0.414541769489719, 0.42016579197290554, 0.43672865715023934, 0.3970657474470927, 0.4120631425404081, 0.4072342333657777, 0.4361969401840778, 0.40728934426043256, 0.4082725720443562, 0.43000999789721533, 0.4488575828864294, 0.4490402939628956, 0.42708765610358584, 0.4119907574152903, 0.4171651719850214, 0.46127884272559017, 0.424621216852364, 0.42663686249551236, 0.443727373143238, 0.4234333980655042, 0.4163527505554478, 0.41962003758446514, 0.43725314623146666, 0.4135830312939909, 0.4485339165719993, 0.438206387026345, 0.42548432996423513, 0.4139385414218493, 0.422614497990877, 0.44258191624619797, 0.5020544356643679, 0.42181872726217207, 0.45136614169414135, 0.42586187040889817, 0.4617275644605066, 0.42785005136758236, 0.4181436715811929, 0.43456211805745376, 0.456493051905258, 0.4419752404130265, 0.439949865836431, 0.458016514120733, 0.45355927485370023, 0.4266304138993077, 0.4774111132959233, 0.43147677636942733, 0.42984621565533326, 0.4257584124226888, 0.41347907219703, 0.44227188734301165, 0.44509480125270784, 0.42359101505694435, 0.45620652116542937, 0.4532003666402078, 0.4544208156419735, 0.4288948582868804, 0.45772276510365817, 0.4652134206776014, 0.4481767811538542, 0.45044724102926387, 0.45808432965228957, 0.4832801581086481, 0.4330355538119215, 0.450099578350965, 0.44405912482818843, 0.4245691280629413, 0.47918898603130206, 0.4441465862088508, 0.4232571859679678, 0.42079631317699073, 0.46119900758140814, 0.4447093260310152, 0.45832808096619215, 0.4615354106998911, 0.46208279893970955, 0.44396297247651234, 0.44644354116719437, 0.4640259245666219, 0.466397661642701, 0.46452038604811785, 0.4498248620908342, 0.4491576017337103, 0.465446551038208, 0.47035092626716574, 0.4713521593012938, 0.4597488452506927, 0.4512224776633814, 0.48416280633240355, 0.4734927043455708, 0.46729338517887337, 0.46055449181357305, 0.44595291037751617, 0.45059747884606977, 0.4550898942740305, 0.4553583972247354, 0.484807043427638, 0.47273430763972085, 0.45941312947109636, 0.4590342458538419, 0.4751177701332113, 0.47533442642466694, 0.45637929342760175, 0.4544383191900766, 0.4463540438526109, 0.45517611408414427, 0.4642113188975582, 0.44581558580930325, 0.4676268981386195, 0.46861990691874833, 0.47670730448090565, 0.4613685188541079, 0.46828112538502203, 0.47280027833310706, 0.44121663249097764, 0.4606796685479554, 0.5259090004415781, 0.48354477134040175, 0.469667333146265, 0.4692415113266393, 0.45400641179944884, 0.47124824484846756, 0.4692198993460111, 0.44249545135900525, 0.4556816854969571, 0.4647824307538423, 0.47515893393360514, 0.44086019356972445, 0.47874322776481804, 0.48824521492911027, 0.4832109012732319, 0.45331234720138397, 0.46669590011166007, 0.4659509803990231, 0.4592359331253843, 0.4794528412395248, 0.4734935767528619, 0.4946094546649678, 0.47707713397183255, 0.4516702479244593, 0.44164761501893984, 0.48553112554637823, 0.4603754159190929, 0.4560155915005096, 0.48063205852739366, 0.4870615708185177, 0.503982417563926, 0.4785489882890354, 0.44539653152848285, 0.4539821001874539, 0.47518135870204253]\n",
        "test_acc_list_const = [71.6080208973571, 85.50245851259987, 89.67040565457899, 89.58589428395821, 90.46173939766442, 90.5577750460971, 91.50276582667486, 92.0520897357099, 91.96757836508912, 92.47464658881377, 92.6820835894284, 92.88952059004302, 93.56177012907192, 93.4580516287646, 93.66548862937923, 93.80377996312231, 93.34665027658266, 93.71158574062692, 93.5041487400123, 93.57329440688383, 94.06499692685925, 93.33896742470804, 93.72311001843885, 92.94330055316533, 93.68853718500307, 94.18792255685311, 93.96127842655194, 93.91518131530424, 93.82682851874615, 94.03042409342348, 94.1917639827904, 93.77304855562384, 93.9420712968654, 93.68853718500307, 94.08420405654579, 93.77304855562384, 93.91518131530424, 94.14566687154272, 93.93822987092808, 94.14566687154272, 94.06499692685925, 93.73079287031346, 93.8921327596804, 94.07267977873387, 94.01889981561156, 94.22633681622618, 93.86140135218193, 93.90749846342962, 93.97664413030117, 94.07652120467118, 94.03042409342348, 93.97280270436386, 94.21865396435157, 94.07267977873387, 94.09956976029503, 94.21865396435157, 94.30700676090964, 93.88444990780577, 94.06883835279656, 94.30316533497235, 94.23017824216349, 94.19944683466503, 94.05731407498463, 94.36846957590657, 94.26475107559926, 94.04578979717272, 94.31468961278426, 94.13030116779349, 94.3262138905962, 94.06499692685925, 93.98432698217579, 94.34926244622004, 94.3262138905962, 94.10341118623234, 94.35694529809466, 93.99969268592501, 94.21481253841426, 94.41840811309157, 94.13414259373079, 94.23017824216349, 94.39920098340504, 94.10341118623234, 94.0381069452981, 94.36846957590657, 94.44529809465274, 94.29548248309773, 94.39920098340504, 94.2839582052858, 94.34542102028273, 94.51444376152428, 94.34926244622004, 94.1418254456054, 94.45682237246466, 94.5221266133989, 94.41072526121697, 94.45298094652735, 93.9958512599877, 94.24938537185002, 94.57974800245852, 94.1418254456054, 94.18792255685311, 94.34926244622004, 94.48371235402581, 94.39535955746773, 94.39151813153042, 94.39535955746773, 94.41072526121697, 94.1418254456054, 94.24554394591273, 94.39535955746773, 94.49907805777505, 94.3262138905962, 94.22633681622618, 94.34542102028273, 94.42224953902888, 94.27627535341118, 94.46450522433928, 94.1840811309158, 94.1917639827904, 94.50291948371235, 94.09956976029503, 94.27243392747388, 94.38383527965581, 94.40304240934235, 94.10725261216963, 94.36462814996926, 94.52980946527352, 94.36078672403197, 94.50676090964966, 94.1917639827904, 94.25706822372464, 94.31084818684695, 94.43761524277812, 94.36462814996926, 94.42224953902888, 94.21097111247695, 94.38383527965581, 94.18023970497849, 94.21865396435157, 94.5259680393362, 94.53365089121081, 94.21865396435157, 94.56822372464659, 94.21865396435157, 94.31468961278426, 94.41072526121697, 94.50291948371235, 94.35310387215735, 94.51060233558697, 94.26090964966195, 94.3338967424708, 94.4299323909035, 94.40688383527966, 94.66425937307929, 94.41072526121697, 94.39535955746773, 94.23786109403811, 94.44145666871543, 94.30316533497235, 94.3761524277812, 94.04578979717272, 94.36846957590657, 94.3300553165335, 94.44145666871543, 94.64505224339274, 93.99200983405039, 94.53365089121081, 94.52980946527352, 94.3799938537185, 94.38767670559312, 94.54133374308543, 94.36078672403197, 94.44913952059004, 94.28779963122311, 94.48755377996312, 94.51828518746159, 94.4721880762139, 94.45682237246466, 94.39920098340504, 94.51444376152428, 94.10341118623234, 94.61432083589429, 94.41840811309157, 94.21865396435157, 94.46450522433928, 94.44913952059004, 94.4798709280885, 94.61432083589429, 94.22633681622618, 94.63352796558083, 94.51060233558697, 94.44145666871543, 94.5221266133989, 94.50291948371235, 93.93438844499079, 94.4721880762139, 94.48371235402581, 94.63736939151813, 94.5759065765212, 94.61047940995698, 94.39920098340504, 94.56822372464659, 94.21481253841426, 94.59895513214505, 94.32237246465888, 94.59127228027043, 94.25322679778733, 94.4299323909035, 94.54517516902274, 94.3338967424708, 94.40688383527966, 94.08420405654579, 94.5259680393362, 94.59895513214505, 94.6681007990166, 94.61047940995698, 94.44529809465274, 94.48371235402581, 94.48755377996312, 94.67962507682851, 94.36846957590657, 94.09572833435772, 94.35694529809466, 94.43377381684081, 94.55669944683467, 94.61047940995698, 94.36462814996926, 94.6220036877689, 94.69499078057775, 94.39535955746773, 94.47602950215119, 94.43761524277812, 94.3300553165335, 94.68730792870313, 94.56822372464659, 94.45298094652735, 94.61816226183159, 94.40304240934235, 94.48371235402581, 94.62968653964352, 94.54517516902274, 94.51828518746159, 94.60663798401967, 94.55669944683467, 94.50676090964966, 94.52980946527352, 94.59511370620774, 94.46066379840197, 94.40304240934235, 94.49139520590043, 94.4299323909035, 94.34157959434542, 94.6681007990166, 94.58358942839583, 94.61047940995698, 94.34926244622004, 94.57974800245852, 94.50676090964966, 94.71035648432698, 94.70651505838967, 94.5221266133989, 94.3761524277812, 94.50291948371235, 94.59127228027043, 94.52980946527352, 94.41456668715428, 94.53749231714812, 94.29932390903504, 94.5720651505839, 94.68730792870313, 94.61816226183159, 94.55285802089736, 94.6681007990166, 94.70651505838967, 94.59127228027043, 94.40304240934235, 94.69114935464044, 94.68730792870313, 94.5221266133989, 94.45682237246466, 94.51828518746159, 94.57974800245852, 94.61432083589429, 94.6220036877689, 94.34542102028273, 94.53749231714812, 94.40688383527966, 94.49523663183774, 94.42609096496619, 94.6757836508912, 94.42224953902888, 94.43377381684081, 94.56438229870928, 94.65273509526736, 94.49139520590043, 94.4721880762139, 94.3338967424708, 94.56054087277197, 94.70651505838967, 94.45682237246466]\n",
        "train_loss_list_cosine = [1.4922631353059113, 0.4829037673670425, 0.3722163860390826, 0.33177220926375245, 0.30060175129876227, 0.2739472426776964, 0.2571369293057499, 0.24421742562517565, 0.23246381484315323, 0.22585582048670064, 0.2115350698794776, 0.2027042203842786, 0.19336028488953586, 0.18796989869901803, 0.17635167322467335, 0.17350608562712425, 0.16392932351688705, 0.15824737119157786, 0.15103116432623812, 0.14280527628211148, 0.13911624957976465, 0.12778460819445814, 0.12645664817696503, 0.12117257786202963, 0.11404008718162048, 0.11024573898614261, 0.10301367604352918, 0.09962807324661957, 0.09590673247266429, 0.09140933892338822, 0.08890839097617563, 0.08312069913650028, 0.07912878103322049, 0.07539688910895247, 0.0703017507051307, 0.06730901317161112, 0.06545131962347604, 0.06006853946239806, 0.05934957680948644, 0.055661115047034776, 0.05501487654656535, 0.04808283364715756, 0.048917624036854686, 0.04978224897863177, 0.04599738403836765, 0.043476243917943865, 0.04154425250810538, 0.044289368833225914, 0.039924735993999975, 0.038134503976484824, 0.03573395804436448, 0.03633099462935413, 0.03407486775469762, 0.03466019954098273, 0.03175460545031914, 0.030751291346965928, 0.02861118235254312, 0.026954070920645837, 0.026345375887917098, 0.027870583739497655, 0.02564559653584358, 0.02595152790995118, 0.02664908538916373, 0.017370514487667858, 0.02682043477352759, 0.02288949422978072, 0.023653404380517905, 0.021177587269342865, 0.01825827019425427, 0.020935813946993946, 0.02142309610224594, 0.021006980622225117, 0.017297085196421504, 0.020975425832167893, 0.016085938927383088, 0.019187606733535582, 0.016529617174421272, 0.017658664713450265, 0.017461861724943822, 0.01473946469002416, 0.013888732157426767, 0.018813401251478592, 0.018245284999345322, 0.013429369967483517, 0.013213716673361565, 0.014179550147890478, 0.013165565478723676, 0.013442610755386424, 0.011938090494525969, 0.013952531346849035, 0.012777711567658235, 0.01361946394184095, 0.01415981548655886, 0.0132249127186142, 0.008202303269795845, 0.013768190907590946, 0.01187117434464689, 0.011072834069390408, 0.011375907309212407, 0.008056683960140176, 0.010813868758558904, 0.010755819991023294, 0.011055263281323623, 0.009854590676922565, 0.009897416544746738, 0.007694702545891319, 0.010974907935630917, 0.009073475042950686, 0.00957946586983009, 0.008521491170432235, 0.009181163735296882, 0.008033073926435217, 0.0075308985053745, 0.007736907337884813, 0.008905222457779166, 0.0075535059801524005, 0.008956405532912782, 0.00652504011866664, 0.007371700082288889, 0.007105531808951359, 0.009608815764438242, 0.006718999051766881, 0.006360923469938631, 0.006009158956909616, 0.005984226889401126, 0.006414240613540077, 0.008046470651133564, 0.006813095370703897, 0.004941387222356937, 0.006961080895634556, 0.0061504495415253034, 0.0044804993527444515, 0.005129817030924186, 0.0051584241011990275, 0.005904302868943308, 0.007040096893751609, 0.006497695825108162, 0.006231063241578372, 0.0045172427992303645, 0.004859256595798965, 0.005357292924210756, 0.0036115553579709734, 0.004059441841032762, 0.003976077818785122, 0.004686579279045762, 0.0047765986038821135, 0.0052203105924466305, 0.004452093922426263, 0.00366650621972908, 0.004813907496713725, 0.003619882242262452, 0.003379426909955734, 0.003642032724296165, 0.003996376050219234, 0.0027063930590780893, 0.0038246032934270147, 0.0035842454059730284, 0.0031291756938454703, 0.003011805406823518, 0.004053137657470737, 0.003489878551598058, 0.002363212944410382, 0.003003648613003568, 0.002955618500260455, 0.002723948648240625, 0.0037992216377342554, 0.0022372855783796795, 0.002972214323449715, 0.0029076020603400095, 0.002062946104120749, 0.0018581666937362376, 0.002313281514427136, 0.0030319557007007047, 0.0023327526890917524, 0.001145549262753536, 0.0016048090303144983, 0.0012414230531554533, 0.0032581396387386516, 0.0029362018629135516, 0.001846583708184242, 0.0012200965248589121, 0.001480849612214921, 0.002236391388236972, 0.0015825234965880586, 0.0009787926420108245, 0.001177633173575748, 0.0017891079141621141, 0.001687879517755926, 0.0027874750058864543, 0.0015881228617325382, 0.0013229923813940598, 0.0013549324787301051, 0.0013446073458035835, 0.0015000026920124656, 0.0012523066383357765, 0.000858102498131121, 0.001035546338261591, 0.0019881639752974524, 0.0016324728715111814, 0.001434380197954095, 0.0009871888154247414, 0.0008098082351434123, 0.0006027996978315856, 0.0008483088922217938, 0.0007788997386943644, 0.0014305688407895846, 0.0007751793022755421, 0.0005314110757599325, 0.0008008613197451418, 0.0005263145778683319, 0.0005657160187561332, 0.0008550505100606119, 0.0004061993359216422, 0.0008238756363850807, 0.0004022425374623086, 0.0007475109217440491, 0.0008010522570415786, 0.0008719455779264424, 0.0008102537067081855, 0.0006836875085969626, 0.000508847326879101, 0.00048076220441707043, 0.0003452522178527452, 0.0005887742460472115, 0.0007575123468680768, 0.0007316459814930954, 0.0006069562958639393, 0.00040482217438505993, 0.000413289390024444, 0.0002512041357034269, 0.00035321101188540014, 0.0002289985919775472, 0.00031013075728369315, 0.00048718760862263036, 0.00024815363102110633, 0.0002127035124524638, 0.00040036594692248164, 0.00022304668872061623, 0.0002539508311412804, 0.00033001078329677927, 0.0005046333238317957, 0.0002472649033615957, 0.00045928622355630194, 0.00022344069632032276, 0.00026002821110843074, 0.00015903297622683538, 0.00040398000405654787, 0.0002816041986039678, 0.00024018690979448482, 0.00029882328487923265, 0.00034577957207554183, 0.00028206157859465874, 0.00017730721445880266, 0.00011439717115794378, 0.00023043857095496602, 0.00013369225500067483, 0.00022097409422235014, 0.00013705560435689807, 0.000107482878806249, 0.0002181373778419168, 9.028549420162597e-05, 8.780925534434591e-05, 0.00015708037552040194, 0.00013996531600239778, 0.00015848159901898015, 9.64823931015096e-05, 0.00021168788567095436, 0.000136050276398834, 0.00010472044780222292, 0.0002206477002030788, 0.00013934975601720095, 0.00010074895917807085, 9.32180454911494e-05, 0.0001472607405713623, 0.00011569483834266491, 7.036176643037457e-05, 0.0001351957850342979, 0.000159376835863944, 0.00010674569268919185, 0.0001185344251126515, 8.718404802724013e-05, 0.00011900450702660869, 0.00012125460894170754, 0.00011230571605721275, 0.00010454212349596384, 0.0001487989837777661, 6.830416546227417e-05, 6.393169394098347e-05, 0.00020005663507281302, 0.00010431355027100938, 0.00017086659031600928, 7.99108349451789e-05, 0.00011386882648142751, 9.478551902827757e-05, 0.00010300004370941282, 8.344318782375742e-05, 0.0001274786575664539, 8.525987476745569e-05, 7.452874058836112e-05, 0.00011550953129777401]\n",
        "train_acc_list_cosine = [47.73742721016411, 84.54632080465855, 88.4425622022234, 89.96717840127052, 90.86712546320804, 91.64849126521969, 92.33245103229221, 92.79195341450503, 93.21757543673901, 93.32980412916888, 93.78295394388566, 94.04129168872419, 94.3928004235045, 94.53679195341451, 94.81206987824245, 95.05134992059291, 95.20169401799895, 95.48755955532027, 95.65484383271573, 95.92165166754897, 95.9640021175225, 96.39385918475384, 96.32186341979883, 96.55479089465325, 96.74748544203283, 96.74748544203283, 97.07146638433034, 97.24722075172049, 97.19640021175225, 97.32980412916888, 97.39332980412917, 97.62413975648491, 97.67496029645315, 97.79777660137638, 97.90365272631021, 97.96717840127052, 98.07940709370037, 98.21492853361568, 98.19163578613023, 98.32715722604553, 98.32292218104817, 98.55584965590259, 98.57702488088935, 98.43938591847538, 98.58973001588141, 98.68713605082054, 98.66807834833246, 98.61302276336686, 98.80359978824775, 98.78454208575967, 98.83748014822658, 98.80783483324511, 98.86077289571202, 98.89253573319216, 98.9433562731604, 99.02805717310747, 99.05982001058761, 99.13816834303864, 99.14452091053468, 99.0979354155638, 99.13393329804128, 99.1148755955532, 99.10217046056114, 99.45579671784013, 99.15510852302806, 99.22710428798305, 99.22498676548439, 99.32451032292218, 99.39650608787719, 99.32662784542086, 99.27368978295394, 99.3774483853891, 99.41132874536792, 99.29698253043938, 99.45367919534145, 99.3668607728957, 99.47061937533087, 99.43885653785071, 99.43250397035469, 99.51932239280042, 99.5404976177872, 99.33509793541556, 99.3774483853891, 99.5299100052938, 99.54473266278454, 99.5129698253044, 99.57014293276866, 99.57014293276866, 99.6294335627316, 99.48967707781895, 99.55320275277924, 99.5659078877713, 99.53202752779248, 99.58073054526204, 99.73742721016411, 99.53838009528852, 99.61461090524087, 99.6294335627316, 99.63366860772896, 99.7289571201694, 99.63155108523029, 99.64213869772367, 99.61884595023822, 99.71201694017999, 99.69084171519323, 99.74589730015882, 99.63790365272631, 99.72260455267337, 99.69719428268925, 99.72260455267337, 99.66754896770779, 99.70354685018528, 99.73954473266278, 99.72683959767072, 99.67601905770249, 99.73319216516676, 99.71625198517734, 99.80518793012176, 99.7289571201694, 99.77130757014294, 99.6929592376919, 99.7924827951297, 99.77766013763896, 99.784012705135, 99.80730545262044, 99.78613022763366, 99.76071995764956, 99.79036527263102, 99.83906829010058, 99.76707252514558, 99.7924827951297, 99.86659608258337, 99.84118581259926, 99.8284806776072, 99.79883536262572, 99.77977766013764, 99.79671784012704, 99.79671784012704, 99.83059820010588, 99.84965590259397, 99.8369507676019, 99.86871360508205, 99.89412387506617, 99.87083112758073, 99.85177342509265, 99.85177342509265, 99.81365802011646, 99.85389094759132, 99.87294865007941, 99.84118581259926, 99.87294865007941, 99.89200635256749, 99.88353626257279, 99.87294865007941, 99.90682901005823, 99.87506617257809, 99.87506617257809, 99.89624139756485, 99.90047644256221, 99.85812599258867, 99.88988883006881, 99.91953414505029, 99.90894653255691, 99.89412387506617, 99.90047644256221, 99.88988883006881, 99.92165166754897, 99.88777130757015, 99.90047644256221, 99.92588671254632, 99.93223928004235, 99.91953414505029, 99.88565378507147, 99.91953414505029, 99.95764955002647, 99.94917946003176, 99.95341450502912, 99.90471148755955, 99.91529910005293, 99.94070937003706, 99.95764955002647, 99.95764955002647, 99.93435680254103, 99.94706193753309, 99.9724722075172, 99.96611964002118, 99.9364743250397, 99.94282689253573, 99.91953414505029, 99.94706193753309, 99.94917946003176, 99.96188459502382, 99.95341450502912, 99.94917946003176, 99.95129698253044, 99.96823716251986, 99.97458973001588, 99.9364743250397, 99.9555320275278, 99.95129698253044, 99.9640021175225, 99.97458973001588, 99.97458973001588, 99.9640021175225, 99.97882477501324, 99.94706193753309, 99.97458973001588, 99.97882477501324, 99.9724722075172, 99.98094229751192, 99.98305982001058, 99.96188459502382, 99.98305982001058, 99.97035468501853, 99.98517734250926, 99.97458973001588, 99.95976707252514, 99.9640021175225, 99.9640021175225, 99.97670725251456, 99.98729486500794, 99.97670725251456, 99.98305982001058, 99.97670725251456, 99.97882477501324, 99.97882477501324, 99.97670725251456, 99.98305982001058, 99.98729486500794, 99.98517734250926, 99.98941238750662, 99.98941238750662, 99.98941238750662, 99.98729486500794, 99.98941238750662, 99.9915299100053, 99.98094229751192, 99.9915299100053, 99.98517734250926, 99.98094229751192, 99.98729486500794, 99.98941238750662, 99.98094229751192, 99.98941238750662, 99.98305982001058, 99.99364743250398, 99.98517734250926, 99.98729486500794, 99.98941238750662, 99.9915299100053, 99.98094229751192, 99.98305982001058, 99.9915299100053, 99.99576495500264, 99.98517734250926, 99.9915299100053, 99.98941238750662, 99.99576495500264, 99.99576495500264, 99.98941238750662, 99.99364743250398, 100.0, 99.9915299100053, 99.9915299100053, 99.98941238750662, 99.99576495500264, 99.98729486500794, 99.99576495500264, 99.99364743250398, 99.9915299100053, 99.99364743250398, 99.99364743250398, 99.99576495500264, 99.98941238750662, 99.9915299100053, 99.99788247750132, 99.98941238750662, 99.9915299100053, 99.99788247750132, 99.99576495500264, 99.99576495500264, 99.99364743250398, 99.9915299100053, 99.99576495500264, 99.99364743250398, 99.99364743250398, 99.99788247750132, 100.0, 99.98941238750662, 99.99576495500264, 99.99364743250398, 99.99576495500264, 99.9915299100053, 99.99576495500264, 99.99364743250398, 99.99576495500264, 99.9915299100053, 99.99788247750132, 99.99576495500264, 99.99576495500264]\n",
        "test_loss_list_cosine = [0.8128060245630788, 0.43769084055926283, 0.3670157931160693, 0.3468363919094497, 0.320717461845454, 0.29874755628407, 0.2880513122414841, 0.27089574385215254, 0.2712526289636598, 0.25813622246770296, 0.27607545629143715, 0.2508813175281473, 0.23157318663217275, 0.23963026514313385, 0.23259784851004095, 0.21799716919514478, 0.2553474395992417, 0.23297278822271847, 0.23411084861293727, 0.24059130224015782, 0.22228851220479198, 0.23491562519441633, 0.23474602972832964, 0.24284341398115253, 0.2531394691699568, 0.2231898330809439, 0.23778781842659502, 0.2260036576612323, 0.2408501879476449, 0.2531653588601187, 0.2437402953521586, 0.25621635543511195, 0.25155149374668506, 0.25073284860335143, 0.24820336959708264, 0.2527083886933385, 0.265060295743466, 0.2638632501977697, 0.27953584988911945, 0.28330178079469237, 0.2898024614425559, 0.285278165183377, 0.28805743263322203, 0.29073836449898927, 0.3027702011898452, 0.3096775676069014, 0.30173745603921515, 0.3021223259468873, 0.30385272365574745, 0.3038429137702812, 0.31642839670473455, 0.31861640270068947, 0.3238279426963452, 0.3290770745598802, 0.3349987664123011, 0.3213929162525079, 0.33029763011590524, 0.34525904689422426, 0.3447993094028503, 0.330581444653445, 0.321795464146371, 0.3393226447423883, 0.33919910747813536, 0.36219359443102983, 0.34984895151437206, 0.34508059543155717, 0.3312660858683361, 0.3478064838449891, 0.37045160054649207, 0.36637233636871563, 0.3615635437069132, 0.34741060503338483, 0.36895488022698786, 0.3510999241478595, 0.3899413192069487, 0.3609655104285362, 0.3731252212287383, 0.38873775874940203, 0.3712333329387155, 0.37048297439354894, 0.39439879441816433, 0.373478519492874, 0.36712522848564033, 0.3914665420420979, 0.38469330903471394, 0.3913668092230664, 0.38145966287337096, 0.37784753624788103, 0.38290000464036766, 0.3881754014622785, 0.39466715339279057, 0.38420211229765533, 0.3928751589150588, 0.3871026979576723, 0.39617241206852827, 0.42028651262323063, 0.38088225707521334, 0.3810242618087168, 0.38620046643978534, 0.41002833922667536, 0.40598774530157883, 0.4145270490324965, 0.4107616166036357, 0.39369509952124576, 0.3983155651949346, 0.4124191829971239, 0.40144294117778245, 0.4075972944425017, 0.41423957538791, 0.4235737093589634, 0.4338786544680011, 0.41868019625818464, 0.4211389536400983, 0.42915419865783083, 0.43854167406428973, 0.44444839669135855, 0.4239410953870153, 0.4140438513154639, 0.4214930013826519, 0.4469692508014394, 0.42807763744620425, 0.4140391830473627, 0.40559402319109616, 0.4296429092794949, 0.447968863443855, 0.4537148197985017, 0.4160180985562357, 0.42932759126757875, 0.4655364652474721, 0.44017627959450084, 0.4476152099646153, 0.45807484502666723, 0.4661675389518267, 0.46650424552624864, 0.4483312015951264, 0.43905310305839806, 0.4307253006571794, 0.4221340075886224, 0.43024291792957514, 0.4534414633743319, 0.439005637879246, 0.455770535284982, 0.4492703357884916, 0.46906485925337266, 0.4463952654680493, 0.4687405661497192, 0.4440763243108842, 0.4453616064936653, 0.466629167502819, 0.4585120371433304, 0.45512742501017, 0.4547710154278606, 0.46565503765847166, 0.47993873200361054, 0.46249571734783695, 0.47604353346076667, 0.4764781709778689, 0.4798887720623729, 0.47362343249294686, 0.477798045156341, 0.475341152007162, 0.46233028874677773, 0.47649588846765895, 0.4905768512090778, 0.49597619045023605, 0.4826731762720966, 0.46256263993963526, 0.4635071344308409, 0.4836937281412675, 0.4737923650753538, 0.47685492698641896, 0.49306667890107514, 0.5079087016615542, 0.47638218596294596, 0.47670910018496215, 0.5044073596980203, 0.5164688170211864, 0.5278736670528922, 0.4981932266542286, 0.4995226736783105, 0.5136372742845732, 0.47614785384697217, 0.5164846121424845, 0.4969261799825985, 0.504426286427998, 0.5065735648545966, 0.5321676667674682, 0.5085760319283615, 0.5058678923307133, 0.505933805387102, 0.5226816843154237, 0.5136628734705714, 0.49541927424862103, 0.5014571096729853, 0.5036171923942782, 0.4995590589264883, 0.5177521720453275, 0.5068038706028578, 0.5099713094623796, 0.4981300873846254, 0.5104206896560523, 0.5262805088377539, 0.5344062875499767, 0.5306157725908812, 0.5357992586351055, 0.5134817360370767, 0.5160781835849562, 0.5250964877469575, 0.5374465012448091, 0.5358220952946473, 0.5378625886514783, 0.5421092164064711, 0.5452210189199403, 0.5411839835076392, 0.5549019162050065, 0.5455894065546054, 0.5472353847934773, 0.5522625445497825, 0.5455132025983367, 0.538114388518985, 0.5509232937326363, 0.536594347077577, 0.5530879087015694, 0.5591187921909652, 0.5578411847065368, 0.5505201403601163, 0.564152417302716, 0.5473365803345052, 0.5458330702511411, 0.5650019636080947, 0.5671965913600562, 0.5513512968934853, 0.5562908629117552, 0.5620296227371356, 0.5617975970529312, 0.5581122653555426, 0.5675084015614736, 0.5657756982859699, 0.5526409040846606, 0.5721038452220634, 0.5690990948042942, 0.5608643038853474, 0.5609114988775556, 0.560604001846692, 0.5697500870121397, 0.5728723204822517, 0.5595448732640886, 0.5570606883866632, 0.5764513230818671, 0.556780993596048, 0.5647389884220948, 0.5543315944429624, 0.5563175826808255, 0.5460904273889301, 0.5574481183030716, 0.552725079560689, 0.5607348622220075, 0.5605667959076955, 0.5626120614498515, 0.5649992083380507, 0.5637176143309083, 0.5681553194373526, 0.5555440176634446, 0.5632415316604059, 0.5714907293874478, 0.5667051952526284, 0.5770144956528812, 0.5655391789224072, 0.57731314366444, 0.5691993516018413, 0.5740196594384079, 0.5696190810367446, 0.5625503671834332, 0.5508576962696251, 0.5676857517648708, 0.575283505166333, 0.5721206341997958, 0.5749247564988977, 0.5699750101832929, 0.580029735533411, 0.5632244463092374, 0.5753009883919731, 0.5712149130710054, 0.5708004219147066, 0.5698616763567734, 0.5771289975888495, 0.577879674463332, 0.5692181185472245, 0.591953951887102, 0.5678756476480368, 0.5756764801849118, 0.578166978474816, 0.5680953554498652, 0.5808629620039616, 0.5650976837804431, 0.5722748214018294, 0.5711645722097042, 0.5751072838172024, 0.5794820262486681, 0.5856684064892047]\n",
        "test_acc_list_cosine = [75.21511985248924, 86.48970497848802, 88.72157344806392, 89.39766441303011, 90.33113091579594, 91.21850030731407, 91.4259373079287, 92.23647818070067, 92.30562384757222, 92.6859250153657, 91.92532267977873, 92.7819606637984, 93.44652735095268, 93.21988322065151, 93.53872157344806, 93.85371850030731, 92.85494775660726, 93.56561155500921, 93.33512599877075, 93.48494161032575, 93.96896127842655, 93.75768285187462, 93.72311001843885, 93.5579287031346, 93.30439459127228, 94.19560540872772, 93.9459127228027, 94.2340196681008, 93.98048555623848, 93.700061462815, 94.24554394591273, 93.68085433312845, 93.82298709280884, 94.05731407498463, 94.24938537185002, 94.29164105716042, 93.83835279655808, 94.26090964966195, 94.03042409342348, 93.97280270436386, 94.00353411186232, 94.06883835279656, 94.13414259373079, 94.00737553779963, 94.12261831591887, 93.93822987092808, 94.16871542716656, 93.9958512599877, 93.96127842655194, 94.22249539028887, 93.93438844499079, 93.91518131530424, 94.0918869084204, 94.01121696373694, 94.15719114935465, 94.29164105716042, 94.31853103872157, 94.01889981561156, 94.15719114935465, 93.88060848186846, 94.54133374308543, 94.34542102028273, 94.14950829748003, 94.09572833435772, 94.36078672403197, 94.21481253841426, 94.3262138905962, 94.27627535341118, 93.92286416717886, 94.05731407498463, 94.09572833435772, 94.41456668715428, 94.24554394591273, 94.31853103872157, 94.24170251997542, 94.06115550092194, 94.0419483712354, 93.95359557467732, 94.30316533497235, 94.42224953902888, 94.1917639827904, 94.02274124154886, 94.43761524277812, 94.12645974185618, 94.20712968653964, 94.40304240934235, 94.35694529809466, 94.29164105716042, 94.48371235402581, 94.13030116779349, 94.31853103872157, 94.3338967424708, 94.41840811309157, 94.66041794714198, 94.48755377996312, 94.09956976029503, 94.26090964966195, 94.34157959434542, 94.38767670559312, 94.4798709280885, 94.38767670559312, 94.36078672403197, 94.16871542716656, 94.64505224339274, 94.36846957590657, 94.48371235402581, 94.28011677934849, 94.46450522433928, 94.28779963122311, 94.36078672403197, 94.35694529809466, 94.55285802089736, 94.35310387215735, 94.48755377996312, 94.34926244622004, 94.47602950215119, 94.64889366933005, 94.46450522433928, 94.44145666871543, 94.54133374308543, 94.52980946527352, 94.54901659496005, 94.69883220651506, 94.58743085433314, 94.16871542716656, 94.41456668715428, 94.56438229870928, 94.5759065765212, 94.21097111247695, 94.54517516902274, 94.44529809465274, 94.46834665027659, 94.27627535341118, 94.34157959434542, 94.71035648432698, 94.49139520590043, 94.60663798401967, 94.75645359557468, 94.74108789182544, 94.66041794714198, 94.59895513214505, 94.66041794714198, 94.57974800245852, 94.57974800245852, 94.71035648432698, 94.54901659496005, 94.7679778733866, 94.7679778733866, 94.56438229870928, 94.67194222495391, 94.66425937307929, 94.74492931776275, 94.79486785494775, 94.44529809465274, 94.73724646588813, 94.43761524277812, 94.72956361401353, 94.72572218807622, 94.74877074370006, 94.66041794714198, 94.4798709280885, 94.80639213275968, 94.61432083589429, 94.50676090964966, 94.70651505838967, 94.70651505838967, 94.59895513214505, 94.78334357713584, 94.76413644744929, 94.79870928088506, 94.69883220651506, 94.59511370620774, 94.42609096496619, 94.87169637369392, 94.86785494775661, 94.74492931776275, 94.74108789182544, 94.29932390903504, 94.86785494775661, 94.84864781807006, 94.68346650276582, 94.94852489244008, 94.69499078057775, 94.82175783650891, 94.86017209588199, 94.89090350338046, 94.6258451137062, 94.82559926244622, 94.83328211432084, 94.90242778119237, 94.88322065150584, 94.89090350338046, 94.96004917025199, 94.87553779963122, 94.9139520590043, 94.91779348494161, 94.99462200368777, 94.94852489244008, 94.85248924400737, 94.83712354025815, 94.90626920712968, 94.76029502151198, 94.78334357713584, 94.99078057775046, 94.88706207744315, 95.02151198524892, 94.97925629993854, 94.97541487400123, 94.87553779963122, 94.97541487400123, 94.88706207744315, 94.76413644744929, 94.84096496619546, 94.9638905961893, 94.96004917025199, 95.01382913337432, 94.99846342962508, 94.91011063306699, 94.92163491087892, 94.93315918869084, 94.9139520590043, 95.12907191149354, 95.03687768899816, 94.8640135218193, 94.82559926244622, 94.98693915181315, 94.90626920712968, 94.95236631837739, 94.98309772587585, 94.92931776275353, 94.74877074370006, 95.11754763368162, 95.02151198524892, 95.15596189305471, 94.98693915181315, 94.89858635525508, 94.99462200368777, 94.98693915181315, 95.07145052243392, 94.9638905961893, 94.97925629993854, 94.9638905961893, 94.98309772587585, 94.99462200368777, 94.95236631837739, 95.059926244622, 95.01767055931161, 95.11754763368162, 95.02919483712354, 95.13291333743085, 95.09834050399509, 95.109864781807, 95.159803318992, 95.16748617086662, 94.98693915181315, 95.22126613398893, 95.06376767055932, 95.12138905961893, 95.1521204671174, 95.04071911493547, 95.16364474492931, 95.17132759680393, 95.09834050399509, 95.1521204671174, 95.08681622618316, 95.059926244622, 95.20974185617702, 95.059926244622, 95.02151198524892, 95.14443761524278, 95.109864781807, 95.12907191149354, 95.17901044867855, 95.20974185617702, 95.17132759680393, 95.17132759680393, 95.19437615242778, 95.02151198524892, 95.17132759680393, 95.12907191149354, 95.19437615242778, 95.19821757836509, 95.12907191149354, 95.09449907805778, 95.109864781807, 95.24431468961278, 95.12907191149354, 95.16748617086662, 95.01382913337432, 95.22894898586355, 95.06760909649662, 95.11370620774431, 95.21742470805162, 95.159803318992, 95.22894898586355, 95.14059618930547, 95.109864781807, 95.14827904118009, 95.13675476336816, 95.09449907805778]\n",
        "train_loss_list_step = [1.5044204033810271, 0.49208608810817644, 0.37830603914209177, 0.33356322168415475, 0.30051283241529775, 0.2731129997548695, 0.25931339448464275, 0.24751879757416603, 0.23341218238762076, 0.22551919120114025, 0.21245744068246225, 0.20461816869696303, 0.19351729513385754, 0.18888680500591673, 0.1790498158513369, 0.17313903582794718, 0.16563977062863708, 0.15938490460599017, 0.1541340289319434, 0.14407484958246147, 0.13983443950815252, 0.13245242257630277, 0.12638695052443805, 0.12125968426304458, 0.11788717211605249, 0.11147532753424269, 0.10493720971996869, 0.10114793143257862, 0.0986312177905505, 0.09325535373060603, 0.06156983597173802, 0.050082290781091464, 0.04566037622696864, 0.042090283896077454, 0.037587769078418896, 0.03525592588446025, 0.034040165755286976, 0.031216950466235478, 0.030938032096463855, 0.029107897011765093, 0.02691679777953582, 0.024817724369887797, 0.02327188092970408, 0.022008026162917072, 0.021083637177603455, 0.020199724932410693, 0.020445302296316296, 0.01811823503557229, 0.016885934063002497, 0.016305384605991728, 0.015734232263797315, 0.01582022506202922, 0.01498541162761943, 0.013855328222454038, 0.012501976389670114, 0.011526370763357194, 0.012135067156394237, 0.011748958516110058, 0.01063293406628148, 0.010577014861956521, 0.009060478683824204, 0.00822005160518879, 0.007903504668233972, 0.0073660259777244665, 0.007953700653634155, 0.006759841606805863, 0.0069558188274406844, 0.007026840372753726, 0.006894602971896024, 0.0063807274687209935, 0.006584282455781946, 0.006663436767551165, 0.006154599042466102, 0.006103539849245962, 0.006048463034234704, 0.00614482418276391, 0.006322457585395453, 0.006028014029933677, 0.006245754332408476, 0.00581059281254909, 0.005709030747089944, 0.005923201890116833, 0.00537758718227695, 0.005807922808844647, 0.005243748521360034, 0.004974289045318631, 0.005323306708443634, 0.005775554073696229, 0.005093053230784864, 0.005536427160324011, 0.0052108629591284105, 0.004749226128792285, 0.005127630246161776, 0.005065120122888249, 0.004685268356975264, 0.005243820296808683, 0.0049121043785080205, 0.005093204319377071, 0.005370734221114888, 0.005008859195898547, 0.005249464887712999, 0.004638611593474268, 0.004728934781005616, 0.005035177127017733, 0.004871168427265863, 0.005197190184529169, 0.004969428170602996, 0.004836069228517858, 0.005308753678475464, 0.004877718586694237, 0.0046394178739258, 0.004715173941094246, 0.005071522070231865, 0.005326596872854554, 0.004782932753934018, 0.00452435254143787, 0.005128751716483132, 0.004405792564614916, 0.005072576416472445, 0.004683375086275378, 0.004745950062742184, 0.005199869099014116, 0.004980171865863766, 0.004840858398257126, 0.0046297258561474545, 0.004515150684752434, 0.005211585615480793, 0.0046955638231178315, 0.004485690257187711, 0.004824536544108968, 0.004908301922912914, 0.005288558260384356, 0.004324457162535601, 0.004497020012851448, 0.004832310812420976, 0.004629586771150122, 0.00469785524222389, 0.0051049072852895816, 0.004760202388700317, 0.00467975922823293, 0.004756802660750659, 0.004577458516745399, 0.0046352100029977686, 0.0045855785586362005, 0.0045302773612948275, 0.00527981271794404, 0.004842230399004512, 0.005128007588957292, 0.0047336897144657575, 0.00452429031405685, 0.004871112167589785, 0.004707888855899686, 0.004987144301444176, 0.004576647475924656, 0.0050210763291275344, 0.004869554478370365, 0.005145938421950075, 0.0049324849194394005, 0.005007630027365255, 0.004694439974090723, 0.005161294266647788, 0.005086089323139447, 0.0045723088694465674, 0.0049315163145030455, 0.005043683056833177, 0.004782498019489051, 0.005208640262013842, 0.004906343821498318, 0.004504839811582297, 0.004765471186647242, 0.004636792729359163, 0.0048650642755604795, 0.00440384292277286, 0.005331020985203442, 0.004309070902481564, 0.005132160800841049, 0.0048202084057818, 0.0042395592619208135, 0.004579358937877841, 0.004854094481356265, 0.004861267090400903, 0.004641431140822869, 0.004752275634260134, 0.005280723348089562, 0.0049866622946490744, 0.004713922302353166, 0.0049851976106240516, 0.0049522482596579456, 0.0047414097434517405, 0.005198805679831853, 0.005367297160049249, 0.004801289511232152, 0.004936103088396973, 0.004513620775553264, 0.005461517685133368, 0.005038318795404084, 0.004621399451298942, 0.004861979541443869, 0.004728190580562456, 0.00424972596940481, 0.004389661508080736, 0.004675524760544651, 0.004678315578823907, 0.005468495250993464, 0.004904287783485815, 0.004628283689890138, 0.004957924133196958, 0.004207717516179686, 0.004497190112673026, 0.0054004669855449725, 0.004738672002041044, 0.004832978636525801, 0.004593934473861766, 0.00428500973297249, 0.004800252157330026, 0.004579809123577946, 0.0047766751335504105, 0.004927973183699149, 0.003962443585616099, 0.005313499298157754, 0.005448946752569696, 0.004626204171514537, 0.00486521731347835, 0.00464669697920995, 0.00473364783223967, 0.0051715146905747635, 0.004559630797677459, 0.004807513805560901, 0.004414798407140251, 0.0055058369074104515, 0.004735360263094689, 0.00523965430700253, 0.004729446927025576, 0.0046202187999946675, 0.004473174700221971, 0.005083680975845281, 0.00473606128646736, 0.004358955291625135, 0.004462744230992882, 0.00445035025452959, 0.004661599569843557, 0.004881452044679059, 0.004753526896741532, 0.004433994836329803, 0.004789544894031266, 0.004920090539939323, 0.005374149948520697, 0.004787621462456112, 0.004773729829575745, 0.0044936602159881545, 0.004565336937149469, 0.004793149001421886, 0.004954056282359589, 0.00447947610881707, 0.004911790105014495, 0.004962967739427566, 0.004856595992693377, 0.0049429370942105965, 0.004495315723300048, 0.005116432885005868, 0.004663471084030195, 0.005105866048162296, 0.0048063792890729005, 0.004996042520567828, 0.0048235019935209174, 0.0052012274302681025, 0.004979272351725164, 0.004979745390991236, 0.005170751338667516, 0.004858217863982788, 0.004663840651548111, 0.005053441917670759, 0.004584236903877065, 0.004780888221675258, 0.004612757356412744, 0.004703525326821731, 0.005128545914503725, 0.005097627184756964, 0.004888794860495936, 0.004546376856262884, 0.0049637655482588335, 0.004738951851973477, 0.005256142429007358, 0.004507868475276601, 0.004676521270572799, 0.0042853716939203765, 0.004519075942008067, 0.004505532296253924, 0.004830542344546013, 0.004257976696643534, 0.004718432969060035, 0.004490681710655531, 0.004793198622190861, 0.004690485046958793, 0.004381172440937899, 0.004597538895479108, 0.0050219146489527905, 0.004799934776746264, 0.004984567811975109, 0.004093347564194513]\n",
        "train_acc_list_step = [47.394388565378506, 84.38962413975648, 88.32609846479619, 89.65802011646373, 90.93912122816305, 91.76071995764956, 92.30492323980943, 92.69878242456326, 93.1371095817893, 93.283218634198, 93.84859714134463, 93.97988353626258, 94.4203282159873, 94.56643726839597, 94.83324510322922, 95.06405505558496, 95.14452091053468, 95.38168343038645, 95.61461090524087, 95.86871360508205, 95.94282689253573, 96.23928004235044, 96.33456855479089, 96.51667548967708, 96.58231868713605, 96.79618845950239, 96.97829539438857, 97.11805187930122, 97.2006352567496, 97.23451561672843, 98.32927474854421, 98.71254632080466, 98.81630492323981, 98.94970884065643, 99.04499735309687, 99.05558496559026, 99.12969825304394, 99.22075172048703, 99.16993118051879, 99.24616199047115, 99.32027527792482, 99.33721545791424, 99.38168343038645, 99.40285865537321, 99.45156167284277, 99.46426680783483, 99.43885653785071, 99.49814716781366, 99.51932239280042, 99.55320275277924, 99.58073054526204, 99.56379036527264, 99.56379036527264, 99.6209634727369, 99.65907887771307, 99.68237162519851, 99.61884595023822, 99.62731604023293, 99.69507676019057, 99.72260455267337, 99.73742721016411, 99.7564849126522, 99.76919004764426, 99.79883536262572, 99.7649550026469, 99.83906829010058, 99.80307040762308, 99.81154049761778, 99.81154049761778, 99.83271572260455, 99.8009528851244, 99.82424563260984, 99.8369507676019, 99.82424563260984, 99.83906829010058, 99.83483324510323, 99.8369507676019, 99.84965590259397, 99.82636315510852, 99.83059820010588, 99.83906829010058, 99.84118581259926, 99.86236103758603, 99.85600847008999, 99.86236103758603, 99.87083112758073, 99.8369507676019, 99.8284806776072, 99.84753838009529, 99.85600847008999, 99.85389094759132, 99.87083112758073, 99.85812599258867, 99.86024351508735, 99.88353626257279, 99.85600847008999, 99.87506617257809, 99.85600847008999, 99.86236103758603, 99.84753838009529, 99.85600847008999, 99.87930121757543, 99.86236103758603, 99.8644785600847, 99.85177342509265, 99.86024351508735, 99.85389094759132, 99.85812599258867, 99.84330333509793, 99.86024351508735, 99.87718369507677, 99.86871360508205, 99.85812599258867, 99.84753838009529, 99.86659608258337, 99.87083112758073, 99.85389094759132, 99.88988883006881, 99.86024351508735, 99.87718369507677, 99.86659608258337, 99.84118581259926, 99.87083112758073, 99.85812599258867, 99.88777130757015, 99.88988883006881, 99.84542085759661, 99.87294865007941, 99.87083112758073, 99.85177342509265, 99.86024351508735, 99.86236103758603, 99.87506617257809, 99.8644785600847, 99.86024351508735, 99.86659608258337, 99.86871360508205, 99.85600847008999, 99.87083112758073, 99.87294865007941, 99.87294865007941, 99.88353626257279, 99.86236103758603, 99.87930121757543, 99.87930121757543, 99.86024351508735, 99.86024351508735, 99.83483324510323, 99.8644785600847, 99.89200635256749, 99.87083112758073, 99.84965590259397, 99.84965590259397, 99.88141874007411, 99.84753838009529, 99.87506617257809, 99.85389094759132, 99.85812599258867, 99.86236103758603, 99.85600847008999, 99.85600847008999, 99.84118581259926, 99.87930121757543, 99.86024351508735, 99.85812599258867, 99.86871360508205, 99.85389094759132, 99.86236103758603, 99.87718369507677, 99.85812599258867, 99.87083112758073, 99.85600847008999, 99.87506617257809, 99.87506617257809, 99.88777130757015, 99.85812599258867, 99.85600847008999, 99.88777130757015, 99.86659608258337, 99.87718369507677, 99.86659608258337, 99.86659608258337, 99.88141874007411, 99.85177342509265, 99.85600847008999, 99.87718369507677, 99.86236103758603, 99.87930121757543, 99.87506617257809, 99.85177342509265, 99.84118581259926, 99.86236103758603, 99.85812599258867, 99.87506617257809, 99.82636315510852, 99.85600847008999, 99.87718369507677, 99.88353626257279, 99.87294865007941, 99.89624139756485, 99.88353626257279, 99.87718369507677, 99.87083112758073, 99.84542085759661, 99.8644785600847, 99.88141874007411, 99.85600847008999, 99.89412387506617, 99.88141874007411, 99.85177342509265, 99.85389094759132, 99.87930121757543, 99.88565378507147, 99.88353626257279, 99.86871360508205, 99.86871360508205, 99.8644785600847, 99.86871360508205, 99.89624139756485, 99.84330333509793, 99.84542085759661, 99.88353626257279, 99.86236103758603, 99.87930121757543, 99.87506617257809, 99.87083112758073, 99.87294865007941, 99.87294865007941, 99.86871360508205, 99.84330333509793, 99.86659608258337, 99.84542085759661, 99.87506617257809, 99.8644785600847, 99.89200635256749, 99.84542085759661, 99.86871360508205, 99.87506617257809, 99.88141874007411, 99.87083112758073, 99.87083112758073, 99.86659608258337, 99.86659608258337, 99.86871360508205, 99.88565378507147, 99.87083112758073, 99.84753838009529, 99.88777130757015, 99.87718369507677, 99.88353626257279, 99.87083112758073, 99.86236103758603, 99.87718369507677, 99.88777130757015, 99.84965590259397, 99.85600847008999, 99.86236103758603, 99.85812599258867, 99.87718369507677, 99.8644785600847, 99.86024351508735, 99.87506617257809, 99.8644785600847, 99.8644785600847, 99.87930121757543, 99.85177342509265, 99.86024351508735, 99.85177342509265, 99.84118581259926, 99.86871360508205, 99.87718369507677, 99.86024351508735, 99.88988883006881, 99.8644785600847, 99.88141874007411, 99.87506617257809, 99.85177342509265, 99.86871360508205, 99.87506617257809, 99.87506617257809, 99.87930121757543, 99.86871360508205, 99.85812599258867, 99.88988883006881, 99.88353626257279, 99.89412387506617, 99.87930121757543, 99.86659608258337, 99.86659608258337, 99.88988883006881, 99.87083112758073, 99.87506617257809, 99.87083112758073, 99.87718369507677, 99.87930121757543, 99.87083112758073, 99.84330333509793, 99.87718369507677, 99.87294865007941, 99.88565378507147]\n",
        "test_loss_list_step = [0.7521707053278007, 0.41922322909037274, 0.3967930445191907, 0.35680990534670215, 0.3250766685049908, 0.30643289908766747, 0.27803952721696273, 0.26461896110399097, 0.26403681638047977, 0.2668783114309989, 0.2609864034708224, 0.25131638298797254, 0.24851202544774495, 0.24535505795011334, 0.23515790532909187, 0.23713360856488055, 0.2406249167215006, 0.24633904404061682, 0.22870721204169825, 0.23081522924350759, 0.23210598513776182, 0.23380596216256713, 0.23264140811036615, 0.2492445185597913, 0.24672274596477842, 0.23322659500819795, 0.2332557491848574, 0.2325650899324055, 0.2574915187433362, 0.23942345477567584, 0.21860766188953729, 0.22490006466122234, 0.22599033247588166, 0.2330661828027052, 0.23813083913980745, 0.2410290090677639, 0.25265564182408007, 0.25388093285408675, 0.25897152009694013, 0.2653597001095905, 0.267472947769634, 0.2757880237756991, 0.28352711534164116, 0.2891207381023788, 0.2937827925471699, 0.3016136303891008, 0.29683313752506296, 0.3059604737939605, 0.31242154799766986, 0.313900034299449, 0.3219801964347853, 0.33177046875889393, 0.3355082166837711, 0.33843776536192377, 0.34364481102309974, 0.3455076480613035, 0.34563379169569586, 0.35321818167051555, 0.35429473890576, 0.35551488774317297, 0.36010582940470354, 0.35364405492631096, 0.3636273516743791, 0.36364757276012327, 0.35665451670887277, 0.3549169595261999, 0.3703063836905594, 0.36219835961146246, 0.3605355029998749, 0.3578162646861564, 0.36774474406140106, 0.3633771698702784, 0.3653042366746448, 0.3691165424956411, 0.36504066177625577, 0.3715900047225695, 0.377250660302154, 0.3705682765610297, 0.3703488810170515, 0.37351591648606985, 0.37677567382343113, 0.37160208523638694, 0.3709423184650494, 0.3721590631769276, 0.3763345616011351, 0.3800408639488559, 0.37680745827874135, 0.3771674718214747, 0.3848353975142042, 0.3798498407590623, 0.3773468048494382, 0.3819653425073507, 0.3785710880149375, 0.37396970703654614, 0.37354924148131236, 0.38423583746029466, 0.3775230319622685, 0.36990976165614875, 0.3731101816017911, 0.3812584620443921, 0.3834761079017292, 0.38286398570327196, 0.3858671422372116, 0.3799955192816389, 0.3770175949378195, 0.3851937071338077, 0.38235137271968755, 0.38205398502303106, 0.38025479872400564, 0.3843826920378442, 0.38355313671533675, 0.3811237645043316, 0.37951354816665545, 0.3756716687428564, 0.38028262046110983, 0.38163883488296585, 0.38113248269712807, 0.38293040138395396, 0.38826611508414444, 0.3821931903378344, 0.3927168071534777, 0.3814124613087259, 0.38783683029788674, 0.384523262598497, 0.3777104146395098, 0.3834854347001323, 0.37351655306331083, 0.37643494215958256, 0.38091917734081837, 0.3816218973667014, 0.38416977602915436, 0.3851156278796421, 0.3824646536297366, 0.37491797317988146, 0.3818769982979432, 0.37861419655382633, 0.3842346754938583, 0.37655236199498177, 0.37527537754024654, 0.37987153332077844, 0.3830455071018899, 0.3794875234803733, 0.38347619624041457, 0.39262797689868834, 0.37537328259763764, 0.3836479137067263, 0.3833252173236699, 0.37210422399563386, 0.387080483238998, 0.3719306717816211, 0.37743640282446994, 0.3764378684846794, 0.37271541687568616, 0.37936363588361177, 0.3804472682075392, 0.371879458409168, 0.38374935142586336, 0.3781576618488294, 0.3732976428844838, 0.3825277503210065, 0.37921793430167083, 0.38190348801550034, 0.3811836604795912, 0.3839620574760963, 0.3847266497711341, 0.3755092674263698, 0.38182204500680755, 0.3782161524446279, 0.383978412504874, 0.3785792852560168, 0.37529283202728075, 0.38586872027200814, 0.37986113718144743, 0.3781509461410928, 0.385330743640296, 0.3866525995362477, 0.3780062812672672, 0.38378071400574315, 0.38093069549102115, 0.38920295537065935, 0.38671026251041424, 0.3797818386251582, 0.3798417299438049, 0.3740828680422376, 0.38145504804218516, 0.38200287843196123, 0.3875016655645096, 0.38280217152308016, 0.38698648213974984, 0.3808485120260978, 0.38649215569317924, 0.3831135995238654, 0.3805198727321683, 0.38389426964682105, 0.38486584148132336, 0.3807648518761876, 0.3849117392856701, 0.38554816227406263, 0.38357163151251333, 0.3802410184734446, 0.3815109624947403, 0.382021243195506, 0.3853015545938237, 0.377494075719048, 0.38711871966427447, 0.3774435311857173, 0.3905071147407095, 0.3806642469603057, 0.38146250298721535, 0.3838621310104488, 0.39097126777849944, 0.38649382303450625, 0.3796885114930132, 0.38229777593183895, 0.3814927838633166, 0.37877432031410874, 0.37605211639082897, 0.37402715621625676, 0.3860787514177169, 0.3734693876285033, 0.3757060868665576, 0.3836308258823028, 0.3818031575019453, 0.3754675344144012, 0.3817046451035376, 0.3788763102984019, 0.38524902579100695, 0.3789480691410455, 0.383543092720941, 0.39350814419780294, 0.3787268260402568, 0.3791520138837251, 0.3838299135951435, 0.3793616016559741, 0.3767985271874304, 0.3790097143866268, 0.37912912847583785, 0.38894346707007466, 0.3742955804382469, 0.38342078903909116, 0.3848988146238102, 0.37459168269061577, 0.38242888645561157, 0.3820874952601598, 0.38645922241951614, 0.37736037103276626, 0.37371054134450343, 0.3787742013148233, 0.3864014833873394, 0.38398269388605566, 0.3823276355716528, 0.38692381239368345, 0.38753888685731036, 0.37855833295878827, 0.37346514290673477, 0.37298866356814314, 0.3816173101830132, 0.3767242920311058, 0.3841047031056209, 0.38750297252965327, 0.369205586767445, 0.3753658247490724, 0.38275528518373475, 0.3800859667141648, 0.3870932775020015, 0.37551862878414494, 0.3840912418421723, 0.3726249620291021, 0.37941730108486454, 0.38384662088298915, 0.37976392629720707, 0.37775822057772207, 0.38208118103006306, 0.3800909294399853, 0.37907081163104844, 0.3812881442672555, 0.3761177863743083, 0.38076628101350485, 0.37963548230518607, 0.3693312196718419, 0.37269937157557875, 0.3810739266987452, 0.37098004634254705, 0.38030854867332997, 0.3809964715083148, 0.37681868622152537, 0.3750186215574835, 0.3816620926098788, 0.3736031973351012, 0.38245252669588026, 0.3819549302798787, 0.38419706475756626, 0.3735217913966991, 0.3787470100191878, 0.37715579814040195, 0.3867923977976555, 0.3815506379960068, 0.3814866198135503, 0.3853403181933305, 0.3877864530602214]\n",
        "test_acc_list_step = [75.81054087277197, 86.9660417947142, 87.58451137062077, 89.28242163491088, 90.1160110633067, 91.05331899200984, 91.73325138291334, 92.37092808850646, 92.390135218193, 92.24031960663798, 92.37092808850646, 92.9778733866011, 93.1737861094038, 93.04701905347265, 93.41963736939152, 93.37738168408113, 93.3159188690842, 93.0278119237861, 93.7077443146896, 93.72695144437616, 93.68853718500307, 93.78073140749846, 93.73847572218807, 93.26213890596189, 93.30055316533497, 93.93438844499079, 94.06883835279656, 94.11877688998156, 93.6040258143823, 94.0419483712354, 94.87553779963122, 94.83712354025815, 94.81023355869699, 94.87169637369392, 94.91779348494161, 94.89474492931777, 94.86785494775661, 94.84096496619546, 94.71419791026429, 94.56054087277197, 94.75645359557468, 94.73724646588813, 94.64889366933005, 94.64889366933005, 94.69499078057775, 94.72956361401353, 94.6757836508912, 94.74492931776275, 94.6220036877689, 94.65273509526736, 94.54901659496005, 94.53749231714812, 94.63352796558083, 94.61816226183159, 94.61432083589429, 94.5221266133989, 94.64121081745544, 94.56054087277197, 94.74492931776275, 94.61047940995698, 94.59127228027043, 94.75645359557468, 94.75645359557468, 94.66425937307929, 94.73340503995082, 94.6757836508912, 94.53365089121081, 94.64505224339274, 94.66041794714198, 94.70267363245236, 94.6258451137062, 94.75645359557468, 94.80255070682237, 94.67962507682851, 94.58358942839583, 94.54517516902274, 94.6258451137062, 94.7180393362016, 94.66425937307929, 94.6681007990166, 94.61432083589429, 94.78718500307313, 94.76029502151198, 94.68730792870313, 94.64121081745544, 94.62968653964352, 94.69114935464044, 94.58743085433314, 94.58358942839583, 94.64889366933005, 94.69114935464044, 94.70651505838967, 94.70267363245236, 94.66041794714198, 94.69883220651506, 94.61432083589429, 94.71419791026429, 94.82559926244622, 94.69883220651506, 94.72572218807622, 94.49523663183774, 94.75261216963737, 94.61047940995698, 94.64505224339274, 94.64889366933005, 94.66041794714198, 94.70267363245236, 94.55669944683467, 94.65657652120467, 94.6681007990166, 94.77566072526122, 94.85633066994468, 94.66041794714198, 94.78334357713584, 94.79102642901044, 94.63352796558083, 94.78718500307313, 94.64505224339274, 94.56438229870928, 94.73340503995082, 94.61047940995698, 94.68730792870313, 94.70267363245236, 94.61432083589429, 94.67194222495391, 94.77181929932391, 94.73724646588813, 94.64505224339274, 94.69499078057775, 94.65273509526736, 94.69883220651506, 94.6757836508912, 94.71035648432698, 94.69883220651506, 94.67194222495391, 94.66425937307929, 94.6757836508912, 94.76029502151198, 94.75645359557468, 94.70267363245236, 94.64505224339274, 94.74108789182544, 94.54133374308543, 94.54901659496005, 94.74877074370006, 94.67962507682851, 94.67194222495391, 94.67194222495391, 94.69883220651506, 94.8140749846343, 94.64121081745544, 94.7180393362016, 94.74492931776275, 94.78334357713584, 94.65273509526736, 94.5759065765212, 94.61816226183159, 94.61047940995698, 94.76413644744929, 94.79870928088506, 94.67962507682851, 94.69114935464044, 94.72572218807622, 94.60663798401967, 94.5720651505839, 94.75261216963737, 94.80255070682237, 94.70267363245236, 94.66041794714198, 94.66425937307929, 94.69114935464044, 94.72188076213891, 94.68730792870313, 94.55285802089736, 94.75645359557468, 94.64505224339274, 94.7180393362016, 94.53749231714812, 94.72572218807622, 94.65657652120467, 94.6258451137062, 94.61432083589429, 94.72188076213891, 94.77566072526122, 94.62968653964352, 94.66041794714198, 94.54901659496005, 94.6757836508912, 94.66425937307929, 94.55669944683467, 94.7679778733866, 94.74877074370006, 94.68346650276582, 94.63352796558083, 94.5759065765212, 94.61047940995698, 94.66425937307929, 94.71419791026429, 94.59895513214505, 94.61816226183159, 94.72956361401353, 94.7180393362016, 94.67194222495391, 94.68346650276582, 94.59127228027043, 94.76029502151198, 94.69499078057775, 94.66041794714198, 94.6681007990166, 94.69499078057775, 94.64889366933005, 94.67962507682851, 94.78718500307313, 94.67962507682851, 94.77950215119853, 94.6757836508912, 94.74492931776275, 94.84096496619546, 94.73340503995082, 94.72188076213891, 94.69883220651506, 94.61047940995698, 94.5720651505839, 94.72572218807622, 94.70651505838967, 94.6681007990166, 94.6258451137062, 94.64121081745544, 94.65657652120467, 94.48371235402581, 94.6757836508912, 94.73724646588813, 94.59895513214505, 94.70651505838967, 94.76029502151198, 94.75645359557468, 94.76413644744929, 94.6220036877689, 94.72188076213891, 94.7180393362016, 94.66425937307929, 94.6681007990166, 94.62968653964352, 94.71419791026429, 94.68346650276582, 94.69883220651506, 94.69114935464044, 94.70267363245236, 94.60663798401967, 94.67194222495391, 94.55285802089736, 94.59895513214505, 94.66041794714198, 94.61432083589429, 94.59895513214505, 94.73724646588813, 94.63352796558083, 94.73724646588813, 94.77566072526122, 94.60279655808236, 94.76413644744929, 94.74108789182544, 94.65273509526736, 94.58743085433314, 94.59511370620774, 94.80255070682237, 94.61047940995698, 94.71035648432698, 94.65273509526736, 94.6681007990166, 94.75261216963737, 94.72188076213891, 94.61816226183159, 94.65657652120467, 94.79870928088506, 94.64121081745544, 94.66425937307929, 94.69883220651506, 94.64889366933005, 94.83712354025815, 94.69883220651506, 94.71419791026429, 94.77181929932391, 94.63352796558083, 94.74492931776275, 94.69883220651506, 94.61816226183159, 94.73340503995082, 94.73724646588813, 94.67194222495391, 94.72956361401353, 94.69114935464044, 94.77181929932391, 94.65657652120467, 94.68346650276582, 94.60663798401967, 94.64889366933005, 94.72188076213891, 94.70651505838967, 94.70267363245236]\n",
        "train_loss_list_linear = [1.0505396097010067, 0.41031305952285363, 0.34015994405924144, 0.3038505317194029, 0.27671615290771007, 0.2539030461090044, 0.24293845730422312, 0.2313264879753919, 0.22205986383403867, 0.21279181394635177, 0.20017173882260877, 0.19267895106014196, 0.18516845405505603, 0.18218227440546844, 0.17281363284604012, 0.16757936995264475, 0.1608044776490064, 0.1526946394343363, 0.1503949542921087, 0.14460325269861435, 0.1379334318267338, 0.13074936200347212, 0.12652073063834654, 0.12361395870888137, 0.11955785878043027, 0.11495751238915171, 0.10918548092407586, 0.10549298947964741, 0.10151599690843081, 0.09746526491886312, 0.09452159306524084, 0.09095732697353857, 0.08772212413056352, 0.08348307219462667, 0.08420775942554884, 0.07539896097104885, 0.07707164484154443, 0.07315899130855473, 0.07151561531902047, 0.06873627238071506, 0.06587666986752452, 0.060232834865876696, 0.059981180607343754, 0.06175297403930204, 0.05708458852451951, 0.05296678836066225, 0.057111382709063976, 0.053133565748266894, 0.05266129370491073, 0.050845057070684346, 0.04849550221115351, 0.0469664877891379, 0.04695639163596419, 0.04652371517443136, 0.04273645039004221, 0.042981587787635805, 0.040151388551610795, 0.03779632822411049, 0.04038962408859879, 0.04276739005321198, 0.035970243897294085, 0.0389290425395164, 0.034595519276827996, 0.034754975707721986, 0.03550548981976138, 0.034968214722219, 0.033513369056052934, 0.033747713374943145, 0.03146642715614011, 0.03088276023026253, 0.03096695340504784, 0.030255960882325888, 0.029922214430077934, 0.026458163596317426, 0.030674233368527256, 0.026803990912770092, 0.028383832357679988, 0.029045142340322395, 0.026921882543196714, 0.025758919344589033, 0.027054627095336514, 0.026907914471629563, 0.02701804478293749, 0.026367548554233428, 0.0261913293705708, 0.02126370853306549, 0.023870387598386333, 0.02343241305678558, 0.026290576351723865, 0.024439686884867598, 0.02106365902915024, 0.022621461096346544, 0.022915108798849105, 0.026723837544939667, 0.021479926218290063, 0.022599881692684752, 0.02025709556021947, 0.019150306689010323, 0.0245601841217654, 0.02191897199554167, 0.01751039288937257, 0.019330726730524307, 0.019164910448808477, 0.024538845911642237, 0.015059424295682626, 0.020382445553086728, 0.022040061389306198, 0.018767614002140155, 0.019355695793985044, 0.019107737408797596, 0.017818392037986942, 0.017417942697963446, 0.021768966118750215, 0.017869346996114355, 0.017951215525590914, 0.01640004834187341, 0.01777675135988997, 0.019044938084980482, 0.015788522874557873, 0.022552578193564623, 0.019296770346089745, 0.014811130384146363, 0.015027894850487148, 0.018469126043487265, 0.018098258266041212, 0.014053442152699729, 0.015989337644331865, 0.018125206405116463, 0.016060621967932922, 0.014774189744400855, 0.01385672290374157, 0.018077892058706895, 0.01727216446030504, 0.014268980369684513, 0.015724211079724697, 0.016640492273927766, 0.014277730624145401, 0.016971004403142326, 0.014619164786088283, 0.016830329990812937, 0.01374343925469295, 0.012524507506886185, 0.01549116191725552, 0.013598875393053052, 0.0151209402418879, 0.014543565561276227, 0.01439005304061679, 0.01173475706005888, 0.016134371224896974, 0.011917180406560032, 0.012485065904832735, 0.0135999512963786, 0.01591276152397482, 0.011587380154317354, 0.009772636422980987, 0.01344329923276031, 0.015756404174912624, 0.014378462400149205, 0.014014234783006126, 0.012014912663851542, 0.01611480384725613, 0.012844832076313767, 0.013578993814619104, 0.011672752483787808, 0.014098815763512592, 0.012935566670426325, 0.015032568787632202, 0.012224024389351091, 0.014372231721552152, 0.011108738588597336, 0.011174887407980897, 0.016397042357398143, 0.007919598232576864, 0.011932518933157513, 0.012465056306042874, 0.012968409797498517, 0.012325529049170838, 0.011099239793170491, 0.011867815633636468, 0.012848886264346762, 0.012182228031996032, 0.011562556203482023, 0.010714672763688681, 0.013554639555182596, 0.012572727286869529, 0.01000095399527329, 0.011995266647266443, 0.011573533052920085, 0.012627172824654724, 0.013339461832399402, 0.010677528724291735, 0.011543188395016422, 0.010650319310586942, 0.009674460366150607, 0.010641892690030602, 0.012230279986791725, 0.01145689182891605, 0.01050705676828164, 0.00970218700554869, 0.011088927940358716, 0.010147759896973753, 0.0144252521625949, 0.009280225094541092, 0.0071248399373703905, 0.013807516299508516, 0.01255990872180107, 0.011576532107942895, 0.008876261175843192, 0.009988300831966212, 0.00985363164915767, 0.01084158176923064, 0.011923880645647825, 0.010890684340816107, 0.011154608579179295, 0.010417346872992216, 0.00848533340091224, 0.009280508183635208, 0.012399288785308195, 0.0072954312946646145, 0.007857306191068237, 0.013135696996073112, 0.007985612870672613, 0.009203406998578557, 0.011825267453099425, 0.009498726522558978, 0.010254976841146922, 0.012558721663513307, 0.007996204799592626, 0.007471766532690587, 0.01177365344033834, 0.010632048703338191, 0.01163114502161057, 0.008471069070895856, 0.008329393120841747, 0.010546008776983772, 0.007213423388353836, 0.009344507342992069, 0.01120725078885267, 0.008723668579122892, 0.008438797396795254, 0.009862698597215867, 0.011641287900165775, 0.009182740705942006, 0.00702334287046593, 0.010164535000019891, 0.008744396335386742, 0.009129313499368456, 0.008084669785307412, 0.010208096987843342, 0.011367271971373962, 0.007117051272343865, 0.008831601460626934, 0.00922884266564445, 0.009587360389017516, 0.008383878850809764, 0.007664171105706518, 0.010088089144474219, 0.007183663350923383, 0.010528553138330638, 0.010388069153280813, 0.008873608166863086, 0.007134666277026189, 0.00842848956719368, 0.008079520347828268, 0.009280271344362845, 0.008955260842407897, 0.010437573963957794, 0.007464409461843186, 0.00868264229136562, 0.008687793158964795, 0.008216140737795655, 0.007891892086514802, 0.007512410591053153, 0.008665417990315836, 0.008976994133145053, 0.008162402152780846, 0.008810677293791461, 0.006835815719566462, 0.007850608918254282, 0.010575828147088042, 0.008934358710317107, 0.0076927311314263745, 0.007455576667292384, 0.005771527193367357, 0.008088972039366073, 0.011438270648105747, 0.005829402718917685, 0.009437662637821316, 0.006043440636937449, 0.007854135620251515, 0.009180552848274084, 0.010004485826416422, 0.007954190094466415, 0.0069836680795018575, 0.006469878575636705, 0.008808609063802835, 0.007791922163181544, 0.006387679697369947, 0.009282423594551585, 0.007415674569260415]\n",
        "train_acc_list_linear = [64.05293806246691, 87.14875595553202, 89.55002646903124, 90.75913181577555, 91.62308099523557, 92.33456855479089, 92.87030174695606, 93.13922710428798, 93.25780836421387, 93.82106934886183, 94.23822128110112, 94.36315510852303, 94.60667019587083, 94.74007411328745, 95.01111699311805, 95.09158284806776, 95.25674960296453, 95.67178401270513, 95.61461090524087, 95.74377977766014, 95.9915299100053, 96.22869242985706, 96.29645314981472, 96.45526733721546, 96.42985706723134, 96.65431445209106, 96.86818422445738, 96.87241926945474, 97.08628904182108, 97.16040232927475, 97.2641609317099, 97.3361566966649, 97.35309687665432, 97.51191106405506, 97.51191106405506, 97.76601376389624, 97.7342509264161, 97.88035997882477, 97.82953943885654, 97.96717840127052, 98.00317628374802, 98.16622551614611, 98.13658020116463, 98.11540497617787, 98.1937533086289, 98.37374272101641, 98.19798835362626, 98.44150344097406, 98.33562731604023, 98.41821069348862, 98.43303335097936, 98.4923239809423, 98.5643197458973, 98.4923239809423, 98.66807834833246, 98.64902064584436, 98.69348861831656, 98.76971942826893, 98.68925357331922, 98.64690312334568, 98.82053996823716, 98.74219163578613, 98.81842244573849, 98.86500794070938, 98.88194812069878, 98.78877713075701, 98.90100582318688, 98.90312334568554, 98.96664902064585, 98.97723663313923, 98.98358920063525, 99.0428798305982, 99.01111699311805, 99.12122816304924, 98.98570672313393, 99.10852302805718, 99.09158284806776, 99.05134992059291, 99.1233456855479, 99.1868713605082, 99.0873478030704, 99.10217046056114, 99.08523028057174, 99.13181577554262, 99.11064055055584, 99.33721545791424, 99.22710428798305, 99.17628374801482, 99.0979354155638, 99.20169401799895, 99.2694547379566, 99.2419269454738, 99.25463208046585, 99.08099523557438, 99.26733721545791, 99.2419269454738, 99.30968766543144, 99.3223928004235, 99.1868713605082, 99.25463208046585, 99.40921122286925, 99.36262572789836, 99.3499205929063, 99.16357861302276, 99.52355743779778, 99.31815775542616, 99.2779248279513, 99.37109581789306, 99.3499205929063, 99.37321334039174, 99.36050820539968, 99.42826892535733, 99.22075172048703, 99.41979883536263, 99.3943885653785, 99.46426680783483, 99.39015352038115, 99.33509793541556, 99.47697194282689, 99.28427739544733, 99.36050820539968, 99.5574377977766, 99.5044997353097, 99.41556379036527, 99.38803599788248, 99.5489677077819, 99.49179460031763, 99.37956590788777, 99.47697194282689, 99.5214399152991, 99.55108523028058, 99.42403388035999, 99.45156167284277, 99.51720487030175, 99.47273689782953, 99.42403388035999, 99.5299100052938, 99.41132874536792, 99.48755955532027, 99.46638433033351, 99.53202752779248, 99.57014293276866, 99.49179460031763, 99.53626257278984, 99.50026469031233, 99.50026469031233, 99.5299100052938, 99.61884595023822, 99.45579671784013, 99.59978824775013, 99.55108523028058, 99.55532027527792, 99.47273689782953, 99.62519851773425, 99.67813658020117, 99.57437797776602, 99.45579671784013, 99.5299100052938, 99.50873478030704, 99.61672842773955, 99.46214928533615, 99.59555320275278, 99.57014293276866, 99.6209634727369, 99.53414505029116, 99.58920063525674, 99.5044997353097, 99.57014293276866, 99.53626257278984, 99.58073054526204, 99.6209634727369, 99.46850185283219, 99.72260455267337, 99.58920063525674, 99.55955532027528, 99.56379036527264, 99.60825833774484, 99.64849126521969, 99.60402329274748, 99.5574377977766, 99.60825833774484, 99.61884595023822, 99.63790365272631, 99.56802541026998, 99.58284806776072, 99.66754896770779, 99.58708311275808, 99.62519851773425, 99.57226045526734, 99.59767072525146, 99.64213869772367, 99.60614081524616, 99.63366860772896, 99.66331392271043, 99.65907887771307, 99.58920063525674, 99.61884595023822, 99.67813658020117, 99.66119640021175, 99.62731604023293, 99.68237162519851, 99.57437797776602, 99.66543144520911, 99.80307040762308, 99.5574377977766, 99.5659078877713, 99.63366860772896, 99.69931180518793, 99.66966649020645, 99.67178401270513, 99.63578613022763, 99.59767072525146, 99.63790365272631, 99.66119640021175, 99.64425622022235, 99.72683959767072, 99.68660667019587, 99.61249338274219, 99.7734250926416, 99.7480148226575, 99.56379036527264, 99.74589730015882, 99.67813658020117, 99.62731604023293, 99.69931180518793, 99.65484383271573, 99.59767072525146, 99.7649550026469, 99.73319216516676, 99.61037586024352, 99.65060878771837, 99.62731604023293, 99.72260455267337, 99.73107464266808, 99.66543144520911, 99.76071995764956, 99.71413446267867, 99.62731604023293, 99.74166225516146, 99.71201694017999, 99.67178401270513, 99.58708311275808, 99.69507676019057, 99.73954473266278, 99.68025410269983, 99.71836950767602, 99.70566437268396, 99.73107464266808, 99.65272631021705, 99.61037586024352, 99.79460031762838, 99.71201694017999, 99.68872419269455, 99.70778189518263, 99.7564849126522, 99.7564849126522, 99.66543144520911, 99.76919004764426, 99.64213869772367, 99.66966649020645, 99.71836950767602, 99.76071995764956, 99.73319216516676, 99.75224986765484, 99.6929592376919, 99.67813658020117, 99.65907887771307, 99.72472207517205, 99.68872419269455, 99.71413446267867, 99.72472207517205, 99.76071995764956, 99.7924827951297, 99.69719428268925, 99.68448914769719, 99.73107464266808, 99.73742721016411, 99.76283748014822, 99.75224986765484, 99.66543144520911, 99.70354685018528, 99.7564849126522, 99.76283748014822, 99.79460031762838, 99.72260455267337, 99.64002117522499, 99.79036527263102, 99.69084171519323, 99.8009528851244, 99.75860243515088, 99.73107464266808, 99.6569613552144, 99.75013234515616, 99.77977766013764, 99.79883536262572, 99.70142932768661, 99.69507676019057, 99.7734250926416, 99.70989941768131, 99.7480148226575]\n",
        "test_loss_list_linear = [0.6001535044873462, 0.4386981564993952, 0.32313791202271686, 0.3436826935001448, 0.3047397654576629, 0.28500792238057826, 0.26378655251042515, 0.2683426228297107, 0.27285771396960695, 0.2532431707516605, 0.25363739693135606, 0.25782927618745494, 0.2381599084200228, 0.24098271157081222, 0.23853215436432876, 0.22876223501767598, 0.2459365823762674, 0.24074384005849853, 0.23645990466078123, 0.23522190126937395, 0.23930488064812094, 0.23977934446770185, 0.22856533881642072, 0.2503986795633739, 0.2523015177096514, 0.2286284007497278, 0.2522817155821066, 0.23435633048853455, 0.25106761294106644, 0.23668714861075082, 0.25767872133748787, 0.2464016547937896, 0.24990028208669493, 0.25964026233437015, 0.2509775684014255, 0.25643125852095144, 0.25149472640352505, 0.2636343431750349, 0.2690670864802657, 0.2728121512952973, 0.2877597091719508, 0.2628066184400928, 0.289077399882908, 0.2637001336643509, 0.2718517802947876, 0.30822780637034014, 0.2853905560412243, 0.3029168626914422, 0.2886001042948634, 0.30125397385335434, 0.2918206257928236, 0.31290997323744435, 0.3016124471894228, 0.3364011060409978, 0.30226883032888757, 0.30097986143264993, 0.3160975992350894, 0.324009220079318, 0.2896068644545534, 0.28848243194321793, 0.30809056048518885, 0.31027250127026845, 0.31317287329219134, 0.3204711799728958, 0.323861251740406, 0.30745547297684583, 0.313345615642474, 0.31225476020435783, 0.32664194748755176, 0.3649349614393477, 0.3289772248781268, 0.34484266127715363, 0.34519970382326376, 0.3562348909566508, 0.34551431653181125, 0.3507686852495752, 0.3557186407749267, 0.34874562685396154, 0.3626312471414898, 0.32565479080977977, 0.3328241787111277, 0.3502718099739914, 0.35965223179436195, 0.33991847547026827, 0.32748869253212914, 0.3568636128751963, 0.366436490642966, 0.36107295554350405, 0.34370773008056715, 0.343383331070928, 0.3387923375794701, 0.33830804925631075, 0.3589262347485797, 0.3675877243079537, 0.3818419510498643, 0.35536084654649686, 0.35281344309595286, 0.3716038850629154, 0.35846942082485733, 0.34315043032242387, 0.36869288876871853, 0.3778895241226636, 0.38406652862242624, 0.352233542257226, 0.3620971898712656, 0.37449508531055614, 0.37493528568131085, 0.36601132105159406, 0.36861560567665624, 0.3717369966793294, 0.369726698087784, 0.3781300349087984, 0.35664984070714195, 0.3536169459751132, 0.406203392372631, 0.3737115380445532, 0.36973171939562055, 0.37459768973948326, 0.40706704461983606, 0.3881958369896108, 0.37344259142364356, 0.3670044747710812, 0.3696818688847855, 0.37015472998952165, 0.38189552233134416, 0.3749138325744984, 0.3891030573742647, 0.3678013856029686, 0.36681280497863306, 0.3926687809620418, 0.4037954283184281, 0.4014408321065061, 0.3803745116673264, 0.38791271775741787, 0.37617705127808687, 0.3729255223537193, 0.36608954668775495, 0.3800407153572522, 0.37864029130843635, 0.3903196998415332, 0.39593674976597815, 0.4233312764953749, 0.41837985076092404, 0.4037898524353902, 0.39368393412772934, 0.40711697678574743, 0.3831657852119237, 0.39936033349630295, 0.4122653530234946, 0.4055917443908459, 0.391819414573119, 0.3998463283493823, 0.3856644867325896, 0.38932094629853964, 0.45811930652159977, 0.4117749379582557, 0.3846923142269838, 0.38948512334814844, 0.3886060610632686, 0.4141315968488069, 0.39312611405244646, 0.40893315564037536, 0.3917342342217179, 0.4033093715232669, 0.40686407040658534, 0.402124198153615, 0.3760559221218322, 0.41189432279298116, 0.3945868524777539, 0.4064665962668026, 0.4281328501380688, 0.3944055610610282, 0.4145982712644207, 0.39619766517231864, 0.4008463170269833, 0.4100334781368135, 0.3959438812681565, 0.4048078361277779, 0.3898382809506181, 0.3940078625638111, 0.41857934771699135, 0.40016016322553305, 0.42573420590191496, 0.4076968614815497, 0.40349324788971275, 0.41680151768320917, 0.4404344489323158, 0.4126104597257925, 0.41489401694464806, 0.39322311091510687, 0.39995222047482637, 0.4001541677862406, 0.40168759471955984, 0.4003755842317261, 0.4116448443930815, 0.3964004161512004, 0.40526396525092423, 0.40040231844885094, 0.436148803023731, 0.42100324081804824, 0.42921065237811384, 0.4016612854548821, 0.4237121128860642, 0.439825722326835, 0.40309608828586835, 0.4022408921143734, 0.40526854241376414, 0.4139130270638156, 0.42377193091327653, 0.4159748113506437, 0.43089511207140546, 0.41587188575124623, 0.42511460325662415, 0.42877695572507735, 0.4061464395730154, 0.4108063448129185, 0.4237177971266575, 0.4147245909142144, 0.4100258317867331, 0.44727529514599224, 0.40763759292552576, 0.4096112189969669, 0.4398777195818576, 0.4397024883450392, 0.42169872507014694, 0.4525160300362782, 0.4039532243748944, 0.42747145336048276, 0.41558541071500776, 0.4243580105614063, 0.4347197997529863, 0.40417124533697085, 0.4213481634563091, 0.42768024219492196, 0.41689553120922224, 0.42403014976640835, 0.43499788091353636, 0.40865986559576556, 0.39625785907949596, 0.41788156942793114, 0.42710175029203, 0.39860118992453186, 0.40182735008534554, 0.43774937425612237, 0.4086107819651564, 0.42035614554861594, 0.42495340735231546, 0.42730405055943477, 0.41829457241749646, 0.4210991954211803, 0.39986408456210415, 0.43789072635163573, 0.4252349973893633, 0.4179426681058591, 0.4189265777229094, 0.42167082347327334, 0.4152061831133038, 0.4278368429582128, 0.42154261065354826, 0.39703031598279875, 0.42471260858663157, 0.4301079974747172, 0.4101321147526523, 0.4378960633869557, 0.412081252308745, 0.4338206772089881, 0.4061669551530013, 0.4237026169088067, 0.4324107744557527, 0.4283796655671561, 0.4480068746077664, 0.42207027151815446, 0.4178310304186216, 0.42660820346289113, 0.423226847276822, 0.44835605301127274, 0.4472741484733335, 0.4282900923771747, 0.4323618656535651, 0.4345184702519784, 0.4196194001455225, 0.4023636024691822, 0.4263699218977754, 0.45100487594940136, 0.4451090616046214, 0.4251364004779972, 0.4428678105822673, 0.42934233425459, 0.4463761615374025, 0.42917647955519167, 0.43650849757935195, 0.40754444616865, 0.4072841090405835, 0.43068673907249583, 0.42376545924857695, 0.4177722467438263, 0.41679786920105794, 0.4319006550381435, 0.42252690807057947, 0.4652784176572573]\n",
        "test_acc_list_linear = [81.16548862937923, 86.56653349723418, 90.2581438229871, 89.47065150583897, 90.86893054701905, 91.77934849416103, 92.38629379225569, 92.26336816226183, 92.23263675476336, 92.60525507068223, 92.71665642286416, 92.73586355255071, 93.29287031346036, 93.27366318377382, 93.4081130915796, 93.58481868469576, 93.13921327596803, 93.3620159803319, 93.6078672403196, 93.46957590657652, 93.5540872771973, 93.73847572218807, 93.77688998156115, 93.21220036877689, 93.30055316533497, 94.06883835279656, 93.46957590657652, 94.04963122311001, 93.73847572218807, 93.8959741856177, 93.44652735095268, 93.91518131530424, 93.73847572218807, 93.6578057775046, 93.9881684081131, 93.98432698217579, 93.95743700061463, 93.98432698217579, 93.8921327596804, 93.92670559311617, 93.68469575906576, 94.3338967424708, 93.6040258143823, 94.10341118623234, 94.14950829748003, 93.79609711124769, 94.17255685310387, 93.64244007375538, 93.9958512599877, 93.56945298094652, 94.00353411186232, 93.67317148125385, 93.96127842655194, 93.50799016594961, 93.99969268592501, 94.3300553165335, 93.97664413030117, 93.78457283343577, 94.16871542716656, 94.10341118623234, 94.16103257529196, 94.16487400122925, 94.05731407498463, 94.11877688998156, 94.12645974185618, 94.16487400122925, 94.17639827904118, 94.17639827904118, 94.18792255685311, 94.02658266748617, 94.1379840196681, 93.90365703749232, 93.80762138905962, 93.93054701905348, 94.00353411186232, 93.98432698217579, 93.97280270436386, 94.06499692685925, 93.99969268592501, 94.25706822372464, 94.41840811309157, 94.01505838967425, 93.78457283343577, 94.14566687154272, 94.25706822372464, 94.04963122311001, 93.8959741856177, 93.97280270436386, 94.13030116779349, 94.41072526121697, 94.20712968653964, 94.31084818684695, 93.77304855562384, 93.85755992624462, 93.78073140749846, 94.21865396435157, 94.3262138905962, 93.96896127842655, 94.20328826060233, 94.43377381684081, 94.19944683466503, 94.17639827904118, 93.82298709280884, 94.3761524277812, 94.31084818684695, 94.13030116779349, 94.18023970497849, 94.27627535341118, 94.14950829748003, 94.30700676090964, 94.41456668715428, 94.35310387215735, 94.10341118623234, 94.3262138905962, 93.75, 94.23786109403811, 94.28779963122311, 93.94975414874001, 93.8921327596804, 93.83066994468346, 94.08420405654579, 94.37231100184388, 94.17255685310387, 94.44913952059004, 94.14950829748003, 94.3761524277812, 94.22249539028887, 94.21865396435157, 94.53365089121081, 94.14566687154272, 94.16487400122925, 93.91133988936693, 93.98432698217579, 94.2839582052858, 94.19944683466503, 94.21481253841426, 94.3338967424708, 93.98432698217579, 94.18792255685311, 94.03042409342348, 94.27627535341118, 93.83066994468346, 94.12261831591887, 94.02274124154886, 94.13030116779349, 94.0880454824831, 94.52980946527352, 94.0880454824831, 94.02658266748617, 94.3262138905962, 94.31084818684695, 94.42609096496619, 94.39535955746773, 94.42224953902888, 93.99969268592501, 94.16871542716656, 94.35310387215735, 94.23017824216349, 94.26475107559926, 94.39535955746773, 94.49139520590043, 94.26090964966195, 94.51444376152428, 94.40304240934235, 94.15719114935465, 94.16103257529196, 94.41840811309157, 94.17639827904118, 94.31853103872157, 94.31853103872157, 94.00737553779963, 94.3338967424708, 94.34926244622004, 94.3338967424708, 94.24554394591273, 94.51060233558697, 94.39151813153042, 94.27243392747388, 94.43377381684081, 94.3262138905962, 94.23017824216349, 94.24170251997542, 93.86908420405655, 94.13030116779349, 94.29164105716042, 94.35694529809466, 94.19560540872772, 94.3799938537185, 94.3338967424708, 94.25322679778733, 94.2839582052858, 94.2340196681008, 94.26859250153657, 94.69114935464044, 94.16487400122925, 94.46834665027659, 94.27243392747388, 94.36846957590657, 94.10725261216963, 94.43377381684081, 94.23786109403811, 94.46450522433928, 94.32237246465888, 94.28779963122311, 94.11493546404425, 94.16871542716656, 94.4299323909035, 94.30700676090964, 94.19560540872772, 94.46066379840197, 94.17639827904118, 94.53365089121081, 94.16871542716656, 94.29164105716042, 94.29548248309773, 94.49523663183774, 94.25706822372464, 94.10341118623234, 94.64889366933005, 94.30316533497235, 94.42609096496619, 94.44529809465274, 94.19944683466503, 94.1917639827904, 94.44529809465274, 94.16103257529196, 94.3338967424708, 94.54901659496005, 94.4721880762139, 94.31084818684695, 94.39920098340504, 94.49139520590043, 94.3761524277812, 94.45298094652735, 94.31468961278426, 94.45682237246466, 94.30316533497235, 94.55285802089736, 94.4299323909035, 94.40688383527966, 94.34542102028273, 94.4299323909035, 94.49139520590043, 94.27243392747388, 94.47602950215119, 94.26090964966195, 94.24554394591273, 94.54517516902274, 94.3300553165335, 94.29548248309773, 94.61432083589429, 94.11877688998156, 94.40304240934235, 94.27243392747388, 94.59511370620774, 94.2839582052858, 94.39151813153042, 94.39535955746773, 94.21865396435157, 94.49523663183774, 94.24554394591273, 94.51060233558697, 94.59127228027043, 94.43377381684081, 94.31853103872157, 94.46834665027659, 94.43761524277812, 94.43761524277812, 94.37231100184388, 94.34157959434542, 94.00353411186232, 94.38383527965581, 94.58358942839583, 94.5759065765212, 94.41840811309157, 94.07652120467118, 94.09956976029503, 94.3799938537185, 94.30700676090964, 94.29932390903504, 94.38383527965581, 94.5259680393362, 94.26859250153657, 94.35310387215735, 94.36462814996926, 94.37231100184388, 94.38767670559312, 94.35694529809466, 94.36846957590657, 94.34542102028273, 94.40304240934235, 94.28779963122311, 94.46834665027659, 94.56054087277197, 94.62968653964352, 94.35310387215735, 94.43761524277812, 94.39535955746773, 94.26475107559926, 93.79609711124769]\n",
        "train_loss_list_exp = [1.5108453227575556, 0.45569133027620756, 0.38955149030297753, 0.38201055714108434, 0.3791554563736851, 0.37846633355791975, 0.37789585104156637, 0.37833547551780533, 0.37885424543202406, 0.3798585463830126, 0.3801606303146538, 0.3785690202864851, 0.380058984244419, 0.38191306041831247, 0.3755665107309657, 0.37845069464790787, 0.3810173268240642, 0.37906573632060675, 0.38025176412044825, 0.37710853672124506, 0.37910957913088605, 0.3795646985129612, 0.37768419939004955, 0.37915716253645054, 0.3763544963304266, 0.37615088787343764, 0.3767496614356028, 0.3780435439209305, 0.37970296291477956, 0.3768931365190806, 0.3782231926433439, 0.37863624968179843, 0.3801447201146666, 0.38008787327504095, 0.3784985868184547, 0.3775513489152681, 0.3797982176387213, 0.378436125835106, 0.37775709227656284, 0.3784049479618951, 0.3812676381047179, 0.377607957215167, 0.37778531927564923, 0.3785970114110931, 0.3763440459320539, 0.37659880587563604, 0.38260392961786366, 0.3788121801404772, 0.3782783241937477, 0.37851579224837184, 0.37789375260270386, 0.37802613358995135, 0.3780238080800064, 0.38144587775879113, 0.3779912972595634, 0.3791698661600025, 0.3766854087996289, 0.37804330437163997, 0.3786587099718854, 0.37654282059772876, 0.3787634488609102, 0.3799068733524824, 0.37713607115958764, 0.37751451530430696, 0.3801226066299247, 0.37911422368956776, 0.3805197463610631, 0.37700172774190827, 0.3788724349847008, 0.3784166157326401, 0.38057625192775313, 0.37780569122251134, 0.38091700400924944, 0.3794297932528545, 0.3814112742338077, 0.37960589101644066, 0.37835461256626823, 0.37888884483798735, 0.3795582245439695, 0.37849634607148364, 0.37682519744082194, 0.37698636250444223, 0.3814956169464401, 0.3775139360570003, 0.379286357057773, 0.37696852730864755, 0.3763741816123973, 0.37921798265561824, 0.3787178158921601, 0.3787029756682352, 0.37783306570557074, 0.3772085670452454, 0.377733004771597, 0.3765660253401371, 0.3802021978912638, 0.37975252381346736, 0.3792028047528047, 0.3785102287039847, 0.3774594428739574, 0.3806928960773034, 0.3792232088441771, 0.37827566011649805, 0.38008303063994825, 0.3778988100325835, 0.3803016372974003, 0.3791544923614357, 0.38108806501882186, 0.37818956795100594, 0.37843327103106955, 0.38068289166382013, 0.38067130910025704, 0.37755737811084683, 0.37938234137325755, 0.3798758971497295, 0.37914535011540906, 0.37674440685811084, 0.37839110369281714, 0.3803737539505248, 0.3769939240966709, 0.37618244623104086, 0.3802966764625818, 0.376914295521855, 0.38093100847590583, 0.3785027291797364, 0.38015424458153524, 0.3799492670753138, 0.3770360491140102, 0.3781222916311688, 0.37643939103214397, 0.37902817707559283, 0.37899113222350916, 0.3788869072428241, 0.37866583002130516, 0.38024979207896925, 0.37998249028433306, 0.37665241209633626, 0.3791340853221371, 0.3768656629776244, 0.3781943121738227, 0.37956079872966136, 0.3790555945660687, 0.3767165967280949, 0.3785593254860178, 0.3785436793071468, 0.3786095209157241, 0.37977585394369556, 0.37895426606421223, 0.37726105127715803, 0.3811360982456181, 0.3777372095239195, 0.37985739951857384, 0.3775850888190231, 0.37942486108964696, 0.3774096647575296, 0.37866538746893247, 0.37956950698441605, 0.37765448986676325, 0.3765703885535884, 0.37493878506063444, 0.3787174464322041, 0.3790939331539278, 0.3798395115024029, 0.3802033216972661, 0.3778447469237051, 0.37922620373528176, 0.3788863822696655, 0.3776389633010073, 0.37965354569720705, 0.37847416466329153, 0.38210083584636856, 0.3769269231858292, 0.376283608316406, 0.3791458204105941, 0.3800827085891067, 0.3800524467940576, 0.379729083156198, 0.3786869500145357, 0.37784998191566, 0.37798632447150987, 0.37959555187199495, 0.3772686913326827, 0.37737370587299834, 0.3774894487647829, 0.3773925192149351, 0.37866038096144916, 0.3799699583673865, 0.37930802181161194, 0.37929151229419034, 0.37872534558217374, 0.38051237880699035, 0.3785012537224829, 0.37907243408969427, 0.3786347356188265, 0.3772817448306536, 0.37789186423386983, 0.377803510807071, 0.37891541296227516, 0.3768928705596019, 0.3764786839808229, 0.37735731827049723, 0.37620092403436417, 0.37594923223583354, 0.37946950848186567, 0.37934725721515616, 0.3799315985383057, 0.37742675118006985, 0.3800657158137014, 0.38043200723362486, 0.3790072623468673, 0.37812678907621844, 0.380248889528962, 0.3795400029679301, 0.37716066687895355, 0.37926767851279036, 0.3811982564002195, 0.3785968935344277, 0.37829239452434427, 0.3791591095003655, 0.37750050801087204, 0.3755794913788152, 0.37707439347657407, 0.3798365030023787, 0.37901620768757693, 0.3787338199815776, 0.3768027005237616, 0.378330920527621, 0.3773879486774688, 0.37904529988281127, 0.3793277707364824, 0.37877471587522243, 0.3773436505135482, 0.38000925620235404, 0.37747299065434836, 0.3783611831707037, 0.38008849536823386, 0.3802644786069064, 0.37820831300604957, 0.37898807813158525, 0.37832132050500006, 0.37651883493755567, 0.3780955279745707, 0.3794448867157546, 0.37681427719147226, 0.37846876878725483, 0.37892088152690306, 0.3757547806433546, 0.3790287462272618, 0.3771775669764051, 0.37960463952081314, 0.38023487892415786, 0.3773157599818739, 0.3790636116734688, 0.37847773605568946, 0.3790109474969104, 0.3778621018094422, 0.3795850692483468, 0.38052020777208695, 0.3777503734681664, 0.38023589062819957, 0.37878056504539037, 0.3778544519813403, 0.3769152445117956, 0.37817029428837423, 0.3794189135879682, 0.3814815942268708, 0.3767595124357761, 0.376037456559618, 0.3787438214067521, 0.37781988387185383, 0.3780568046621514, 0.3789186481295562, 0.3792363849031893, 0.3795603767723895, 0.38027571985715125, 0.37710057130350977, 0.37790428061633896, 0.3758348726887044, 0.37957349484205893, 0.3801340398184329, 0.37570041014250055, 0.37696901714898706, 0.3780564808748602, 0.37865450675409984, 0.37989892315896867, 0.37623435913062675, 0.3776249932402841, 0.3777484252120098, 0.37681428954853274, 0.3777670302203677, 0.37880285431537525, 0.3775242522641572, 0.37820892516513505, 0.37701137581976446, 0.378751233704691, 0.37953954490865793, 0.3773617081364319, 0.3809620241324107, 0.37960273697770386, 0.3808248356428896, 0.37698043551709914]\n",
        "train_acc_list_exp = [47.21651667548968, 85.66437268395977, 87.928004235045, 88.11858125992589, 88.25410269984118, 88.07623080995235, 88.23928004235044, 88.18422445738486, 88.14187400741133, 88.09740603493913, 88.06564319745897, 88.14399152991001, 88.0084700899947, 87.9915299100053, 88.28163049232398, 88.06776071995765, 88.09105346744309, 88.11434621492853, 87.96188459502382, 88.11222869242985, 88.12916887241927, 88.13975648491265, 88.2435150873478, 88.16728427739545, 88.09105346744309, 88.20116463737428, 88.20328215987296, 88.01482265749074, 88.0084700899947, 88.20116463737428, 88.15034409740603, 88.22445738485972, 88.04870301746956, 88.06776071995765, 88.28586553732133, 88.19269454737956, 88.11011116993119, 88.18422445738486, 88.20539968237162, 88.08046585494971, 88.071995764955, 88.23292747485442, 88.23292747485442, 88.15881418740074, 88.26680783483324, 88.32398094229751, 88.06140815246162, 88.18210693488618, 88.17363684489148, 88.15881418740074, 88.13975648491265, 88.15034409740603, 88.12281630492323, 88.01694017998942, 88.19481206987824, 88.18634197988354, 88.3070407623081, 88.16093170989942, 88.16516675489677, 88.19481206987824, 88.25622022233986, 88.12493382742191, 88.1355214399153, 88.16093170989942, 88.09105346744309, 88.24563260984648, 88.11858125992589, 88.10799364743251, 88.09317098994177, 88.09528851244045, 88.0359978824775, 88.17787188988883, 88.10375860243515, 88.1355214399153, 88.08470089994707, 88.09105346744309, 88.23928004235044, 88.12916887241927, 88.10375860243515, 88.26045526733722, 88.24775013234516, 88.16093170989942, 88.05717310746427, 88.11858125992589, 88.15034409740603, 88.24563260984648, 88.14822657490735, 88.05293806246691, 88.1630492323981, 88.12281630492323, 88.32821598729487, 88.21386977236634, 88.23928004235044, 88.26892535733192, 88.23928004235044, 87.91529910005293, 88.05293806246691, 88.16093170989942, 88.26892535733192, 88.10587612493383, 88.13128639491795, 88.28374801482266, 88.21810481736368, 88.20328215987296, 88.18422445738486, 88.09740603493913, 88.00635256749602, 88.08681842244575, 88.10375860243515, 88.09528851244045, 88.06140815246162, 88.16093170989942, 88.10799364743251, 88.12493382742191, 88.08681842244575, 88.29433562731604, 88.08258337744839, 88.0084700899947, 88.17363684489148, 88.27527792482795, 88.10587612493383, 88.12069878242457, 87.98941238750662, 88.22022233986236, 88.12916887241927, 88.12281630492323, 88.14399152991001, 88.16516675489677, 88.14399152991001, 88.09528851244045, 88.18845950238222, 88.19692959237692, 88.15246161990471, 88.06564319745897, 88.12069878242457, 88.25833774483854, 88.15246161990471, 88.25622022233986, 88.11646373742721, 88.10375860243515, 88.11434621492853, 88.18422445738486, 88.14610905240868, 88.27316040232928, 88.15881418740074, 88.16728427739545, 88.13763896241397, 88.08681842244575, 88.03811540497618, 88.15457914240339, 88.15034409740603, 88.28586553732133, 88.13975648491265, 88.29010058231869, 88.25622022233986, 88.21810481736368, 88.22445738485972, 88.27316040232928, 88.2710428798306, 88.11222869242985, 88.02541026998412, 88.02541026998412, 88.08258337744839, 88.14610905240868, 88.09740603493913, 88.18634197988354, 88.18634197988354, 88.17998941238751, 88.11646373742721, 88.06140815246162, 88.05293806246691, 88.2710428798306, 88.071995764955, 88.22233986236104, 88.09317098994177, 88.12493382742191, 88.18634197988354, 88.11434621492853, 88.21810481736368, 88.00211752249868, 88.11858125992589, 88.27527792482795, 88.30280571731075, 88.24563260984648, 88.15246161990471, 88.12493382742191, 88.11434621492853, 88.02117522498676, 88.24563260984648, 88.06987824245633, 88.23716251985178, 88.0635256749603, 88.14399152991001, 88.22445738485972, 88.19269454737956, 88.26045526733722, 88.1630492323981, 88.1990471148756, 88.14399152991001, 88.27527792482795, 88.24563260984648, 88.32609846479619, 88.02117522498676, 88.15669666490207, 88.071995764955, 88.16728427739545, 88.15034409740603, 88.12281630492323, 88.15881418740074, 88.17787188988883, 88.06987824245633, 88.08470089994707, 88.11646373742721, 88.13763896241397, 88.1355214399153, 88.18845950238222, 88.18845950238222, 88.14399152991001, 88.25410269984118, 88.22445738485972, 88.17998941238751, 88.07623080995235, 88.14187400741133, 88.09952355743779, 88.31127580730545, 88.19481206987824, 88.2265749073584, 88.10164107993647, 88.12705134992059, 88.1905770248809, 88.11434621492853, 88.13128639491795, 88.22233986236104, 88.17998941238751, 88.13128639491795, 88.12705134992059, 88.24563260984648, 88.16940179989412, 88.06987824245633, 88.24986765484384, 88.12493382742191, 88.15457914240339, 88.22869242985706, 88.06987824245633, 88.08470089994707, 88.23928004235044, 88.07411328745368, 88.215987294865, 88.15881418740074, 88.14822657490735, 88.12069878242457, 88.10799364743251, 88.27527792482795, 88.00423504499736, 88.17363684489148, 88.18422445738486, 88.1355214399153, 88.18634197988354, 88.08893594494441, 88.11011116993119, 88.09105346744309, 88.2265749073584, 88.20539968237162, 88.13763896241397, 88.11646373742721, 88.29221810481737, 88.20116463737428, 88.18845950238222, 88.14399152991001, 88.0359978824775, 88.15246161990471, 88.14399152991001, 88.07834833245103, 88.30068819481207, 88.21386977236634, 88.1715193223928, 88.34515616728427, 87.99788247750132, 87.91106405505559, 88.15034409740603, 88.15669666490207, 88.1630492323981, 88.13975648491265, 88.08470089994707, 88.3705664372684, 88.22869242985706, 88.1355214399153, 88.17787188988883, 88.14610905240868, 88.18210693488618, 88.27527792482795, 88.17363684489148, 88.15034409740603, 88.26469031233457, 88.14187400741133, 88.26045526733722, 88.08681842244575, 88.09952355743779, 88.16728427739545, 88.12916887241927]\n",
        "test_loss_list_exp = [0.7599668520338395, 0.41391250622623105, 0.392150557216476, 0.3908533724207504, 0.3910430484980929, 0.3915113131059151, 0.3893689420439449, 0.39278720669886646, 0.3913472319642703, 0.3898209324654411, 0.3919214537622882, 0.3930370972729197, 0.3875842901567618, 0.3887199363579937, 0.3931986349178295, 0.3915082841527228, 0.39034265929869577, 0.38962281656031517, 0.39410267273585003, 0.3923792628680958, 0.3913208600498882, 0.3940731910806076, 0.3901035269978, 0.3910912872091228, 0.38958175787154364, 0.3905664257266942, 0.3923723243323027, 0.3930160653795682, 0.3910279740013328, 0.39261947053612445, 0.39096589437594603, 0.391885179076709, 0.3932027472730945, 0.39465218753206965, 0.38952960351518556, 0.3913427036182553, 0.39291733357251857, 0.38893387598149914, 0.3907097995865579, 0.39178018352272465, 0.38997077613192443, 0.38897235888768644, 0.39309656094102297, 0.3918713939686616, 0.38930126476813764, 0.3919200319431576, 0.3909892667742336, 0.3936515018782195, 0.3921337866900014, 0.3897011807444049, 0.389769053050116, 0.3902200594106141, 0.38991651070468564, 0.3911624582228707, 0.3900841569491461, 0.38808364508783116, 0.39078373330480914, 0.3911517692693308, 0.3905084923494096, 0.392090796986047, 0.3915708720245782, 0.3868667685664168, 0.3898552243469977, 0.3904008638186782, 0.38936665559224054, 0.3908520120323873, 0.3909977381574173, 0.3894340494538055, 0.3913114227938886, 0.38867319550584345, 0.3922208594340904, 0.39250611046365663, 0.3933124074748918, 0.3923281282916957, 0.3923263843445217, 0.3901771641537255, 0.39134884019400556, 0.38913627334085166, 0.39259269424513277, 0.39072176375809836, 0.3921530346806143, 0.39100976697370116, 0.39079241250075547, 0.3899397780643959, 0.3929170014373228, 0.3889102427398457, 0.38894158966985404, 0.39256049473496046, 0.3897372295020842, 0.39233241295989824, 0.39279353041567056, 0.394793822411813, 0.3923672336865874, 0.38789207575952306, 0.391120845853698, 0.3914312515042576, 0.3890334714715387, 0.38947177671042144, 0.3914659348334752, 0.39052898429480254, 0.38922284959870224, 0.3926088816541083, 0.3930806327684253, 0.392626012584158, 0.3940140974580073, 0.3908073946687521, 0.3892480945762466, 0.3914746798428835, 0.3904998555925547, 0.39119182118013796, 0.39440940546931, 0.39126060596283746, 0.39380357363352586, 0.3891267492344566, 0.3960636570027061, 0.3884448780879086, 0.39201007885675804, 0.3887008518418845, 0.3946489935704306, 0.39012787235425966, 0.3926868131201641, 0.391174876587648, 0.39185114438627283, 0.3916316426121721, 0.39595334516728625, 0.3925237079315326, 0.3923376457510041, 0.3896145831574412, 0.3903454670719072, 0.38917045835770814, 0.3901947463552157, 0.38894958538459795, 0.39138510881685745, 0.3923698270729944, 0.391075956543871, 0.3886808121905607, 0.3895190705855687, 0.3937717227666986, 0.3892171628334943, 0.3928420579462659, 0.3929640487128613, 0.39179296280239145, 0.39022867213569434, 0.3915944700585861, 0.3908092556338684, 0.38998721832153843, 0.3919305083360158, 0.38930609106433156, 0.3915616778620318, 0.39198073642510994, 0.3898476078083702, 0.3942314390750492, 0.39122209778311207, 0.39321639089315547, 0.39556199356037025, 0.39069073004465477, 0.3901906652631713, 0.3918502590089452, 0.3921655998656563, 0.3897442833027419, 0.39050330440787706, 0.39026205139417275, 0.3908781868716081, 0.39245978443353785, 0.3903481831007144, 0.39445868105280635, 0.3892170712351799, 0.390042986282531, 0.3925706706941128, 0.39200084298556925, 0.3911351076528138, 0.39240403268851487, 0.3938527383348521, 0.3900003031480546, 0.3922225654709573, 0.3932921053001694, 0.39253465655971975, 0.3926194997540876, 0.3874349548097919, 0.39200115612908903, 0.3948955665908608, 0.3898700127998988, 0.38938687602971117, 0.3864065995257275, 0.39098570473930416, 0.3909385864641152, 0.3916171494067884, 0.39381099733359676, 0.3898175518740626, 0.3911994169740116, 0.39155133265782804, 0.39230194549058, 0.39114902234252763, 0.39017920955723406, 0.3944379236622184, 0.3916288679283039, 0.39200210498244153, 0.39363550146420795, 0.3911365672361617, 0.39186333141782703, 0.39117470471297994, 0.38984077394593, 0.39264219977399883, 0.3900822729018389, 0.38898184559508864, 0.3910406724202867, 0.3907987319809549, 0.39212526841198697, 0.39124271427007284, 0.3914998725202738, 0.38839635442869336, 0.3937654222781752, 0.3902626017875531, 0.3955241435883092, 0.39323169840317146, 0.39330630157800284, 0.38655056892072454, 0.39017754106544983, 0.3952399448436849, 0.39261257144458156, 0.3932858498073092, 0.39499423879326556, 0.3910665978111473, 0.3922448316041161, 0.39132243789294185, 0.3905644398547855, 0.39066209877822916, 0.39234474681171716, 0.3916584701806891, 0.39128884439374884, 0.38904376978091165, 0.39183107588221044, 0.3900741520611679, 0.3930635173969409, 0.38995324082526506, 0.39311547854951784, 0.3887742017121876, 0.3909773443113355, 0.38996322007448064, 0.38965144855718986, 0.39268765284442436, 0.390111534545819, 0.3925885803559247, 0.39064669915858435, 0.3895316495030534, 0.3907163414154567, 0.3907886814399093, 0.39067917981860684, 0.3914143468673323, 0.39072976911477014, 0.39038341521632436, 0.39487345085716713, 0.39242782728636966, 0.39340144921751585, 0.391638440317383, 0.38982952050134245, 0.39072682591629965, 0.39341319571523103, 0.39195014886996327, 0.3917578032203749, 0.38969211536003096, 0.3894471714601797, 0.3898716885961738, 0.39170538400318106, 0.39051353675769823, 0.39179608466870647, 0.3921622397998969, 0.39439454747765673, 0.39445579307628614, 0.39132673534400325, 0.3867330071972866, 0.3894542139388767, 0.39122700530524346, 0.3883669243431559, 0.38984382444737004, 0.3917064682817927, 0.3907441727670969, 0.39336200226463525, 0.3893578349083078, 0.3913763885696729, 0.3897980788320887, 0.3937839002293699, 0.39068301957027585, 0.3921215317997278, 0.39208485865417647, 0.3916279115513259, 0.3911278141918136, 0.39544760121726524, 0.38938475297946556, 0.39043475139667005, 0.39209672136634005, 0.39414026142627584, 0.3913420901871195, 0.39028101735839654, 0.3931575333516972, 0.39237413212072614, 0.38880399651094977, 0.3887612786801422, 0.3894933131979961, 0.3917112931901333]\n",
        "test_acc_list_exp = [75.58773816840811, 87.06976029502151, 87.53073140749846, 87.8918254456054, 87.76889981561156, 87.74969268592501, 87.98786109403811, 87.76121696373694, 87.87645974185618, 87.71896127842655, 87.74969268592501, 87.79578979717272, 87.96481253841426, 87.81115550092194, 87.81499692685925, 87.80731407498463, 87.9417639827904, 87.93023970497849, 87.76505838967425, 87.78042409342348, 87.66133988936693, 87.7458512599877, 87.7458512599877, 87.85725261216963, 87.66518131530424, 87.71127842655194, 87.65365703749232, 87.66133988936693, 87.74200983405039, 87.7458512599877, 87.85725261216963, 87.89566687154272, 87.64981561155501, 87.61140135218193, 88.02243392747388, 87.85725261216963, 87.50384142593731, 87.91871542716656, 87.82652120467118, 87.80731407498463, 87.82652120467118, 87.93792255685311, 87.6920712968654, 87.72280270436386, 87.82267977873387, 87.81499692685925, 87.87645974185618, 87.66133988936693, 87.8418869084204, 87.76889981561156, 87.88030116779349, 87.78042409342348, 87.7919483712354, 87.79963122311001, 87.81883835279656, 87.8918254456054, 87.92639827904118, 87.94944683466503, 87.78426551936079, 87.8918254456054, 87.7881069452981, 88.00322679778733, 87.87645974185618, 87.7881069452981, 87.68054701905348, 87.81499692685925, 87.71896127842655, 87.82267977873387, 87.7919483712354, 87.81883835279656, 87.83036263060848, 87.78042409342348, 87.78042409342348, 87.71511985248924, 87.72664413030117, 87.82267977873387, 87.80731407498463, 87.88030116779349, 87.83036263060848, 87.84956976029503, 87.91487400122925, 87.73048555623848, 87.81115550092194, 87.73048555623848, 87.7381684081131, 87.72664413030117, 87.8918254456054, 87.66518131530424, 87.8418869084204, 87.7919483712354, 87.69975414874001, 87.79578979717272, 87.77658266748617, 87.90334972341734, 87.75353411186232, 87.83036263060848, 87.86877688998156, 87.86877688998156, 87.7919483712354, 87.84572833435772, 87.91871542716656, 87.84956976029503, 87.83036263060848, 87.80347264904732, 87.70743700061463, 87.75353411186232, 87.9840196681008, 87.70359557467732, 87.84956976029503, 87.87645974185618, 87.7381684081131, 87.81115550092194, 87.6459741856177, 87.7458512599877, 87.51920712968654, 88.05700676090964, 87.79578979717272, 87.91871542716656, 87.56914566687155, 87.82267977873387, 87.73432698217579, 87.75737553779963, 87.81115550092194, 87.85341118623234, 87.74969268592501, 87.68054701905348, 87.63444990780577, 87.83420405654579, 87.91103257529196, 87.76505838967425, 87.80731407498463, 88.04548248309773, 87.81883835279656, 87.82652120467118, 87.6920712968654, 87.76121696373694, 87.74200983405039, 87.80347264904732, 87.96481253841426, 87.8380454824831, 87.6459741856177, 87.85341118623234, 87.92255685310387, 87.68438844499079, 87.7458512599877, 87.78426551936079, 87.81883835279656, 87.98017824216349, 87.88030116779349, 87.6421327596804, 87.9340811309158, 87.65365703749232, 87.68438844499079, 87.60371850030731, 87.66133988936693, 87.6459741856177, 87.78042409342348, 87.77658266748617, 87.58451137062077, 87.97633681622618, 87.83420405654579, 87.73048555623848, 87.84572833435772, 87.75737553779963, 87.76121696373694, 87.63829133374308, 87.91103257529196, 87.75353411186232, 87.57298709280884, 87.70743700061463, 87.73048555623848, 87.77658266748617, 87.76505838967425, 88.02243392747388, 87.84572833435772, 87.73048555623848, 87.88414259373079, 87.69975414874001, 87.88030116779349, 87.88030116779349, 87.68054701905348, 87.92639827904118, 87.95712968653964, 87.91103257529196, 87.68054701905348, 87.76121696373694, 87.78426551936079, 87.67286416717886, 87.67670559311617, 87.88414259373079, 87.79578979717272, 87.66133988936693, 87.83036263060848, 87.86493546404425, 87.78426551936079, 87.67286416717886, 87.81115550092194, 87.6421327596804, 87.70359557467732, 87.71127842655194, 87.84956976029503, 87.82267977873387, 87.7919483712354, 87.71127842655194, 87.83036263060848, 87.8879840196681, 87.76121696373694, 87.73048555623848, 87.76121696373694, 87.8380454824831, 87.75353411186232, 87.72280270436386, 87.84956976029503, 87.59987707437, 87.6959127228027, 87.62292562999386, 88.03779963122311, 87.77658266748617, 87.7381684081131, 87.80731407498463, 87.56530424093424, 87.63060848186846, 87.69975414874001, 87.71896127842655, 87.89566687154272, 87.83420405654579, 87.76121696373694, 87.63060848186846, 87.79578979717272, 87.65365703749232, 87.82267977873387, 87.85341118623234, 87.85341118623234, 87.80347264904732, 87.87645974185618, 87.84572833435772, 87.96097111247695, 87.84572833435772, 87.71511985248924, 87.85725261216963, 87.78426551936079, 87.78426551936079, 87.61908420405655, 87.86493546404425, 87.96097111247695, 87.86109403810694, 87.75737553779963, 87.84956976029503, 87.7881069452981, 87.84572833435772, 87.70359557467732, 87.71127842655194, 87.65749846342962, 87.62292562999386, 87.71896127842655, 87.96865396435157, 87.97633681622618, 87.76121696373694, 87.75353411186232, 87.81115550092194, 87.8879840196681, 87.82652120467118, 87.87645974185618, 87.68438844499079, 87.8918254456054, 87.63060848186846, 87.76505838967425, 87.5960356484327, 87.66133988936693, 87.9840196681008, 87.88030116779349, 87.86877688998156, 87.77274124154886, 87.9417639827904, 87.90719114935465, 87.7881069452981, 87.69975414874001, 87.77658266748617, 87.84572833435772, 87.67286416717886, 87.80731407498463, 87.74200983405039, 87.71896127842655, 87.6459741856177, 87.68438844499079, 87.86877688998156, 87.82267977873387, 87.78426551936079, 87.72664413030117, 87.98017824216349, 87.66518131530424, 87.6959127228027, 87.75353411186232, 87.91487400122925, 87.89566687154272, 87.72280270436386, 87.91487400122925, 87.88030116779349, 87.91103257529196, 87.75353411186232]\n",
        "\n",
        "\n",
        "\n",
        "# train_loss_list_001 = [1.5218167792493924, 1.0340943300305083, 0.7934107637633911, 0.6561045421959874, 0.5800367868936862, 0.5100019117132925, 0.4574989944982072, 0.4208847186245476, 0.3881891872079228, 0.35861467820006054, 0.3339165490084944, 0.3111862796849717, 0.2948872684575498, 0.27018505394363557, 0.25883166084940823]\n",
        "# train_acc_list_001 = [44.0975, 63.055, 72.2075, 77.0575, 79.93, 82.43, 84.0575, 85.47, 86.5625, 87.675, 88.48, 89.16, 89.645, 90.5075, 90.9225]\n",
        "# test_loss_list_001 = [1.3665137306044373, 1.0809951907471766, 0.8369579171832604, 0.7580914105041118, 0.6663558483123779, 0.7236429437806334, 0.5709026199352892, 0.5137608224832559, 0.5140901245648348, 0.48385591937016836, 0.47491426897954336, 0.5022718974306614, 0.5243912806993798, 0.42129093069064466, 0.4074777747634091]\n",
        "# test_acc_list_001 = [52.13, 63.83, 71.47, 73.82, 77.34, 76.49, 80.87, 82.32, 82.77, 83.6, 84.17, 83.3, 83.55, 86.28, 86.86]\n",
        "# train_loss_list_01 = [1.8962965864723864, 1.472022865146113, 1.2269341696184664, 1.0348994015885618, 0.9050399440165144, 0.7892040218027255, 0.6947498847120486, 0.6181783355272616, 0.5774767158892208, 0.5502305545936377, 0.5206893583456167, 0.5026034010104097, 0.4774538204311944, 0.4683271567471111, 0.4492297477710742]\n",
        "# train_acc_list_01 = [31.045, 45.415, 55.22, 62.9525, 67.66, 72.095, 75.7825, 78.6325, 80.1775, 80.8325, 82.04, 82.7775, 83.69, 83.865, 84.6225]\n",
        "# test_loss_list_01 = [1.6557437422909314, 1.5662637073782426, 1.1888765285286722, 1.1435910184172136, 0.9932384845576708, 0.7845515208908275, 0.7405012536652481, 0.7023888849004915, 0.6914112718799447, 0.8937227891970284, 0.6744754122027868, 0.7125071339969393, 0.595223272148567, 0.6645651912387414, 0.5624623864511901]\n",
        "# test_acc_list_01 = [38.54, 45.39, 57.61, 59.19, 65.26, 72.6, 74.56, 76.24, 76.7, 71.16, 77.34, 75.59, 80.06, 77.85, 81.28]\n",
        "# train_loss_list_0001 = [1.6901012103016766, 1.282657652045972, 1.0717586383652002, 0.940055356619838, 0.8364242675205389, 0.7547609810821545, 0.6785155514749094, 0.6320573160061821, 0.5852954303875518, 0.5420240767466755, 0.506621429809747, 0.47925190327647393, 0.4483506758563435, 0.4286465794800189, 0.40272510780122717]\n",
        "# train_acc_list_0001 = [37.2, 53.36, 61.4225, 66.4725, 70.2, 73.2275, 76.08, 77.8025, 79.53, 80.905, 82.42, 83.1425, 84.5575, 85.195, 86.005]\n",
        "# test_loss_list_0001 = [1.4665760239468346, 1.2375031430510026, 1.0847761902628066, 1.0535234567485279, 0.8635394460038294, 0.757294207434111, 0.7295623666877988, 0.7312412850464447, 0.7336276015148887, 0.6307676260984396, 0.6266382736495778, 0.6370392079594769, 0.5392829055273081, 0.5410988666588747, 0.5530912065053288]\n",
        "# test_acc_list_0001 = [46.98, 55.76, 61.4, 63.25, 69.13, 73.23, 74.32, 74.64, 73.96, 78.38, 78.25, 78.58, 81.0, 81.49, 81.32]\n",
        "\n",
        "train_loss_list_cut = [1.613359474907287, 1.186605121571416, 0.9613291156558564, 0.8175660327981455, 0.7329503829105974, 0.6675834229198127, 0.6155491586500844, 0.5636878716298186, 0.5260912411319562, 0.5027581219094249, 0.4726218120834698, 0.44723697268543916, 0.43208082353535554, 0.41164722000828946, 0.3974420052652542, 0.3826391176103403, 0.35894755928661115, 0.3507439031863746, 0.34107909901454425, 0.32801356564124173, 0.3124787288542373, 0.31465199137457645, 0.298767308029123, 0.28487236778766584, 0.27646335530966615, 0.27320462827103587, 0.26612033452184053, 0.26308306258993025, 0.2525483318411123, 0.24124891794146822, 0.24217433363389665, 0.23168825182004477, 0.23012469901730076, 0.21693348256162942, 0.21930161923074876, 0.21653421889669217, 0.21434410071125426, 0.20718723166579256, 0.2052558541250305, 0.19716152494041303, 0.1941497164983719, 0.19097941378339792, 0.18447992457939794, 0.18228636479701477, 0.17657887242948667, 0.17477260330043282, 0.1733962548212312, 0.17061091698825168, 0.16732201631219623, 0.1637549052556483, 0.1613706673700779, 0.16051728827075457, 0.15993363755389142, 0.15627589414770993, 0.15532852677158274, 0.15280420149858007, 0.15101519260353174, 0.14880275238341037, 0.13978494027742563, 0.14830744789002803, 0.14331960949463585, 0.13924681502409256, 0.13707038418601114, 0.1308897960062225, 0.13411890785581768, 0.13409768981627002, 0.13344513029217148, 0.1273199183205827, 0.1283481349103367, 0.12471981016924968, 0.1280518789260913, 0.12418704933394639, 0.12167530884139073, 0.12545658500430684, 0.11795977773234105, 0.11789289145423962, 0.12059933257202943, 0.11666790398355491, 0.11807365684558789, 0.11512304616931338, 0.11561917723082125, 0.11491954412323217, 0.11419005636661388, 0.10912806965624944, 0.11002509410198504, 0.11631911551466766, 0.10705401468129394, 0.1092875181331326, 0.10784903160942058, 0.10965043486069186, 0.10153051761511606, 0.1083350477579493, 0.10791226423467501, 0.10326221440665828, 0.10488084647149895, 0.100699621469925, 0.1017464189983595, 0.1026811115919782, 0.10191941613587327, 0.10146693645503384, 0.0997033108370944, 0.09700418662386961, 0.09839744385653221, 0.09619725261109706, 0.0990427272757307, 0.0953601595407096, 0.09479633424157342, 0.09373613773062588, 0.09360281742228486, 0.09450248972224161, 0.0905749333885531, 0.09483718122251499, 0.094004499824188, 0.09207623065731967, 0.0874123571696468, 0.08727556101073282, 0.09273509363444468, 0.08398670141106121, 0.08792692410964935, 0.08606394986732128, 0.08853992108648387, 0.08270674364302105, 0.08553103019539922, 0.08325290489799013, 0.08070171735109613, 0.08357380274028633, 0.0855067997409132, 0.07974059663760585, 0.07902837820208301, 0.07707989068862539, 0.08178071627101768, 0.07777025827025168, 0.0837722305065622, 0.07823536036232599, 0.07849429675731986, 0.08163950307229266, 0.0772132684176151, 0.0742708618630664, 0.07653025378839086, 0.07370652056361635, 0.07428970992767488, 0.066969591103637, 0.07278624822656377, 0.0711755252714022, 0.07434361697005007, 0.0714579665521606, 0.06873778960468194, 0.06797652607099317, 0.06636746319385763, 0.06754642966004035, 0.07100995908232448, 0.07219889727584757, 0.06691185421884631, 0.06648011294047768, 0.06790807842018125, 0.06884961091755583, 0.06649817791454994, 0.06520170254853015, 0.06067689375196116, 0.06284649541583685, 0.058577289441808726, 0.06126952098277859, 0.05914597977369357, 0.06332467258952487, 0.057384233588513474, 0.055878468872473455, 0.0626550735429691, 0.05836306477245241, 0.058223563505294985, 0.05526084442453358, 0.05660795328168633, 0.0533848489447238, 0.05570016410677863, 0.052253410154685806, 0.053411352781418224, 0.05460695667971913, 0.051183629400147417, 0.053861447934287425, 0.05485730031029152, 0.05130477134769146, 0.04791999825934014, 0.0483061204941128, 0.04967576256259895, 0.045770330684848676, 0.04519799896501028, 0.04884168341720161, 0.04494558678963742, 0.04718005882141689, 0.044031581079688506, 0.04476618649699865, 0.043659136812098494, 0.04084714540554145, 0.04083325983599399, 0.04264434854375026, 0.04191593292166297, 0.03981593097694004, 0.043085993813594785, 0.040142026240416705, 0.03974097740119353, 0.037398707843162474, 0.03773854231051268, 0.035977833013172256, 0.037191637455464936, 0.03843252851401631, 0.03596398131787991, 0.03615279636307123, 0.03480513299854038, 0.034845688106557623, 0.03309281391743273, 0.03143419835645075, 0.030938323252438643, 0.031188237175833397, 0.032819104747632485, 0.03191146987844437, 0.03148931583624702, 0.030030576116479815, 0.03224378252050843, 0.03121553240239787, 0.02776832960601063, 0.02880164314531528, 0.029192425741311222, 0.030026258470062107, 0.029695636324269085, 0.027793155418494687, 0.026486221334551828, 0.026275856554293976, 0.025177531725408646, 0.025483246410664278, 0.023590462400574986, 0.025644099438033356, 0.027068594341187146, 0.02301408553766771, 0.02522345676426047, 0.023035302803220865, 0.021253975071444418, 0.02299421675913869, 0.022151207279476424, 0.02280317421388119, 0.022020146825734655, 0.019744288622618865, 0.021754962591507946, 0.02058648679832431, 0.019481258098625193, 0.018746506298904102, 0.02082187346794223, 0.019484504032284973, 0.018540539403487676, 0.018026220258518744, 0.018277281947350635, 0.019090669363150937, 0.018498932709917426, 0.016855266360646357, 0.017087985687076854, 0.01755107378292555, 0.0162497674808287, 0.016687223960023624, 0.01735119057555918, 0.016141063884457057, 0.015963702109648276, 0.01607858874766638, 0.015593683267140184, 0.017362511261907843, 0.015086121411217502, 0.014080422949388076, 0.01572396905130198, 0.01647015061964409, 0.015150130807850569, 0.014522404045821688, 0.015314155997271045, 0.014406249093742798, 0.014474769565128028, 0.014898723138633151, 0.014346211947822056, 0.013767596740370836, 0.012941224452929376, 0.01388948843457376, 0.013728754877591856, 0.012941669198511817, 0.013923830428598122, 0.01301820403204475, 0.013493596776626622, 0.014167216677105608, 0.012224345361983505, 0.013615870727791479, 0.014401413207968916, 0.013055011124228137, 0.013154573026321495, 0.013493608308438295, 0.012507462098944587, 0.012766335765792492, 0.013181844661488367, 0.013105310207699982, 0.014206890060682409, 0.01374277858861005, 0.012588148265660475, 0.013434472534368141, 0.013608296886800576, 0.012433568900153517, 0.012731193249246266, 0.012850580789437429]\n",
        "train_acc_list_cut = [40.9, 57.2025, 65.81, 71.35, 74.33, 76.565, 78.5275, 80.4, 81.7075, 82.5125, 83.585, 84.51, 85.1225, 85.4725, 85.9375, 86.5425, 87.3725, 87.685, 88.0775, 88.5075, 89.195, 88.7475, 89.465, 89.89, 90.23, 90.4575, 90.6025, 90.64, 91.0475, 91.385, 91.485, 91.8375, 91.855, 92.455, 92.2575, 92.455, 92.4025, 92.6625, 92.92, 93.085, 93.19, 93.35, 93.635, 93.515, 93.77, 93.805, 93.8825, 93.9425, 94.15, 94.27, 94.3925, 94.3325, 94.4625, 94.545, 94.6175, 94.6675, 94.66, 94.7825, 95.1875, 94.845, 94.9125, 95.1, 95.2425, 95.45, 95.28, 95.2525, 95.3375, 95.6875, 95.58, 95.74, 95.56, 95.7275, 95.7375, 95.7325, 95.96, 96.0225, 95.8975, 95.9725, 95.9975, 96.035, 95.935, 96.0, 96.1025, 96.3225, 96.3075, 96.0575, 96.32, 96.3125, 96.3075, 96.26, 96.575, 96.3125, 96.3875, 96.495, 96.4125, 96.585, 96.49, 96.455, 96.4925, 96.4725, 96.6375, 96.6125, 96.715, 96.775, 96.675, 96.7725, 96.7825, 96.7825, 96.775, 96.835, 96.9425, 96.72, 96.7375, 96.8475, 97.035, 97.0625, 96.845, 97.2175, 97.06, 97.1575, 97.055, 97.2325, 97.125, 97.2375, 97.315, 97.2125, 97.1925, 97.2775, 97.3325, 97.45, 97.25, 97.395, 97.23, 97.3725, 97.355, 97.2325, 97.4625, 97.4875, 97.4075, 97.545, 97.5525, 97.7425, 97.545, 97.6125, 97.505, 97.635, 97.7125, 97.72, 97.8225, 97.78, 97.6175, 97.5975, 97.86, 97.7825, 97.7725, 97.7525, 97.8075, 97.7875, 97.9775, 97.96, 98.1425, 97.9675, 98.0225, 97.88, 98.1, 98.2375, 97.9275, 97.995, 98.0375, 98.2175, 98.12, 98.26, 98.185, 98.2625, 98.275, 98.23, 98.285, 98.2475, 98.195, 98.36, 98.48, 98.4025, 98.315, 98.4725, 98.5175, 98.4075, 98.5175, 98.46, 98.535, 98.48, 98.6225, 98.6675, 98.72, 98.5975, 98.6575, 98.68, 98.525, 98.7025, 98.715, 98.78, 98.8, 98.845, 98.7975, 98.75, 98.835, 98.8225, 98.8975, 98.9, 98.925, 99.045, 98.995, 99.0175, 99.0025, 98.97, 98.9425, 99.075, 98.945, 98.9975, 99.07, 99.0825, 99.0525, 99.0625, 99.0075, 99.085, 99.2075, 99.19, 99.2475, 99.1775, 99.265, 99.1775, 99.1425, 99.255, 99.21, 99.215, 99.34, 99.245, 99.29, 99.28, 99.3, 99.3725, 99.3175, 99.3475, 99.4, 99.41, 99.2975, 99.4, 99.44, 99.45, 99.4325, 99.425, 99.4025, 99.4775, 99.44, 99.4275, 99.495, 99.4925, 99.4575, 99.485, 99.5075, 99.5, 99.5075, 99.425, 99.535, 99.5825, 99.4725, 99.5025, 99.5375, 99.545, 99.53, 99.58, 99.5525, 99.5425, 99.5775, 99.58, 99.635, 99.605, 99.59, 99.605, 99.6075, 99.65, 99.59, 99.58, 99.6125, 99.575, 99.5775, 99.59, 99.59, 99.585, 99.6075, 99.5975, 99.61, 99.635, 99.5625, 99.6, 99.6175, 99.5925, 99.6075, 99.6225, 99.5925, 99.615]\n",
        "test_loss_list_cut = [1.4791154242769073, 1.1658613644068754, 0.9872214477273482, 0.8314349047745331, 0.9944802085055581, 0.7577941908112055, 0.7259182379215579, 0.6789853301229356, 0.7040932129455518, 0.6710618870167793, 0.5932643009891992, 0.7190747709968422, 0.5915243693544895, 0.5677886009216309, 0.5548223072214972, 0.48938161738311187, 0.5485320487354375, 0.5049594323846358, 0.5024378439293632, 0.5253199633163742, 0.545006987037538, 0.49844539429568036, 0.4618456284456615, 0.47183522431156305, 0.45087659849396233, 0.5046738835075234, 0.4737275925618184, 0.517040255326259, 0.47144785334792316, 0.43287935023066365, 0.4623851370585116, 0.4191177538301371, 0.40598977762687055, 0.39758752437332007, 0.429098657414883, 0.49839673049842254, 0.4335803312214115, 0.4391304107406471, 0.4258851580604722, 0.4272765200349349, 0.41979367031326775, 0.3996517728023891, 0.39644326382799994, 0.4050762924966933, 0.37220431883123856, 0.41266086810751806, 0.4058349334363696, 0.40598829731911046, 0.4262330528301529, 0.4142030260608166, 0.35964680000951016, 0.3874821408262736, 0.3994222947313816, 0.4099999762411359, 0.48654232843767237, 0.40172827545600603, 0.3953292958344085, 0.4276507317642622, 0.37284702835958217, 0.3759660202113888, 0.39387373701681067, 0.3743562951118131, 0.3970871818593786, 0.41597020795828177, 0.3505105532800095, 0.3577760239190693, 0.3745738009486017, 0.36575221789034107, 0.3922628566056867, 0.3804579684628716, 0.3428110468991195, 0.38194986197012887, 0.35429555196550827, 0.361780740603616, 0.3707258914467655, 0.3518540302786646, 0.3650943885875654, 0.3918004062356828, 0.3596177604756778, 0.37847324338140365, 0.40541056722779817, 0.39005918272688417, 0.3564605245107337, 0.36085661019705517, 0.41434526443481445, 0.3446986880860751, 0.3807661754043796, 0.3560769367444364, 0.3401542149389846, 0.33212360739707947, 0.34799107928064804, 0.40249927549422543, 0.3623131317428396, 0.38307136523572705, 0.3425910604905479, 0.3611237068153635, 0.37625472587120684, 0.3761880048845388, 0.3633357567500465, 0.32559320141997516, 0.3548705181743525, 0.37774981407425073, 0.33313915548445305, 0.3662335353938839, 0.34161262233046036, 0.33970868606356125, 0.33961755310810066, 0.35591573598264137, 0.36459438993206505, 0.34952369747282586, 0.3646538981908484, 0.3669853182155875, 0.346222849586342, 0.3648104143293598, 0.35329970227012153, 0.3857896882521955, 0.3397268560491031, 0.3996561256390584, 0.344681443670128, 0.35413239726537393, 0.3316430355174632, 0.38716104249410993, 0.32942197028594683, 0.35217226400405544, 0.37594665285152723, 0.34076649818239335, 0.3449836048898818, 0.3587528810470919, 0.3401296163284326, 0.3270592946983591, 0.32152744599535493, 0.3485121142260636, 0.33158980649483355, 0.35384547361467455, 0.3337541961971718, 0.31837486522861674, 0.32609030859002586, 0.35300223284129856, 0.31153832630643363, 0.331745189390605, 0.31228505301324627, 0.32915962798685966, 0.307383563322357, 0.3250211654584619, 0.32271280979053885, 0.3312963891444327, 0.3233849070494688, 0.3239633294789097, 0.3277184535996823, 0.3104742332538472, 0.32017407990709135, 0.32736679232573207, 0.31899695456782473, 0.29834681513566, 0.3152913172033769, 0.3275818183452268, 0.33088456924203075, 0.3209677238630343, 0.32485942636864096, 0.3357626995708369, 0.3363142457755306, 0.3152533290506918, 0.3161260941171948, 0.3332944193595572, 0.3038376487697227, 0.34032498092591007, 0.31091189195838154, 0.31926076189626623, 0.31000442501110365, 0.3010354260855083, 0.30018055627617657, 0.3127445231510114, 0.30238260064698474, 0.3182603884724122, 0.31473101213385785, 0.29884613908921615, 0.31703367735011667, 0.2913209448886823, 0.2881702622280845, 0.31681924888604807, 0.29637292385855807, 0.2983416555614411, 0.29558402111258686, 0.29196765764227395, 0.3100640736048735, 0.31831720773177813, 0.29677640637264974, 0.30746520772764957, 0.2865563590503946, 0.30253688547807406, 0.28961686959749533, 0.29738174973032144, 0.29680030708071553, 0.29409151684634294, 0.2892140953601161, 0.3005977722851536, 0.2880074861872045, 0.27800113717211955, 0.2859659475993507, 0.2899065754270252, 0.28989569729642023, 0.29461942848902717, 0.27184448862754845, 0.2677759771482854, 0.2699777761214896, 0.3000830202540265, 0.2926526506301723, 0.2815253124395503, 0.2683991437094121, 0.2708052956982504, 0.2659032362737233, 0.29111536092396023, 0.27172746575331386, 0.2721299055633666, 0.27334422420097304, 0.27823255380874945, 0.2846702391022368, 0.26914313680763485, 0.2824036598299878, 0.26155152982926066, 0.28833567049307157, 0.2757935132595557, 0.28530820940114276, 0.26061007040965406, 0.2689024790932861, 0.2690693513502049, 0.2623352914859977, 0.26749783254499676, 0.2726406627629377, 0.2707079539570627, 0.2750299688947352, 0.2752766144237941, 0.26105580568502224, 0.2643242926348614, 0.2672247627302061, 0.27170711321921287, 0.28075962617427486, 0.25020905593528026, 0.24921881190583675, 0.26484523855055436, 0.2614167183637619, 0.27086676525164255, 0.2653467998474459, 0.25267240150442605, 0.2534334840653818, 0.26007933452536786, 0.2619169997640803, 0.2546714186857018, 0.2537678420732293, 0.27184398217669015, 0.24711663049610355, 0.26054487415129624, 0.2526520066246202, 0.2480248349565494, 0.24464330218638045, 0.2543100225208681, 0.2467851509587674, 0.2543815068806274, 0.2550846805112271, 0.24686935845809646, 0.25780645163753363, 0.24647484842357756, 0.2498779943849467, 0.24236657725104802, 0.25234464791756644, 0.23720906674861908, 0.2444623167378993, 0.24947976245533063, 0.24974906840656377, 0.24106483623574052, 0.24051216914306714, 0.24444433122496062, 0.240749250955974, 0.24741494844231424, 0.24492908486082585, 0.24239608881217015, 0.24886162715810764, 0.24737394035239763, 0.24470583080679556, 0.23975468323200563, 0.25973454749659647, 0.24880992423129988, 0.25671024592239644, 0.2394170629072793, 0.2508298989526833, 0.25579182813061946, 0.23148325047915494, 0.2473917234736153, 0.23667787901962858, 0.24764571838741062, 0.23395111053427564, 0.23682133947746664, 0.2453216223777095, 0.2395489558200293, 0.24663213140602352, 0.2379017510934721, 0.24195825148232375, 0.23623901610321638, 0.24765730121090443, 0.24364126761314236]\n",
        "test_acc_list_cut = [47.45, 60.0, 65.86, 71.15, 68.02, 74.04, 74.75, 76.5, 76.38, 77.35, 79.87, 75.79, 80.02, 80.57, 81.27, 83.4, 81.97, 82.87, 83.07, 82.72, 81.44, 83.43, 84.51, 84.41, 84.85, 83.96, 84.21, 82.61, 85.09, 85.42, 85.35, 86.23, 86.96, 86.92, 85.98, 84.21, 85.86, 85.85, 85.91, 86.93, 86.74, 86.97, 87.32, 87.19, 87.67, 86.55, 86.81, 87.37, 86.84, 87.3, 88.46, 87.92, 87.66, 87.19, 85.59, 87.67, 87.69, 86.81, 88.24, 88.24, 87.26, 88.01, 87.61, 87.88, 88.96, 88.76, 88.97, 88.92, 87.96, 88.36, 88.67, 87.88, 88.96, 88.67, 88.46, 88.99, 88.87, 88.21, 88.92, 88.53, 88.17, 88.26, 89.01, 88.57, 87.5, 89.68, 88.57, 88.93, 89.45, 89.58, 89.15, 88.23, 88.64, 88.35, 89.49, 89.34, 89.23, 88.47, 88.78, 89.82, 89.47, 89.04, 89.77, 88.77, 89.85, 89.69, 89.79, 89.18, 89.03, 89.57, 89.04, 89.3, 89.43, 89.17, 89.25, 88.69, 89.73, 88.08, 89.49, 89.59, 90.02, 88.41, 89.94, 89.77, 88.87, 89.91, 89.6, 89.29, 90.01, 90.18, 90.39, 89.52, 90.2, 89.82, 89.98, 90.55, 90.27, 89.84, 90.81, 90.16, 90.58, 90.17, 91.06, 90.49, 90.02, 90.36, 90.15, 90.41, 90.29, 91.05, 90.46, 90.37, 90.3, 91.15, 90.77, 90.49, 90.34, 90.44, 90.56, 90.45, 90.12, 90.36, 91.18, 90.31, 91.04, 90.24, 90.89, 90.56, 91.21, 91.14, 90.88, 90.8, 90.98, 90.69, 90.8, 91.24, 90.76, 91.31, 91.56, 90.68, 90.91, 91.29, 91.61, 91.43, 91.23, 90.98, 91.61, 91.17, 91.7, 91.2, 92.06, 91.39, 91.26, 91.56, 91.42, 91.42, 91.78, 91.94, 91.69, 91.44, 92.0, 91.36, 91.88, 92.07, 92.17, 91.33, 91.74, 91.8, 92.13, 92.09, 92.31, 91.55, 92.01, 92.26, 92.4, 91.8, 91.83, 92.3, 92.02, 92.56, 92.16, 92.24, 91.79, 92.59, 92.35, 92.7, 92.71, 92.45, 92.55, 92.34, 92.02, 92.56, 92.65, 92.35, 92.53, 92.23, 92.47, 92.65, 92.84, 92.75, 92.85, 92.58, 92.62, 92.95, 92.94, 92.61, 92.87, 92.93, 92.82, 92.37, 92.94, 92.82, 92.81, 93.03, 93.0, 92.85, 93.13, 92.88, 92.75, 93.02, 92.82, 93.14, 92.91, 93.07, 92.55, 93.2, 93.19, 93.02, 93.01, 93.03, 93.35, 93.0, 93.14, 92.89, 93.14, 93.2, 93.12, 93.15, 93.17, 93.22, 93.14, 93.1, 93.0, 93.72, 92.96, 92.94, 93.39, 92.96, 93.36, 93.22, 93.51, 93.28, 93.27, 93.3, 93.29, 93.18, 93.24, 93.64, 93.27, 93.29]\n",
        "train_loss_list_wd5e4 = [1.5027774480965952, 1.0276084239490497, 0.7975675012356938, 0.6686166104988549, 0.5772798815474343, 0.5116038408142309, 0.461431055594557, 0.4285996074493701, 0.3872021212459753, 0.36954540337998265, 0.33349453384122146, 0.3191225348760526, 0.29570154136362164, 0.2778870724736692, 0.26335706169041584, 0.2490621807809455, 0.23208605314786443, 0.22031858939522753, 0.20280856089279675, 0.1948807320465295, 0.18225760984058959, 0.17901248411058238, 0.16944760515000493, 0.15725475339034495, 0.15281064094255525, 0.14829571347552747, 0.1351967308396539, 0.1339439639982324, 0.12818257354747373, 0.1161808082328056, 0.11971007128612111, 0.11157218916728474, 0.11184166256969158, 0.1071621371736637, 0.09870970028396041, 0.09892762057221355, 0.0908012348063552, 0.08706318784933596, 0.08129636609492401, 0.0844175164608624, 0.07711375228608378, 0.0727171496318552, 0.07881306375439365, 0.06988748621207456, 0.06977271015187517, 0.07213704411785443, 0.06465452981178467, 0.059860886116259206, 0.06565443985164165, 0.055741984914904966, 0.05937281503273656, 0.054797398726851604, 0.04783525239057339, 0.057667528927778475, 0.0519976342906253, 0.05075258484479195, 0.0479780873997857, 0.051437813953047216, 0.044275228567897514, 0.05218542266202668, 0.047849885717677045, 0.04727344876809861, 0.04353566757275369, 0.05274154159564751, 0.03900954714669778, 0.038132579750217756, 0.0438798694252468, 0.04052391117319655, 0.0444194049285814, 0.03921739171041896, 0.04322019030456059, 0.03981477512677495, 0.04147126957232627, 0.03451747147622295, 0.03873391680645581, 0.03558630044438159, 0.03961444919863448, 0.03973888016243379, 0.03753592373844915, 0.03485113727642943, 0.036234460864811184, 0.03558692057875875, 0.03258687717030747, 0.03053688010224662, 0.03501797043614256, 0.02845712277083137, 0.03135093330991821, 0.03356501266074638, 0.03350056209170019, 0.03376623985473626, 0.0308699009333032, 0.02893257925968272, 0.027391545217150984, 0.0286959023021471, 0.030725791665781942, 0.026020749267517997, 0.03082717279829204, 0.02599965874775173, 0.03394242989184995, 0.03584211132750986, 0.032864981473753815, 0.03261832850271116, 0.03152383655345383, 0.028458409716550725, 0.0237263294655425, 0.02717581400335335, 0.028603503928362084, 0.02477863742285572, 0.026328867512603348, 0.02738554204796283, 0.024270188131818946, 0.02639033293170027, 0.028431153394275915, 0.0267944870994221, 0.022552848333112014, 0.027474652415665147, 0.022996818839450376, 0.02170721744931044, 0.024480172923168005, 0.025365150559651918, 0.02576148252154644, 0.024583259466499946, 0.026209578144188506, 0.022519710091368172, 0.022190634743682446, 0.017703767737207082, 0.019398964044396966, 0.02643031852926833, 0.02380849324953787, 0.02303797679733092, 0.015461873901173318, 0.016369462731820994, 0.023921750444984333, 0.024145581647188375, 0.024439221988923062, 0.021081448698151536, 0.019269814025595806, 0.01770727836546271, 0.015750449192457307, 0.018357921256341586, 0.016851160442084668, 0.016779405207331545, 0.01922887265860749, 0.01491048913590384, 0.017494613310685175, 0.020450935306655357, 0.019253915843045036, 0.01861391122789143, 0.017561238870908397, 0.01683490170342937, 0.012796001498417827, 0.016780197644567552, 0.014076976357370067, 0.0157260496032457, 0.016004126569547784, 0.017361198594353307, 0.01750588047760018, 0.014100992143118439, 0.012314881729229857, 0.008250951137283621, 0.013473853790245878, 0.010151084203766985, 0.009842516266005918, 0.010620841719269657, 0.009969146477769667, 0.011355855877097613, 0.007406337388096372, 0.008986727931512931, 0.009986997959159387, 0.009125781858848712, 0.007472109663621567, 0.008835015864224551, 0.004292508458737998, 0.005812553818874692, 0.00677099364538924, 0.0067322269050826946, 0.004625315853274061, 0.005215383676132158, 0.006344635733596671, 0.005187604376992669, 0.003839952308653345, 0.003965519353213271, 0.002669242659610467, 0.002117635275329502, 0.002775355486739308, 0.00265736014845546, 0.0022718249013076145, 0.0020042899910497447, 0.0016117119838441976, 0.0014573458997613063, 0.001228732236187917, 0.0012267742422409356, 0.0014500122589130586, 0.0014915024765362493, 0.0012808567260394986, 0.0011165388441382767, 0.0012906825464864531, 0.001357362313477245, 0.0012554813951131897, 0.0011106275753623928, 0.0011766462907799112, 0.0011652772541354281, 0.0014127067940387006, 0.0012243740158691145, 0.0011723753179798421, 0.0012344509843341149, 0.0012428820435963451, 0.0011997493186641259, 0.001336276902967451, 0.0011452813391029217, 0.0011089205116661378, 0.001165861148571673, 0.0012621701129313452, 0.0011624022613475902, 0.0012330106737020725, 0.0011809163343030425, 0.001101714661181235, 0.0011684441624340205, 0.0011685377142329615, 0.0012322912058129478, 0.0011872104088293787, 0.001143343054307119, 0.0011744529526597394, 0.001203257890125088, 0.00115107328663714, 0.0011390011661653273, 0.0012292457904782706, 0.0011242386569215443, 0.0011883258320072802, 0.0011454876748467097, 0.001156560716423066, 0.0011528850767014627, 0.001165562277152456, 0.0011236844305395365, 0.0011447015948914967, 0.0012182490931773268, 0.0011218772364628559, 0.001162500955319157, 0.0011594401151403047, 0.001198594366161587, 0.0011824657069370388, 0.001172395464897263, 0.0012272381267998333, 0.001153445542647173, 0.0012023703809961462, 0.0011459320135334262, 0.0012018852982485589, 0.0011949562754023809, 0.0011886567682230149, 0.0011700176551697043, 0.0011815206049539792, 0.0011973182879798947, 0.0012122210003242206, 0.0011632622707971392, 0.0011874910569734894, 0.0011854940763534828, 0.0011964635884451765, 0.0011862511147771734, 0.001194577806401617, 0.0012562638589407142, 0.0012219592303293534, 0.0011857493995159221, 0.0011942158280217204, 0.0011750017043601828, 0.001190489213080547, 0.0011951297786780082, 0.0011906062601321635, 0.0011730845402048442, 0.0012332784984939206, 0.0012139175384760664, 0.0011773592329021698, 0.0012054313082075394, 0.0011924652807851926, 0.0012579563214404942, 0.001199958441122926, 0.0012173140406632386, 0.0012009810876122083, 0.001210526931255806, 0.0011779946152689143, 0.0011719051533164427, 0.001181355180083432, 0.0011896568855580191, 0.0012324937115670345, 0.0011941396765940534, 0.0011866521274376386, 0.0011868935034494287, 0.0011929624456207093, 0.0012042453618922506, 0.001203572630105665, 0.001169494700063163, 0.0012129276695946893, 0.0012054597970452337, 0.0012071873559750402, 0.0011781047958621797, 0.0011885599540055584, 0.0011778895794518126, 0.0011938168243624079, 0.0011808463227607833, 0.0012280858834437763, 0.001201700613523027]\n",
        "train_acc_list_wd5e4 = [44.9675, 63.1875, 71.8725, 76.5, 79.885, 82.1125, 83.93, 85.2275, 86.5525, 87.015, 88.4975, 88.8775, 89.675, 90.22, 90.8925, 91.28, 91.83, 92.26, 92.8925, 93.18, 93.5725, 93.6975, 94.0725, 94.48, 94.7275, 94.8025, 95.185, 95.255, 95.445, 96.0025, 95.7275, 96.06, 96.1225, 96.2175, 96.5175, 96.4475, 96.815, 96.9275, 97.2, 97.03, 97.3525, 97.5325, 97.2775, 97.6, 97.6325, 97.4725, 97.8175, 97.9575, 97.6775, 98.13, 97.9825, 98.115, 98.465, 98.025, 98.2125, 98.315, 98.395, 98.265, 98.535, 98.2375, 98.3425, 98.4175, 98.59, 98.165, 98.775, 98.7575, 98.4825, 98.6875, 98.425, 98.76, 98.555, 98.6875, 98.565, 98.86, 98.715, 98.8725, 98.665, 98.645, 98.765, 98.8425, 98.845, 98.8475, 98.95, 99.0225, 98.8175, 99.0825, 98.97, 98.91, 98.885, 98.87, 98.975, 99.0475, 99.1425, 99.065, 99.0175, 99.1325, 98.9975, 99.205, 98.9175, 98.85, 98.9625, 98.9, 99.02, 99.0675, 99.295, 99.1675, 99.08, 99.225, 99.1425, 99.1, 99.2525, 99.155, 99.0975, 99.165, 99.3, 99.1275, 99.3225, 99.3125, 99.245, 99.1725, 99.1575, 99.2175, 99.1825, 99.32, 99.29, 99.48, 99.44, 99.17, 99.25, 99.32, 99.5675, 99.5, 99.275, 99.2075, 99.2025, 99.3425, 99.4, 99.5125, 99.5125, 99.495, 99.4975, 99.4925, 99.4125, 99.565, 99.465, 99.365, 99.4025, 99.4375, 99.445, 99.4825, 99.645, 99.4825, 99.62, 99.5275, 99.53, 99.43, 99.485, 99.61, 99.6675, 99.785, 99.5775, 99.705, 99.725, 99.6975, 99.69, 99.705, 99.8125, 99.765, 99.725, 99.77, 99.81, 99.785, 99.92, 99.8575, 99.8125, 99.82, 99.9025, 99.885, 99.84, 99.88, 99.935, 99.93, 99.965, 99.9775, 99.9475, 99.9625, 99.955, 99.98, 99.9875, 99.9925, 100.0, 99.995, 99.985, 99.9925, 99.9975, 100.0, 99.99, 99.9925, 99.9975, 99.9975, 99.995, 99.9925, 99.9925, 99.995, 99.9975, 99.9975, 99.9975, 99.9975, 99.99, 100.0, 100.0, 100.0, 99.9925, 100.0, 99.995, 100.0, 100.0, 99.9975, 99.9975, 99.995, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 99.9975, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 99.9975, 100.0, 99.9975, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
        "test_loss_list_wd5e4 = [1.2136726432208773, 0.9851384208172183, 0.8816822442827346, 0.9342003608051734, 0.7205261165582681, 0.7135021192363545, 0.6295640034766137, 0.6222952057289172, 0.5453219817409033, 0.6333636732041081, 0.5167707955535454, 0.5517654562298255, 0.4426909998247895, 0.41966562520099593, 0.5765619779689403, 0.43953118633620347, 0.3954082934916774, 0.399195172552821, 0.3910487050874324, 0.38879678803908674, 0.4299736600133437, 0.38120186121403415, 0.4164479757788815, 0.3728801941947092, 0.42476696719097184, 0.395566382551495, 0.39108878091166294, 0.390310141670553, 0.4194243029703068, 0.3851487408333187, 0.43473647101015983, 0.3949222198770016, 0.43027733530424817, 0.46044455280032337, 0.4314384918801392, 0.402132544525062, 0.4030761315098292, 0.4495672079958493, 0.39670435857923725, 0.3508086872251728, 0.43815861830982983, 0.4294034951849829, 0.44646918698202204, 0.4012679084192348, 0.3856019262648836, 0.38729274404954306, 0.41598545373240603, 0.4074918993666202, 0.3850251690873617, 0.36644049479237084, 0.4081271881166893, 0.3885994453596163, 0.3874774380952497, 0.38053672147702566, 0.46447082222262515, 0.3927165820628782, 0.38685785233974457, 0.3861157114181337, 0.37443527598169785, 0.3745674545629115, 0.42922606705864774, 0.3947119893906992, 0.4066375641128685, 0.4015842970413498, 0.36048868430566183, 0.35823154091080534, 0.36034491677072983, 0.3813755610321142, 0.35023685667333726, 0.3729325658347033, 0.3793654409767706, 0.3758758772400361, 0.386005234491976, 0.3741716125720664, 0.38768857264820533, 0.4233778007800066, 0.3564016471935224, 0.37897610098500795, 0.40855275943309444, 0.37117944054211244, 0.4877219747138929, 0.32491220892230166, 0.3434305104273784, 0.3840078609653666, 0.3549162511584125, 0.3900517902419537, 0.4008802462227737, 0.37963688882845864, 0.41286554225260697, 0.3738790906116932, 0.39308812431519546, 0.40336327028425434, 0.3903807069681868, 0.3731421724527697, 0.36139429652992683, 0.36114918450011485, 0.3433755166545699, 0.33576442904864684, 0.38036759027951883, 0.389648184368882, 0.3580984584515608, 0.38816530191445653, 0.35682666716696343, 0.3467950366343124, 0.3625389872283875, 0.35996044842125496, 0.3454359777366059, 0.3356767878690852, 0.37700900737243365, 0.35454515490350846, 0.33726465683194656, 0.37725700768111625, 0.37593507747861404, 0.3295244945189621, 0.3762158106399488, 0.38346752543238144, 0.3287717970677569, 0.3532322215128548, 0.3622320907402642, 0.3615872695853439, 0.3733052865047998, 0.33632627308745927, 0.3659961585757099, 0.34503319652020176, 0.3413062731299219, 0.3364775993778736, 0.3670855132462103, 0.34390894257569615, 0.35701036585282675, 0.33068783996225914, 0.3194702649606934, 0.35311298242098166, 0.35698866938488394, 0.35237887407405466, 0.38091013099573834, 0.3747274860551086, 0.3419938689168495, 0.3380954069427297, 0.36431473842527295, 0.37605810872738876, 0.3960524099537089, 0.33081852295730685, 0.36062877370586877, 0.3480613913528527, 0.37270384611962715, 0.34597224412085137, 0.3434583605090274, 0.34766560349660586, 0.3527573428199261, 0.3291342975218085, 0.3357704000759728, 0.3537662517798098, 0.35035382720488534, 0.3609619809291031, 0.36449365525306027, 0.38716287873213806, 0.36449907418293287, 0.3543851946162272, 0.3296925408553474, 0.33976211521444444, 0.36568557687952546, 0.3350432302944268, 0.35040959508358677, 0.33734601575739775, 0.35390251950372625, 0.325316196189651, 0.3150268595995782, 0.34807058489775355, 0.34181052182294147, 0.30128647698254524, 0.3265819955098478, 0.3273799891901922, 0.3255926795964, 0.3264697471374198, 0.31853930699297145, 0.3249078211905081, 0.30419793300613573, 0.31979161029375053, 0.33804947467921653, 0.30469325590360014, 0.32201941085012653, 0.32308190551739707, 0.3179449903531165, 0.29340258193544194, 0.30763082790978347, 0.3099403906660744, 0.3032598841133751, 0.30206969182325316, 0.28878650833157044, 0.298862298172486, 0.2903934564394287, 0.2809895239298857, 0.2981501730936992, 0.2927116846925096, 0.2824138938626157, 0.28358515229406234, 0.2825708462844921, 0.29461616579490374, 0.2836763092988654, 0.28226735923863666, 0.2746177826878391, 0.27155414657502236, 0.2885775946174996, 0.27818745195488387, 0.27747765494675575, 0.28425973069064225, 0.27348717754777474, 0.27310062048933176, 0.2786915644626074, 0.2682647790429713, 0.27626944993492925, 0.28479830703780623, 0.27027927178748046, 0.2733822947344448, 0.2701373880233946, 0.2778020071549506, 0.2666197860542732, 0.26562596329405336, 0.2745143292070944, 0.27233143329997606, 0.26666624231051794, 0.2696480587407758, 0.2745003360736219, 0.27249939272863954, 0.27024260172738307, 0.2686373513144783, 0.27092717569085617, 0.2656354494392872, 0.26311222001721585, 0.26525769873133187, 0.26492436928085134, 0.2699424793731563, 0.2719281587419631, 0.27291557249389115, 0.2715623058095763, 0.26247314769255964, 0.25891310165200054, 0.27840734131728545, 0.27305792734215534, 0.2638986749645275, 0.26656815414375895, 0.2615214324544502, 0.2578285336494446, 0.27850422602665575, 0.2716222282829164, 0.2712610442238518, 0.2711411705594274, 0.26657556308598457, 0.265236472498767, 0.2611617881663238, 0.27041642509306535, 0.25506745703235456, 0.2576812288806408, 0.2628553329577929, 0.2742771508195732, 0.26676151354478883, 0.26692218276896057, 0.27390673850910574, 0.2696215535360801, 0.2623596463022353, 0.2678805201585534, 0.2617303827140905, 0.2673761573254685, 0.2679485789016832, 0.26076228487529335, 0.26153764751138564, 0.2555880120283441, 0.2642273437939113, 0.25984900718248344, 0.26822723297378687, 0.2618812537645992, 0.26794138192376005, 0.2702119716832155, 0.27756816747633717, 0.2612695039450368, 0.2614873691072947, 0.2663590583148636, 0.25799272407459306, 0.2680966036417816, 0.27000968761836425, 0.2693696271015119, 0.25710588410685337, 0.26840231454447855, 0.2647050721924516, 0.2639507615019249, 0.27469288246541085, 0.2502636852898175, 0.26998129741677757, 0.2636978205623506, 0.2510043778279914, 0.26851412955718706, 0.2697306358833102, 0.26649072681423985, 0.2556019111336032, 0.2660581706256806, 0.27122338370809074, 0.2661679719067827, 0.2716638084170939, 0.2666107994280284, 0.25817724770005746]\n",
        "test_acc_list_wd5e4 = [57.49, 65.32, 68.85, 69.25, 75.11, 76.47, 79.04, 78.91, 81.38, 79.78, 83.31, 82.27, 85.39, 86.04, 81.19, 85.57, 86.87, 86.8, 87.39, 87.89, 86.55, 87.98, 87.07, 88.42, 87.53, 87.54, 88.73, 88.01, 87.66, 88.41, 87.62, 88.47, 87.7, 87.31, 87.93, 88.33, 88.35, 87.92, 88.93, 89.82, 88.12, 87.97, 88.15, 89.02, 89.51, 89.51, 88.97, 88.91, 89.73, 90.22, 89.19, 89.69, 89.73, 89.78, 87.74, 89.45, 89.75, 89.88, 89.95, 90.06, 88.65, 89.62, 89.26, 89.31, 90.25, 90.69, 90.2, 90.24, 90.68, 90.65, 90.07, 89.97, 90.12, 90.12, 89.86, 88.49, 90.59, 90.09, 89.83, 90.61, 87.36, 91.49, 90.93, 90.05, 90.69, 89.85, 89.84, 90.3, 89.98, 90.2, 90.1, 89.68, 89.81, 90.13, 90.46, 90.27, 91.08, 91.19, 90.42, 89.93, 90.76, 90.14, 90.89, 90.66, 90.76, 91.03, 91.41, 90.93, 90.67, 90.6, 91.03, 90.24, 90.46, 91.57, 90.73, 90.03, 91.58, 90.88, 90.92, 90.68, 90.39, 91.06, 90.78, 91.26, 91.24, 91.34, 90.73, 90.8, 90.85, 91.48, 91.91, 91.43, 90.71, 90.95, 90.31, 90.52, 91.39, 91.55, 90.75, 90.81, 90.03, 91.94, 90.96, 91.39, 90.87, 91.48, 91.35, 91.49, 91.32, 91.69, 92.11, 91.03, 91.27, 91.16, 91.05, 90.7, 91.13, 91.46, 91.82, 91.81, 91.49, 91.99, 91.79, 91.73, 91.2, 91.93, 92.24, 91.43, 91.47, 92.96, 92.19, 92.09, 92.48, 92.02, 92.34, 92.2, 92.68, 92.39, 91.92, 92.75, 92.45, 92.44, 92.63, 93.15, 92.81, 92.81, 92.69, 92.93, 93.25, 92.81, 93.06, 93.3, 93.25, 93.18, 93.24, 93.27, 93.09, 92.97, 93.34, 93.48, 93.5, 93.69, 92.96, 93.63, 93.47, 93.2, 93.34, 93.33, 93.45, 93.63, 93.43, 93.45, 93.43, 93.28, 93.34, 93.46, 93.25, 93.5, 93.43, 93.59, 93.52, 93.6, 93.48, 93.48, 93.64, 93.39, 93.43, 93.53, 93.86, 93.52, 93.62, 93.52, 93.53, 93.39, 93.49, 93.6, 93.69, 93.23, 93.55, 93.52, 93.32, 93.61, 93.58, 93.53, 93.55, 93.62, 93.58, 93.59, 93.54, 93.48, 93.56, 93.71, 93.59, 93.5, 93.49, 93.58, 93.58, 93.58, 93.57, 93.58, 93.4, 93.52, 93.66, 93.54, 93.58, 93.84, 93.78, 93.32, 93.76, 93.65, 93.72, 93.33, 93.67, 93.51, 93.87, 93.69, 93.5, 93.81, 93.66, 93.74, 93.39, 93.78, 93.7, 93.6, 93.74, 93.79, 93.85, 93.54, 93.78, 93.83, 93.51, 93.47, 93.65, 93.63, 93.65, 93.52, 93.58, 93.48, 93.61, 93.69]\n",
        "train_loss_list_wd1e2 = [1.5167798984545868, 1.027339467034934, 0.8097865730047987, 0.719615878769384, 0.658255658305872, 0.615522557935014, 0.5982994875207115, 0.5846719085789336, 0.5773727887164289, 0.5627464520664641, 0.5591158762145728, 0.5449530482292175, 0.5340716247550976, 0.5340199959925569, 0.5181504983109788, 0.5176402507498622, 0.5080099205810803, 0.49660853408396055, 0.49530376631992695, 0.4915612660848295, 0.4967052273856946, 0.49324704406741327, 0.49331052558490646, 0.48590693801355817, 0.4841031406443721, 0.48545389605787237, 0.4772332070734554, 0.47386771064406386, 0.47569251241394506, 0.4783006565639386, 0.4702882940967243, 0.47064500009289945, 0.47294196457908555, 0.45738911800110305, 0.4716209914928046, 0.4719133836011917, 0.4696217343068351, 0.45638318650257853, 0.46723246879090136, 0.4585420537870913, 0.45578641851489154, 0.465418814565427, 0.4614397684415689, 0.45631032715590236, 0.4486660618370714, 0.4534833081804525, 0.45102608156280394, 0.4530490816782077, 0.45389505011585957, 0.45014332449093414, 0.4494515047096216, 0.450234164349949, 0.4440193151037533, 0.44662567982658413, 0.44600409516892114, 0.4423091958125178, 0.44665925180950106, 0.4360342098120302, 0.43951448350668715, 0.44023644362394804, 0.4460902175964258, 0.43867638192999475, 0.4361597361465612, 0.433228444367552, 0.43528716051921296, 0.4372344275061696, 0.4323961653838904, 0.43725441934201664, 0.43545952991555675, 0.42910580998792436, 0.43043560903674116, 0.43121091731059286, 0.42305412998024267, 0.42636741055086397, 0.4268225592831834, 0.42100020314748293, 0.41761867097391486, 0.4185335077702428, 0.42368483900452575, 0.41616832524442826, 0.41972924578494536, 0.410687822789049, 0.4157286212562372, 0.4089399065358189, 0.40666280982022085, 0.4132289454198112, 0.40205181121064454, 0.40999404700419395, 0.4086824649819932, 0.40080371218177074, 0.4015931721788626, 0.4063145552580349, 0.4066784158110999, 0.40118428522024674, 0.4009731699483463, 0.3969333416547257, 0.39313930387314133, 0.39997971186432213, 0.3931039036653293, 0.38969586989559685, 0.3916486538351534, 0.38877204070076016, 0.38579398788773595, 0.3810211131557489, 0.3882365185803118, 0.38886873219340756, 0.3846093181984874, 0.38404670505287547, 0.3830401858392234, 0.3767934866701833, 0.37473060204959907, 0.37977902538860187, 0.37377259620843223, 0.38002860750824496, 0.36324887853651383, 0.3618837351235338, 0.36737791051308566, 0.3669434030311176, 0.3633396982099302, 0.36406448107367506, 0.3577735069365547, 0.353955817536805, 0.3599519186888259, 0.35612359438270047, 0.35251178142552175, 0.35027061312343366, 0.3470285452497653, 0.3470985202934034, 0.3467607243468586, 0.33784743396047584, 0.34203178333207823, 0.3446982724312395, 0.3408664211678429, 0.3342146307420426, 0.33859410405920715, 0.3276973366737366, 0.33356578295794537, 0.33072001608415913, 0.32800227998735043, 0.32374877627855675, 0.32380120865643597, 0.323726001829385, 0.3161135405397263, 0.32177544154298193, 0.3120566315639514, 0.31642273844431, 0.308783637401395, 0.32058241168340557, 0.31076529688728505, 0.3038082039489533, 0.2967669633678354, 0.30553271728582654, 0.29508178397870294, 0.2989721709070876, 0.2943779766654816, 0.29553699583862536, 0.28911615298769344, 0.29165190915330147, 0.29362286717746966, 0.28382395979124136, 0.2868227225999101, 0.2803722090138414, 0.27424085373505236, 0.28322451868758036, 0.2771561364777172, 0.2667812052816629, 0.2687817446578044, 0.26123926229179856, 0.2610385168474703, 0.2666228299799819, 0.2595565085784315, 0.25669008474380445, 0.2462938559798006, 0.2528521096982514, 0.24889454671654837, 0.2559791601504, 0.24216721735347194, 0.2481234513056545, 0.2393635288356973, 0.23972950325891995, 0.23211169509461133, 0.23631990731905061, 0.2304189305621595, 0.22999336732366976, 0.2226063944994451, 0.22054641979475753, 0.21703065643771388, 0.21686043295140464, 0.21233341681024137, 0.21059229460577614, 0.20723726490911204, 0.20685803997345245, 0.20887376510868438, 0.1978470858531638, 0.2038871747569535, 0.19181662209974693, 0.19145933929057166, 0.1935519773167924, 0.19051632768334673, 0.18543817138614746, 0.17970925828995415, 0.18026368427105224, 0.17701446067410917, 0.17569915164773836, 0.16774458888049323, 0.17213442190862693, 0.1631373402100211, 0.16456650209407836, 0.16205769175062545, 0.1562980950569002, 0.1515140765891098, 0.15663365386545466, 0.15088544290857955, 0.14032981417382867, 0.14692522343783715, 0.14212213777981628, 0.1392525550894463, 0.13808176975947217, 0.12566476896071968, 0.13361202100642955, 0.1256047902753749, 0.1272383151462855, 0.11795103549957275, 0.11710383291966237, 0.11266100632782561, 0.11401164303191554, 0.106656764119197, 0.10556819721961173, 0.10439886295566925, 0.10726791070387386, 0.09460471522884246, 0.0962810536590628, 0.09549028507341592, 0.09219917137259112, 0.0865226152379768, 0.08575040581651008, 0.08219950793745419, 0.07716210633992387, 0.07188480715163219, 0.08065827630650692, 0.06764008765355847, 0.06986326664186324, 0.07430443038551, 0.06353453783609997, 0.06618601354523398, 0.057043209338721375, 0.051298089039782745, 0.054104091200870445, 0.04759680214352882, 0.053709484345187394, 0.046893905104396824, 0.043303748533224905, 0.04337597922228586, 0.0364828174844527, 0.035764870218956434, 0.03372384082918731, 0.03319300545718723, 0.03330101266193885, 0.03186584125978116, 0.027693336871199715, 0.026877068286648573, 0.026169767811561166, 0.0256406356053897, 0.025631253985456005, 0.024362976106401448, 0.023579948727553264, 0.02450076937556457, 0.023537636362611296, 0.02355496631786465, 0.023737981790504136, 0.022986546093330215, 0.023408689955695748, 0.023427624023332, 0.02295914088813261, 0.023163542187156768, 0.023356065636102003, 0.023212027953217584, 0.02309029637434231, 0.022946245367296586, 0.022692412935411587, 0.02339219701842378, 0.023122945366004784, 0.023124087244843522, 0.02343619195893169, 0.022982527528660365, 0.023213101919895163, 0.02317301971248735, 0.022974888207956245, 0.023228626388806503, 0.02310882277263049, 0.023166305644396014, 0.022929255026407518, 0.02320044601377778, 0.02337465347787633, 0.02341003445224069, 0.022785558921698566, 0.023203995150213423, 0.023422795285384494, 0.023028960588355414, 0.023342699217148863]\n",
        "train_acc_list_wd1e2 = [44.33, 63.2525, 71.6725, 75.2775, 77.445, 79.1175, 80.16, 80.8725, 80.74, 81.4475, 81.53, 82.1375, 82.65, 82.6, 82.96, 83.1775, 83.415, 83.7225, 83.915, 84.0275, 83.7325, 83.8675, 83.855, 84.1725, 84.09, 84.1675, 84.56, 84.5075, 84.6425, 84.4775, 84.74, 84.7025, 84.6975, 85.115, 84.7575, 84.705, 84.82, 85.3025, 84.7525, 85.0175, 85.3575, 84.9425, 85.02, 85.27, 85.5525, 85.28, 85.3825, 85.275, 85.2875, 85.52, 85.4475, 85.18, 85.4925, 85.525, 85.6125, 85.75, 85.4725, 85.7925, 85.8875, 85.5825, 85.7075, 85.8275, 86.0, 86.11, 85.95, 85.9425, 85.93, 85.89, 85.92, 86.29, 86.0525, 86.085, 86.41, 86.3275, 86.2675, 86.5075, 86.5475, 86.4525, 86.245, 86.705, 86.37, 86.7625, 86.705, 86.8875, 86.9225, 86.7075, 87.0625, 86.7475, 87.0175, 87.1175, 87.105, 86.8725, 87.09, 87.08, 87.1425, 87.405, 87.4725, 87.09, 87.4975, 87.53, 87.6025, 87.6175, 87.61, 87.685, 87.5425, 87.6175, 87.635, 87.8375, 87.78, 87.9375, 88.0275, 87.7925, 88.2425, 87.6675, 88.5325, 88.5275, 88.3, 88.4075, 88.42, 88.435, 88.6525, 88.795, 88.54, 88.7475, 88.7525, 88.77, 88.8375, 88.9225, 88.9325, 89.41, 89.1775, 89.045, 89.1575, 89.53, 89.3225, 89.855, 89.5225, 89.5275, 89.67, 89.66, 89.7775, 89.81, 90.0825, 89.9125, 90.125, 90.0025, 90.1125, 89.94, 90.265, 90.4675, 90.795, 90.4, 90.8425, 90.7075, 90.79, 90.6825, 91.02, 91.01, 90.935, 91.225, 90.995, 91.265, 91.47, 91.1175, 91.3675, 91.805, 91.6375, 91.945, 92.08, 91.76, 91.995, 92.1175, 92.48, 92.1875, 92.3625, 92.13, 92.595, 92.445, 92.71, 92.67, 93.005, 92.7375, 92.9675, 93.0725, 93.2, 93.335, 93.5175, 93.475, 93.62, 93.7325, 93.7275, 93.7925, 93.66, 94.175, 93.9525, 94.44, 94.3725, 94.22, 94.4575, 94.51, 94.8425, 94.695, 94.77, 94.87, 95.23, 94.9475, 95.2325, 95.23, 95.28, 95.5575, 95.725, 95.595, 95.7125, 96.0975, 95.9275, 95.985, 96.155, 96.23, 96.7125, 96.4, 96.5725, 96.5825, 96.8275, 96.9275, 97.0525, 96.9475, 97.2575, 97.295, 97.4575, 97.2975, 97.66, 97.6175, 97.5925, 97.81, 98.01, 97.9825, 98.08, 98.2975, 98.5225, 98.105, 98.595, 98.47, 98.3475, 98.6925, 98.59, 99.0025, 99.1525, 99.095, 99.27, 99.0025, 99.2475, 99.4225, 99.43, 99.6275, 99.635, 99.6975, 99.715, 99.7175, 99.7575, 99.885, 99.895, 99.9175, 99.925, 99.9225, 99.9675, 99.99, 99.955, 99.9825, 99.985, 99.98, 99.9875, 99.985, 99.9875, 99.995, 99.9925, 99.995, 99.9975, 99.9975, 99.9975, 99.9975, 99.995, 99.9975, 99.9975, 99.9975, 99.9925, 99.995, 99.995, 99.9975, 99.9925, 99.9975, 99.9975, 99.9925, 99.9925, 99.9975, 100.0, 99.99, 99.9975, 99.9975, 99.9925, 99.995]\n",
        "test_loss_list_wd1e2 = [1.3725743022146104, 1.1929742784439763, 0.9703254548809196, 0.8823626479016075, 0.7912342133401316, 0.7744240081762965, 0.9433422028263913, 0.698423402218879, 0.6534588306765013, 0.8720321919344649, 0.9694693926014478, 0.7136559109144573, 1.0214628554597687, 0.8677132740805421, 0.7085553053059156, 0.6858387778076944, 0.8705045671402654, 0.7853590740433222, 0.6123643494859526, 0.8236432565918451, 0.639409672987612, 0.7271271928956237, 0.6982333724257312, 0.8511822268932681, 0.8309068830707406, 0.8227003050755851, 0.7763343317599236, 1.2531145765811582, 0.7305624854715564, 0.7311744048625608, 0.9035976242415512, 0.6800222517568854, 0.5853997695295117, 0.7430345084093795, 0.8785871493665478, 1.108451329454591, 0.6588784128050261, 0.7044716858411137, 0.6049494954604137, 0.6143702955185613, 0.7461452868920339, 0.6528882425797137, 0.8694826870024959, 0.5893921456005, 0.6651792013192479, 0.6350515420678295, 0.6344684543488901, 0.6994746138777914, 0.5725251994555509, 0.6624086785920059, 0.9637637417527694, 0.8048486015464686, 0.6040883234030083, 0.7363305929340894, 0.6205344230313844, 0.6153643851793265, 0.7306718335875982, 0.6487590738489658, 0.7805856805813464, 0.6067438793333271, 0.6581050104732755, 0.6229219368741482, 0.5560055741026432, 0.6660232819333861, 0.7725009058095231, 0.6505244955231871, 0.5968554544297955, 0.7541794965538797, 0.6514349609990663, 0.7917481374137009, 0.6845550778545911, 0.7952898657774623, 0.6756763978849484, 0.7763081943687005, 0.8057050523878653, 0.7995553084566623, 0.6522585726991484, 0.5840945632397374, 0.6991102891632274, 0.6438415571104122, 0.5860964950126938, 0.635802041126203, 0.9848449245283876, 0.7002730075317093, 0.7162983300565164, 0.637317259100419, 1.0616072612472727, 0.5819362533997886, 0.6480541802659819, 0.7091145688974405, 0.532154082497464, 0.7218736040441296, 1.2941748115080822, 0.5242264851739135, 0.5975038005581385, 0.5866859227041655, 0.7909847196144394, 0.5758454048935371, 0.613415573971181, 0.6326792579662951, 0.6269164990775192, 0.5572914639605752, 0.5726913866362994, 0.6187993967080418, 0.6928256652023219, 0.5965810301183145, 0.6376756267457069, 0.5003311121011083, 0.5492256042323534, 0.601341659509683, 0.68496092286291, 0.5872152938118463, 0.5926056911673727, 0.5051948205579685, 0.5469717281528667, 0.7418177361729779, 0.7266407209106639, 0.7612239318557933, 0.4831244179719611, 0.6568866824801964, 0.7060362685330307, 0.693530101564866, 0.5567308860489085, 0.7726584718197207, 0.7781753985187675, 0.761501886422121, 0.6479162256928939, 0.7326241222363484, 0.5910413627383075, 0.7026853682119635, 0.7172112155564224, 0.6026144582259504, 0.5826918271523488, 0.5626278546037553, 0.49854521132722684, 0.6062672617314737, 0.5234693847125089, 0.5272691702540917, 0.6541247133967243, 0.42634944221641446, 0.638887640041641, 0.49163430740561664, 0.8650434718856329, 0.6591806264617776, 0.5614948665039449, 0.5188067898720126, 0.583303042982198, 0.5776442757135705, 0.7945110775247405, 0.4383143160162093, 0.5127309507961515, 0.48725567663772196, 0.5552749682830859, 0.5127951763098753, 0.5552909845792795, 0.6396092316017875, 0.6915487266039546, 0.5918025759202016, 0.6109406363360489, 0.49616090745865543, 0.4317665135935892, 0.5233679349663891, 0.46711384986020343, 0.5448861080634443, 0.705641579401644, 0.48266834171512457, 0.4491133007067668, 0.6736038417755803, 0.41231273925757106, 0.4804556973372834, 0.49244492793384986, 0.5156307778780973, 0.5698083575013317, 0.4579203664502011, 0.5245823471606532, 0.4540955895864511, 0.4462763463394551, 0.47661840538435346, 0.3877083959081505, 0.5359865875938271, 0.47837658622596835, 0.5976610659044, 0.4057789820281765, 0.4568231581132623, 0.5033613733852966, 0.4880643072007578, 0.421935570013674, 0.3864406771674941, 0.3924215535951566, 0.4215288765822785, 0.3856588805778117, 0.39591050393219235, 0.3960021594657174, 0.4162447214881076, 0.4317766793921024, 0.43716210089152374, 0.3752851269290417, 0.3684254226428044, 0.4046018751738947, 0.4570601512736912, 0.42681945275656785, 0.3557315610254867, 0.38338644142392314, 0.45626604632486273, 0.48571689672107937, 0.4053226277420792, 0.38042039244989806, 0.40194732573213454, 0.3663549528846258, 0.40040300088592723, 0.33492799480504626, 0.34661694832994966, 0.3733026383421089, 0.31432308200039444, 0.35983454105974755, 0.3449418092075783, 0.38464838134337076, 0.34258481332018403, 0.3416274746384802, 0.3719708046203927, 0.33954131207134153, 0.3303550594969641, 0.3256681533176688, 0.3663260734911206, 0.305223887479758, 0.3600897785229019, 0.359975767286518, 0.3134645165144643, 0.31159543180013005, 0.2859196728920635, 0.3370714645031132, 0.30697164241271685, 0.3175534585231467, 0.31721039870871776, 0.3941425471743451, 0.34323022165630435, 0.2986761056169679, 0.2926558808812612, 0.298173848963991, 0.29151669446426104, 0.3160338721509221, 0.2907146306920655, 0.29263262648748445, 0.27918298995193047, 0.2787177439166021, 0.27893129094869273, 0.27565936725350876, 0.2982580420337146, 0.32113587752550465, 0.28794968552604505, 0.2873940949764433, 0.2720843562031094, 0.255550967647305, 0.24216599573817435, 0.2502805740584301, 0.2626812101542195, 0.2440197715842271, 0.24417917901956582, 0.2448149986470802, 0.2319961440148233, 0.22655500405574147, 0.2284358123058005, 0.2258316146422036, 0.22443584173540526, 0.21244820348824126, 0.21419980961687957, 0.20566311539917054, 0.20899182690095297, 0.20759060931733891, 0.20997956381002558, 0.2037774644131902, 0.20164045950845827, 0.20583347683843178, 0.209160113353518, 0.2061407360000701, 0.1988746996827518, 0.2016011231307742, 0.20003076869098446, 0.20136920108070858, 0.20208889264849167, 0.19937377427763578, 0.20227546367464186, 0.19644954659139055, 0.2079336524386949, 0.19830586704649503, 0.2019951339763931, 0.2011260839202736, 0.1998056554341618, 0.20407455923813808, 0.20218830621695216, 0.20711066649307178, 0.20228053129549267, 0.1982952310126039, 0.1989742641018916, 0.20098775002775313, 0.19563924860727938, 0.20572516359860385, 0.19656268205446534, 0.19923276595677, 0.1936172448287282]\n",
        "test_acc_list_wd1e2 = [52.13, 58.0, 66.47, 68.48, 71.93, 73.38, 68.66, 76.35, 78.12, 70.27, 67.31, 76.04, 66.26, 71.09, 76.74, 77.08, 70.56, 74.21, 79.8, 73.72, 78.4, 76.32, 76.58, 71.8, 72.87, 73.02, 74.38, 58.91, 74.56, 75.75, 69.39, 78.21, 80.51, 75.74, 71.26, 65.74, 77.96, 76.81, 80.1, 79.77, 74.87, 77.76, 72.43, 80.66, 78.24, 79.29, 79.0, 77.18, 81.23, 77.41, 70.82, 74.14, 79.96, 75.72, 79.75, 80.63, 76.93, 78.66, 73.71, 79.9, 78.5, 79.77, 82.0, 78.63, 74.67, 77.94, 80.15, 75.57, 78.33, 74.1, 77.43, 74.34, 78.18, 75.35, 74.2, 73.88, 78.85, 80.44, 76.92, 78.07, 80.31, 78.92, 68.42, 77.39, 76.56, 79.35, 67.33, 80.82, 78.55, 77.35, 82.3, 77.25, 61.02, 82.77, 79.87, 81.04, 73.94, 81.59, 79.69, 78.21, 79.9, 81.84, 81.13, 80.0, 77.16, 80.57, 79.35, 83.74, 81.48, 80.41, 78.15, 80.5, 80.9, 83.64, 82.31, 75.42, 76.71, 75.99, 84.67, 79.01, 76.87, 77.9, 81.65, 74.49, 73.92, 75.98, 78.22, 76.52, 80.6, 77.42, 76.86, 80.4, 80.66, 81.63, 84.08, 80.38, 83.18, 82.73, 78.42, 86.07, 78.92, 84.8, 74.23, 78.45, 82.33, 83.06, 80.78, 81.68, 75.94, 85.72, 83.08, 84.02, 81.86, 83.38, 81.98, 79.64, 79.1, 80.78, 80.36, 83.85, 86.0, 82.75, 85.08, 82.98, 77.55, 84.19, 85.7, 77.95, 86.42, 84.35, 83.96, 82.9, 81.53, 85.4, 83.22, 85.75, 85.89, 84.67, 87.57, 83.16, 85.08, 81.54, 87.43, 85.54, 83.96, 84.48, 86.54, 87.69, 87.78, 86.26, 87.73, 87.31, 87.28, 86.94, 86.55, 86.06, 88.29, 88.32, 87.19, 85.53, 86.85, 89.14, 88.15, 85.86, 84.75, 87.47, 88.24, 87.6, 88.28, 87.78, 89.53, 89.36, 88.74, 90.36, 88.87, 89.5, 87.99, 89.19, 89.71, 88.6, 89.51, 90.06, 90.36, 88.83, 90.54, 89.04, 89.2, 90.63, 90.62, 91.59, 90.18, 90.87, 90.09, 90.93, 88.63, 89.71, 91.42, 91.72, 91.15, 91.61, 90.94, 91.63, 91.49, 92.14, 92.34, 92.5, 92.22, 91.41, 90.91, 92.27, 92.05, 92.77, 93.17, 93.48, 93.25, 92.79, 93.44, 93.49, 93.54, 93.83, 93.98, 93.84, 94.0, 94.24, 94.31, 94.42, 94.67, 94.34, 94.28, 94.61, 94.58, 94.66, 94.53, 94.5, 94.65, 94.62, 94.71, 94.73, 94.69, 94.61, 94.78, 94.59, 94.83, 94.68, 94.68, 94.76, 94.76, 94.77, 94.61, 94.75, 94.64, 94.51, 94.78, 94.81, 94.72, 94.81, 94.72, 94.92, 94.66, 94.96]\n",
        "train_loss_list_300const = [1.5072595094339536, 1.0231038059670323, 0.7837633473423723, 0.6642771163306678, 0.5742667439265754, 0.5122711343315843, 0.4693711296723673, 0.42363618176204326, 0.38468483099922207, 0.3563146945386649, 0.32989333312922775, 0.3088601354402475, 0.28732080238695723, 0.2773427833050204, 0.255741254994854, 0.2355493075502947, 0.22032112675829055, 0.20569888635660513, 0.19303267375348857, 0.18455393645710078, 0.1760997775430306, 0.1610161360388937, 0.15303532645915643, 0.14379756043132502, 0.13295495837998275, 0.12522198601414603, 0.12198948780425821, 0.10830174501949606, 0.1100872532187845, 0.1019453627054398, 0.10001775390876178, 0.0945176584318804, 0.09056858165552631, 0.08527834246905086, 0.07428395165243563, 0.073358194616894, 0.0680886913840763, 0.06761494047416094, 0.06709703737006972, 0.05811852629716023, 0.05694502567985473, 0.054410113292499285, 0.048847372924831635, 0.05006321483610061, 0.04818616545470521, 0.04477997207657074, 0.04385583504540518, 0.044935471408021524, 0.04291497548661245, 0.03747522448019955, 0.03720155920232304, 0.03781872331246603, 0.03907512170084106, 0.03808571144822426, 0.0337266662997155, 0.03255701906303652, 0.02515740589887356, 0.024542150663780255, 0.02793147737166276, 0.02593546934959928, 0.02561265908809492, 0.027039424086304994, 0.02195272859767639, 0.022481467933937037, 0.023805265842651288, 0.019805631663177174, 0.020343415882660094, 0.022202910110800743, 0.021919588551681024, 0.025648440866278835, 0.024350768256822405, 0.02123939903258099, 0.01514708600285948, 0.013058545996253482, 0.016306638918867626, 0.01685327520953289, 0.016574702121484014, 0.016640731977996603, 0.01489181877258933, 0.015860306949298176, 0.014099853219945703, 0.013995581963445438, 0.016321878165777286, 0.013422961495113753, 0.010429043487289373, 0.015911166273230783, 0.011775680992100685, 0.014917363881686935, 0.018574517294552517, 0.012787643851754746, 0.014939473080523192, 0.01176600590211186, 0.012982209259662748, 0.008858771788844534, 0.009820060386200072, 0.009505460298391053, 0.011240429890194399, 0.010675670454989353, 0.010158317344160542, 0.006973291840534449, 0.006880928409012844, 0.005811905768775168, 0.007408292713742807, 0.007451252999362914, 0.010354434164527717, 0.01092628369954178, 0.012062771040645908, 0.007832180905609038, 0.010386954512029251, 0.010358608426182522, 0.00795716502255174, 0.0063932851670936985, 0.006447436372195698, 0.004587642372207018, 0.007233876957458231, 0.007980881170325053, 0.010550779263485601, 0.012090449684582942, 0.009726078081678914, 0.00813099715300611, 0.006632155526443176, 0.00597371292983393, 0.004589174477389631, 0.003584162614903578, 0.005510094879151764, 0.007690454193111211, 0.006682628281595362, 0.005014813251454181, 0.005320725576812532, 0.007166650853292368, 0.007646939404855167, 0.0068286928395223144, 0.008263683283372191, 0.007336255545465137, 0.007655256824561777, 0.007321817508736698, 0.00431125137086996, 0.003226296991330126, 0.004760576777890367, 0.0063299430897483565, 0.004795740673675748, 0.0033784159969536777, 0.0035024787487254625, 0.005299788615376221, 0.003612780912293712, 0.0032062702902157835, 0.004884716233539565, 0.005524865538374664, 0.0067742976795348445, 0.00882896750121604, 0.005893147790118473, 0.00848282681003614, 0.007084418307778205, 0.0041311039470466075, 0.003654746470935981, 0.0033575089104424215, 0.0021545471597365076, 0.003389105463073925, 0.004264044310640164, 0.004854566317398292, 0.0023904619371291056, 0.002829549484107492, 0.002053513441469564, 0.002324710492194019, 0.0021624633618179965, 0.00212872851480438, 0.0023564651491906493, 0.003220773860256611, 0.002329243483711235, 0.0029083483295660978, 0.001973968918878853, 0.00209470545457433, 0.0020779970437868503, 0.0028365065618229167, 0.003357957487961407, 0.0030591108087931595, 0.002803605341979417, 0.0023873494445828206, 0.0023525519992746557, 0.002546498180350455, 0.0016468311933363184, 0.004213925738022168, 0.003313322557223379, 0.003231858396061877, 0.002629125782985243, 0.0029141006389791914, 0.003536610023096286, 0.004142602702752726, 0.005321672680347896, 0.005746499752380254, 0.0031256999963618884, 0.0032810929011951122, 0.0043974452997825285, 0.005239838635538117, 0.004797415686162696, 0.0032062243664227707, 0.003533768229311691, 0.0029152913773755726, 0.0041634554793835415, 0.0021060470147562338, 0.005407320675629354, 0.006459823155465985, 0.004450791821244248, 0.004911506875814143, 0.003201270101376409, 0.0062131848153570805, 0.003254528715726523, 0.003693553237162894, 0.00231178058878833, 0.002128929863538825, 0.0034310214809528308, 0.0021012531412071865, 0.0037686275719023775, 0.0026402074050687757, 0.0015023963813803039, 0.0016959904317550368, 0.0023601827229728445, 0.0016248808494453031, 0.001744074698825616, 0.0017098960009427912, 0.001745167300260726, 0.001709676564420346, 0.0014354304178802375, 0.0017766182493312826, 0.0017726632421619605, 0.005174195695061486, 0.00404562306744882, 0.0019462628384505156, 0.0021156957895648327, 0.003270384036677395, 0.004819947180601103, 0.004801801557994203, 0.003892041585816488, 0.003026418740098223, 0.0016867717583173738, 0.00243665445885507, 0.001924227454584904, 0.0032565365431517818, 0.0025015898762403913, 0.004080375838660543, 0.0034671720829839744, 0.002242059777764631, 0.00174158359747937, 0.0023047459024574768, 0.0018280270336568435, 0.003222634576515772, 0.002665815578360469, 0.002171202788719592, 0.004139460606425585, 0.005407126804135614, 0.003192231660575842, 0.0023893227233684547, 0.0017126825603354112, 0.0012934664354602478, 0.0024260497488599003, 0.0017768104415345862, 0.001707909157712362, 0.0025417737575718987, 0.0026618135311662788, 0.0013963878702470945, 0.001963627433890095, 0.0015268292372885813, 0.0010931521974983068, 0.0014422832873635296, 0.0014909466068760181, 0.0013153712134022825, 0.0010111237624123271, 0.0009827128673119666, 0.0005205708235784471, 0.0008319928409817137, 0.0007095568490024797, 0.00037128912167129697, 0.0006035467810239169, 0.0011062782132314122, 0.000675376708713361, 0.0006885260622773773, 0.00101789951790438, 0.0006217252514125575, 0.0007492425983336682, 0.0021772955004840527, 0.0016886424110349782, 0.0013806177742215066, 0.001364799225246333, 0.0013202676723717748, 0.0013149851532095538, 0.0010957021063876863, 0.0007910217270622839, 0.0011311705605573214, 0.000875898118397272, 0.0015793223269400525, 0.0013224604617411992, 0.0022564116427801696, 0.0026849516309372387, 0.0020301068258375223, 0.0006494859589330384, 0.0005210057817731313, 0.000972889763034617, 0.0010060088542230168, 0.0009481032836110347, 0.001107927744007216]\n",
        "train_acc_list_300const = [44.7525, 63.5, 72.3975, 76.605, 79.975, 82.355, 83.73, 85.3525, 86.61, 87.5825, 88.51, 89.1375, 90.01, 90.3875, 91.105, 91.8175, 92.1325, 92.8725, 93.125, 93.5025, 93.6875, 94.285, 94.705, 94.91, 95.275, 95.5425, 95.6975, 96.1225, 96.08, 96.3625, 96.4425, 96.6725, 96.75, 96.98, 97.385, 97.405, 97.635, 97.55, 97.615, 97.935, 97.9225, 98.115, 98.2575, 98.245, 98.31, 98.4675, 98.44, 98.46, 98.4925, 98.7, 98.6225, 98.6575, 98.6, 98.63, 98.765, 98.835, 99.135, 99.13, 99.0825, 99.0975, 99.1425, 99.085, 99.25, 99.21, 99.1575, 99.3575, 99.285, 99.2325, 99.23, 99.12, 99.155, 99.3075, 99.505, 99.57, 99.4, 99.43, 99.425, 99.4425, 99.495, 99.4975, 99.4875, 99.5125, 99.4175, 99.5525, 99.6225, 99.455, 99.59, 99.475, 99.355, 99.5525, 99.4975, 99.6075, 99.545, 99.7325, 99.655, 99.6675, 99.655, 99.63, 99.65, 99.7875, 99.8125, 99.82, 99.7425, 99.7475, 99.66, 99.6325, 99.55, 99.74, 99.685, 99.6475, 99.7425, 99.7775, 99.77, 99.86, 99.7625, 99.7475, 99.6, 99.5925, 99.68, 99.7275, 99.775, 99.8, 99.8475, 99.895, 99.82, 99.7325, 99.7725, 99.8375, 99.8025, 99.7625, 99.725, 99.7525, 99.705, 99.7675, 99.735, 99.72, 99.8675, 99.8975, 99.845, 99.7875, 99.8475, 99.8725, 99.8825, 99.8425, 99.865, 99.905, 99.8525, 99.805, 99.7725, 99.695, 99.8, 99.7325, 99.76, 99.8675, 99.8725, 99.9125, 99.9425, 99.9, 99.86, 99.83, 99.935, 99.8975, 99.9425, 99.9225, 99.9325, 99.9325, 99.9275, 99.8825, 99.9275, 99.8975, 99.9425, 99.92, 99.94, 99.8975, 99.895, 99.89, 99.9075, 99.9225, 99.9175, 99.91, 99.9525, 99.855, 99.885, 99.88, 99.915, 99.9, 99.8775, 99.8475, 99.8175, 99.7875, 99.905, 99.8825, 99.8775, 99.8225, 99.8675, 99.875, 99.89, 99.9025, 99.8725, 99.9275, 99.825, 99.77, 99.86, 99.815, 99.905, 99.78, 99.8975, 99.8875, 99.925, 99.9225, 99.8875, 99.9375, 99.8775, 99.91, 99.96, 99.9425, 99.9175, 99.9525, 99.945, 99.9425, 99.945, 99.935, 99.96, 99.9375, 99.9275, 99.8525, 99.8525, 99.945, 99.935, 99.8825, 99.8275, 99.825, 99.8725, 99.8875, 99.9425, 99.9125, 99.925, 99.89, 99.9175, 99.8725, 99.8975, 99.925, 99.9475, 99.9325, 99.93, 99.8975, 99.88, 99.925, 99.8575, 99.8025, 99.895, 99.9175, 99.9475, 99.955, 99.9225, 99.94, 99.9325, 99.92, 99.8875, 99.9625, 99.925, 99.95, 99.965, 99.955, 99.9625, 99.9675, 99.9725, 99.97, 99.9925, 99.9725, 99.9825, 99.9925, 99.9825, 99.9625, 99.9875, 99.9725, 99.9625, 99.985, 99.975, 99.9375, 99.9475, 99.9525, 99.9625, 99.965, 99.97, 99.97, 99.9775, 99.9675, 99.9775, 99.9525, 99.95, 99.9225, 99.905, 99.945, 99.985, 99.985, 99.9675, 99.98, 99.9725, 99.95]\n",
        "test_loss_list_300const = [1.3109248876571655, 0.9655934632578983, 0.8187996443313889, 0.689740173047102, 0.6772998514809186, 0.6437552631655826, 0.5553377157525171, 0.5520454826234262, 0.5120236824584913, 0.5249090968053552, 0.47987785124326054, 0.45728976138030425, 0.4652932938895648, 0.49931400976603546, 0.5139825019655349, 0.46358482577378235, 0.5389019496078733, 0.4064606526229955, 0.4374082180895383, 0.41569147004356866, 0.42417793334284914, 0.43346760355973546, 0.4166975364654879, 0.3910007622045807, 0.4190592248983021, 0.4890744652174696, 0.4130742766811878, 0.44733505535729323, 0.3960334837813921, 0.4205396575263784, 0.4506959509623202, 0.45510448968108697, 0.4337734672464902, 0.4140701989961576, 0.44726073873948446, 0.43756397483469567, 0.4081488196985631, 0.4404116733164727, 0.4069605218836024, 0.4365550541802298, 0.4524993513581119, 0.4810156839180596, 0.4475790313150309, 0.43388396841061266, 0.4388974361781833, 0.4250442619565167, 0.4161135846678215, 0.4599215022370785, 0.4951787300502198, 0.5083512184740622, 0.4711726530443264, 0.5019661293754095, 0.42351165593047685, 0.4634768115946009, 0.472742423415184, 0.4849290226267863, 0.4477674431061443, 0.48274370330043986, 0.4825481849757931, 0.4942950615777245, 0.5367305686202231, 0.4768067407834379, 0.4528346563441844, 0.49772302598892887, 0.43751826391944404, 0.4484408743019345, 0.5320610732217378, 0.47107771444547025, 0.4548007876058168, 0.48090932169292544, 0.47415826705437675, 0.44294909170911284, 0.408969871039632, 0.4602267291349701, 0.4937338570627985, 0.5272085793033431, 0.5100355125680754, 0.46345121894456165, 0.4728180213442332, 0.48767814587188674, 0.4734298460468461, 0.5140062019794802, 0.48423475234568875, 0.4522600043800813, 0.4885755015324943, 0.4898544026515152, 0.48496218667000157, 0.5125900219324269, 0.4700437818146959, 0.45802032928678055, 0.49178935455370554, 0.48068239813364005, 0.4772832633196553, 0.45595825010839897, 0.4741470496865767, 0.508940930796575, 0.47970542651188525, 0.5067461744139466, 0.47261701335635364, 0.4504903811442701, 0.4822925106634068, 0.4916953529360928, 0.4976545244078093, 0.5076064607010612, 0.512105936019481, 0.5002134619634363, 0.4895322826466983, 0.46702025679847864, 0.5068643896640102, 0.5748100429773331, 0.4924963390148139, 0.49676062770282164, 0.5234567566385752, 0.49199416045146654, 0.5275697438400003, 0.5010442611160157, 0.49138097276416004, 0.5251847050989731, 0.5231706929169123, 0.4994612190919586, 0.475140321481077, 0.4973598703553405, 0.46391360763507555, 0.4987307276718224, 0.5342934448507768, 0.490154218258737, 0.5101715593209749, 0.5079800128182278, 0.4876681245580504, 0.5047554036107245, 0.4933491258681575, 0.5017587872622888, 0.5059225696928894, 0.4965471655507631, 0.5347162768999233, 0.5293986288995682, 0.48946061645504796, 0.4993787820957884, 0.5046113696468028, 0.5203786311270315, 0.5163537305367144, 0.4821115889692608, 0.554601405617557, 0.5177212712130969, 0.5428616695011719, 0.5283017215094988, 0.5123567226566846, 0.5303934999281847, 0.5816303280335439, 0.5290483341941351, 0.5672795996069908, 0.579266353309909, 0.5199603319545335, 0.4895725855721703, 0.5205931299472157, 0.5044250222323816, 0.5172668089029155, 0.48294846157107174, 0.5366536256255983, 0.5250458570220803, 0.5189918728568886, 0.5009268411918532, 0.5045282197526738, 0.5237355313346356, 0.511520393098457, 0.525018301002587, 0.510031249515618, 0.5295086546411997, 0.4966339695302746, 0.52833208498321, 0.49447107315063477, 0.5136264913444277, 0.5089094453031504, 0.5464415133376664, 0.5187408701528476, 0.5190243262656128, 0.5516869642689258, 0.5397132114519047, 0.5154607352576678, 0.5146083532820774, 0.5073600661339639, 0.5701480022148241, 0.5735466538728038, 0.5491804146691214, 0.5422320174454134, 0.5446482123453406, 0.5426935783669918, 0.5710887145015258, 0.5220382987887044, 0.5417104106915148, 0.5754493603223487, 0.533619965744924, 0.5762778842185117, 0.5338592627380467, 0.5573781338296359, 0.5375887101775483, 0.5312822029183183, 0.5451974290647085, 0.5270344963933848, 0.5519347873669637, 0.5583887022884586, 0.5747697632524031, 0.53732075910025, 0.544713698799097, 0.54281858609447, 0.5719384611407413, 0.5202449362111997, 0.5084290615742719, 0.5141259058366848, 0.5029577503475962, 0.5330851511864723, 0.5383445264040669, 0.5445871983147874, 0.5439003353254704, 0.5215786695480347, 0.5109690418349037, 0.530744302310521, 0.49466055395859704, 0.5258592778368841, 0.5579611717522899, 0.5201689512292041, 0.5321517630091196, 0.5553653376011909, 0.5559994182254695, 0.5492764958475209, 0.5437155616811559, 0.5355881104552294, 0.5277321902634222, 0.539707180819934, 0.5771049804325346, 0.5852305162933809, 0.5669170301171798, 0.5253017652261106, 0.5338942474579509, 0.5554900363653521, 0.5405757495119602, 0.5658218766315074, 0.5613152086734772, 0.5465129902468452, 0.5272626559945601, 0.5295005417134189, 0.5047299167777919, 0.49300466383559793, 0.5492192791609825, 0.553901323977905, 0.5375248759607726, 0.5450990347168113, 0.5740641949674751, 0.6136820884067801, 0.549185292064389, 0.5533378690103942, 0.5597253417289709, 0.5323915726776365, 0.5313113266720048, 0.5539356519149828, 0.5392684795056717, 0.5539341314306742, 0.5655378125890901, 0.5571187838346143, 0.5493852450877805, 0.541150254162052, 0.5641609287903279, 0.5385742417619198, 0.5644041978860204, 0.5555187009180649, 0.5374931981669197, 0.5337770737801926, 0.5295022774157645, 0.5276252516085589, 0.526966481646405, 0.5239434323356121, 0.5202406580689587, 0.5424046025057382, 0.5546739329642887, 0.5492268700765658, 0.5330030725726599, 0.5302840090250667, 0.559327697074866, 0.5229975948982601, 0.5579739920323408, 0.5483722508519511, 0.5500919239807732, 0.5255736416276497, 0.5679198548763613, 0.5679364106323146, 0.5342161246115649, 0.5500478484208071, 0.5313761402157289, 0.5704804831479169, 0.5673190015780775, 0.5638997756604907, 0.5896313563932346, 0.5433095578528657, 0.5565140822642967, 0.5584272151883645, 0.5537656983242759, 0.548184348058097, 0.5406601837352861, 0.555926272952104, 0.5514698358653467]\n",
        "test_acc_list_300const = [53.98, 65.75, 72.1, 76.14, 77.31, 77.93, 81.02, 81.31, 82.75, 82.79, 83.69, 84.8, 85.35, 83.73, 83.79, 85.27, 84.08, 86.95, 86.6, 86.76, 87.11, 87.3, 88.07, 88.24, 87.9, 86.96, 88.42, 88.0, 88.7, 88.74, 87.84, 87.77, 88.76, 88.95, 88.48, 88.71, 89.77, 88.58, 89.26, 89.81, 89.22, 88.58, 89.44, 89.89, 89.57, 89.96, 89.93, 89.62, 88.5, 89.02, 89.32, 89.09, 90.25, 89.9, 90.15, 89.83, 90.56, 89.5, 90.0, 89.84, 89.55, 90.01, 90.27, 89.89, 90.63, 90.74, 89.52, 90.6, 90.0, 90.07, 90.5, 91.04, 91.19, 90.84, 90.5, 89.92, 90.31, 90.59, 90.56, 90.44, 90.87, 90.43, 90.47, 91.12, 90.58, 90.7, 90.83, 90.28, 91.18, 91.08, 90.76, 90.93, 91.05, 91.44, 91.19, 90.73, 91.14, 90.84, 91.04, 91.48, 90.9, 91.52, 91.12, 91.39, 90.99, 90.96, 91.09, 91.44, 91.0, 90.32, 91.25, 91.42, 91.16, 91.08, 90.75, 91.24, 91.32, 90.74, 90.9, 91.13, 91.6, 91.41, 91.7, 91.22, 90.61, 91.46, 90.95, 91.5, 91.36, 91.43, 91.4, 90.91, 91.13, 91.28, 91.29, 91.08, 91.73, 91.34, 91.19, 91.15, 91.59, 91.75, 91.13, 91.5, 91.47, 91.33, 91.42, 91.52, 90.83, 91.11, 91.09, 90.95, 91.6, 91.74, 91.37, 91.74, 91.61, 91.83, 90.89, 91.51, 91.79, 91.72, 92.0, 91.68, 91.82, 91.75, 91.83, 91.66, 91.79, 91.57, 91.82, 91.86, 91.98, 91.55, 91.74, 91.64, 91.27, 91.43, 91.88, 91.69, 92.03, 91.23, 91.2, 91.33, 91.81, 91.53, 91.45, 91.15, 91.51, 90.91, 91.23, 91.95, 91.46, 91.29, 91.48, 91.44, 91.69, 91.54, 91.51, 91.46, 91.42, 91.05, 91.18, 91.19, 91.56, 91.29, 91.73, 91.76, 91.56, 91.49, 91.62, 91.53, 91.7, 91.61, 91.97, 91.83, 92.04, 91.91, 91.65, 91.62, 91.6, 91.81, 91.53, 91.51, 91.84, 91.74, 91.75, 91.74, 91.9, 91.56, 91.06, 91.42, 91.51, 91.83, 91.39, 91.63, 91.73, 91.66, 91.55, 91.7, 91.4, 91.93, 92.04, 91.7, 91.47, 91.45, 91.64, 91.32, 90.81, 91.54, 91.31, 91.47, 92.03, 91.83, 91.86, 91.87, 91.91, 91.64, 91.84, 91.63, 91.8, 91.87, 92.0, 91.71, 91.79, 91.94, 92.08, 91.91, 92.31, 92.23, 92.29, 92.38, 92.07, 92.0, 91.81, 92.24, 92.44, 92.16, 92.14, 91.99, 91.69, 91.77, 92.03, 91.96, 91.84, 92.05, 91.96, 91.89, 92.32, 91.83, 91.85, 91.69, 92.02, 91.96, 91.93, 92.11, 91.92, 92.17, 91.77, 92.18]\n",
        "train_loss_list_300cosine = [1.5071465310197287, 1.0219155532864337, 0.7846573638839843, 0.6613810150958479, 0.5711964959153732, 0.5125089824770968, 0.4690145238900718, 0.4279545375142996, 0.38901165142036476, 0.3574826151799089, 0.32995736313323243, 0.3070905926033331, 0.29151374520585177, 0.27276346844415694, 0.2528251954637016, 0.2403833875164818, 0.21865639683251945, 0.21172414217798854, 0.1913816854833795, 0.18111336490692803, 0.1749152649943821, 0.16864872572663875, 0.15266839607645527, 0.1438077868649754, 0.131705386099962, 0.12894651846002086, 0.12342969215097138, 0.11197239956773888, 0.10989574266198915, 0.10079097377058988, 0.08904699315302098, 0.09436483237856684, 0.08817791208898583, 0.08477092711939313, 0.08056267116147393, 0.07277504891490404, 0.07563522219146117, 0.07128996417795222, 0.057986204402324874, 0.06231818245218013, 0.05127317121716591, 0.05075172336099628, 0.05728455614774657, 0.05022484378014414, 0.047201051832007145, 0.04299503102446326, 0.04491299623028396, 0.03698193365499711, 0.03486268078765502, 0.03927968754864539, 0.03724308135345007, 0.03970887712738551, 0.03285741158493292, 0.028173236158518744, 0.028217624198978605, 0.029066849599247234, 0.028810474211156678, 0.026399405389732587, 0.025763797607963174, 0.022101726805115827, 0.0206387186558328, 0.023366123397712605, 0.019345413963012873, 0.024243142143205797, 0.02154286678515214, 0.01816358644301042, 0.021265280448992292, 0.015330405338271595, 0.017995145478354284, 0.014342598256486924, 0.015467149512258362, 0.016130026668673175, 0.013699235769971377, 0.011888031450363573, 0.010789586022736494, 0.010900068600833514, 0.009051817458605739, 0.010178615377595523, 0.011202018590738473, 0.014404841935349116, 0.01132253416424867, 0.010159075095953521, 0.008117529459464283, 0.009489624014945505, 0.010137052083415494, 0.008486186185057242, 0.006968282311358831, 0.007533594105837527, 0.008950318053382235, 0.008889065595822946, 0.007539918537286241, 0.0057594785649967666, 0.006437454009137117, 0.005282934063498991, 0.004347972699934572, 0.005166493265034323, 0.004321007760212431, 0.00471395535047535, 0.0032943137242381028, 0.004311655514717282, 0.003059743971269304, 0.0039670848353404575, 0.0030527470893090434, 0.003868208768115135, 0.0034133147897016363, 0.004668654355293675, 0.0059490043992545235, 0.0041872456292248, 0.0038983581991906308, 0.0037588840379914716, 0.003916245875395213, 0.0034797456036773718, 0.0029193548814658767, 0.0028640743580595858, 0.0023959889678485742, 0.0022046099687237837, 0.00213721157060805, 0.001905863403416865, 0.0018298605383502951, 0.0019732695092532088, 0.0012731390455790247, 0.0014643412322186396, 0.0014584561287045482, 0.0013225552577550437, 0.0012107872296070626, 0.0009283569928718647, 0.0009475817591004316, 0.0009087429049996629, 0.0012826446498267045, 0.0011837309814490989, 0.0007089067522449444, 0.0007332131970385923, 0.0012466143751911653, 0.0009211793556768187, 0.000789917121722373, 0.0007207691697882189, 0.0007975770474887959, 0.0006758498559978029, 0.0007351151072098131, 0.0010837652537124284, 0.000724128287233449, 0.0005810306201093711, 0.00047435187363775613, 0.0004135937659276342, 0.0007063415897789764, 0.000588239943727893, 0.0005826207090225168, 0.0003262833067903336, 0.00038663124462745416, 0.0004899335792227164, 0.00045023311883942547, 0.00032505322259691985, 0.0004240722920772437, 0.000434831311982059, 0.0005759489223388422, 0.0003049708239195287, 0.00034293277186921384, 0.00026971405763318425, 0.00031280760296981017, 0.0007162304362590164, 0.0005976533334854375, 0.00044388456952353377, 0.00032143614894833265, 0.0003041571174924992, 0.00035453209945005774, 0.00023628432779605936, 0.00022958813212190958, 0.00018880277622619602, 0.00021482868581873839, 0.0001753957263578302, 0.00041403284841927493, 0.00021361697475566445, 0.0002520289759267065, 0.00019671708842346002, 0.00019921282705291114, 0.00030435736312682684, 0.00029258786672637244, 0.00027827393721283804, 0.0002795331743087422, 0.00024584021705562536, 0.00020301578391222058, 0.00015932558792188886, 0.00035799621748254593, 0.00028251744383299383, 0.00021141435477205867, 0.00022243324366817674, 0.00020670883468984464, 0.00021545639445687408, 0.00016235478455645368, 0.00022787957680421982, 0.00016665564254608194, 0.0001477420311949244, 0.00018142510538173126, 0.00034612401611176406, 0.00015685088123029036, 0.00019157101443428062, 0.00019864877790692532, 0.00020180520096545342, 0.00012184150966041865, 0.00010955794329265264, 0.0002272417015756974, 0.00012062133251651577, 0.00024990162183020963, 0.00019493304157317115, 0.00011319477053125949, 0.0001146485327846252, 0.0001126736392463569, 0.00013831783792320434, 0.0001353729226037266, 0.0001630674412422994, 0.0001457879433627867, 0.00012657002445250193, 0.00020756496892448712, 0.0001576078423556483, 0.00016432452238864054, 0.00013326239761994157, 0.000126308732199333, 0.00011329932245867901, 9.373737983756487e-05, 0.00012264910064071485, 9.72924513225386e-05, 9.851183427198616e-05, 0.00014160107466290727, 0.00019542356384294292, 0.00010866884136099055, 0.00012103338623344784, 0.00010338596340528902, 0.00012947641171964864, 0.00016585586701756884, 0.00012378471780928916, 0.00012914159678432927, 8.873532887934217e-05, 0.00012277717895865415, 0.00010324342793536724, 0.00011429319050957435, 0.00014108263730566994, 0.00011344413554959498, 0.0001314912391754855, 0.00014366292956832224, 0.00010670398796356165, 9.395798729230545e-05, 9.690826291712445e-05, 8.27918457593515e-05, 7.49523732641198e-05, 0.000116572610751607, 0.00010861793663089236, 0.00010167052199556757, 0.0001307339418356521, 0.00010273090034757616, 0.00012697175096751327, 0.00012768910120719, 7.014640353397078e-05, 9.454710011857551e-05, 0.00011274885802570828, 0.000126904926870751, 9.48965783575255e-05, 9.193459106337249e-05, 8.93238301681306e-05, 0.00010053854063013211, 0.00010620467252830129, 0.00014532459377346714, 8.020320023135105e-05, 7.178885715230178e-05, 0.00011935103254135713, 9.011012685912722e-05, 8.608197714011839e-05, 9.837138899922847e-05, 0.00010366790116276302, 8.538921241570249e-05, 0.0001675107899030607, 7.792623628388836e-05, 6.928247518943407e-05, 8.20774587304151e-05, 0.00011248917587745207, 7.720731971381645e-05, 6.833156445040754e-05, 0.00010231887682267187, 9.609293597030557e-05, 7.292958810775106e-05, 0.00010774314482761675, 9.255914947628301e-05, 0.0001237650751620048, 9.64451171900979e-05, 0.0001169090761506665, 0.0001072614105620238, 9.686175892860217e-05, 0.00011957684801773822, 0.00011583886435356528, 0.0001275175088925282, 6.546866618231805e-05, 9.038513419941541e-05, 0.00010006681310687346, 9.007086580128472e-05, 0.00022282703456296465, 8.881661869528004e-05, 8.626907432722472e-05, 9.961434439616733e-05, 0.0001082291823644854, 0.00010509098167338485, 7.50494904148657e-05]\n",
        "train_acc_list_300cosine = [44.7275, 63.7, 72.3575, 76.9475, 80.11, 82.24, 83.6775, 85.0225, 86.4575, 87.51, 88.3775, 89.1875, 89.8275, 90.4725, 91.27, 91.6125, 92.195, 92.6575, 93.2125, 93.6125, 93.7775, 93.95, 94.65, 94.79, 95.3675, 95.5025, 95.58, 96.0475, 96.075, 96.4175, 96.79, 96.66, 96.8425, 97.0175, 97.1425, 97.4075, 97.345, 97.375, 97.945, 97.73, 98.185, 98.1775, 98.0075, 98.175, 98.36, 98.465, 98.4525, 98.72, 98.815, 98.635, 98.6625, 98.58, 98.825, 99.075, 99.0425, 98.985, 98.965, 99.1025, 99.0875, 99.26, 99.2775, 99.1925, 99.3375, 99.1475, 99.2425, 99.425, 99.2375, 99.475, 99.38, 99.505, 99.465, 99.4675, 99.5625, 99.605, 99.64, 99.6425, 99.71, 99.66, 99.625, 99.4775, 99.625, 99.6725, 99.745, 99.7, 99.67, 99.7325, 99.785, 99.7425, 99.675, 99.6725, 99.7625, 99.83, 99.7925, 99.8025, 99.8675, 99.8425, 99.8625, 99.85, 99.905, 99.8775, 99.9, 99.8825, 99.895, 99.8625, 99.9025, 99.8625, 99.7975, 99.8725, 99.87, 99.89, 99.88, 99.87, 99.9125, 99.915, 99.9375, 99.9475, 99.94, 99.945, 99.94, 99.9425, 99.9775, 99.96, 99.9575, 99.965, 99.9725, 99.975, 99.98, 99.9825, 99.9675, 99.965, 99.9925, 99.985, 99.9675, 99.975, 99.9725, 99.975, 99.98, 99.985, 99.9875, 99.9725, 99.9825, 99.9925, 99.9925, 99.9975, 99.98, 99.99, 99.9875, 99.9975, 99.995, 99.985, 99.99, 100.0, 99.9925, 99.9875, 99.9825, 100.0, 99.9975, 99.9975, 99.9925, 99.9825, 99.9875, 99.99, 99.9975, 99.9975, 99.9925, 100.0, 99.9975, 99.9975, 100.0, 100.0, 99.985, 99.9975, 99.9975, 99.995, 99.9975, 99.9975, 99.9975, 99.9975, 99.9925, 99.995, 99.9975, 100.0, 99.9875, 99.9925, 100.0, 99.995, 100.0, 99.995, 100.0, 99.9925, 100.0, 100.0, 99.9975, 99.99, 100.0, 99.9975, 99.995, 99.995, 100.0, 100.0, 99.9925, 100.0, 99.995, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 99.9975, 99.995, 99.9975, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9925, 100.0, 100.0, 100.0, 99.9975, 99.9975, 100.0, 99.9975, 100.0, 99.9975, 100.0, 99.9975, 100.0, 99.9975, 99.9975, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 99.9975, 100.0, 100.0, 99.9975, 100.0, 99.9925, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
        "test_loss_list_300cosine = [1.3360588429849358, 1.0448881915852994, 0.8597957295707509, 0.6716653414164917, 0.6908234833162042, 0.5932456066336813, 0.5461285729197007, 0.5097410641139066, 0.5401990885221506, 0.5333668193485164, 0.4556268532819386, 0.4484600973657415, 0.4620721623112884, 0.4327629621647581, 0.47597266639335245, 0.48141292925876905, 0.49725772666780255, 0.40901502231253856, 0.4323547872938687, 0.4539363452150852, 0.4590581006641629, 0.40523579641233515, 0.4201337242428261, 0.418421223005162, 0.41465936016432847, 0.43959930557993393, 0.3880773431892636, 0.397792113732688, 0.4114573869524123, 0.3806772181127645, 0.40738119260419775, 0.41724546894996983, 0.3961485907246795, 0.40635761147058463, 0.41718924026700516, 0.4624185173571864, 0.4003025793199298, 0.4194344795202907, 0.43098732470711576, 0.42882050819034817, 0.4735664082101629, 0.4482132949029343, 0.4824381698913212, 0.489483314035814, 0.42275018914590906, 0.46854421854773654, 0.46698710643037966, 0.45736848336609104, 0.48327861801732946, 0.42990076523038406, 0.4146351450228993, 0.4377061692596991, 0.4491257427991191, 0.4464895281987854, 0.5250044391502308, 0.4606155977596211, 0.48777404917946343, 0.46350483313391483, 0.45923378441152696, 0.47999939944925185, 0.4535947281725799, 0.45670295101177844, 0.4861161712604233, 0.5072417436521265, 0.45164442854591563, 0.4723219115145599, 0.465178365948834, 0.48657907800206657, 0.4763214318435403, 0.4628955114491378, 0.4531608254094667, 0.4590076025150999, 0.48202957307236105, 0.47070856392383575, 0.4524376158472858, 0.5099793840435487, 0.4644981126619291, 0.4989790280785742, 0.4910671897704088, 0.5239907608756537, 0.4774412533527688, 0.4662430659879612, 0.47549027996727183, 0.49829778188391577, 0.4727332616531396, 0.48873594582458085, 0.4645166827153556, 0.4946923785949055, 0.5139467286158211, 0.504527775924417, 0.4836999862631665, 0.4878289546770386, 0.49920391819522353, 0.48420851479602767, 0.4741327323113816, 0.4969299920374834, 0.492643963309783, 0.4669509222613105, 0.5004429721002337, 0.5008861348976062, 0.48978376954416686, 0.48975364958183676, 0.4894806569135642, 0.4939129582688778, 0.5057695177914221, 0.483693698161765, 0.49143673158899137, 0.508361778493169, 0.49146506284611136, 0.5179333079464828, 0.4985507955845398, 0.48249870953680596, 0.4882995514552804, 0.48072621575261976, 0.4871305170692975, 0.495542240859587, 0.502074130728275, 0.4848601612486417, 0.46474564952563635, 0.45505768362479876, 0.480818674534182, 0.48904268066339857, 0.48706334348343594, 0.5078222957215731, 0.48777740590180024, 0.48671044654484036, 0.4677445718759223, 0.4843851542925533, 0.47715435280830043, 0.4916676714827743, 0.48873102721534195, 0.4853220989432516, 0.495768607794484, 0.48384790348855755, 0.494273115373865, 0.520792075538937, 0.48109641991838625, 0.4784795025104209, 0.5150823619546769, 0.49928266564501994, 0.508841446871999, 0.4865191377039197, 0.49814116068278685, 0.4979177566268776, 0.509591865388653, 0.5287647111506402, 0.5030483018748367, 0.47968817134446734, 0.5014746417350406, 0.5026743098904815, 0.5065745726416383, 0.4991808756242824, 0.49776733959022956, 0.4859552224980125, 0.505850138354905, 0.507838894294787, 0.4913963109632082, 0.4985096752643585, 0.5035310268779344, 0.49438839943348606, 0.5127879598095447, 0.49104869393985484, 0.4965999275445938, 0.500093090760557, 0.5078368173747123, 0.49182856611058684, 0.4928507318225088, 0.5030448830957654, 0.4847751912436908, 0.5120558350146571, 0.4888622204336939, 0.49651806179103974, 0.49612243831912173, 0.4962354428783248, 0.513516125611112, 0.49529338731795924, 0.4980339683309386, 0.5014159933109826, 0.49898972069915337, 0.507745862573008, 0.52068718268147, 0.5003404364555697, 0.525249100938628, 0.5042423366368571, 0.5168220468714267, 0.5160682231565065, 0.5124318371467953, 0.5218248301291768, 0.4837766302914559, 0.5026627190505402, 0.5024134409201296, 0.512225415887712, 0.5186735810358313, 0.5077592483426951, 0.4826306046187123, 0.512151868848861, 0.5008148086976402, 0.4929636383924303, 0.4970756280648557, 0.5172987021977389, 0.4966934053580972, 0.5119075952451441, 0.4916618470149704, 0.5191661170389079, 0.5202390191298497, 0.5187712847432003, 0.5312336112690877, 0.5149811487409133, 0.5015346613488619, 0.4897740053225167, 0.5123436318922646, 0.4950245019001297, 0.5100116333629512, 0.517671759935874, 0.49631667703012877, 0.5063869094924082, 0.5044590200804457, 0.5116969626161116, 0.5092341914018498, 0.5033747812237921, 0.5193329711880865, 0.5127834038266653, 0.5309269350918033, 0.49558807419070716, 0.513400316049781, 0.5085111194396321, 0.510913455599471, 0.4971718424105946, 0.5032308197851423, 0.5266274438251423, 0.49809465491319005, 0.5013407922432392, 0.4893049780703798, 0.5038147557385361, 0.5071697546334206, 0.5146695088736618, 0.49518201302123976, 0.5041418398105646, 0.5182757000379925, 0.5224867431046087, 0.4978565979230253, 0.507878503939019, 0.49611272823206987, 0.5108789631837531, 0.5073691453737549, 0.5223453206163419, 0.5107548187805128, 0.5007183399004272, 0.5212629056429561, 0.49115426876122437, 0.5297577135925051, 0.5027483824310424, 0.5041316181798524, 0.4906521471618097, 0.493595665768732, 0.502438643687888, 0.4948954885896248, 0.4982064822806588, 0.5073866029328937, 0.5064849340462987, 0.5071891489662702, 0.5056039866390107, 0.5256269764673861, 0.4950309816417815, 0.5119404613594466, 0.4975051161231874, 0.4961673260866841, 0.5196594443125061, 0.4737729119914996, 0.5019824165332166, 0.4959078891367852, 0.5141448318203793, 0.49820214887208575, 0.5068159803182264, 0.4858330869221989, 0.496248337852804, 0.4940428052899204, 0.503951388069346, 0.49354483621029915, 0.4873908001788055, 0.5022429193876967, 0.50107072425794, 0.49141834751714636, 0.50169266326518, 0.4897840660584124, 0.5037467987099781, 0.501852917708928, 0.5113212121061131, 0.5046284511873994, 0.4889931109132646, 0.507928684731073, 0.4926022017681146, 0.5067250309865686, 0.510816790823695, 0.5253809748948375, 0.5128096621247786, 0.4980110616623601, 0.5035155990832969, 0.5060317163603215, 0.49634331974047646]\n",
        "test_acc_list_300cosine = [52.55, 63.48, 70.79, 76.77, 77.07, 79.66, 81.7, 82.61, 82.36, 83.15, 84.72, 85.19, 85.13, 85.7, 85.08, 84.65, 84.64, 87.17, 86.56, 86.12, 86.83, 87.48, 87.37, 87.5, 88.14, 87.33, 89.04, 88.78, 88.32, 89.27, 88.82, 88.61, 89.75, 88.78, 89.11, 88.37, 89.51, 89.38, 88.67, 89.45, 88.48, 89.32, 89.16, 88.72, 89.82, 89.02, 88.77, 89.55, 89.58, 90.05, 90.18, 90.23, 89.78, 90.12, 89.23, 89.82, 89.9, 89.95, 89.87, 90.25, 90.38, 90.39, 90.25, 89.66, 90.88, 90.49, 90.4, 90.26, 90.13, 91.0, 90.89, 90.98, 90.93, 90.94, 91.42, 90.54, 91.11, 90.89, 90.63, 90.02, 91.17, 90.81, 91.24, 90.97, 91.14, 91.35, 91.47, 91.32, 90.83, 90.87, 91.23, 90.95, 91.06, 91.39, 91.41, 90.98, 91.19, 91.65, 91.43, 90.97, 90.93, 91.36, 91.17, 91.47, 91.56, 91.59, 91.49, 90.95, 91.39, 91.46, 91.51, 91.64, 91.6, 91.65, 91.39, 91.51, 91.58, 92.1, 91.79, 91.97, 91.75, 91.74, 91.57, 91.96, 91.91, 92.0, 92.15, 92.02, 91.96, 91.79, 91.77, 92.07, 91.99, 92.41, 92.06, 91.7, 92.06, 92.22, 91.85, 91.93, 92.2, 92.1, 92.04, 91.91, 91.85, 91.94, 91.89, 92.15, 92.18, 91.94, 92.03, 92.19, 92.17, 92.45, 92.43, 91.98, 92.28, 92.25, 92.4, 92.24, 92.02, 92.17, 92.01, 92.24, 92.28, 92.14, 92.37, 92.14, 92.25, 92.3, 92.22, 92.12, 92.18, 92.11, 92.29, 92.04, 92.02, 92.2, 92.37, 92.34, 92.25, 92.27, 91.84, 92.13, 92.18, 91.97, 92.19, 91.85, 91.98, 92.21, 92.11, 92.25, 92.17, 92.13, 92.4, 92.37, 92.21, 92.18, 92.44, 92.29, 92.36, 92.11, 92.35, 92.51, 91.96, 92.18, 92.12, 92.41, 92.3, 92.57, 92.18, 92.45, 92.11, 92.13, 92.52, 92.18, 92.1, 92.1, 92.24, 92.59, 92.23, 92.27, 92.27, 92.48, 92.31, 92.38, 92.04, 92.33, 92.33, 92.15, 92.27, 92.48, 92.52, 92.61, 92.2, 92.3, 92.31, 92.55, 92.25, 92.12, 92.24, 92.12, 92.47, 92.29, 92.22, 92.34, 92.4, 92.34, 92.23, 92.56, 92.08, 92.09, 92.33, 92.22, 92.44, 92.34, 92.41, 92.41, 92.2, 92.35, 92.42, 92.43, 91.96, 92.21, 92.27, 92.41, 92.21, 92.36, 92.66, 92.32, 92.33, 92.37, 92.2, 92.16, 92.31, 92.34, 92.35, 92.43, 92.49, 92.47, 92.15, 92.23, 92.61, 92.26, 92.76, 92.44, 92.32, 92.18, 92.38, 92.38, 92.28, 92.54, 92.5, 92.4, 92.13, 92.37, 92.31, 92.37, 92.12, 92.62]\n"
      ],
      "metadata": {
        "id": "ilmYYfIElcAU"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list_01)), train_acc_list_01, 'b')\n",
        "plt.plot(range(len(train_acc_list_001)), train_acc_list_001, 'r')\n",
        "plt.plot(range(len(train_acc_list_0001)), train_acc_list_0001, 'g')\n",
        "\n",
        "plt.plot(range(len(test_acc_list_01)), test_acc_list_01, color='b', linestyle='--')\n",
        "plt.plot(range(len(test_acc_list_001)), test_acc_list_001,color='r', linestyle='--')\n",
        "plt.plot(range(len(test_acc_list_0001)), test_acc_list_0001, color='g', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Combined accuracy\")\n",
        "plt.legend(['train with lr = 1e-1', 'train with lr = 1e-2','train with lr = 1e-3',\n",
        "            'test with lr = 1e-1','test with lr = 1e-2', 'test with lr = 1e-3'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ubm3Y5CSnZ0D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ddf5c83c-16d9-4d8e-a4a7-f267469fec90"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list_01)), train_loss_list_01, 'b')\n",
        "plt.plot(range(len(train_loss_list_001)), train_loss_list_001, 'r')\n",
        "plt.plot(range(len(train_loss_list_0001)), train_loss_list_0001, 'g')\n",
        "\n",
        "plt.plot(range(len(test_loss_list_01)), test_loss_list_01, color='b', linestyle='--')\n",
        "plt.plot(range(len(test_loss_list_001)), test_loss_list_001,color='r', linestyle='--')\n",
        "plt.plot(range(len(test_loss_list_0001)), test_loss_list_0001, color='g', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Combined loss\")\n",
        "plt.legend(['train with lr = 1e-1', 'train with lr = 1e-2','train with lr = 1e-3',\n",
        "            'test with lr = 1e-1','test with lr = 1e-2', 'test with lr = 1e-3'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QnOTaNoesBEb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6ae6f65e-824e-44b4-d6f7-32d0cd4fdbb3"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}