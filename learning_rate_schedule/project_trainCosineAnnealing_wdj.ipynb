{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f4747ad51be44c2997a4110d54e6f7fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a5bf204bbdf434db7785ac3ab008ac8",
              "IPY_MODEL_e896ac8cfecb4afab488eb1d7c99e0fd",
              "IPY_MODEL_ea785cb9f42a41908669bc2602632710"
            ],
            "layout": "IPY_MODEL_ca63f6af6ca84cf09581e126b665b62e"
          }
        },
        "2a5bf204bbdf434db7785ac3ab008ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9dcf5fde57d4522b37c529521299a96",
            "placeholder": "​",
            "style": "IPY_MODEL_434513c1caaa493c9789fad3ea4ece93",
            "value": "100%"
          }
        },
        "e896ac8cfecb4afab488eb1d7c99e0fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9025f21020ab492ab29c886ab59950ce",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11e7717b5e564e75acdc8d2e93133252",
            "value": 182040794
          }
        },
        "ea785cb9f42a41908669bc2602632710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b05ae5408c9543dbb1904f5317662ddf",
            "placeholder": "​",
            "style": "IPY_MODEL_d3994261970540aaa7a6f1b430a77ebc",
            "value": " 182040794/182040794 [00:04&lt;00:00, 57758796.77it/s]"
          }
        },
        "ca63f6af6ca84cf09581e126b665b62e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9dcf5fde57d4522b37c529521299a96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "434513c1caaa493c9789fad3ea4ece93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9025f21020ab492ab29c886ab59950ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11e7717b5e564e75acdc8d2e93133252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b05ae5408c9543dbb1904f5317662ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3994261970540aaa7a6f1b430a77ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74dade94af524a9b9e2a423c1b01c4b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26774894c1114ba0b592f606e6faceff",
              "IPY_MODEL_a9b667e33363460a828d9336296ed71d",
              "IPY_MODEL_160a89ee0acb46548cbf2469002743a7"
            ],
            "layout": "IPY_MODEL_c2e8e6eb6d634b0eb3eb57e9572b7f1e"
          }
        },
        "26774894c1114ba0b592f606e6faceff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f01af65c5cd749d9a9cfa7d2a1b03204",
            "placeholder": "​",
            "style": "IPY_MODEL_4a49fa54b98f49208245b6260ed71cca",
            "value": "100%"
          }
        },
        "a9b667e33363460a828d9336296ed71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ae1e4e429254dc78cfb85ffb71f436c",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4c9713188d545188e86bd9ed4bdc301",
            "value": 64275384
          }
        },
        "160a89ee0acb46548cbf2469002743a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e2abd0d87c9464fb8235ba75b788804",
            "placeholder": "​",
            "style": "IPY_MODEL_a6429898e6b0486cb77514b4ad772f6a",
            "value": " 64275384/64275384 [00:02&lt;00:00, 55036947.99it/s]"
          }
        },
        "c2e8e6eb6d634b0eb3eb57e9572b7f1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f01af65c5cd749d9a9cfa7d2a1b03204": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a49fa54b98f49208245b6260ed71cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ae1e4e429254dc78cfb85ffb71f436c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4c9713188d545188e86bd9ed4bdc301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e2abd0d87c9464fb8235ba75b788804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6429898e6b0486cb77514b4ad772f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzGPN_g_nabA"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    #transforms.RandomErasing(value=get_mean_color()),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "f4747ad51be44c2997a4110d54e6f7fa",
            "2a5bf204bbdf434db7785ac3ab008ac8",
            "e896ac8cfecb4afab488eb1d7c99e0fd",
            "ea785cb9f42a41908669bc2602632710",
            "ca63f6af6ca84cf09581e126b665b62e",
            "b9dcf5fde57d4522b37c529521299a96",
            "434513c1caaa493c9789fad3ea4ece93",
            "9025f21020ab492ab29c886ab59950ce",
            "11e7717b5e564e75acdc8d2e93133252",
            "b05ae5408c9543dbb1904f5317662ddf",
            "d3994261970540aaa7a6f1b430a77ebc",
            "74dade94af524a9b9e2a423c1b01c4b4",
            "26774894c1114ba0b592f606e6faceff",
            "a9b667e33363460a828d9336296ed71d",
            "160a89ee0acb46548cbf2469002743a7",
            "c2e8e6eb6d634b0eb3eb57e9572b7f1e",
            "f01af65c5cd749d9a9cfa7d2a1b03204",
            "4a49fa54b98f49208245b6260ed71cca",
            "8ae1e4e429254dc78cfb85ffb71f436c",
            "e4c9713188d545188e86bd9ed4bdc301",
            "5e2abd0d87c9464fb8235ba75b788804",
            "a6429898e6b0486cb77514b4ad772f6a"
          ]
        },
        "id": "CHgyN-h7wG9v",
        "outputId": "30c2f269-4c64-4b72-e5a4-cd17ed711a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4747ad51be44c2997a4110d54e6f7fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74dade94af524a9b9e2a423c1b01c4b4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBHcId1WwXXZ",
        "outputId": "b08624fa-79b8-445f-e22b-d84a700a459a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "metadata": {
        "id": "2h9M8BQcwbKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "metadata": {
        "id": "G1laNDB_wdqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PAECJE7gwf17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "metadata": {
        "id": "2oGZBboFwpyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "_iHqpCMdw0YN",
        "outputId": "07c7f711-7b6e-4c75-dffa-e624de7c5857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsoNo8QIxS1a",
        "outputId": "6258d91f-8762-4603-879e-8493aca0105d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7tYsoawxRl6",
        "outputId": "c8046cf5-6dfa-4c77-e882-73dfb33bddcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3446, accuracy : 16.62\n",
            "iteration : 100, loss : 2.2784, accuracy : 18.96\n",
            "iteration : 150, loss : 2.1875, accuracy : 22.05\n",
            "iteration : 200, loss : 2.0291, accuracy : 27.65\n",
            "iteration : 250, loss : 1.8490, accuracy : 34.45\n",
            "iteration : 300, loss : 1.6793, accuracy : 40.77\n",
            "iteration : 350, loss : 1.5395, accuracy : 46.03\n",
            "Epoch :   1, training loss : 1.4923, training accuracy : 47.74, test loss : 0.8128, test accuracy : 75.22\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.5750, accuracy : 81.09\n",
            "iteration : 100, loss : 0.5543, accuracy : 81.94\n",
            "iteration : 150, loss : 0.5332, accuracy : 82.66\n",
            "iteration : 200, loss : 0.5207, accuracy : 83.16\n",
            "iteration : 250, loss : 0.5051, accuracy : 83.70\n",
            "iteration : 300, loss : 0.4921, accuracy : 84.24\n",
            "iteration : 350, loss : 0.4869, accuracy : 84.41\n",
            "Epoch :   2, training loss : 0.4829, training accuracy : 84.55, test loss : 0.4377, test accuracy : 86.49\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.3975, accuracy : 87.38\n",
            "iteration : 100, loss : 0.4028, accuracy : 87.48\n",
            "iteration : 150, loss : 0.3876, accuracy : 87.96\n",
            "iteration : 200, loss : 0.3804, accuracy : 88.09\n",
            "iteration : 250, loss : 0.3759, accuracy : 88.30\n",
            "iteration : 300, loss : 0.3750, accuracy : 88.36\n",
            "iteration : 350, loss : 0.3727, accuracy : 88.42\n",
            "Epoch :   3, training loss : 0.3722, training accuracy : 88.44, test loss : 0.3670, test accuracy : 88.72\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3334, accuracy : 89.84\n",
            "iteration : 100, loss : 0.3428, accuracy : 89.66\n",
            "iteration : 150, loss : 0.3366, accuracy : 89.77\n",
            "iteration : 200, loss : 0.3404, accuracy : 89.61\n",
            "iteration : 250, loss : 0.3369, accuracy : 89.76\n",
            "iteration : 300, loss : 0.3344, accuracy : 89.85\n",
            "iteration : 350, loss : 0.3320, accuracy : 89.99\n",
            "Epoch :   4, training loss : 0.3318, training accuracy : 89.97, test loss : 0.3468, test accuracy : 89.40\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.2916, accuracy : 91.00\n",
            "iteration : 100, loss : 0.2929, accuracy : 90.99\n",
            "iteration : 150, loss : 0.2974, accuracy : 90.72\n",
            "iteration : 200, loss : 0.3027, accuracy : 90.65\n",
            "iteration : 250, loss : 0.3045, accuracy : 90.70\n",
            "iteration : 300, loss : 0.3003, accuracy : 90.85\n",
            "iteration : 350, loss : 0.3009, accuracy : 90.84\n",
            "Epoch :   5, training loss : 0.3006, training accuracy : 90.87, test loss : 0.3207, test accuracy : 90.33\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.2781, accuracy : 91.78\n",
            "iteration : 100, loss : 0.2791, accuracy : 91.44\n",
            "iteration : 150, loss : 0.2805, accuracy : 91.42\n",
            "iteration : 200, loss : 0.2785, accuracy : 91.48\n",
            "iteration : 250, loss : 0.2801, accuracy : 91.46\n",
            "iteration : 300, loss : 0.2774, accuracy : 91.51\n",
            "iteration : 350, loss : 0.2754, accuracy : 91.61\n",
            "Epoch :   6, training loss : 0.2739, training accuracy : 91.65, test loss : 0.2987, test accuracy : 91.22\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.2492, accuracy : 92.69\n",
            "iteration : 100, loss : 0.2595, accuracy : 92.16\n",
            "iteration : 150, loss : 0.2620, accuracy : 92.09\n",
            "iteration : 200, loss : 0.2616, accuracy : 92.13\n",
            "iteration : 250, loss : 0.2577, accuracy : 92.28\n",
            "iteration : 300, loss : 0.2568, accuracy : 92.27\n",
            "iteration : 350, loss : 0.2573, accuracy : 92.32\n",
            "Epoch :   7, training loss : 0.2571, training accuracy : 92.33, test loss : 0.2881, test accuracy : 91.43\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.2385, accuracy : 93.12\n",
            "iteration : 100, loss : 0.2431, accuracy : 92.91\n",
            "iteration : 150, loss : 0.2451, accuracy : 92.79\n",
            "iteration : 200, loss : 0.2427, accuracy : 92.86\n",
            "iteration : 250, loss : 0.2431, accuracy : 92.83\n",
            "iteration : 300, loss : 0.2474, accuracy : 92.73\n",
            "iteration : 350, loss : 0.2453, accuracy : 92.78\n",
            "Epoch :   8, training loss : 0.2442, training accuracy : 92.79, test loss : 0.2709, test accuracy : 92.24\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.2315, accuracy : 93.39\n",
            "iteration : 100, loss : 0.2284, accuracy : 93.36\n",
            "iteration : 150, loss : 0.2315, accuracy : 93.23\n",
            "iteration : 200, loss : 0.2354, accuracy : 93.14\n",
            "iteration : 250, loss : 0.2339, accuracy : 93.19\n",
            "iteration : 300, loss : 0.2342, accuracy : 93.22\n",
            "iteration : 350, loss : 0.2332, accuracy : 93.19\n",
            "Epoch :   9, training loss : 0.2325, training accuracy : 93.22, test loss : 0.2713, test accuracy : 92.31\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2230, accuracy : 93.58\n",
            "iteration : 100, loss : 0.2227, accuracy : 93.47\n",
            "iteration : 150, loss : 0.2203, accuracy : 93.54\n",
            "iteration : 200, loss : 0.2214, accuracy : 93.46\n",
            "iteration : 250, loss : 0.2229, accuracy : 93.38\n",
            "iteration : 300, loss : 0.2220, accuracy : 93.36\n",
            "iteration : 350, loss : 0.2264, accuracy : 93.29\n",
            "Epoch :  10, training loss : 0.2259, training accuracy : 93.33, test loss : 0.2581, test accuracy : 92.69\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.1985, accuracy : 94.20\n",
            "iteration : 100, loss : 0.2117, accuracy : 93.78\n",
            "iteration : 150, loss : 0.2073, accuracy : 93.89\n",
            "iteration : 200, loss : 0.2071, accuracy : 93.96\n",
            "iteration : 250, loss : 0.2123, accuracy : 93.78\n",
            "iteration : 300, loss : 0.2111, accuracy : 93.77\n",
            "iteration : 350, loss : 0.2112, accuracy : 93.77\n",
            "Epoch :  11, training loss : 0.2115, training accuracy : 93.78, test loss : 0.2761, test accuracy : 91.93\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.1920, accuracy : 94.53\n",
            "iteration : 100, loss : 0.1947, accuracy : 94.31\n",
            "iteration : 150, loss : 0.2052, accuracy : 94.00\n",
            "iteration : 200, loss : 0.2059, accuracy : 94.05\n",
            "iteration : 250, loss : 0.2044, accuracy : 94.08\n",
            "iteration : 300, loss : 0.2028, accuracy : 94.06\n",
            "iteration : 350, loss : 0.2011, accuracy : 94.08\n",
            "Epoch :  12, training loss : 0.2027, training accuracy : 94.04, test loss : 0.2509, test accuracy : 92.78\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.1946, accuracy : 94.53\n",
            "iteration : 100, loss : 0.1979, accuracy : 94.41\n",
            "iteration : 150, loss : 0.2002, accuracy : 94.33\n",
            "iteration : 200, loss : 0.1972, accuracy : 94.41\n",
            "iteration : 250, loss : 0.1939, accuracy : 94.51\n",
            "iteration : 300, loss : 0.1946, accuracy : 94.46\n",
            "iteration : 350, loss : 0.1947, accuracy : 94.38\n",
            "Epoch :  13, training loss : 0.1934, training accuracy : 94.39, test loss : 0.2316, test accuracy : 93.45\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.1822, accuracy : 94.69\n",
            "iteration : 100, loss : 0.1819, accuracy : 94.69\n",
            "iteration : 150, loss : 0.1821, accuracy : 94.73\n",
            "iteration : 200, loss : 0.1844, accuracy : 94.66\n",
            "iteration : 250, loss : 0.1858, accuracy : 94.62\n",
            "iteration : 300, loss : 0.1870, accuracy : 94.54\n",
            "iteration : 350, loss : 0.1873, accuracy : 94.54\n",
            "Epoch :  14, training loss : 0.1880, training accuracy : 94.54, test loss : 0.2396, test accuracy : 93.22\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.1554, accuracy : 95.53\n",
            "iteration : 100, loss : 0.1622, accuracy : 95.24\n",
            "iteration : 150, loss : 0.1632, accuracy : 95.13\n",
            "iteration : 200, loss : 0.1713, accuracy : 94.89\n",
            "iteration : 250, loss : 0.1727, accuracy : 94.88\n",
            "iteration : 300, loss : 0.1748, accuracy : 94.84\n",
            "iteration : 350, loss : 0.1753, accuracy : 94.85\n",
            "Epoch :  15, training loss : 0.1764, training accuracy : 94.81, test loss : 0.2326, test accuracy : 93.54\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.1802, accuracy : 94.67\n",
            "iteration : 100, loss : 0.1756, accuracy : 94.88\n",
            "iteration : 150, loss : 0.1713, accuracy : 94.98\n",
            "iteration : 200, loss : 0.1707, accuracy : 95.04\n",
            "iteration : 250, loss : 0.1727, accuracy : 95.00\n",
            "iteration : 300, loss : 0.1724, accuracy : 95.09\n",
            "iteration : 350, loss : 0.1736, accuracy : 95.07\n",
            "Epoch :  16, training loss : 0.1735, training accuracy : 95.05, test loss : 0.2180, test accuracy : 93.85\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.1747, accuracy : 95.16\n",
            "iteration : 100, loss : 0.1665, accuracy : 95.28\n",
            "iteration : 150, loss : 0.1619, accuracy : 95.36\n",
            "iteration : 200, loss : 0.1612, accuracy : 95.34\n",
            "iteration : 250, loss : 0.1622, accuracy : 95.30\n",
            "iteration : 300, loss : 0.1628, accuracy : 95.24\n",
            "iteration : 350, loss : 0.1631, accuracy : 95.25\n",
            "Epoch :  17, training loss : 0.1639, training accuracy : 95.20, test loss : 0.2553, test accuracy : 92.85\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.1537, accuracy : 95.61\n",
            "iteration : 100, loss : 0.1659, accuracy : 95.41\n",
            "iteration : 150, loss : 0.1575, accuracy : 95.62\n",
            "iteration : 200, loss : 0.1546, accuracy : 95.62\n",
            "iteration : 250, loss : 0.1580, accuracy : 95.46\n",
            "iteration : 300, loss : 0.1587, accuracy : 95.46\n",
            "iteration : 350, loss : 0.1584, accuracy : 95.49\n",
            "Epoch :  18, training loss : 0.1582, training accuracy : 95.49, test loss : 0.2330, test accuracy : 93.57\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.1442, accuracy : 96.02\n",
            "iteration : 100, loss : 0.1425, accuracy : 95.98\n",
            "iteration : 150, loss : 0.1469, accuracy : 95.82\n",
            "iteration : 200, loss : 0.1435, accuracy : 95.89\n",
            "iteration : 250, loss : 0.1433, accuracy : 95.86\n",
            "iteration : 300, loss : 0.1462, accuracy : 95.79\n",
            "iteration : 350, loss : 0.1498, accuracy : 95.71\n",
            "Epoch :  19, training loss : 0.1510, training accuracy : 95.65, test loss : 0.2341, test accuracy : 93.34\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.1253, accuracy : 96.42\n",
            "iteration : 100, loss : 0.1359, accuracy : 96.23\n",
            "iteration : 150, loss : 0.1381, accuracy : 96.03\n",
            "iteration : 200, loss : 0.1374, accuracy : 96.07\n",
            "iteration : 250, loss : 0.1382, accuracy : 96.01\n",
            "iteration : 300, loss : 0.1386, accuracy : 96.02\n",
            "iteration : 350, loss : 0.1417, accuracy : 95.96\n",
            "Epoch :  20, training loss : 0.1428, training accuracy : 95.92, test loss : 0.2406, test accuracy : 93.48\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.1392, accuracy : 96.14\n",
            "iteration : 100, loss : 0.1400, accuracy : 95.93\n",
            "iteration : 150, loss : 0.1420, accuracy : 95.87\n",
            "iteration : 200, loss : 0.1397, accuracy : 95.92\n",
            "iteration : 250, loss : 0.1381, accuracy : 95.94\n",
            "iteration : 300, loss : 0.1379, accuracy : 95.98\n",
            "iteration : 350, loss : 0.1377, accuracy : 95.98\n",
            "Epoch :  21, training loss : 0.1391, training accuracy : 95.96, test loss : 0.2223, test accuracy : 93.97\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.1203, accuracy : 96.59\n",
            "iteration : 100, loss : 0.1269, accuracy : 96.52\n",
            "iteration : 150, loss : 0.1284, accuracy : 96.46\n",
            "iteration : 200, loss : 0.1272, accuracy : 96.46\n",
            "iteration : 250, loss : 0.1252, accuracy : 96.47\n",
            "iteration : 300, loss : 0.1265, accuracy : 96.42\n",
            "iteration : 350, loss : 0.1277, accuracy : 96.40\n",
            "Epoch :  22, training loss : 0.1278, training accuracy : 96.39, test loss : 0.2349, test accuracy : 93.76\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.1281, accuracy : 96.48\n",
            "iteration : 100, loss : 0.1237, accuracy : 96.48\n",
            "iteration : 150, loss : 0.1244, accuracy : 96.38\n",
            "iteration : 200, loss : 0.1255, accuracy : 96.32\n",
            "iteration : 250, loss : 0.1256, accuracy : 96.33\n",
            "iteration : 300, loss : 0.1269, accuracy : 96.33\n",
            "iteration : 350, loss : 0.1268, accuracy : 96.32\n",
            "Epoch :  23, training loss : 0.1265, training accuracy : 96.32, test loss : 0.2347, test accuracy : 93.72\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.1223, accuracy : 96.75\n",
            "iteration : 100, loss : 0.1194, accuracy : 96.72\n",
            "iteration : 150, loss : 0.1135, accuracy : 96.78\n",
            "iteration : 200, loss : 0.1163, accuracy : 96.69\n",
            "iteration : 250, loss : 0.1184, accuracy : 96.70\n",
            "iteration : 300, loss : 0.1166, accuracy : 96.67\n",
            "iteration : 350, loss : 0.1188, accuracy : 96.58\n",
            "Epoch :  24, training loss : 0.1212, training accuracy : 96.55, test loss : 0.2428, test accuracy : 93.56\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.0917, accuracy : 97.22\n",
            "iteration : 100, loss : 0.0969, accuracy : 97.07\n",
            "iteration : 150, loss : 0.1032, accuracy : 96.94\n",
            "iteration : 200, loss : 0.1068, accuracy : 96.88\n",
            "iteration : 250, loss : 0.1084, accuracy : 96.88\n",
            "iteration : 300, loss : 0.1120, accuracy : 96.77\n",
            "iteration : 350, loss : 0.1125, accuracy : 96.78\n",
            "Epoch :  25, training loss : 0.1140, training accuracy : 96.75, test loss : 0.2531, test accuracy : 93.30\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.1085, accuracy : 96.84\n",
            "iteration : 100, loss : 0.1020, accuracy : 96.98\n",
            "iteration : 150, loss : 0.1026, accuracy : 96.93\n",
            "iteration : 200, loss : 0.1058, accuracy : 96.91\n",
            "iteration : 250, loss : 0.1055, accuracy : 96.88\n",
            "iteration : 300, loss : 0.1088, accuracy : 96.80\n",
            "iteration : 350, loss : 0.1105, accuracy : 96.75\n",
            "Epoch :  26, training loss : 0.1102, training accuracy : 96.75, test loss : 0.2232, test accuracy : 94.20\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.0852, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0964, accuracy : 97.32\n",
            "iteration : 150, loss : 0.1020, accuracy : 97.14\n",
            "iteration : 200, loss : 0.1046, accuracy : 97.06\n",
            "iteration : 250, loss : 0.1048, accuracy : 97.04\n",
            "iteration : 300, loss : 0.1034, accuracy : 97.07\n",
            "iteration : 350, loss : 0.1029, accuracy : 97.06\n",
            "Epoch :  27, training loss : 0.1030, training accuracy : 97.07, test loss : 0.2378, test accuracy : 93.95\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.0879, accuracy : 97.30\n",
            "iteration : 100, loss : 0.0874, accuracy : 97.45\n",
            "iteration : 150, loss : 0.0965, accuracy : 97.33\n",
            "iteration : 200, loss : 0.0968, accuracy : 97.29\n",
            "iteration : 250, loss : 0.0981, accuracy : 97.28\n",
            "iteration : 300, loss : 0.0991, accuracy : 97.27\n",
            "iteration : 350, loss : 0.0992, accuracy : 97.25\n",
            "Epoch :  28, training loss : 0.0996, training accuracy : 97.25, test loss : 0.2260, test accuracy : 94.23\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.0810, accuracy : 97.72\n",
            "iteration : 100, loss : 0.0838, accuracy : 97.61\n",
            "iteration : 150, loss : 0.0942, accuracy : 97.33\n",
            "iteration : 200, loss : 0.0962, accuracy : 97.28\n",
            "iteration : 250, loss : 0.0958, accuracy : 97.28\n",
            "iteration : 300, loss : 0.0961, accuracy : 97.20\n",
            "iteration : 350, loss : 0.0969, accuracy : 97.17\n",
            "Epoch :  29, training loss : 0.0959, training accuracy : 97.20, test loss : 0.2409, test accuracy : 93.98\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.0855, accuracy : 97.56\n",
            "iteration : 100, loss : 0.0854, accuracy : 97.59\n",
            "iteration : 150, loss : 0.0830, accuracy : 97.57\n",
            "iteration : 200, loss : 0.0856, accuracy : 97.51\n",
            "iteration : 250, loss : 0.0867, accuracy : 97.49\n",
            "iteration : 300, loss : 0.0887, accuracy : 97.42\n",
            "iteration : 350, loss : 0.0915, accuracy : 97.33\n",
            "Epoch :  30, training loss : 0.0914, training accuracy : 97.33, test loss : 0.2532, test accuracy : 93.70\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.0847, accuracy : 97.42\n",
            "iteration : 100, loss : 0.0822, accuracy : 97.52\n",
            "iteration : 150, loss : 0.0847, accuracy : 97.52\n",
            "iteration : 200, loss : 0.0831, accuracy : 97.54\n",
            "iteration : 250, loss : 0.0870, accuracy : 97.45\n",
            "iteration : 300, loss : 0.0887, accuracy : 97.42\n",
            "iteration : 350, loss : 0.0894, accuracy : 97.38\n",
            "Epoch :  31, training loss : 0.0889, training accuracy : 97.39, test loss : 0.2437, test accuracy : 94.25\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.0773, accuracy : 97.89\n",
            "iteration : 100, loss : 0.0769, accuracy : 97.82\n",
            "iteration : 150, loss : 0.0770, accuracy : 97.83\n",
            "iteration : 200, loss : 0.0779, accuracy : 97.77\n",
            "iteration : 250, loss : 0.0806, accuracy : 97.69\n",
            "iteration : 300, loss : 0.0817, accuracy : 97.64\n",
            "iteration : 350, loss : 0.0829, accuracy : 97.62\n",
            "Epoch :  32, training loss : 0.0831, training accuracy : 97.62, test loss : 0.2562, test accuracy : 93.68\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.0681, accuracy : 97.97\n",
            "iteration : 100, loss : 0.0680, accuracy : 97.94\n",
            "iteration : 150, loss : 0.0708, accuracy : 97.89\n",
            "iteration : 200, loss : 0.0728, accuracy : 97.85\n",
            "iteration : 250, loss : 0.0765, accuracy : 97.75\n",
            "iteration : 300, loss : 0.0779, accuracy : 97.75\n",
            "iteration : 350, loss : 0.0783, accuracy : 97.70\n",
            "Epoch :  33, training loss : 0.0791, training accuracy : 97.67, test loss : 0.2516, test accuracy : 93.82\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.0774, accuracy : 97.69\n",
            "iteration : 100, loss : 0.0731, accuracy : 97.93\n",
            "iteration : 150, loss : 0.0759, accuracy : 97.87\n",
            "iteration : 200, loss : 0.0755, accuracy : 97.88\n",
            "iteration : 250, loss : 0.0729, accuracy : 97.94\n",
            "iteration : 300, loss : 0.0742, accuracy : 97.87\n",
            "iteration : 350, loss : 0.0753, accuracy : 97.80\n",
            "Epoch :  34, training loss : 0.0754, training accuracy : 97.80, test loss : 0.2507, test accuracy : 94.06\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.0641, accuracy : 97.83\n",
            "iteration : 100, loss : 0.0611, accuracy : 98.00\n",
            "iteration : 150, loss : 0.0643, accuracy : 97.96\n",
            "iteration : 200, loss : 0.0657, accuracy : 97.94\n",
            "iteration : 250, loss : 0.0686, accuracy : 97.91\n",
            "iteration : 300, loss : 0.0691, accuracy : 97.92\n",
            "iteration : 350, loss : 0.0705, accuracy : 97.90\n",
            "Epoch :  35, training loss : 0.0703, training accuracy : 97.90, test loss : 0.2482, test accuracy : 94.25\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.0574, accuracy : 98.17\n",
            "iteration : 100, loss : 0.0603, accuracy : 98.16\n",
            "iteration : 150, loss : 0.0626, accuracy : 98.12\n",
            "iteration : 200, loss : 0.0643, accuracy : 98.06\n",
            "iteration : 250, loss : 0.0645, accuracy : 98.07\n",
            "iteration : 300, loss : 0.0664, accuracy : 98.01\n",
            "iteration : 350, loss : 0.0678, accuracy : 97.94\n",
            "Epoch :  36, training loss : 0.0673, training accuracy : 97.97, test loss : 0.2527, test accuracy : 94.29\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.0640, accuracy : 98.19\n",
            "iteration : 100, loss : 0.0700, accuracy : 97.96\n",
            "iteration : 150, loss : 0.0670, accuracy : 98.11\n",
            "iteration : 200, loss : 0.0628, accuracy : 98.21\n",
            "iteration : 250, loss : 0.0618, accuracy : 98.22\n",
            "iteration : 300, loss : 0.0634, accuracy : 98.13\n",
            "iteration : 350, loss : 0.0652, accuracy : 98.08\n",
            "Epoch :  37, training loss : 0.0655, training accuracy : 98.08, test loss : 0.2651, test accuracy : 93.84\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.0535, accuracy : 98.42\n",
            "iteration : 100, loss : 0.0564, accuracy : 98.27\n",
            "iteration : 150, loss : 0.0558, accuracy : 98.30\n",
            "iteration : 200, loss : 0.0588, accuracy : 98.23\n",
            "iteration : 250, loss : 0.0570, accuracy : 98.27\n",
            "iteration : 300, loss : 0.0588, accuracy : 98.23\n",
            "iteration : 350, loss : 0.0604, accuracy : 98.21\n",
            "Epoch :  38, training loss : 0.0601, training accuracy : 98.21, test loss : 0.2639, test accuracy : 94.26\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.0565, accuracy : 98.28\n",
            "iteration : 100, loss : 0.0589, accuracy : 98.17\n",
            "iteration : 150, loss : 0.0587, accuracy : 98.18\n",
            "iteration : 200, loss : 0.0599, accuracy : 98.21\n",
            "iteration : 250, loss : 0.0617, accuracy : 98.15\n",
            "iteration : 300, loss : 0.0610, accuracy : 98.16\n",
            "iteration : 350, loss : 0.0600, accuracy : 98.18\n",
            "Epoch :  39, training loss : 0.0593, training accuracy : 98.19, test loss : 0.2795, test accuracy : 94.03\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.0487, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0484, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0487, accuracy : 98.49\n",
            "iteration : 200, loss : 0.0525, accuracy : 98.39\n",
            "iteration : 250, loss : 0.0560, accuracy : 98.32\n",
            "iteration : 300, loss : 0.0560, accuracy : 98.34\n",
            "iteration : 350, loss : 0.0565, accuracy : 98.31\n",
            "Epoch :  40, training loss : 0.0557, training accuracy : 98.33, test loss : 0.2833, test accuracy : 93.97\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.0438, accuracy : 98.62\n",
            "iteration : 100, loss : 0.0518, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0528, accuracy : 98.39\n",
            "iteration : 200, loss : 0.0526, accuracy : 98.40\n",
            "iteration : 250, loss : 0.0549, accuracy : 98.32\n",
            "iteration : 300, loss : 0.0553, accuracy : 98.31\n",
            "iteration : 350, loss : 0.0551, accuracy : 98.33\n",
            "Epoch :  41, training loss : 0.0550, training accuracy : 98.32, test loss : 0.2898, test accuracy : 94.00\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.0469, accuracy : 98.42\n",
            "iteration : 100, loss : 0.0497, accuracy : 98.50\n",
            "iteration : 150, loss : 0.0458, accuracy : 98.62\n",
            "iteration : 200, loss : 0.0445, accuracy : 98.70\n",
            "iteration : 250, loss : 0.0442, accuracy : 98.69\n",
            "iteration : 300, loss : 0.0464, accuracy : 98.60\n",
            "iteration : 350, loss : 0.0480, accuracy : 98.56\n",
            "Epoch :  42, training loss : 0.0481, training accuracy : 98.56, test loss : 0.2853, test accuracy : 94.07\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.0416, accuracy : 98.70\n",
            "iteration : 100, loss : 0.0445, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0459, accuracy : 98.63\n",
            "iteration : 200, loss : 0.0443, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0451, accuracy : 98.68\n",
            "iteration : 300, loss : 0.0463, accuracy : 98.63\n",
            "iteration : 350, loss : 0.0482, accuracy : 98.59\n",
            "Epoch :  43, training loss : 0.0489, training accuracy : 98.58, test loss : 0.2881, test accuracy : 94.13\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.0435, accuracy : 98.72\n",
            "iteration : 100, loss : 0.0452, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0457, accuracy : 98.60\n",
            "iteration : 200, loss : 0.0471, accuracy : 98.55\n",
            "iteration : 250, loss : 0.0492, accuracy : 98.48\n",
            "iteration : 300, loss : 0.0496, accuracy : 98.47\n",
            "iteration : 350, loss : 0.0500, accuracy : 98.44\n",
            "Epoch :  44, training loss : 0.0498, training accuracy : 98.44, test loss : 0.2907, test accuracy : 94.01\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.0395, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0411, accuracy : 98.80\n",
            "iteration : 150, loss : 0.0420, accuracy : 98.73\n",
            "iteration : 200, loss : 0.0434, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0428, accuracy : 98.65\n",
            "iteration : 300, loss : 0.0450, accuracy : 98.60\n",
            "iteration : 350, loss : 0.0456, accuracy : 98.59\n",
            "Epoch :  45, training loss : 0.0460, training accuracy : 98.59, test loss : 0.3028, test accuracy : 94.12\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.0483, accuracy : 98.62\n",
            "iteration : 100, loss : 0.0423, accuracy : 98.71\n",
            "iteration : 150, loss : 0.0407, accuracy : 98.78\n",
            "iteration : 200, loss : 0.0414, accuracy : 98.73\n",
            "iteration : 250, loss : 0.0429, accuracy : 98.71\n",
            "iteration : 300, loss : 0.0436, accuracy : 98.69\n",
            "iteration : 350, loss : 0.0431, accuracy : 98.71\n",
            "Epoch :  46, training loss : 0.0435, training accuracy : 98.69, test loss : 0.3097, test accuracy : 93.94\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.0346, accuracy : 98.91\n",
            "iteration : 100, loss : 0.0339, accuracy : 98.92\n",
            "iteration : 150, loss : 0.0362, accuracy : 98.83\n",
            "iteration : 200, loss : 0.0383, accuracy : 98.75\n",
            "iteration : 250, loss : 0.0401, accuracy : 98.68\n",
            "iteration : 300, loss : 0.0404, accuracy : 98.70\n",
            "iteration : 350, loss : 0.0412, accuracy : 98.67\n",
            "Epoch :  47, training loss : 0.0415, training accuracy : 98.67, test loss : 0.3017, test accuracy : 94.17\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.0409, accuracy : 98.75\n",
            "iteration : 100, loss : 0.0407, accuracy : 98.74\n",
            "iteration : 150, loss : 0.0413, accuracy : 98.72\n",
            "iteration : 200, loss : 0.0420, accuracy : 98.73\n",
            "iteration : 250, loss : 0.0435, accuracy : 98.64\n",
            "iteration : 300, loss : 0.0438, accuracy : 98.63\n",
            "iteration : 350, loss : 0.0447, accuracy : 98.60\n",
            "Epoch :  48, training loss : 0.0443, training accuracy : 98.61, test loss : 0.3021, test accuracy : 94.00\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.0347, accuracy : 99.06\n",
            "iteration : 100, loss : 0.0342, accuracy : 98.99\n",
            "iteration : 150, loss : 0.0349, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0353, accuracy : 98.96\n",
            "iteration : 250, loss : 0.0370, accuracy : 98.89\n",
            "iteration : 300, loss : 0.0386, accuracy : 98.84\n",
            "iteration : 350, loss : 0.0393, accuracy : 98.83\n",
            "Epoch :  49, training loss : 0.0399, training accuracy : 98.80, test loss : 0.3039, test accuracy : 93.96\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.0365, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0368, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0362, accuracy : 98.88\n",
            "iteration : 200, loss : 0.0394, accuracy : 98.78\n",
            "iteration : 250, loss : 0.0384, accuracy : 98.80\n",
            "iteration : 300, loss : 0.0380, accuracy : 98.81\n",
            "iteration : 350, loss : 0.0380, accuracy : 98.79\n",
            "Epoch :  50, training loss : 0.0381, training accuracy : 98.78, test loss : 0.3038, test accuracy : 94.22\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.0279, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0293, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0293, accuracy : 99.03\n",
            "iteration : 200, loss : 0.0316, accuracy : 98.98\n",
            "iteration : 250, loss : 0.0332, accuracy : 98.92\n",
            "iteration : 300, loss : 0.0336, accuracy : 98.90\n",
            "iteration : 350, loss : 0.0349, accuracy : 98.86\n",
            "Epoch :  51, training loss : 0.0357, training accuracy : 98.84, test loss : 0.3164, test accuracy : 93.93\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.0307, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0326, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0333, accuracy : 98.93\n",
            "iteration : 200, loss : 0.0338, accuracy : 98.93\n",
            "iteration : 250, loss : 0.0344, accuracy : 98.89\n",
            "iteration : 300, loss : 0.0357, accuracy : 98.83\n",
            "iteration : 350, loss : 0.0360, accuracy : 98.83\n",
            "Epoch :  52, training loss : 0.0363, training accuracy : 98.81, test loss : 0.3186, test accuracy : 93.92\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.0343, accuracy : 98.81\n",
            "iteration : 100, loss : 0.0322, accuracy : 98.91\n",
            "iteration : 150, loss : 0.0331, accuracy : 98.89\n",
            "iteration : 200, loss : 0.0339, accuracy : 98.86\n",
            "iteration : 250, loss : 0.0342, accuracy : 98.82\n",
            "iteration : 300, loss : 0.0338, accuracy : 98.88\n",
            "iteration : 350, loss : 0.0339, accuracy : 98.86\n",
            "Epoch :  53, training loss : 0.0341, training accuracy : 98.86, test loss : 0.3238, test accuracy : 94.09\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.0314, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0342, accuracy : 98.87\n",
            "iteration : 150, loss : 0.0360, accuracy : 98.83\n",
            "iteration : 200, loss : 0.0362, accuracy : 98.83\n",
            "iteration : 250, loss : 0.0357, accuracy : 98.85\n",
            "iteration : 300, loss : 0.0349, accuracy : 98.89\n",
            "iteration : 350, loss : 0.0350, accuracy : 98.88\n",
            "Epoch :  54, training loss : 0.0347, training accuracy : 98.89, test loss : 0.3291, test accuracy : 94.01\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.0288, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0304, accuracy : 98.99\n",
            "iteration : 150, loss : 0.0283, accuracy : 99.06\n",
            "iteration : 200, loss : 0.0296, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0306, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0311, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0312, accuracy : 98.95\n",
            "Epoch :  55, training loss : 0.0318, training accuracy : 98.94, test loss : 0.3350, test accuracy : 94.16\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.0299, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0300, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0271, accuracy : 99.16\n",
            "iteration : 200, loss : 0.0273, accuracy : 99.15\n",
            "iteration : 250, loss : 0.0294, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0294, accuracy : 99.06\n",
            "iteration : 350, loss : 0.0306, accuracy : 99.03\n",
            "Epoch :  56, training loss : 0.0308, training accuracy : 99.03, test loss : 0.3214, test accuracy : 94.29\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.0287, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0267, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0264, accuracy : 99.12\n",
            "iteration : 200, loss : 0.0257, accuracy : 99.16\n",
            "iteration : 250, loss : 0.0279, accuracy : 99.06\n",
            "iteration : 300, loss : 0.0279, accuracy : 99.06\n",
            "iteration : 350, loss : 0.0282, accuracy : 99.08\n",
            "Epoch :  57, training loss : 0.0286, training accuracy : 99.06, test loss : 0.3303, test accuracy : 94.32\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.0278, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0278, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0276, accuracy : 99.11\n",
            "iteration : 200, loss : 0.0280, accuracy : 99.12\n",
            "iteration : 250, loss : 0.0270, accuracy : 99.14\n",
            "iteration : 300, loss : 0.0267, accuracy : 99.15\n",
            "iteration : 350, loss : 0.0266, accuracy : 99.15\n",
            "Epoch :  58, training loss : 0.0270, training accuracy : 99.14, test loss : 0.3453, test accuracy : 94.02\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.0218, accuracy : 99.28\n",
            "iteration : 100, loss : 0.0212, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0250, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0257, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0256, accuracy : 99.14\n",
            "iteration : 300, loss : 0.0263, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0261, accuracy : 99.15\n",
            "Epoch :  59, training loss : 0.0263, training accuracy : 99.14, test loss : 0.3448, test accuracy : 94.16\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.0309, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0263, accuracy : 99.17\n",
            "iteration : 150, loss : 0.0255, accuracy : 99.15\n",
            "iteration : 200, loss : 0.0245, accuracy : 99.19\n",
            "iteration : 250, loss : 0.0261, accuracy : 99.14\n",
            "iteration : 300, loss : 0.0259, accuracy : 99.16\n",
            "iteration : 350, loss : 0.0273, accuracy : 99.11\n",
            "Epoch :  60, training loss : 0.0279, training accuracy : 99.10, test loss : 0.3306, test accuracy : 93.88\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.0288, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0269, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0273, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0259, accuracy : 99.11\n",
            "iteration : 250, loss : 0.0261, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0267, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0264, accuracy : 99.11\n",
            "Epoch :  61, training loss : 0.0256, training accuracy : 99.13, test loss : 0.3218, test accuracy : 94.54\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.0164, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0180, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0217, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0227, accuracy : 99.21\n",
            "iteration : 250, loss : 0.0237, accuracy : 99.17\n",
            "iteration : 300, loss : 0.0247, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0253, accuracy : 99.14\n",
            "Epoch :  62, training loss : 0.0260, training accuracy : 99.11, test loss : 0.3393, test accuracy : 94.35\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.0314, accuracy : 98.89\n",
            "iteration : 100, loss : 0.0296, accuracy : 98.98\n",
            "iteration : 150, loss : 0.0264, accuracy : 99.12\n",
            "iteration : 200, loss : 0.0264, accuracy : 99.11\n",
            "iteration : 250, loss : 0.0260, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0259, accuracy : 99.13\n",
            "iteration : 350, loss : 0.0264, accuracy : 99.11\n",
            "Epoch :  63, training loss : 0.0266, training accuracy : 99.10, test loss : 0.3392, test accuracy : 94.15\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.0208, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0176, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0170, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0160, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0159, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0158, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0172, accuracy : 99.46\n",
            "Epoch :  64, training loss : 0.0174, training accuracy : 99.46, test loss : 0.3622, test accuracy : 94.10\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.0143, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0195, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0226, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0260, accuracy : 99.18\n",
            "iteration : 250, loss : 0.0280, accuracy : 99.13\n",
            "iteration : 300, loss : 0.0276, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0265, accuracy : 99.16\n",
            "Epoch :  65, training loss : 0.0268, training accuracy : 99.16, test loss : 0.3498, test accuracy : 94.36\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.0179, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0209, accuracy : 99.30\n",
            "iteration : 150, loss : 0.0202, accuracy : 99.30\n",
            "iteration : 200, loss : 0.0212, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0220, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0222, accuracy : 99.26\n",
            "iteration : 350, loss : 0.0225, accuracy : 99.25\n",
            "Epoch :  66, training loss : 0.0229, training accuracy : 99.23, test loss : 0.3451, test accuracy : 94.21\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.0260, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0231, accuracy : 99.24\n",
            "iteration : 150, loss : 0.0225, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0227, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0243, accuracy : 99.20\n",
            "iteration : 300, loss : 0.0243, accuracy : 99.20\n",
            "iteration : 350, loss : 0.0240, accuracy : 99.22\n",
            "Epoch :  67, training loss : 0.0237, training accuracy : 99.22, test loss : 0.3313, test accuracy : 94.33\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.0146, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0188, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0190, accuracy : 99.37\n",
            "iteration : 200, loss : 0.0202, accuracy : 99.35\n",
            "iteration : 250, loss : 0.0209, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0210, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0215, accuracy : 99.32\n",
            "Epoch :  68, training loss : 0.0212, training accuracy : 99.32, test loss : 0.3478, test accuracy : 94.28\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.0154, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0158, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0168, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0175, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0176, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0175, accuracy : 99.44\n",
            "iteration : 350, loss : 0.0180, accuracy : 99.40\n",
            "Epoch :  69, training loss : 0.0183, training accuracy : 99.40, test loss : 0.3705, test accuracy : 93.92\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.0215, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0212, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0221, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0228, accuracy : 99.29\n",
            "iteration : 250, loss : 0.0221, accuracy : 99.30\n",
            "iteration : 300, loss : 0.0214, accuracy : 99.31\n",
            "iteration : 350, loss : 0.0211, accuracy : 99.32\n",
            "Epoch :  70, training loss : 0.0209, training accuracy : 99.33, test loss : 0.3664, test accuracy : 94.06\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.0180, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.40\n",
            "iteration : 150, loss : 0.0166, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0173, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0185, accuracy : 99.35\n",
            "iteration : 300, loss : 0.0209, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0215, accuracy : 99.27\n",
            "Epoch :  71, training loss : 0.0214, training accuracy : 99.27, test loss : 0.3616, test accuracy : 94.10\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.0203, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0219, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0212, accuracy : 99.34\n",
            "iteration : 200, loss : 0.0226, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0220, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0215, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0213, accuracy : 99.37\n",
            "Epoch :  72, training loss : 0.0210, training accuracy : 99.38, test loss : 0.3474, test accuracy : 94.41\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.0131, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0141, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0160, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0151, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0159, accuracy : 99.45\n",
            "iteration : 300, loss : 0.0166, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0172, accuracy : 99.41\n",
            "Epoch :  73, training loss : 0.0173, training accuracy : 99.41, test loss : 0.3690, test accuracy : 94.25\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.0223, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0213, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0197, accuracy : 99.34\n",
            "iteration : 200, loss : 0.0197, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0199, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0208, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0210, accuracy : 99.30\n",
            "Epoch :  74, training loss : 0.0210, training accuracy : 99.30, test loss : 0.3511, test accuracy : 94.32\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.0130, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0134, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0145, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0152, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0153, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0160, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0160, accuracy : 99.45\n",
            "Epoch :  75, training loss : 0.0161, training accuracy : 99.45, test loss : 0.3899, test accuracy : 94.24\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.0129, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0147, accuracy : 99.50\n",
            "iteration : 150, loss : 0.0150, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0153, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0156, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0169, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0187, accuracy : 99.37\n",
            "Epoch :  76, training loss : 0.0192, training accuracy : 99.37, test loss : 0.3610, test accuracy : 94.06\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.0131, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0144, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0148, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0144, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0142, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0154, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0158, accuracy : 99.49\n",
            "Epoch :  77, training loss : 0.0165, training accuracy : 99.47, test loss : 0.3731, test accuracy : 94.04\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.0162, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0199, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0189, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0185, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0182, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0172, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0173, accuracy : 99.44\n",
            "Epoch :  78, training loss : 0.0177, training accuracy : 99.44, test loss : 0.3887, test accuracy : 93.95\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.0158, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0179, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0169, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0170, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0175, accuracy : 99.40\n",
            "iteration : 300, loss : 0.0171, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0174, accuracy : 99.43\n",
            "Epoch :  79, training loss : 0.0175, training accuracy : 99.43, test loss : 0.3712, test accuracy : 94.30\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.0105, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0141, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0137, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0136, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0144, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0147, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0147, accuracy : 99.53\n",
            "Epoch :  80, training loss : 0.0147, training accuracy : 99.52, test loss : 0.3705, test accuracy : 94.42\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.0163, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0146, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0140, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0130, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0132, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0137, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0136, accuracy : 99.55\n",
            "Epoch :  81, training loss : 0.0139, training accuracy : 99.54, test loss : 0.3944, test accuracy : 94.19\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.0201, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0200, accuracy : 99.26\n",
            "iteration : 150, loss : 0.0195, accuracy : 99.30\n",
            "iteration : 200, loss : 0.0189, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0181, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0181, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0191, accuracy : 99.33\n",
            "Epoch :  82, training loss : 0.0188, training accuracy : 99.34, test loss : 0.3735, test accuracy : 94.02\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.0167, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0166, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0180, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0177, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0188, accuracy : 99.35\n",
            "iteration : 300, loss : 0.0187, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0182, accuracy : 99.38\n",
            "Epoch :  83, training loss : 0.0182, training accuracy : 99.38, test loss : 0.3671, test accuracy : 94.44\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.0119, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0105, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0112, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0127, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0122, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0127, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0131, accuracy : 99.55\n",
            "Epoch :  84, training loss : 0.0134, training accuracy : 99.53, test loss : 0.3915, test accuracy : 94.13\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0140, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0128, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0127, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0135, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0131, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0132, accuracy : 99.55\n",
            "Epoch :  85, training loss : 0.0132, training accuracy : 99.54, test loss : 0.3847, test accuracy : 94.21\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.0121, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0105, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0121, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0127, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0133, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0138, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0139, accuracy : 99.53\n",
            "Epoch :  86, training loss : 0.0142, training accuracy : 99.51, test loss : 0.3914, test accuracy : 94.40\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.0126, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0122, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0126, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0128, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0125, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0128, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0133, accuracy : 99.56\n",
            "Epoch :  87, training loss : 0.0132, training accuracy : 99.57, test loss : 0.3815, test accuracy : 94.36\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.0108, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0116, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0116, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0125, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0129, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0135, accuracy : 99.57\n",
            "Epoch :  88, training loss : 0.0134, training accuracy : 99.57, test loss : 0.3778, test accuracy : 94.29\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.0113, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0108, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0104, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0118, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0119, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0121, accuracy : 99.63\n",
            "Epoch :  89, training loss : 0.0119, training accuracy : 99.63, test loss : 0.3829, test accuracy : 94.48\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.0126, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0128, accuracy : 99.61\n",
            "iteration : 150, loss : 0.0135, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0132, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0133, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0131, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0139, accuracy : 99.50\n",
            "Epoch :  90, training loss : 0.0140, training accuracy : 99.49, test loss : 0.3882, test accuracy : 94.13\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.0193, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0151, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0151, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0135, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0130, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0130, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0130, accuracy : 99.55\n",
            "Epoch :  91, training loss : 0.0128, training accuracy : 99.55, test loss : 0.3947, test accuracy : 94.32\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.0101, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0119, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0124, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0128, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0133, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0138, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0138, accuracy : 99.56\n",
            "Epoch :  92, training loss : 0.0136, training accuracy : 99.57, test loss : 0.3842, test accuracy : 94.33\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.0145, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0131, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0134, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0134, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0135, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0137, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0138, accuracy : 99.54\n",
            "Epoch :  93, training loss : 0.0142, training accuracy : 99.53, test loss : 0.3929, test accuracy : 94.42\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.0169, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0161, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0155, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0143, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0147, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0134, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0134, accuracy : 99.58\n",
            "Epoch :  94, training loss : 0.0132, training accuracy : 99.58, test loss : 0.3871, test accuracy : 94.66\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.0077, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0077, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0077, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0082, accuracy : 99.74\n",
            "Epoch :  95, training loss : 0.0082, training accuracy : 99.74, test loss : 0.3962, test accuracy : 94.49\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.0122, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0098, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0093, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0105, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0129, accuracy : 99.56\n",
            "Epoch :  96, training loss : 0.0138, training accuracy : 99.54, test loss : 0.4203, test accuracy : 94.10\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.0122, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0119, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0123, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0121, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0117, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0119, accuracy : 99.62\n",
            "Epoch :  97, training loss : 0.0119, training accuracy : 99.61, test loss : 0.3809, test accuracy : 94.26\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0115, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0112, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.62\n",
            "Epoch :  98, training loss : 0.0111, training accuracy : 99.63, test loss : 0.3810, test accuracy : 94.34\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.0102, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0122, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0118, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0115, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.62\n",
            "Epoch :  99, training loss : 0.0114, training accuracy : 99.63, test loss : 0.3862, test accuracy : 94.39\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0077, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0068, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0065, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0073, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.73\n",
            "Epoch : 100, training loss : 0.0081, training accuracy : 99.73, test loss : 0.4100, test accuracy : 94.48\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.0093, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0093, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0103, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0106, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0111, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.63\n",
            "Epoch : 101, training loss : 0.0108, training accuracy : 99.63, test loss : 0.4060, test accuracy : 94.39\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0083, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0101, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0118, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0110, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0108, accuracy : 99.64\n",
            "Epoch : 102, training loss : 0.0108, training accuracy : 99.64, test loss : 0.4145, test accuracy : 94.36\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.0108, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0104, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0111, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.62\n",
            "Epoch : 103, training loss : 0.0111, training accuracy : 99.62, test loss : 0.4108, test accuracy : 94.17\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.0111, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0113, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0107, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0098, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0098, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0102, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0098, accuracy : 99.72\n",
            "Epoch : 104, training loss : 0.0099, training accuracy : 99.71, test loss : 0.3937, test accuracy : 94.65\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.0079, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0084, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0092, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0099, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0104, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0100, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0099, accuracy : 99.69\n",
            "Epoch : 105, training loss : 0.0099, training accuracy : 99.69, test loss : 0.3983, test accuracy : 94.37\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0061, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0064, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.74\n",
            "Epoch : 106, training loss : 0.0077, training accuracy : 99.75, test loss : 0.4124, test accuracy : 94.48\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0072, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0072, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0089, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0103, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0109, accuracy : 99.65\n",
            "Epoch : 107, training loss : 0.0110, training accuracy : 99.64, test loss : 0.4014, test accuracy : 94.28\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.0102, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0104, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0101, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0088, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0091, accuracy : 99.72\n",
            "Epoch : 108, training loss : 0.0091, training accuracy : 99.72, test loss : 0.4076, test accuracy : 94.46\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.0075, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0076, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0086, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0085, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0093, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0098, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0097, accuracy : 99.70\n",
            "Epoch : 109, training loss : 0.0096, training accuracy : 99.70, test loss : 0.4142, test accuracy : 94.29\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.0107, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0104, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0095, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0085, accuracy : 99.73\n",
            "Epoch : 110, training loss : 0.0085, training accuracy : 99.72, test loss : 0.4236, test accuracy : 94.36\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0092, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0086, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0087, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0088, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0091, accuracy : 99.67\n",
            "Epoch : 111, training loss : 0.0092, training accuracy : 99.67, test loss : 0.4339, test accuracy : 94.36\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.0100, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0098, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0088, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0080, accuracy : 99.70\n",
            "Epoch : 112, training loss : 0.0080, training accuracy : 99.70, test loss : 0.4187, test accuracy : 94.55\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0075, accuracy : 99.75\n",
            "Epoch : 113, training loss : 0.0075, training accuracy : 99.74, test loss : 0.4211, test accuracy : 94.35\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.0102, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0084, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0086, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0081, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0081, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0077, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.72\n",
            "Epoch : 114, training loss : 0.0077, training accuracy : 99.73, test loss : 0.4292, test accuracy : 94.49\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0062, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0086, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0081, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0087, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0088, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0089, accuracy : 99.68\n",
            "Epoch : 115, training loss : 0.0089, training accuracy : 99.68, test loss : 0.4385, test accuracy : 94.35\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0072, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0080, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0073, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0074, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0073, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0073, accuracy : 99.74\n",
            "Epoch : 116, training loss : 0.0076, training accuracy : 99.73, test loss : 0.4444, test accuracy : 94.48\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0083, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0088, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0085, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0089, accuracy : 99.72\n",
            "Epoch : 117, training loss : 0.0090, training accuracy : 99.72, test loss : 0.4239, test accuracy : 94.65\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0064, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0064, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.81\n",
            "Epoch : 118, training loss : 0.0065, training accuracy : 99.81, test loss : 0.4140, test accuracy : 94.46\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.0071, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0075, accuracy : 99.73\n",
            "Epoch : 119, training loss : 0.0074, training accuracy : 99.73, test loss : 0.4215, test accuracy : 94.44\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.0079, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0076, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0070, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0070, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0069, accuracy : 99.78\n",
            "Epoch : 120, training loss : 0.0071, training accuracy : 99.77, test loss : 0.4470, test accuracy : 94.54\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0076, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0077, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0082, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0099, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0096, accuracy : 99.69\n",
            "Epoch : 121, training loss : 0.0096, training accuracy : 99.69, test loss : 0.4281, test accuracy : 94.53\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0074, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0069, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.80\n",
            "Epoch : 122, training loss : 0.0067, training accuracy : 99.79, test loss : 0.4140, test accuracy : 94.55\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0059, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.78\n",
            "Epoch : 123, training loss : 0.0064, training accuracy : 99.78, test loss : 0.4056, test accuracy : 94.70\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0062, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0061, accuracy : 99.78\n",
            "Epoch : 124, training loss : 0.0060, training accuracy : 99.78, test loss : 0.4296, test accuracy : 94.59\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0057, accuracy : 99.82\n",
            "Epoch : 125, training loss : 0.0060, training accuracy : 99.81, test loss : 0.4480, test accuracy : 94.17\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.0074, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0061, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0059, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0062, accuracy : 99.79\n",
            "Epoch : 126, training loss : 0.0064, training accuracy : 99.79, test loss : 0.4537, test accuracy : 94.41\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0073, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0065, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0069, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.77\n",
            "Epoch : 127, training loss : 0.0080, training accuracy : 99.76, test loss : 0.4160, test accuracy : 94.56\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0077, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0072, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0068, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0068, accuracy : 99.79\n",
            "Epoch : 128, training loss : 0.0068, training accuracy : 99.79, test loss : 0.4293, test accuracy : 94.58\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 129, training loss : 0.0049, training accuracy : 99.84, test loss : 0.4655, test accuracy : 94.21\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0057, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0069, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0070, accuracy : 99.77\n",
            "Epoch : 130, training loss : 0.0070, training accuracy : 99.77, test loss : 0.4402, test accuracy : 94.55\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0064, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0064, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0064, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0061, accuracy : 99.79\n",
            "Epoch : 131, training loss : 0.0062, training accuracy : 99.79, test loss : 0.4476, test accuracy : 94.45\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.0033, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 132, training loss : 0.0045, training accuracy : 99.87, test loss : 0.4581, test accuracy : 94.47\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0034, accuracy : 99.92\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.84\n",
            "Epoch : 133, training loss : 0.0051, training accuracy : 99.84, test loss : 0.4662, test accuracy : 94.28\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0064, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.83\n",
            "Epoch : 134, training loss : 0.0052, training accuracy : 99.83, test loss : 0.4665, test accuracy : 94.34\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0063, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0061, accuracy : 99.79\n",
            "Epoch : 135, training loss : 0.0059, training accuracy : 99.80, test loss : 0.4483, test accuracy : 94.71\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0078, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0089, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0076, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0072, accuracy : 99.78\n",
            "Epoch : 136, training loss : 0.0070, training accuracy : 99.78, test loss : 0.4391, test accuracy : 94.49\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0069, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0069, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.80\n",
            "Epoch : 137, training loss : 0.0065, training accuracy : 99.80, test loss : 0.4307, test accuracy : 94.61\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0074, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0068, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0077, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0073, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0069, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0064, accuracy : 99.80\n",
            "Epoch : 138, training loss : 0.0062, training accuracy : 99.80, test loss : 0.4221, test accuracy : 94.76\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.84\n",
            "Epoch : 139, training loss : 0.0045, training accuracy : 99.83, test loss : 0.4302, test accuracy : 94.74\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0037, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 140, training loss : 0.0049, training accuracy : 99.85, test loss : 0.4534, test accuracy : 94.66\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.84\n",
            "Epoch : 141, training loss : 0.0054, training accuracy : 99.84, test loss : 0.4390, test accuracy : 94.60\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0037, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0034, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0036, accuracy : 99.87\n",
            "Epoch : 142, training loss : 0.0036, training accuracy : 99.87, test loss : 0.4558, test accuracy : 94.66\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0036, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0033, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0031, accuracy : 99.91\n",
            "iteration : 300, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.89\n",
            "Epoch : 143, training loss : 0.0041, training accuracy : 99.89, test loss : 0.4493, test accuracy : 94.58\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0027, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0041, accuracy : 99.87\n",
            "Epoch : 144, training loss : 0.0040, training accuracy : 99.87, test loss : 0.4691, test accuracy : 94.58\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0040, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 145, training loss : 0.0047, training accuracy : 99.85, test loss : 0.4464, test accuracy : 94.71\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0027, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0028, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0041, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0044, accuracy : 99.86\n",
            "Epoch : 146, training loss : 0.0048, training accuracy : 99.85, test loss : 0.4687, test accuracy : 94.55\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.81\n",
            "Epoch : 147, training loss : 0.0052, training accuracy : 99.81, test loss : 0.4441, test accuracy : 94.77\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.85\n",
            "Epoch : 148, training loss : 0.0045, training accuracy : 99.85, test loss : 0.4454, test accuracy : 94.77\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0023, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0024, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0027, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0029, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0030, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0032, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0037, accuracy : 99.87\n",
            "Epoch : 149, training loss : 0.0037, training accuracy : 99.87, test loss : 0.4666, test accuracy : 94.56\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0042, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.84\n",
            "Epoch : 150, training loss : 0.0048, training accuracy : 99.84, test loss : 0.4585, test accuracy : 94.67\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0025, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0027, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0031, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0035, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0034, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0036, accuracy : 99.87\n",
            "Epoch : 151, training loss : 0.0036, training accuracy : 99.87, test loss : 0.4551, test accuracy : 94.66\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0026, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0025, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0033, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0034, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0031, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0034, accuracy : 99.90\n",
            "Epoch : 152, training loss : 0.0034, training accuracy : 99.89, test loss : 0.4548, test accuracy : 94.74\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0027, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0037, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0035, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0032, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0035, accuracy : 99.89\n",
            "Epoch : 153, training loss : 0.0036, training accuracy : 99.88, test loss : 0.4657, test accuracy : 94.79\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0031, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0035, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0039, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0040, accuracy : 99.88\n",
            "Epoch : 154, training loss : 0.0040, training accuracy : 99.87, test loss : 0.4799, test accuracy : 94.45\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0034, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0033, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0028, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0028, accuracy : 99.91\n",
            "iteration : 300, loss : 0.0027, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0028, accuracy : 99.90\n",
            "Epoch : 155, training loss : 0.0027, training accuracy : 99.91, test loss : 0.4625, test accuracy : 94.74\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0035, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0032, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0037, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0038, accuracy : 99.88\n",
            "Epoch : 156, training loss : 0.0038, training accuracy : 99.88, test loss : 0.4760, test accuracy : 94.44\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0038, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0037, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0036, accuracy : 99.88\n",
            "Epoch : 157, training loss : 0.0036, training accuracy : 99.88, test loss : 0.4765, test accuracy : 94.73\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0034, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0034, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0035, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0032, accuracy : 99.89\n",
            "Epoch : 158, training loss : 0.0031, training accuracy : 99.90, test loss : 0.4799, test accuracy : 94.73\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0020, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0028, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0025, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0026, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0027, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0026, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0030, accuracy : 99.90\n",
            "Epoch : 159, training loss : 0.0030, training accuracy : 99.90, test loss : 0.4736, test accuracy : 94.75\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0039, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.86\n",
            "Epoch : 160, training loss : 0.0041, training accuracy : 99.86, test loss : 0.4778, test accuracy : 94.66\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0026, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0033, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0040, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0038, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0037, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0035, accuracy : 99.89\n",
            "Epoch : 161, training loss : 0.0035, training accuracy : 99.89, test loss : 0.4753, test accuracy : 94.48\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0024, accuracy : 99.92\n",
            "iteration : 150, loss : 0.0028, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0026, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0025, accuracy : 99.92\n",
            "iteration : 300, loss : 0.0024, accuracy : 99.92\n",
            "iteration : 350, loss : 0.0024, accuracy : 99.92\n",
            "Epoch : 162, training loss : 0.0024, training accuracy : 99.92, test loss : 0.4623, test accuracy : 94.81\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0009, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0031, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0033, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0033, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0033, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0031, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0031, accuracy : 99.91\n",
            "Epoch : 163, training loss : 0.0030, training accuracy : 99.91, test loss : 0.4765, test accuracy : 94.61\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0014, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0026, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0029, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0028, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0026, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0028, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0029, accuracy : 99.90\n",
            "Epoch : 164, training loss : 0.0030, training accuracy : 99.89, test loss : 0.4906, test accuracy : 94.51\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0030, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0027, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0025, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0027, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0027, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0027, accuracy : 99.90\n",
            "Epoch : 165, training loss : 0.0027, training accuracy : 99.90, test loss : 0.4960, test accuracy : 94.71\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0028, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0031, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0033, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0032, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0032, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0034, accuracy : 99.90\n",
            "Epoch : 166, training loss : 0.0038, training accuracy : 99.89, test loss : 0.4827, test accuracy : 94.71\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0027, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0019, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0024, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0024, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0022, accuracy : 99.92\n",
            "iteration : 300, loss : 0.0023, accuracy : 99.92\n",
            "iteration : 350, loss : 0.0022, accuracy : 99.92\n",
            "Epoch : 167, training loss : 0.0022, training accuracy : 99.92, test loss : 0.4626, test accuracy : 94.60\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0017, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0020, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0023, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0024, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0025, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0028, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0030, accuracy : 99.89\n",
            "Epoch : 168, training loss : 0.0030, training accuracy : 99.89, test loss : 0.4635, test accuracy : 94.78\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0022, accuracy : 99.93\n",
            "iteration : 150, loss : 0.0025, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0026, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0028, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0031, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0030, accuracy : 99.90\n",
            "Epoch : 169, training loss : 0.0029, training accuracy : 99.90, test loss : 0.4837, test accuracy : 94.76\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0019, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0019, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0018, accuracy : 99.94\n",
            "iteration : 200, loss : 0.0017, accuracy : 99.94\n",
            "iteration : 250, loss : 0.0018, accuracy : 99.93\n",
            "iteration : 300, loss : 0.0020, accuracy : 99.92\n",
            "iteration : 350, loss : 0.0021, accuracy : 99.92\n",
            "Epoch : 170, training loss : 0.0021, training accuracy : 99.93, test loss : 0.4738, test accuracy : 94.80\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0033, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0033, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0027, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0023, accuracy : 99.92\n",
            "iteration : 250, loss : 0.0023, accuracy : 99.92\n",
            "iteration : 300, loss : 0.0020, accuracy : 99.93\n",
            "iteration : 350, loss : 0.0019, accuracy : 99.93\n",
            "Epoch : 171, training loss : 0.0019, training accuracy : 99.93, test loss : 0.4769, test accuracy : 94.70\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0011, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0015, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0018, accuracy : 99.95\n",
            "iteration : 200, loss : 0.0017, accuracy : 99.94\n",
            "iteration : 250, loss : 0.0017, accuracy : 99.94\n",
            "iteration : 300, loss : 0.0019, accuracy : 99.93\n",
            "iteration : 350, loss : 0.0022, accuracy : 99.92\n",
            "Epoch : 172, training loss : 0.0023, training accuracy : 99.92, test loss : 0.4931, test accuracy : 94.60\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0023, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0020, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0026, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0025, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0026, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0029, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0030, accuracy : 99.88\n",
            "Epoch : 173, training loss : 0.0030, training accuracy : 99.89, test loss : 0.5079, test accuracy : 94.43\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0031, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0027, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0029, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0026, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0026, accuracy : 99.91\n",
            "iteration : 300, loss : 0.0026, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0024, accuracy : 99.92\n",
            "Epoch : 174, training loss : 0.0023, training accuracy : 99.92, test loss : 0.4764, test accuracy : 94.87\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0009, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0010, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0011, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0011, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0011, accuracy : 99.96\n",
            "Epoch : 175, training loss : 0.0011, training accuracy : 99.96, test loss : 0.4767, test accuracy : 94.87\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0013, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0011, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0011, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0015, accuracy : 99.94\n",
            "iteration : 250, loss : 0.0015, accuracy : 99.94\n",
            "iteration : 300, loss : 0.0016, accuracy : 99.95\n",
            "iteration : 350, loss : 0.0015, accuracy : 99.95\n",
            "Epoch : 176, training loss : 0.0016, training accuracy : 99.95, test loss : 0.5044, test accuracy : 94.74\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0011, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0012, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0011, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0010, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0011, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0012, accuracy : 99.95\n",
            "iteration : 350, loss : 0.0013, accuracy : 99.95\n",
            "Epoch : 177, training loss : 0.0012, training accuracy : 99.95, test loss : 0.5165, test accuracy : 94.74\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0023, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0029, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0024, accuracy : 99.92\n",
            "iteration : 250, loss : 0.0024, accuracy : 99.92\n",
            "iteration : 300, loss : 0.0026, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0032, accuracy : 99.91\n",
            "Epoch : 178, training loss : 0.0033, training accuracy : 99.90, test loss : 0.5279, test accuracy : 94.30\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0072, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0037, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0033, accuracy : 99.91\n",
            "iteration : 300, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0030, accuracy : 99.91\n",
            "Epoch : 179, training loss : 0.0029, training accuracy : 99.92, test loss : 0.4982, test accuracy : 94.87\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0017, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0015, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0021, accuracy : 99.94\n",
            "iteration : 200, loss : 0.0021, accuracy : 99.94\n",
            "iteration : 250, loss : 0.0020, accuracy : 99.94\n",
            "iteration : 300, loss : 0.0018, accuracy : 99.95\n",
            "iteration : 350, loss : 0.0018, accuracy : 99.94\n",
            "Epoch : 180, training loss : 0.0018, training accuracy : 99.94, test loss : 0.4995, test accuracy : 94.85\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0023, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0018, accuracy : 99.93\n",
            "iteration : 150, loss : 0.0014, accuracy : 99.95\n",
            "iteration : 200, loss : 0.0013, accuracy : 99.95\n",
            "iteration : 250, loss : 0.0013, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0012, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0012, accuracy : 99.96\n",
            "Epoch : 181, training loss : 0.0012, training accuracy : 99.96, test loss : 0.5136, test accuracy : 94.68\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0013, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0015, accuracy : 99.93\n",
            "iteration : 150, loss : 0.0015, accuracy : 99.94\n",
            "iteration : 200, loss : 0.0014, accuracy : 99.95\n",
            "iteration : 250, loss : 0.0014, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0015, accuracy : 99.95\n",
            "iteration : 350, loss : 0.0016, accuracy : 99.96\n",
            "Epoch : 182, training loss : 0.0015, training accuracy : 99.96, test loss : 0.4761, test accuracy : 94.95\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0013, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0013, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0019, accuracy : 99.94\n",
            "iteration : 200, loss : 0.0024, accuracy : 99.94\n",
            "iteration : 250, loss : 0.0022, accuracy : 99.94\n",
            "iteration : 300, loss : 0.0021, accuracy : 99.94\n",
            "iteration : 350, loss : 0.0023, accuracy : 99.93\n",
            "Epoch : 183, training loss : 0.0022, training accuracy : 99.93, test loss : 0.5165, test accuracy : 94.69\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0020, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0022, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0019, accuracy : 99.95\n",
            "iteration : 200, loss : 0.0019, accuracy : 99.95\n",
            "iteration : 250, loss : 0.0018, accuracy : 99.94\n",
            "iteration : 300, loss : 0.0017, accuracy : 99.94\n",
            "iteration : 350, loss : 0.0016, accuracy : 99.94\n",
            "Epoch : 184, training loss : 0.0016, training accuracy : 99.95, test loss : 0.4969, test accuracy : 94.82\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0010, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0010, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0009, accuracy : 99.97\n",
            "Epoch : 185, training loss : 0.0010, training accuracy : 99.97, test loss : 0.5044, test accuracy : 94.86\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0012, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0012, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0012, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0011, accuracy : 99.97\n",
            "Epoch : 186, training loss : 0.0012, training accuracy : 99.97, test loss : 0.5066, test accuracy : 94.89\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0010, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0014, accuracy : 99.95\n",
            "iteration : 350, loss : 0.0017, accuracy : 99.94\n",
            "Epoch : 187, training loss : 0.0018, training accuracy : 99.94, test loss : 0.5322, test accuracy : 94.63\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0014, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0014, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0012, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0015, accuracy : 99.95\n",
            "iteration : 300, loss : 0.0016, accuracy : 99.94\n",
            "iteration : 350, loss : 0.0016, accuracy : 99.94\n",
            "Epoch : 188, training loss : 0.0017, training accuracy : 99.94, test loss : 0.5086, test accuracy : 94.83\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0033, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0035, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0032, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0029, accuracy : 99.92\n",
            "iteration : 350, loss : 0.0029, accuracy : 99.92\n",
            "Epoch : 189, training loss : 0.0028, training accuracy : 99.92, test loss : 0.5059, test accuracy : 94.83\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0011, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0014, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0017, accuracy : 99.94\n",
            "iteration : 200, loss : 0.0016, accuracy : 99.95\n",
            "iteration : 250, loss : 0.0014, accuracy : 99.95\n",
            "iteration : 300, loss : 0.0018, accuracy : 99.94\n",
            "iteration : 350, loss : 0.0016, accuracy : 99.94\n",
            "Epoch : 190, training loss : 0.0016, training accuracy : 99.95, test loss : 0.5059, test accuracy : 94.90\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0012, accuracy : 99.96\n",
            "Epoch : 191, training loss : 0.0013, training accuracy : 99.95, test loss : 0.5227, test accuracy : 94.88\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0012, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0016, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0015, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0014, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0013, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0014, accuracy : 99.96\n",
            "Epoch : 192, training loss : 0.0014, training accuracy : 99.96, test loss : 0.5137, test accuracy : 94.89\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0017, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0022, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0019, accuracy : 99.94\n",
            "iteration : 200, loss : 0.0017, accuracy : 99.95\n",
            "iteration : 250, loss : 0.0017, accuracy : 99.94\n",
            "iteration : 300, loss : 0.0015, accuracy : 99.95\n",
            "iteration : 350, loss : 0.0014, accuracy : 99.95\n",
            "Epoch : 193, training loss : 0.0013, training accuracy : 99.95, test loss : 0.4954, test accuracy : 94.96\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0015, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0025, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0022, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0020, accuracy : 99.93\n",
            "iteration : 250, loss : 0.0019, accuracy : 99.93\n",
            "iteration : 300, loss : 0.0018, accuracy : 99.94\n",
            "iteration : 350, loss : 0.0016, accuracy : 99.95\n",
            "Epoch : 194, training loss : 0.0015, training accuracy : 99.95, test loss : 0.5015, test accuracy : 94.88\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0021, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0016, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0017, accuracy : 99.95\n",
            "iteration : 200, loss : 0.0018, accuracy : 99.94\n",
            "iteration : 250, loss : 0.0016, accuracy : 99.94\n",
            "iteration : 300, loss : 0.0014, accuracy : 99.95\n",
            "iteration : 350, loss : 0.0013, accuracy : 99.95\n",
            "Epoch : 195, training loss : 0.0013, training accuracy : 99.95, test loss : 0.5036, test accuracy : 94.91\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0010, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0009, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.97\n",
            "Epoch : 196, training loss : 0.0009, training accuracy : 99.97, test loss : 0.4996, test accuracy : 94.92\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0010, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0011, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0011, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0010, accuracy : 99.98\n",
            "Epoch : 197, training loss : 0.0010, training accuracy : 99.97, test loss : 0.5178, test accuracy : 94.99\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0018, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0018, accuracy : 99.93\n",
            "iteration : 150, loss : 0.0020, accuracy : 99.93\n",
            "iteration : 200, loss : 0.0023, accuracy : 99.92\n",
            "iteration : 250, loss : 0.0021, accuracy : 99.93\n",
            "iteration : 300, loss : 0.0019, accuracy : 99.93\n",
            "iteration : 350, loss : 0.0019, accuracy : 99.94\n",
            "Epoch : 198, training loss : 0.0020, training accuracy : 99.94, test loss : 0.5068, test accuracy : 94.95\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0017, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0014, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0014, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0019, accuracy : 99.95\n",
            "iteration : 250, loss : 0.0019, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0017, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0016, accuracy : 99.96\n",
            "Epoch : 199, training loss : 0.0016, training accuracy : 99.96, test loss : 0.5100, test accuracy : 94.85\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0008, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0010, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0009, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0014, accuracy : 99.95\n",
            "iteration : 300, loss : 0.0012, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0014, accuracy : 99.95\n",
            "Epoch : 200, training loss : 0.0014, training accuracy : 99.95, test loss : 0.4981, test accuracy : 94.84\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0020, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0014, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0011, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0011, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0010, accuracy : 99.96\n",
            "Epoch : 201, training loss : 0.0010, training accuracy : 99.96, test loss : 0.5104, test accuracy : 94.91\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.98\n",
            "Epoch : 202, training loss : 0.0008, training accuracy : 99.97, test loss : 0.5263, test accuracy : 94.76\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0004, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch : 203, training loss : 0.0006, training accuracy : 99.97, test loss : 0.5344, test accuracy : 94.78\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0022, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0016, accuracy : 99.92\n",
            "iteration : 150, loss : 0.0012, accuracy : 99.94\n",
            "iteration : 200, loss : 0.0011, accuracy : 99.95\n",
            "iteration : 250, loss : 0.0010, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0010, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0009, accuracy : 99.96\n",
            "Epoch : 204, training loss : 0.0008, training accuracy : 99.96, test loss : 0.5306, test accuracy : 94.99\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.98\n",
            "Epoch : 205, training loss : 0.0008, training accuracy : 99.98, test loss : 0.5358, test accuracy : 94.89\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0009, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0015, accuracy : 99.95\n",
            "iteration : 250, loss : 0.0013, accuracy : 99.96\n",
            "iteration : 300, loss : 0.0013, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0014, accuracy : 99.95\n",
            "Epoch : 206, training loss : 0.0014, training accuracy : 99.95, test loss : 0.5135, test accuracy : 95.02\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0011, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.97\n",
            "Epoch : 207, training loss : 0.0008, training accuracy : 99.97, test loss : 0.5161, test accuracy : 94.98\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.98\n",
            "Epoch : 208, training loss : 0.0005, training accuracy : 99.98, test loss : 0.5251, test accuracy : 94.98\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.97\n",
            "Epoch : 209, training loss : 0.0008, training accuracy : 99.97, test loss : 0.5374, test accuracy : 94.88\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 210, training loss : 0.0005, training accuracy : 99.98, test loss : 0.5358, test accuracy : 94.98\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.98\n",
            "Epoch : 211, training loss : 0.0006, training accuracy : 99.98, test loss : 0.5379, test accuracy : 94.89\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.96\n",
            "Epoch : 212, training loss : 0.0009, training accuracy : 99.96, test loss : 0.5421, test accuracy : 94.76\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 213, training loss : 0.0004, training accuracy : 99.98, test loss : 0.5452, test accuracy : 94.84\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0010, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0010, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0009, accuracy : 99.97\n",
            "Epoch : 214, training loss : 0.0008, training accuracy : 99.97, test loss : 0.5412, test accuracy : 94.96\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 215, training loss : 0.0004, training accuracy : 99.99, test loss : 0.5549, test accuracy : 94.96\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.97\n",
            "Epoch : 216, training loss : 0.0007, training accuracy : 99.97, test loss : 0.5456, test accuracy : 95.01\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0007, accuracy : 99.96\n",
            "Epoch : 217, training loss : 0.0008, training accuracy : 99.96, test loss : 0.5472, test accuracy : 95.00\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0011, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0009, accuracy : 99.95\n",
            "iteration : 200, loss : 0.0011, accuracy : 99.96\n",
            "iteration : 250, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0009, accuracy : 99.96\n",
            "iteration : 350, loss : 0.0009, accuracy : 99.96\n",
            "Epoch : 218, training loss : 0.0009, training accuracy : 99.96, test loss : 0.5523, test accuracy : 94.91\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0013, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0014, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0010, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0007, accuracy : 99.97\n",
            "Epoch : 219, training loss : 0.0008, training accuracy : 99.96, test loss : 0.5455, test accuracy : 94.92\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0007, accuracy : 99.98\n",
            "Epoch : 220, training loss : 0.0007, training accuracy : 99.98, test loss : 0.5381, test accuracy : 94.93\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.99\n",
            "Epoch : 221, training loss : 0.0005, training accuracy : 99.99, test loss : 0.5509, test accuracy : 94.91\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 222, training loss : 0.0005, training accuracy : 99.98, test loss : 0.5366, test accuracy : 95.13\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 223, training loss : 0.0003, training accuracy : 99.98, test loss : 0.5531, test accuracy : 95.04\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0012, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0010, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0006, accuracy : 99.97\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch : 224, training loss : 0.0006, training accuracy : 99.98, test loss : 0.5591, test accuracy : 94.86\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0019, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0013, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0015, accuracy : 99.96\n",
            "iteration : 200, loss : 0.0012, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 300, loss : 0.0009, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0008, accuracy : 99.98\n",
            "Epoch : 225, training loss : 0.0008, training accuracy : 99.98, test loss : 0.5578, test accuracy : 94.83\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0008, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0009, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0007, accuracy : 99.98\n",
            "Epoch : 226, training loss : 0.0007, training accuracy : 99.98, test loss : 0.5505, test accuracy : 94.99\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0006, accuracy : 99.98\n",
            "Epoch : 227, training loss : 0.0006, training accuracy : 99.98, test loss : 0.5642, test accuracy : 94.91\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 228, training loss : 0.0004, training accuracy : 99.98, test loss : 0.5473, test accuracy : 94.95\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.99\n",
            "Epoch : 229, training loss : 0.0004, training accuracy : 99.99, test loss : 0.5458, test accuracy : 94.98\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 230, training loss : 0.0003, training accuracy : 99.99, test loss : 0.5650, test accuracy : 94.93\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0009, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.99\n",
            "Epoch : 231, training loss : 0.0004, training accuracy : 99.99, test loss : 0.5672, test accuracy : 94.75\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 232, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5514, test accuracy : 95.12\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 233, training loss : 0.0003, training accuracy : 99.99, test loss : 0.5563, test accuracy : 95.02\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.99\n",
            "Epoch : 234, training loss : 0.0005, training accuracy : 99.99, test loss : 0.5620, test accuracy : 95.16\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 235, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5618, test accuracy : 94.99\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 236, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5581, test accuracy : 94.90\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 200, loss : 0.0005, accuracy : 99.97\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 237, training loss : 0.0004, training accuracy : 99.98, test loss : 0.5675, test accuracy : 94.99\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 238, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5658, test accuracy : 94.99\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 239, training loss : 0.0003, training accuracy : 99.99, test loss : 0.5526, test accuracy : 95.07\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 240, training loss : 0.0003, training accuracy : 99.98, test loss : 0.5721, test accuracy : 94.96\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0016, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0008, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0006, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0006, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.99\n",
            "Epoch : 241, training loss : 0.0005, training accuracy : 99.99, test loss : 0.5691, test accuracy : 94.98\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 242, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5609, test accuracy : 94.96\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0005, accuracy : 99.98\n",
            "Epoch : 243, training loss : 0.0005, training accuracy : 99.98, test loss : 0.5609, test accuracy : 94.98\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 244, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5606, test accuracy : 94.99\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 245, training loss : 0.0003, training accuracy : 99.98, test loss : 0.5698, test accuracy : 94.95\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 246, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5729, test accuracy : 95.06\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0007, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0006, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 247, training loss : 0.0004, training accuracy : 99.99, test loss : 0.5595, test accuracy : 95.02\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 248, training loss : 0.0003, training accuracy : 99.99, test loss : 0.5571, test accuracy : 95.12\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 249, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5765, test accuracy : 95.03\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.99\n",
            "Epoch : 250, training loss : 0.0003, training accuracy : 99.99, test loss : 0.5568, test accuracy : 95.13\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0010, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0007, accuracy : 99.97\n",
            "iteration : 150, loss : 0.0005, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0004, accuracy : 99.98\n",
            "Epoch : 251, training loss : 0.0003, training accuracy : 99.98, test loss : 0.5647, test accuracy : 95.10\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0003, accuracy : 99.98\n",
            "Epoch : 252, training loss : 0.0003, training accuracy : 99.98, test loss : 0.5543, test accuracy : 95.11\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 253, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5563, test accuracy : 95.16\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 254, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5461, test accuracy : 95.17\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.98\n",
            "Epoch : 255, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5574, test accuracy : 94.99\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 256, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5527, test accuracy : 95.22\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 257, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5607, test accuracy : 95.06\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 258, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5606, test accuracy : 95.12\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 259, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5626, test accuracy : 95.15\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 260, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5650, test accuracy : 95.04\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 261, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5637, test accuracy : 95.16\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 262, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5682, test accuracy : 95.17\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 263, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5555, test accuracy : 95.10\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 264, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5632, test accuracy : 95.15\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 265, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5715, test accuracy : 95.09\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 266, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5667, test accuracy : 95.06\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 267, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5770, test accuracy : 95.21\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 268, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5655, test accuracy : 95.06\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 269, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5773, test accuracy : 95.02\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0003, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 270, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5692, test accuracy : 95.14\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 271, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5740, test accuracy : 95.11\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 272, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5696, test accuracy : 95.13\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 273, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5626, test accuracy : 95.18\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 274, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5509, test accuracy : 95.21\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 275, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5677, test accuracy : 95.17\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0000, accuracy : 100.00\n",
            "Epoch : 276, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5753, test accuracy : 95.17\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 277, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5721, test accuracy : 95.19\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 278, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5749, test accuracy : 95.02\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 279, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5700, test accuracy : 95.17\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 280, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5800, test accuracy : 95.13\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 281, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5632, test accuracy : 95.19\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 282, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5753, test accuracy : 95.20\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 283, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5712, test accuracy : 95.13\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 284, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5708, test accuracy : 95.09\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0000, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 285, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5699, test accuracy : 95.11\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 286, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5771, test accuracy : 95.24\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 287, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5779, test accuracy : 95.13\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 288, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5692, test accuracy : 95.17\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 289, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5920, test accuracy : 95.01\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 290, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5679, test accuracy : 95.23\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0004, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0002, accuracy : 99.99\n",
            "Epoch : 291, training loss : 0.0002, training accuracy : 99.99, test loss : 0.5757, test accuracy : 95.07\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 292, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5782, test accuracy : 95.11\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.98\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 293, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5681, test accuracy : 95.22\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 294, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5809, test accuracy : 95.16\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0003, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.98\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 295, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5651, test accuracy : 95.23\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 296, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5723, test accuracy : 95.14\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0000, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 99.99\n",
            "Epoch : 297, training loss : 0.0001, training accuracy : 99.99, test loss : 0.5712, test accuracy : 95.11\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0002, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 150, loss : 0.0002, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 298, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5751, test accuracy : 95.15\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 300, loss : 0.0001, accuracy : 99.99\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 299, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5795, test accuracy : 95.14\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 100, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 150, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 200, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 250, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 300, loss : 0.0001, accuracy : 100.00\n",
            "iteration : 350, loss : 0.0001, accuracy : 100.00\n",
            "Epoch : 300, training loss : 0.0001, training accuracy : 100.00, test loss : 0.5857, test accuracy : 95.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp-wwJeWxXQt",
        "outputId": "abcfcbcf-fc0c-4ed1-cee9-6d21d3ce0214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5097350562904396, 95.77443146896128)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : cosine annealing\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "sOsdo848xZuy",
        "outputId": "da030910-16d6-4311-ea63-607595653863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : cosine annealing\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "s8861nSOxaak",
        "outputId": "c1d30c20-2e03-4265-f3e0-814173812b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_cosine = {train_loss_list}\") \n",
        "print(f\"train_acc_list_cosine = {train_acc_list}\")\n",
        "print(f\"test_loss_list_cosine = {test_loss_list}\")\n",
        "print(f\"test_acc_list_cosine = {test_acc_list}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkszTB8bxcOV",
        "outputId": "3cb7c3ec-abde-4737-ed28-7756805b7393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_cosine = [1.4922631353059113, 0.4829037673670425, 0.3722163860390826, 0.33177220926375245, 0.30060175129876227, 0.2739472426776964, 0.2571369293057499, 0.24421742562517565, 0.23246381484315323, 0.22585582048670064, 0.2115350698794776, 0.2027042203842786, 0.19336028488953586, 0.18796989869901803, 0.17635167322467335, 0.17350608562712425, 0.16392932351688705, 0.15824737119157786, 0.15103116432623812, 0.14280527628211148, 0.13911624957976465, 0.12778460819445814, 0.12645664817696503, 0.12117257786202963, 0.11404008718162048, 0.11024573898614261, 0.10301367604352918, 0.09962807324661957, 0.09590673247266429, 0.09140933892338822, 0.08890839097617563, 0.08312069913650028, 0.07912878103322049, 0.07539688910895247, 0.0703017507051307, 0.06730901317161112, 0.06545131962347604, 0.06006853946239806, 0.05934957680948644, 0.055661115047034776, 0.05501487654656535, 0.04808283364715756, 0.048917624036854686, 0.04978224897863177, 0.04599738403836765, 0.043476243917943865, 0.04154425250810538, 0.044289368833225914, 0.039924735993999975, 0.038134503976484824, 0.03573395804436448, 0.03633099462935413, 0.03407486775469762, 0.03466019954098273, 0.03175460545031914, 0.030751291346965928, 0.02861118235254312, 0.026954070920645837, 0.026345375887917098, 0.027870583739497655, 0.02564559653584358, 0.02595152790995118, 0.02664908538916373, 0.017370514487667858, 0.02682043477352759, 0.02288949422978072, 0.023653404380517905, 0.021177587269342865, 0.01825827019425427, 0.020935813946993946, 0.02142309610224594, 0.021006980622225117, 0.017297085196421504, 0.020975425832167893, 0.016085938927383088, 0.019187606733535582, 0.016529617174421272, 0.017658664713450265, 0.017461861724943822, 0.01473946469002416, 0.013888732157426767, 0.018813401251478592, 0.018245284999345322, 0.013429369967483517, 0.013213716673361565, 0.014179550147890478, 0.013165565478723676, 0.013442610755386424, 0.011938090494525969, 0.013952531346849035, 0.012777711567658235, 0.01361946394184095, 0.01415981548655886, 0.0132249127186142, 0.008202303269795845, 0.013768190907590946, 0.01187117434464689, 0.011072834069390408, 0.011375907309212407, 0.008056683960140176, 0.010813868758558904, 0.010755819991023294, 0.011055263281323623, 0.009854590676922565, 0.009897416544746738, 0.007694702545891319, 0.010974907935630917, 0.009073475042950686, 0.00957946586983009, 0.008521491170432235, 0.009181163735296882, 0.008033073926435217, 0.0075308985053745, 0.007736907337884813, 0.008905222457779166, 0.0075535059801524005, 0.008956405532912782, 0.00652504011866664, 0.007371700082288889, 0.007105531808951359, 0.009608815764438242, 0.006718999051766881, 0.006360923469938631, 0.006009158956909616, 0.005984226889401126, 0.006414240613540077, 0.008046470651133564, 0.006813095370703897, 0.004941387222356937, 0.006961080895634556, 0.0061504495415253034, 0.0044804993527444515, 0.005129817030924186, 0.0051584241011990275, 0.005904302868943308, 0.007040096893751609, 0.006497695825108162, 0.006231063241578372, 0.0045172427992303645, 0.004859256595798965, 0.005357292924210756, 0.0036115553579709734, 0.004059441841032762, 0.003976077818785122, 0.004686579279045762, 0.0047765986038821135, 0.0052203105924466305, 0.004452093922426263, 0.00366650621972908, 0.004813907496713725, 0.003619882242262452, 0.003379426909955734, 0.003642032724296165, 0.003996376050219234, 0.0027063930590780893, 0.0038246032934270147, 0.0035842454059730284, 0.0031291756938454703, 0.003011805406823518, 0.004053137657470737, 0.003489878551598058, 0.002363212944410382, 0.003003648613003568, 0.002955618500260455, 0.002723948648240625, 0.0037992216377342554, 0.0022372855783796795, 0.002972214323449715, 0.0029076020603400095, 0.002062946104120749, 0.0018581666937362376, 0.002313281514427136, 0.0030319557007007047, 0.0023327526890917524, 0.001145549262753536, 0.0016048090303144983, 0.0012414230531554533, 0.0032581396387386516, 0.0029362018629135516, 0.001846583708184242, 0.0012200965248589121, 0.001480849612214921, 0.002236391388236972, 0.0015825234965880586, 0.0009787926420108245, 0.001177633173575748, 0.0017891079141621141, 0.001687879517755926, 0.0027874750058864543, 0.0015881228617325382, 0.0013229923813940598, 0.0013549324787301051, 0.0013446073458035835, 0.0015000026920124656, 0.0012523066383357765, 0.000858102498131121, 0.001035546338261591, 0.0019881639752974524, 0.0016324728715111814, 0.001434380197954095, 0.0009871888154247414, 0.0008098082351434123, 0.0006027996978315856, 0.0008483088922217938, 0.0007788997386943644, 0.0014305688407895846, 0.0007751793022755421, 0.0005314110757599325, 0.0008008613197451418, 0.0005263145778683319, 0.0005657160187561332, 0.0008550505100606119, 0.0004061993359216422, 0.0008238756363850807, 0.0004022425374623086, 0.0007475109217440491, 0.0008010522570415786, 0.0008719455779264424, 0.0008102537067081855, 0.0006836875085969626, 0.000508847326879101, 0.00048076220441707043, 0.0003452522178527452, 0.0005887742460472115, 0.0007575123468680768, 0.0007316459814930954, 0.0006069562958639393, 0.00040482217438505993, 0.000413289390024444, 0.0002512041357034269, 0.00035321101188540014, 0.0002289985919775472, 0.00031013075728369315, 0.00048718760862263036, 0.00024815363102110633, 0.0002127035124524638, 0.00040036594692248164, 0.00022304668872061623, 0.0002539508311412804, 0.00033001078329677927, 0.0005046333238317957, 0.0002472649033615957, 0.00045928622355630194, 0.00022344069632032276, 0.00026002821110843074, 0.00015903297622683538, 0.00040398000405654787, 0.0002816041986039678, 0.00024018690979448482, 0.00029882328487923265, 0.00034577957207554183, 0.00028206157859465874, 0.00017730721445880266, 0.00011439717115794378, 0.00023043857095496602, 0.00013369225500067483, 0.00022097409422235014, 0.00013705560435689807, 0.000107482878806249, 0.0002181373778419168, 9.028549420162597e-05, 8.780925534434591e-05, 0.00015708037552040194, 0.00013996531600239778, 0.00015848159901898015, 9.64823931015096e-05, 0.00021168788567095436, 0.000136050276398834, 0.00010472044780222292, 0.0002206477002030788, 0.00013934975601720095, 0.00010074895917807085, 9.32180454911494e-05, 0.0001472607405713623, 0.00011569483834266491, 7.036176643037457e-05, 0.0001351957850342979, 0.000159376835863944, 0.00010674569268919185, 0.0001185344251126515, 8.718404802724013e-05, 0.00011900450702660869, 0.00012125460894170754, 0.00011230571605721275, 0.00010454212349596384, 0.0001487989837777661, 6.830416546227417e-05, 6.393169394098347e-05, 0.00020005663507281302, 0.00010431355027100938, 0.00017086659031600928, 7.99108349451789e-05, 0.00011386882648142751, 9.478551902827757e-05, 0.00010300004370941282, 8.344318782375742e-05, 0.0001274786575664539, 8.525987476745569e-05, 7.452874058836112e-05, 0.00011550953129777401]\n",
            "train_acc_list_cosine = [47.73742721016411, 84.54632080465855, 88.4425622022234, 89.96717840127052, 90.86712546320804, 91.64849126521969, 92.33245103229221, 92.79195341450503, 93.21757543673901, 93.32980412916888, 93.78295394388566, 94.04129168872419, 94.3928004235045, 94.53679195341451, 94.81206987824245, 95.05134992059291, 95.20169401799895, 95.48755955532027, 95.65484383271573, 95.92165166754897, 95.9640021175225, 96.39385918475384, 96.32186341979883, 96.55479089465325, 96.74748544203283, 96.74748544203283, 97.07146638433034, 97.24722075172049, 97.19640021175225, 97.32980412916888, 97.39332980412917, 97.62413975648491, 97.67496029645315, 97.79777660137638, 97.90365272631021, 97.96717840127052, 98.07940709370037, 98.21492853361568, 98.19163578613023, 98.32715722604553, 98.32292218104817, 98.55584965590259, 98.57702488088935, 98.43938591847538, 98.58973001588141, 98.68713605082054, 98.66807834833246, 98.61302276336686, 98.80359978824775, 98.78454208575967, 98.83748014822658, 98.80783483324511, 98.86077289571202, 98.89253573319216, 98.9433562731604, 99.02805717310747, 99.05982001058761, 99.13816834303864, 99.14452091053468, 99.0979354155638, 99.13393329804128, 99.1148755955532, 99.10217046056114, 99.45579671784013, 99.15510852302806, 99.22710428798305, 99.22498676548439, 99.32451032292218, 99.39650608787719, 99.32662784542086, 99.27368978295394, 99.3774483853891, 99.41132874536792, 99.29698253043938, 99.45367919534145, 99.3668607728957, 99.47061937533087, 99.43885653785071, 99.43250397035469, 99.51932239280042, 99.5404976177872, 99.33509793541556, 99.3774483853891, 99.5299100052938, 99.54473266278454, 99.5129698253044, 99.57014293276866, 99.57014293276866, 99.6294335627316, 99.48967707781895, 99.55320275277924, 99.5659078877713, 99.53202752779248, 99.58073054526204, 99.73742721016411, 99.53838009528852, 99.61461090524087, 99.6294335627316, 99.63366860772896, 99.7289571201694, 99.63155108523029, 99.64213869772367, 99.61884595023822, 99.71201694017999, 99.69084171519323, 99.74589730015882, 99.63790365272631, 99.72260455267337, 99.69719428268925, 99.72260455267337, 99.66754896770779, 99.70354685018528, 99.73954473266278, 99.72683959767072, 99.67601905770249, 99.73319216516676, 99.71625198517734, 99.80518793012176, 99.7289571201694, 99.77130757014294, 99.6929592376919, 99.7924827951297, 99.77766013763896, 99.784012705135, 99.80730545262044, 99.78613022763366, 99.76071995764956, 99.79036527263102, 99.83906829010058, 99.76707252514558, 99.7924827951297, 99.86659608258337, 99.84118581259926, 99.8284806776072, 99.79883536262572, 99.77977766013764, 99.79671784012704, 99.79671784012704, 99.83059820010588, 99.84965590259397, 99.8369507676019, 99.86871360508205, 99.89412387506617, 99.87083112758073, 99.85177342509265, 99.85177342509265, 99.81365802011646, 99.85389094759132, 99.87294865007941, 99.84118581259926, 99.87294865007941, 99.89200635256749, 99.88353626257279, 99.87294865007941, 99.90682901005823, 99.87506617257809, 99.87506617257809, 99.89624139756485, 99.90047644256221, 99.85812599258867, 99.88988883006881, 99.91953414505029, 99.90894653255691, 99.89412387506617, 99.90047644256221, 99.88988883006881, 99.92165166754897, 99.88777130757015, 99.90047644256221, 99.92588671254632, 99.93223928004235, 99.91953414505029, 99.88565378507147, 99.91953414505029, 99.95764955002647, 99.94917946003176, 99.95341450502912, 99.90471148755955, 99.91529910005293, 99.94070937003706, 99.95764955002647, 99.95764955002647, 99.93435680254103, 99.94706193753309, 99.9724722075172, 99.96611964002118, 99.9364743250397, 99.94282689253573, 99.91953414505029, 99.94706193753309, 99.94917946003176, 99.96188459502382, 99.95341450502912, 99.94917946003176, 99.95129698253044, 99.96823716251986, 99.97458973001588, 99.9364743250397, 99.9555320275278, 99.95129698253044, 99.9640021175225, 99.97458973001588, 99.97458973001588, 99.9640021175225, 99.97882477501324, 99.94706193753309, 99.97458973001588, 99.97882477501324, 99.9724722075172, 99.98094229751192, 99.98305982001058, 99.96188459502382, 99.98305982001058, 99.97035468501853, 99.98517734250926, 99.97458973001588, 99.95976707252514, 99.9640021175225, 99.9640021175225, 99.97670725251456, 99.98729486500794, 99.97670725251456, 99.98305982001058, 99.97670725251456, 99.97882477501324, 99.97882477501324, 99.97670725251456, 99.98305982001058, 99.98729486500794, 99.98517734250926, 99.98941238750662, 99.98941238750662, 99.98941238750662, 99.98729486500794, 99.98941238750662, 99.9915299100053, 99.98094229751192, 99.9915299100053, 99.98517734250926, 99.98094229751192, 99.98729486500794, 99.98941238750662, 99.98094229751192, 99.98941238750662, 99.98305982001058, 99.99364743250398, 99.98517734250926, 99.98729486500794, 99.98941238750662, 99.9915299100053, 99.98094229751192, 99.98305982001058, 99.9915299100053, 99.99576495500264, 99.98517734250926, 99.9915299100053, 99.98941238750662, 99.99576495500264, 99.99576495500264, 99.98941238750662, 99.99364743250398, 100.0, 99.9915299100053, 99.9915299100053, 99.98941238750662, 99.99576495500264, 99.98729486500794, 99.99576495500264, 99.99364743250398, 99.9915299100053, 99.99364743250398, 99.99364743250398, 99.99576495500264, 99.98941238750662, 99.9915299100053, 99.99788247750132, 99.98941238750662, 99.9915299100053, 99.99788247750132, 99.99576495500264, 99.99576495500264, 99.99364743250398, 99.9915299100053, 99.99576495500264, 99.99364743250398, 99.99364743250398, 99.99788247750132, 100.0, 99.98941238750662, 99.99576495500264, 99.99364743250398, 99.99576495500264, 99.9915299100053, 99.99576495500264, 99.99364743250398, 99.99576495500264, 99.9915299100053, 99.99788247750132, 99.99576495500264, 99.99576495500264]\n",
            "test_loss_list_cosine = [0.8128060245630788, 0.43769084055926283, 0.3670157931160693, 0.3468363919094497, 0.320717461845454, 0.29874755628407, 0.2880513122414841, 0.27089574385215254, 0.2712526289636598, 0.25813622246770296, 0.27607545629143715, 0.2508813175281473, 0.23157318663217275, 0.23963026514313385, 0.23259784851004095, 0.21799716919514478, 0.2553474395992417, 0.23297278822271847, 0.23411084861293727, 0.24059130224015782, 0.22228851220479198, 0.23491562519441633, 0.23474602972832964, 0.24284341398115253, 0.2531394691699568, 0.2231898330809439, 0.23778781842659502, 0.2260036576612323, 0.2408501879476449, 0.2531653588601187, 0.2437402953521586, 0.25621635543511195, 0.25155149374668506, 0.25073284860335143, 0.24820336959708264, 0.2527083886933385, 0.265060295743466, 0.2638632501977697, 0.27953584988911945, 0.28330178079469237, 0.2898024614425559, 0.285278165183377, 0.28805743263322203, 0.29073836449898927, 0.3027702011898452, 0.3096775676069014, 0.30173745603921515, 0.3021223259468873, 0.30385272365574745, 0.3038429137702812, 0.31642839670473455, 0.31861640270068947, 0.3238279426963452, 0.3290770745598802, 0.3349987664123011, 0.3213929162525079, 0.33029763011590524, 0.34525904689422426, 0.3447993094028503, 0.330581444653445, 0.321795464146371, 0.3393226447423883, 0.33919910747813536, 0.36219359443102983, 0.34984895151437206, 0.34508059543155717, 0.3312660858683361, 0.3478064838449891, 0.37045160054649207, 0.36637233636871563, 0.3615635437069132, 0.34741060503338483, 0.36895488022698786, 0.3510999241478595, 0.3899413192069487, 0.3609655104285362, 0.3731252212287383, 0.38873775874940203, 0.3712333329387155, 0.37048297439354894, 0.39439879441816433, 0.373478519492874, 0.36712522848564033, 0.3914665420420979, 0.38469330903471394, 0.3913668092230664, 0.38145966287337096, 0.37784753624788103, 0.38290000464036766, 0.3881754014622785, 0.39466715339279057, 0.38420211229765533, 0.3928751589150588, 0.3871026979576723, 0.39617241206852827, 0.42028651262323063, 0.38088225707521334, 0.3810242618087168, 0.38620046643978534, 0.41002833922667536, 0.40598774530157883, 0.4145270490324965, 0.4107616166036357, 0.39369509952124576, 0.3983155651949346, 0.4124191829971239, 0.40144294117778245, 0.4075972944425017, 0.41423957538791, 0.4235737093589634, 0.4338786544680011, 0.41868019625818464, 0.4211389536400983, 0.42915419865783083, 0.43854167406428973, 0.44444839669135855, 0.4239410953870153, 0.4140438513154639, 0.4214930013826519, 0.4469692508014394, 0.42807763744620425, 0.4140391830473627, 0.40559402319109616, 0.4296429092794949, 0.447968863443855, 0.4537148197985017, 0.4160180985562357, 0.42932759126757875, 0.4655364652474721, 0.44017627959450084, 0.4476152099646153, 0.45807484502666723, 0.4661675389518267, 0.46650424552624864, 0.4483312015951264, 0.43905310305839806, 0.4307253006571794, 0.4221340075886224, 0.43024291792957514, 0.4534414633743319, 0.439005637879246, 0.455770535284982, 0.4492703357884916, 0.46906485925337266, 0.4463952654680493, 0.4687405661497192, 0.4440763243108842, 0.4453616064936653, 0.466629167502819, 0.4585120371433304, 0.45512742501017, 0.4547710154278606, 0.46565503765847166, 0.47993873200361054, 0.46249571734783695, 0.47604353346076667, 0.4764781709778689, 0.4798887720623729, 0.47362343249294686, 0.477798045156341, 0.475341152007162, 0.46233028874677773, 0.47649588846765895, 0.4905768512090778, 0.49597619045023605, 0.4826731762720966, 0.46256263993963526, 0.4635071344308409, 0.4836937281412675, 0.4737923650753538, 0.47685492698641896, 0.49306667890107514, 0.5079087016615542, 0.47638218596294596, 0.47670910018496215, 0.5044073596980203, 0.5164688170211864, 0.5278736670528922, 0.4981932266542286, 0.4995226736783105, 0.5136372742845732, 0.47614785384697217, 0.5164846121424845, 0.4969261799825985, 0.504426286427998, 0.5065735648545966, 0.5321676667674682, 0.5085760319283615, 0.5058678923307133, 0.505933805387102, 0.5226816843154237, 0.5136628734705714, 0.49541927424862103, 0.5014571096729853, 0.5036171923942782, 0.4995590589264883, 0.5177521720453275, 0.5068038706028578, 0.5099713094623796, 0.4981300873846254, 0.5104206896560523, 0.5262805088377539, 0.5344062875499767, 0.5306157725908812, 0.5357992586351055, 0.5134817360370767, 0.5160781835849562, 0.5250964877469575, 0.5374465012448091, 0.5358220952946473, 0.5378625886514783, 0.5421092164064711, 0.5452210189199403, 0.5411839835076392, 0.5549019162050065, 0.5455894065546054, 0.5472353847934773, 0.5522625445497825, 0.5455132025983367, 0.538114388518985, 0.5509232937326363, 0.536594347077577, 0.5530879087015694, 0.5591187921909652, 0.5578411847065368, 0.5505201403601163, 0.564152417302716, 0.5473365803345052, 0.5458330702511411, 0.5650019636080947, 0.5671965913600562, 0.5513512968934853, 0.5562908629117552, 0.5620296227371356, 0.5617975970529312, 0.5581122653555426, 0.5675084015614736, 0.5657756982859699, 0.5526409040846606, 0.5721038452220634, 0.5690990948042942, 0.5608643038853474, 0.5609114988775556, 0.560604001846692, 0.5697500870121397, 0.5728723204822517, 0.5595448732640886, 0.5570606883866632, 0.5764513230818671, 0.556780993596048, 0.5647389884220948, 0.5543315944429624, 0.5563175826808255, 0.5460904273889301, 0.5574481183030716, 0.552725079560689, 0.5607348622220075, 0.5605667959076955, 0.5626120614498515, 0.5649992083380507, 0.5637176143309083, 0.5681553194373526, 0.5555440176634446, 0.5632415316604059, 0.5714907293874478, 0.5667051952526284, 0.5770144956528812, 0.5655391789224072, 0.57731314366444, 0.5691993516018413, 0.5740196594384079, 0.5696190810367446, 0.5625503671834332, 0.5508576962696251, 0.5676857517648708, 0.575283505166333, 0.5721206341997958, 0.5749247564988977, 0.5699750101832929, 0.580029735533411, 0.5632244463092374, 0.5753009883919731, 0.5712149130710054, 0.5708004219147066, 0.5698616763567734, 0.5771289975888495, 0.577879674463332, 0.5692181185472245, 0.591953951887102, 0.5678756476480368, 0.5756764801849118, 0.578166978474816, 0.5680953554498652, 0.5808629620039616, 0.5650976837804431, 0.5722748214018294, 0.5711645722097042, 0.5751072838172024, 0.5794820262486681, 0.5856684064892047]\n",
            "test_acc_list_cosine = [75.21511985248924, 86.48970497848802, 88.72157344806392, 89.39766441303011, 90.33113091579594, 91.21850030731407, 91.4259373079287, 92.23647818070067, 92.30562384757222, 92.6859250153657, 91.92532267977873, 92.7819606637984, 93.44652735095268, 93.21988322065151, 93.53872157344806, 93.85371850030731, 92.85494775660726, 93.56561155500921, 93.33512599877075, 93.48494161032575, 93.96896127842655, 93.75768285187462, 93.72311001843885, 93.5579287031346, 93.30439459127228, 94.19560540872772, 93.9459127228027, 94.2340196681008, 93.98048555623848, 93.700061462815, 94.24554394591273, 93.68085433312845, 93.82298709280884, 94.05731407498463, 94.24938537185002, 94.29164105716042, 93.83835279655808, 94.26090964966195, 94.03042409342348, 93.97280270436386, 94.00353411186232, 94.06883835279656, 94.13414259373079, 94.00737553779963, 94.12261831591887, 93.93822987092808, 94.16871542716656, 93.9958512599877, 93.96127842655194, 94.22249539028887, 93.93438844499079, 93.91518131530424, 94.0918869084204, 94.01121696373694, 94.15719114935465, 94.29164105716042, 94.31853103872157, 94.01889981561156, 94.15719114935465, 93.88060848186846, 94.54133374308543, 94.34542102028273, 94.14950829748003, 94.09572833435772, 94.36078672403197, 94.21481253841426, 94.3262138905962, 94.27627535341118, 93.92286416717886, 94.05731407498463, 94.09572833435772, 94.41456668715428, 94.24554394591273, 94.31853103872157, 94.24170251997542, 94.06115550092194, 94.0419483712354, 93.95359557467732, 94.30316533497235, 94.42224953902888, 94.1917639827904, 94.02274124154886, 94.43761524277812, 94.12645974185618, 94.20712968653964, 94.40304240934235, 94.35694529809466, 94.29164105716042, 94.48371235402581, 94.13030116779349, 94.31853103872157, 94.3338967424708, 94.41840811309157, 94.66041794714198, 94.48755377996312, 94.09956976029503, 94.26090964966195, 94.34157959434542, 94.38767670559312, 94.4798709280885, 94.38767670559312, 94.36078672403197, 94.16871542716656, 94.64505224339274, 94.36846957590657, 94.48371235402581, 94.28011677934849, 94.46450522433928, 94.28779963122311, 94.36078672403197, 94.35694529809466, 94.55285802089736, 94.35310387215735, 94.48755377996312, 94.34926244622004, 94.47602950215119, 94.64889366933005, 94.46450522433928, 94.44145666871543, 94.54133374308543, 94.52980946527352, 94.54901659496005, 94.69883220651506, 94.58743085433314, 94.16871542716656, 94.41456668715428, 94.56438229870928, 94.5759065765212, 94.21097111247695, 94.54517516902274, 94.44529809465274, 94.46834665027659, 94.27627535341118, 94.34157959434542, 94.71035648432698, 94.49139520590043, 94.60663798401967, 94.75645359557468, 94.74108789182544, 94.66041794714198, 94.59895513214505, 94.66041794714198, 94.57974800245852, 94.57974800245852, 94.71035648432698, 94.54901659496005, 94.7679778733866, 94.7679778733866, 94.56438229870928, 94.67194222495391, 94.66425937307929, 94.74492931776275, 94.79486785494775, 94.44529809465274, 94.73724646588813, 94.43761524277812, 94.72956361401353, 94.72572218807622, 94.74877074370006, 94.66041794714198, 94.4798709280885, 94.80639213275968, 94.61432083589429, 94.50676090964966, 94.70651505838967, 94.70651505838967, 94.59895513214505, 94.78334357713584, 94.76413644744929, 94.79870928088506, 94.69883220651506, 94.59511370620774, 94.42609096496619, 94.87169637369392, 94.86785494775661, 94.74492931776275, 94.74108789182544, 94.29932390903504, 94.86785494775661, 94.84864781807006, 94.68346650276582, 94.94852489244008, 94.69499078057775, 94.82175783650891, 94.86017209588199, 94.89090350338046, 94.6258451137062, 94.82559926244622, 94.83328211432084, 94.90242778119237, 94.88322065150584, 94.89090350338046, 94.96004917025199, 94.87553779963122, 94.9139520590043, 94.91779348494161, 94.99462200368777, 94.94852489244008, 94.85248924400737, 94.83712354025815, 94.90626920712968, 94.76029502151198, 94.78334357713584, 94.99078057775046, 94.88706207744315, 95.02151198524892, 94.97925629993854, 94.97541487400123, 94.87553779963122, 94.97541487400123, 94.88706207744315, 94.76413644744929, 94.84096496619546, 94.9638905961893, 94.96004917025199, 95.01382913337432, 94.99846342962508, 94.91011063306699, 94.92163491087892, 94.93315918869084, 94.9139520590043, 95.12907191149354, 95.03687768899816, 94.8640135218193, 94.82559926244622, 94.98693915181315, 94.90626920712968, 94.95236631837739, 94.98309772587585, 94.92931776275353, 94.74877074370006, 95.11754763368162, 95.02151198524892, 95.15596189305471, 94.98693915181315, 94.89858635525508, 94.99462200368777, 94.98693915181315, 95.07145052243392, 94.9638905961893, 94.97925629993854, 94.9638905961893, 94.98309772587585, 94.99462200368777, 94.95236631837739, 95.059926244622, 95.01767055931161, 95.11754763368162, 95.02919483712354, 95.13291333743085, 95.09834050399509, 95.109864781807, 95.159803318992, 95.16748617086662, 94.98693915181315, 95.22126613398893, 95.06376767055932, 95.12138905961893, 95.1521204671174, 95.04071911493547, 95.16364474492931, 95.17132759680393, 95.09834050399509, 95.1521204671174, 95.08681622618316, 95.059926244622, 95.20974185617702, 95.059926244622, 95.02151198524892, 95.14443761524278, 95.109864781807, 95.12907191149354, 95.17901044867855, 95.20974185617702, 95.17132759680393, 95.17132759680393, 95.19437615242778, 95.02151198524892, 95.17132759680393, 95.12907191149354, 95.19437615242778, 95.19821757836509, 95.12907191149354, 95.09449907805778, 95.109864781807, 95.24431468961278, 95.12907191149354, 95.16748617086662, 95.01382913337432, 95.22894898586355, 95.06760909649662, 95.11370620774431, 95.21742470805162, 95.159803318992, 95.22894898586355, 95.14059618930547, 95.109864781807, 95.14827904118009, 95.13675476336816, 95.09449907805778]\n"
          ]
        }
      ]
    }
  ]
}