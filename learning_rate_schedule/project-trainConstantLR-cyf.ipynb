{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8ntKy6oKQGJv"
      },
      "outputs": [],
      "source": [
        "# ## Source code for unpickle function: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "# def unpickle(file):\n",
        "#     import pickle\n",
        "#     with open(file, 'rb') as fo:\n",
        "#         dict = pickle.load(fo, encoding='bytes')\n",
        "#     return dict\n",
        "\n",
        "# import numpy as np\n",
        "# def get_mean_color():\n",
        "#     d=unpickle('./data/cifar-10-batches-py/data_batch_1')\n",
        "#     channels = d[b'data']\n",
        "#     for i in range(2,6):\n",
        "#         d=unpickle('./data/cifar-10-batches-py/data_batch_'+str(i))\n",
        "#         channels=np.concatenate((channels, d[b'data']), axis=0)\n",
        "#     r=np.mean(channels[:,:1024])/255  \n",
        "#     g=np.mean(channels[:,1024:2048])/255\n",
        "#     b=np.mean(channels[:,2048:])/255\n",
        "#     return(r,g,b)\n",
        "# get_mean_color()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgAiImV0uURP",
        "outputId": "c4adf409-aeaa-4cca-a28f-d00a815c2f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./data\\train_32x32.mat\n",
            "Using downloaded and verified file: ./data\\test_32x32.mat\n"
          ]
        }
      ],
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    \n",
        "    transforms.ToTensor(),\n",
        "    #transforms.RandomErasing(value=get_mean_color()),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "d6a0bf3a-2d2f-4f76-f11c-5ec665a8b45d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "73257 26032\n"
          ]
        }
      ],
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te71lQ17B1L_"
      },
      "source": [
        "#Data Augmentation\n",
        "Data augmentation performs random modifications of the image as a preprocessing step. It serves the following purposes:\n",
        "1. It increases the amount of data for training.\n",
        "2. By deleting features, it prevents the network from relying on a narrow set of features, which may not generalize.\n",
        "3. By changing features while maintaining the same output, it helps the network become tolerant of changes that do not change the image lab. \n",
        "\n",
        "In short, data augmentation desensitivizes the network, so it extracts features that are invariant to changes that should not affect the prediction. \n",
        "\n",
        "We showcase a few random data augmentation provided by PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "RyG26xoJC0Pa"
      },
      "outputs": [],
      "source": [
        "# import torch.nn as nn\n",
        "# transforms = torch.nn.Sequential(\n",
        "#     T.Resize(256), # resize the short edge to 256.\n",
        "#     T.RandomCrop(224), #randomly crop a 224x224 region from the image\n",
        "#     T.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
        "#     #T.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0.1)\n",
        "#     #T.AutoAugment()\n",
        "# )\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# dog1 = dog1.to(device)\n",
        "# # dog2 = dog2.to(device)\n",
        "\n",
        "# # transformed_dog1 = transforms(dog1)\n",
        "# transformed_dog1 = transforms(dog1)\n",
        "# show([transformed_dog1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    #scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "outputs": [],
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "outputs": [],
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # This is the \"stem\"\n",
        "        # For CIFAR (32x32 images), it does not perform downsampling\n",
        "        # It should downsample for ImageNet\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "e9d8cddb-ef09-4a87-8154-e80569d015c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "74b13d96-61d4-43b8-c27c-b968190ddc62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(class_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ArgupDVRwB8i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3605, accuracy : 15.77\n",
            "iteration : 100, loss : 2.3059, accuracy : 17.33\n",
            "iteration : 150, loss : 2.2518, accuracy : 19.51\n",
            "iteration : 200, loss : 2.1372, accuracy : 23.86\n",
            "iteration : 250, loss : 1.9687, accuracy : 29.94\n",
            "iteration : 300, loss : 1.7984, accuracy : 36.38\n",
            "iteration : 350, loss : 1.6467, accuracy : 42.15\n",
            "Epoch :   1, training loss : 1.5945, training accuracy : 44.03, test loss : 0.9403, test accuracy : 71.61\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.5972, accuracy : 80.56\n",
            "iteration : 100, loss : 0.5706, accuracy : 81.42\n",
            "iteration : 150, loss : 0.5461, accuracy : 82.35\n",
            "iteration : 200, loss : 0.5306, accuracy : 82.92\n",
            "iteration : 250, loss : 0.5108, accuracy : 83.58\n",
            "iteration : 300, loss : 0.4969, accuracy : 84.13\n",
            "iteration : 350, loss : 0.4903, accuracy : 84.38\n",
            "Epoch :   2, training loss : 0.4869, training accuracy : 84.50, test loss : 0.4697, test accuracy : 85.50\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.3953, accuracy : 87.31\n",
            "iteration : 100, loss : 0.3975, accuracy : 87.67\n",
            "iteration : 150, loss : 0.3835, accuracy : 88.08\n",
            "iteration : 200, loss : 0.3782, accuracy : 88.18\n",
            "iteration : 250, loss : 0.3758, accuracy : 88.34\n",
            "iteration : 300, loss : 0.3749, accuracy : 88.46\n",
            "iteration : 350, loss : 0.3721, accuracy : 88.54\n",
            "Epoch :   3, training loss : 0.3711, training accuracy : 88.61, test loss : 0.3431, test accuracy : 89.67\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3366, accuracy : 89.33\n",
            "iteration : 100, loss : 0.3389, accuracy : 89.39\n",
            "iteration : 150, loss : 0.3315, accuracy : 89.68\n",
            "iteration : 200, loss : 0.3350, accuracy : 89.64\n",
            "iteration : 250, loss : 0.3341, accuracy : 89.72\n",
            "iteration : 300, loss : 0.3323, accuracy : 89.75\n",
            "iteration : 350, loss : 0.3293, accuracy : 89.88\n",
            "Epoch :   4, training loss : 0.3297, training accuracy : 89.87, test loss : 0.3384, test accuracy : 89.59\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.2872, accuracy : 90.92\n",
            "iteration : 100, loss : 0.2937, accuracy : 90.89\n",
            "iteration : 150, loss : 0.2974, accuracy : 90.83\n",
            "iteration : 200, loss : 0.3020, accuracy : 90.80\n",
            "iteration : 250, loss : 0.3041, accuracy : 90.80\n",
            "iteration : 300, loss : 0.3001, accuracy : 90.93\n",
            "iteration : 350, loss : 0.2992, accuracy : 90.99\n",
            "Epoch :   5, training loss : 0.2988, training accuracy : 91.02, test loss : 0.3165, test accuracy : 90.46\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.2760, accuracy : 91.98\n",
            "iteration : 100, loss : 0.2761, accuracy : 91.68\n",
            "iteration : 150, loss : 0.2795, accuracy : 91.60\n",
            "iteration : 200, loss : 0.2770, accuracy : 91.67\n",
            "iteration : 250, loss : 0.2779, accuracy : 91.64\n",
            "iteration : 300, loss : 0.2758, accuracy : 91.67\n",
            "iteration : 350, loss : 0.2732, accuracy : 91.74\n",
            "Epoch :   6, training loss : 0.2715, training accuracy : 91.79, test loss : 0.3176, test accuracy : 90.56\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.2533, accuracy : 92.33\n",
            "iteration : 100, loss : 0.2629, accuracy : 92.09\n",
            "iteration : 150, loss : 0.2644, accuracy : 92.04\n",
            "iteration : 200, loss : 0.2641, accuracy : 92.10\n",
            "iteration : 250, loss : 0.2606, accuracy : 92.19\n",
            "iteration : 300, loss : 0.2603, accuracy : 92.20\n",
            "iteration : 350, loss : 0.2602, accuracy : 92.24\n",
            "Epoch :   7, training loss : 0.2604, training accuracy : 92.24, test loss : 0.2828, test accuracy : 91.50\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.2463, accuracy : 92.77\n",
            "iteration : 100, loss : 0.2460, accuracy : 92.75\n",
            "iteration : 150, loss : 0.2452, accuracy : 92.74\n",
            "iteration : 200, loss : 0.2440, accuracy : 92.75\n",
            "iteration : 250, loss : 0.2446, accuracy : 92.80\n",
            "iteration : 300, loss : 0.2484, accuracy : 92.71\n",
            "iteration : 350, loss : 0.2454, accuracy : 92.78\n",
            "Epoch :   8, training loss : 0.2442, training accuracy : 92.81, test loss : 0.2724, test accuracy : 92.05\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.2384, accuracy : 92.97\n",
            "iteration : 100, loss : 0.2326, accuracy : 93.12\n",
            "iteration : 150, loss : 0.2341, accuracy : 92.96\n",
            "iteration : 200, loss : 0.2364, accuracy : 92.90\n",
            "iteration : 250, loss : 0.2346, accuracy : 93.00\n",
            "iteration : 300, loss : 0.2350, accuracy : 93.05\n",
            "iteration : 350, loss : 0.2346, accuracy : 93.03\n",
            "Epoch :   9, training loss : 0.2342, training accuracy : 93.04, test loss : 0.2788, test accuracy : 91.97\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2280, accuracy : 93.31\n",
            "iteration : 100, loss : 0.2262, accuracy : 93.34\n",
            "iteration : 150, loss : 0.2240, accuracy : 93.42\n",
            "iteration : 200, loss : 0.2241, accuracy : 93.37\n",
            "iteration : 250, loss : 0.2240, accuracy : 93.34\n",
            "iteration : 300, loss : 0.2226, accuracy : 93.31\n",
            "iteration : 350, loss : 0.2255, accuracy : 93.34\n",
            "Epoch :  10, training loss : 0.2246, training accuracy : 93.37, test loss : 0.2606, test accuracy : 92.47\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.2041, accuracy : 94.06\n",
            "iteration : 100, loss : 0.2161, accuracy : 93.73\n",
            "iteration : 150, loss : 0.2113, accuracy : 93.85\n",
            "iteration : 200, loss : 0.2109, accuracy : 93.91\n",
            "iteration : 250, loss : 0.2131, accuracy : 93.81\n",
            "iteration : 300, loss : 0.2119, accuracy : 93.77\n",
            "iteration : 350, loss : 0.2118, accuracy : 93.79\n",
            "Epoch :  11, training loss : 0.2117, training accuracy : 93.80, test loss : 0.2570, test accuracy : 92.68\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.1908, accuracy : 94.39\n",
            "iteration : 100, loss : 0.1960, accuracy : 94.30\n",
            "iteration : 150, loss : 0.2065, accuracy : 93.90\n",
            "iteration : 200, loss : 0.2080, accuracy : 93.92\n",
            "iteration : 250, loss : 0.2061, accuracy : 93.94\n",
            "iteration : 300, loss : 0.2038, accuracy : 94.00\n",
            "iteration : 350, loss : 0.2007, accuracy : 94.04\n",
            "Epoch :  12, training loss : 0.2020, training accuracy : 94.01, test loss : 0.2515, test accuracy : 92.89\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.2000, accuracy : 94.28\n",
            "iteration : 100, loss : 0.2019, accuracy : 94.19\n",
            "iteration : 150, loss : 0.2036, accuracy : 94.19\n",
            "iteration : 200, loss : 0.1990, accuracy : 94.29\n",
            "iteration : 250, loss : 0.1960, accuracy : 94.34\n",
            "iteration : 300, loss : 0.1964, accuracy : 94.33\n",
            "iteration : 350, loss : 0.1957, accuracy : 94.32\n",
            "Epoch :  13, training loss : 0.1943, training accuracy : 94.38, test loss : 0.2304, test accuracy : 93.56\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.1833, accuracy : 94.56\n",
            "iteration : 100, loss : 0.1841, accuracy : 94.60\n",
            "iteration : 150, loss : 0.1841, accuracy : 94.74\n",
            "iteration : 200, loss : 0.1843, accuracy : 94.70\n",
            "iteration : 250, loss : 0.1854, accuracy : 94.71\n",
            "iteration : 300, loss : 0.1864, accuracy : 94.65\n",
            "iteration : 350, loss : 0.1869, accuracy : 94.65\n",
            "Epoch :  14, training loss : 0.1875, training accuracy : 94.63, test loss : 0.2339, test accuracy : 93.46\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.1617, accuracy : 95.31\n",
            "iteration : 100, loss : 0.1693, accuracy : 95.16\n",
            "iteration : 150, loss : 0.1686, accuracy : 95.16\n",
            "iteration : 200, loss : 0.1751, accuracy : 95.00\n",
            "iteration : 250, loss : 0.1745, accuracy : 95.02\n",
            "iteration : 300, loss : 0.1768, accuracy : 94.90\n",
            "iteration : 350, loss : 0.1767, accuracy : 94.92\n",
            "Epoch :  15, training loss : 0.1775, training accuracy : 94.89, test loss : 0.2332, test accuracy : 93.67\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.1801, accuracy : 94.59\n",
            "iteration : 100, loss : 0.1718, accuracy : 94.93\n",
            "iteration : 150, loss : 0.1690, accuracy : 94.98\n",
            "iteration : 200, loss : 0.1696, accuracy : 95.01\n",
            "iteration : 250, loss : 0.1718, accuracy : 94.97\n",
            "iteration : 300, loss : 0.1706, accuracy : 95.03\n",
            "iteration : 350, loss : 0.1725, accuracy : 95.01\n",
            "Epoch :  16, training loss : 0.1721, training accuracy : 95.01, test loss : 0.2243, test accuracy : 93.80\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.1763, accuracy : 94.88\n",
            "iteration : 100, loss : 0.1670, accuracy : 95.16\n",
            "iteration : 150, loss : 0.1626, accuracy : 95.26\n",
            "iteration : 200, loss : 0.1632, accuracy : 95.25\n",
            "iteration : 250, loss : 0.1639, accuracy : 95.25\n",
            "iteration : 300, loss : 0.1646, accuracy : 95.22\n",
            "iteration : 350, loss : 0.1653, accuracy : 95.20\n",
            "Epoch :  17, training loss : 0.1655, training accuracy : 95.19, test loss : 0.2389, test accuracy : 93.35\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.1500, accuracy : 95.91\n",
            "iteration : 100, loss : 0.1605, accuracy : 95.73\n",
            "iteration : 150, loss : 0.1543, accuracy : 95.81\n",
            "iteration : 200, loss : 0.1514, accuracy : 95.88\n",
            "iteration : 250, loss : 0.1540, accuracy : 95.70\n",
            "iteration : 300, loss : 0.1546, accuracy : 95.66\n",
            "iteration : 350, loss : 0.1542, accuracy : 95.68\n",
            "Epoch :  18, training loss : 0.1547, training accuracy : 95.67, test loss : 0.2297, test accuracy : 93.71\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.1422, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1436, accuracy : 95.90\n",
            "iteration : 150, loss : 0.1464, accuracy : 95.81\n",
            "iteration : 200, loss : 0.1458, accuracy : 95.79\n",
            "iteration : 250, loss : 0.1470, accuracy : 95.73\n",
            "iteration : 300, loss : 0.1498, accuracy : 95.71\n",
            "iteration : 350, loss : 0.1531, accuracy : 95.60\n",
            "Epoch :  19, training loss : 0.1540, training accuracy : 95.57, test loss : 0.2351, test accuracy : 93.50\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.1310, accuracy : 96.36\n",
            "iteration : 100, loss : 0.1368, accuracy : 96.11\n",
            "iteration : 150, loss : 0.1368, accuracy : 96.08\n",
            "iteration : 200, loss : 0.1364, accuracy : 96.11\n",
            "iteration : 250, loss : 0.1383, accuracy : 96.03\n",
            "iteration : 300, loss : 0.1387, accuracy : 95.98\n",
            "iteration : 350, loss : 0.1415, accuracy : 95.92\n",
            "Epoch :  20, training loss : 0.1421, training accuracy : 95.92, test loss : 0.2337, test accuracy : 93.57\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.1380, accuracy : 96.12\n",
            "iteration : 100, loss : 0.1375, accuracy : 96.00\n",
            "iteration : 150, loss : 0.1350, accuracy : 96.11\n",
            "iteration : 200, loss : 0.1355, accuracy : 96.05\n",
            "iteration : 250, loss : 0.1345, accuracy : 96.09\n",
            "iteration : 300, loss : 0.1356, accuracy : 96.06\n",
            "iteration : 350, loss : 0.1350, accuracy : 96.04\n",
            "Epoch :  21, training loss : 0.1369, training accuracy : 96.02, test loss : 0.2218, test accuracy : 94.06\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.1325, accuracy : 96.16\n",
            "iteration : 100, loss : 0.1328, accuracy : 96.25\n",
            "iteration : 150, loss : 0.1347, accuracy : 96.12\n",
            "iteration : 200, loss : 0.1348, accuracy : 96.00\n",
            "iteration : 250, loss : 0.1327, accuracy : 96.10\n",
            "iteration : 300, loss : 0.1317, accuracy : 96.15\n",
            "iteration : 350, loss : 0.1318, accuracy : 96.17\n",
            "Epoch :  22, training loss : 0.1327, training accuracy : 96.16, test loss : 0.2491, test accuracy : 93.34\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.1316, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1252, accuracy : 96.47\n",
            "iteration : 150, loss : 0.1272, accuracy : 96.29\n",
            "iteration : 200, loss : 0.1255, accuracy : 96.36\n",
            "iteration : 250, loss : 0.1252, accuracy : 96.37\n",
            "iteration : 300, loss : 0.1268, accuracy : 96.34\n",
            "iteration : 350, loss : 0.1265, accuracy : 96.35\n",
            "Epoch :  23, training loss : 0.1264, training accuracy : 96.35, test loss : 0.2340, test accuracy : 93.72\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.1185, accuracy : 96.78\n",
            "iteration : 100, loss : 0.1223, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1199, accuracy : 96.65\n",
            "iteration : 200, loss : 0.1199, accuracy : 96.62\n",
            "iteration : 250, loss : 0.1198, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1183, accuracy : 96.67\n",
            "iteration : 350, loss : 0.1195, accuracy : 96.59\n",
            "Epoch :  24, training loss : 0.1224, training accuracy : 96.55, test loss : 0.2536, test accuracy : 92.94\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.0973, accuracy : 97.22\n",
            "iteration : 100, loss : 0.1011, accuracy : 96.99\n",
            "iteration : 150, loss : 0.1052, accuracy : 96.92\n",
            "iteration : 200, loss : 0.1065, accuracy : 96.91\n",
            "iteration : 250, loss : 0.1100, accuracy : 96.82\n",
            "iteration : 300, loss : 0.1144, accuracy : 96.73\n",
            "iteration : 350, loss : 0.1150, accuracy : 96.76\n",
            "Epoch :  25, training loss : 0.1157, training accuracy : 96.74, test loss : 0.2350, test accuracy : 93.69\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.1001, accuracy : 97.23\n",
            "iteration : 100, loss : 0.0965, accuracy : 97.24\n",
            "iteration : 150, loss : 0.1003, accuracy : 97.07\n",
            "iteration : 200, loss : 0.1045, accuracy : 97.00\n",
            "iteration : 250, loss : 0.1050, accuracy : 96.95\n",
            "iteration : 300, loss : 0.1062, accuracy : 96.93\n",
            "iteration : 350, loss : 0.1090, accuracy : 96.87\n",
            "Epoch :  26, training loss : 0.1087, training accuracy : 96.86, test loss : 0.2248, test accuracy : 94.19\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.0854, accuracy : 97.42\n",
            "iteration : 100, loss : 0.0992, accuracy : 97.02\n",
            "iteration : 150, loss : 0.1069, accuracy : 96.89\n",
            "iteration : 200, loss : 0.1075, accuracy : 96.92\n",
            "iteration : 250, loss : 0.1067, accuracy : 96.93\n",
            "iteration : 300, loss : 0.1060, accuracy : 96.97\n",
            "iteration : 350, loss : 0.1059, accuracy : 96.95\n",
            "Epoch :  27, training loss : 0.1061, training accuracy : 96.96, test loss : 0.2325, test accuracy : 93.96\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.0860, accuracy : 97.53\n",
            "iteration : 100, loss : 0.0885, accuracy : 97.46\n",
            "iteration : 150, loss : 0.0939, accuracy : 97.41\n",
            "iteration : 200, loss : 0.0943, accuracy : 97.33\n",
            "iteration : 250, loss : 0.0981, accuracy : 97.25\n",
            "iteration : 300, loss : 0.0997, accuracy : 97.18\n",
            "iteration : 350, loss : 0.1008, accuracy : 97.12\n",
            "Epoch :  28, training loss : 0.1014, training accuracy : 97.11, test loss : 0.2375, test accuracy : 93.92\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.0837, accuracy : 97.69\n",
            "iteration : 100, loss : 0.0915, accuracy : 97.45\n",
            "iteration : 150, loss : 0.0970, accuracy : 97.41\n",
            "iteration : 200, loss : 0.0991, accuracy : 97.29\n",
            "iteration : 250, loss : 0.0982, accuracy : 97.25\n",
            "iteration : 300, loss : 0.0988, accuracy : 97.21\n",
            "iteration : 350, loss : 0.0988, accuracy : 97.20\n",
            "Epoch :  29, training loss : 0.0985, training accuracy : 97.22, test loss : 0.2436, test accuracy : 93.83\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.0878, accuracy : 97.58\n",
            "iteration : 100, loss : 0.0857, accuracy : 97.60\n",
            "iteration : 150, loss : 0.0853, accuracy : 97.51\n",
            "iteration : 200, loss : 0.0870, accuracy : 97.47\n",
            "iteration : 250, loss : 0.0882, accuracy : 97.44\n",
            "iteration : 300, loss : 0.0894, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0919, accuracy : 97.30\n",
            "Epoch :  30, training loss : 0.0916, training accuracy : 97.32, test loss : 0.2495, test accuracy : 94.03\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.0836, accuracy : 97.50\n",
            "iteration : 100, loss : 0.0819, accuracy : 97.66\n",
            "iteration : 150, loss : 0.0854, accuracy : 97.54\n",
            "iteration : 200, loss : 0.0839, accuracy : 97.54\n",
            "iteration : 250, loss : 0.0884, accuracy : 97.46\n",
            "iteration : 300, loss : 0.0911, accuracy : 97.38\n",
            "iteration : 350, loss : 0.0924, accuracy : 97.33\n",
            "Epoch :  31, training loss : 0.0918, training accuracy : 97.33, test loss : 0.2393, test accuracy : 94.19\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.0758, accuracy : 98.05\n",
            "iteration : 100, loss : 0.0764, accuracy : 97.88\n",
            "iteration : 150, loss : 0.0755, accuracy : 97.84\n",
            "iteration : 200, loss : 0.0777, accuracy : 97.77\n",
            "iteration : 250, loss : 0.0809, accuracy : 97.72\n",
            "iteration : 300, loss : 0.0808, accuracy : 97.73\n",
            "iteration : 350, loss : 0.0827, accuracy : 97.66\n",
            "Epoch :  32, training loss : 0.0832, training accuracy : 97.64, test loss : 0.2544, test accuracy : 93.77\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.0667, accuracy : 98.20\n",
            "iteration : 100, loss : 0.0682, accuracy : 98.15\n",
            "iteration : 150, loss : 0.0672, accuracy : 98.09\n",
            "iteration : 200, loss : 0.0708, accuracy : 97.98\n",
            "iteration : 250, loss : 0.0768, accuracy : 97.80\n",
            "iteration : 300, loss : 0.0787, accuracy : 97.75\n",
            "iteration : 350, loss : 0.0786, accuracy : 97.76\n",
            "Epoch :  33, training loss : 0.0797, training accuracy : 97.72, test loss : 0.2537, test accuracy : 93.94\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.0786, accuracy : 97.81\n",
            "iteration : 100, loss : 0.0786, accuracy : 97.80\n",
            "iteration : 150, loss : 0.0812, accuracy : 97.71\n",
            "iteration : 200, loss : 0.0801, accuracy : 97.74\n",
            "iteration : 250, loss : 0.0772, accuracy : 97.84\n",
            "iteration : 300, loss : 0.0786, accuracy : 97.78\n",
            "iteration : 350, loss : 0.0794, accuracy : 97.73\n",
            "Epoch :  34, training loss : 0.0793, training accuracy : 97.73, test loss : 0.2661, test accuracy : 93.69\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.0664, accuracy : 97.98\n",
            "iteration : 100, loss : 0.0619, accuracy : 98.08\n",
            "iteration : 150, loss : 0.0642, accuracy : 98.09\n",
            "iteration : 200, loss : 0.0648, accuracy : 98.04\n",
            "iteration : 250, loss : 0.0669, accuracy : 97.97\n",
            "iteration : 300, loss : 0.0677, accuracy : 97.94\n",
            "iteration : 350, loss : 0.0694, accuracy : 97.91\n",
            "Epoch :  35, training loss : 0.0698, training accuracy : 97.89, test loss : 0.2672, test accuracy : 94.08\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.0660, accuracy : 98.03\n",
            "iteration : 100, loss : 0.0644, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0668, accuracy : 98.00\n",
            "iteration : 200, loss : 0.0680, accuracy : 97.96\n",
            "iteration : 250, loss : 0.0719, accuracy : 97.82\n",
            "iteration : 300, loss : 0.0708, accuracy : 97.87\n",
            "iteration : 350, loss : 0.0717, accuracy : 97.85\n",
            "Epoch :  36, training loss : 0.0709, training accuracy : 97.87, test loss : 0.2747, test accuracy : 93.77\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.0678, accuracy : 98.11\n",
            "iteration : 100, loss : 0.0659, accuracy : 98.14\n",
            "iteration : 150, loss : 0.0650, accuracy : 98.14\n",
            "iteration : 200, loss : 0.0641, accuracy : 98.12\n",
            "iteration : 250, loss : 0.0655, accuracy : 98.03\n",
            "iteration : 300, loss : 0.0657, accuracy : 98.01\n",
            "iteration : 350, loss : 0.0665, accuracy : 97.97\n",
            "Epoch :  37, training loss : 0.0672, training accuracy : 97.97, test loss : 0.2582, test accuracy : 93.92\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.0576, accuracy : 98.45\n",
            "iteration : 100, loss : 0.0577, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0568, accuracy : 98.35\n",
            "iteration : 200, loss : 0.0617, accuracy : 98.18\n",
            "iteration : 250, loss : 0.0617, accuracy : 98.17\n",
            "iteration : 300, loss : 0.0641, accuracy : 98.12\n",
            "iteration : 350, loss : 0.0655, accuracy : 98.08\n",
            "Epoch :  38, training loss : 0.0647, training accuracy : 98.09, test loss : 0.2603, test accuracy : 94.15\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.0540, accuracy : 98.44\n",
            "iteration : 100, loss : 0.0563, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0600, accuracy : 98.30\n",
            "iteration : 200, loss : 0.0613, accuracy : 98.31\n",
            "iteration : 250, loss : 0.0618, accuracy : 98.28\n",
            "iteration : 300, loss : 0.0614, accuracy : 98.28\n",
            "iteration : 350, loss : 0.0610, accuracy : 98.26\n",
            "Epoch :  39, training loss : 0.0616, training accuracy : 98.24, test loss : 0.2791, test accuracy : 93.94\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.0471, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0578, accuracy : 98.27\n",
            "iteration : 150, loss : 0.0562, accuracy : 98.30\n",
            "iteration : 200, loss : 0.0587, accuracy : 98.28\n",
            "iteration : 250, loss : 0.0604, accuracy : 98.22\n",
            "iteration : 300, loss : 0.0602, accuracy : 98.24\n",
            "iteration : 350, loss : 0.0613, accuracy : 98.20\n",
            "Epoch :  40, training loss : 0.0615, training accuracy : 98.17, test loss : 0.2692, test accuracy : 94.15\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.0503, accuracy : 98.48\n",
            "iteration : 100, loss : 0.0540, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0560, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0565, accuracy : 98.31\n",
            "iteration : 250, loss : 0.0567, accuracy : 98.32\n",
            "iteration : 300, loss : 0.0576, accuracy : 98.30\n",
            "iteration : 350, loss : 0.0572, accuracy : 98.29\n",
            "Epoch :  41, training loss : 0.0571, training accuracy : 98.29, test loss : 0.2764, test accuracy : 94.06\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.0526, accuracy : 98.39\n",
            "iteration : 100, loss : 0.0539, accuracy : 98.39\n",
            "iteration : 150, loss : 0.0499, accuracy : 98.47\n",
            "iteration : 200, loss : 0.0499, accuracy : 98.48\n",
            "iteration : 250, loss : 0.0519, accuracy : 98.42\n",
            "iteration : 300, loss : 0.0531, accuracy : 98.38\n",
            "iteration : 350, loss : 0.0526, accuracy : 98.37\n",
            "Epoch :  42, training loss : 0.0534, training accuracy : 98.35, test loss : 0.2997, test accuracy : 93.73\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.0471, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0459, accuracy : 98.65\n",
            "iteration : 150, loss : 0.0456, accuracy : 98.58\n",
            "iteration : 200, loss : 0.0439, accuracy : 98.61\n",
            "iteration : 250, loss : 0.0442, accuracy : 98.58\n",
            "iteration : 300, loss : 0.0470, accuracy : 98.50\n",
            "iteration : 350, loss : 0.0495, accuracy : 98.45\n",
            "Epoch :  43, training loss : 0.0499, training accuracy : 98.45, test loss : 0.2916, test accuracy : 93.89\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.0409, accuracy : 98.83\n",
            "iteration : 100, loss : 0.0488, accuracy : 98.55\n",
            "iteration : 150, loss : 0.0468, accuracy : 98.61\n",
            "iteration : 200, loss : 0.0491, accuracy : 98.57\n",
            "iteration : 250, loss : 0.0509, accuracy : 98.50\n",
            "iteration : 300, loss : 0.0511, accuracy : 98.49\n",
            "iteration : 350, loss : 0.0510, accuracy : 98.48\n",
            "Epoch :  44, training loss : 0.0507, training accuracy : 98.49, test loss : 0.3004, test accuracy : 94.07\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.0440, accuracy : 98.78\n",
            "iteration : 100, loss : 0.0469, accuracy : 98.61\n",
            "iteration : 150, loss : 0.0463, accuracy : 98.59\n",
            "iteration : 200, loss : 0.0465, accuracy : 98.60\n",
            "iteration : 250, loss : 0.0463, accuracy : 98.62\n",
            "iteration : 300, loss : 0.0474, accuracy : 98.59\n",
            "iteration : 350, loss : 0.0495, accuracy : 98.52\n",
            "Epoch :  45, training loss : 0.0499, training accuracy : 98.50, test loss : 0.2806, test accuracy : 94.02\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.0520, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0440, accuracy : 98.62\n",
            "iteration : 150, loss : 0.0430, accuracy : 98.66\n",
            "iteration : 200, loss : 0.0427, accuracy : 98.67\n",
            "iteration : 250, loss : 0.0441, accuracy : 98.59\n",
            "iteration : 300, loss : 0.0461, accuracy : 98.54\n",
            "iteration : 350, loss : 0.0466, accuracy : 98.54\n",
            "Epoch :  46, training loss : 0.0468, training accuracy : 98.54, test loss : 0.2931, test accuracy : 94.23\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.0474, accuracy : 98.48\n",
            "iteration : 100, loss : 0.0443, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0455, accuracy : 98.61\n",
            "iteration : 200, loss : 0.0453, accuracy : 98.59\n",
            "iteration : 250, loss : 0.0465, accuracy : 98.54\n",
            "iteration : 300, loss : 0.0460, accuracy : 98.56\n",
            "iteration : 350, loss : 0.0471, accuracy : 98.55\n",
            "Epoch :  47, training loss : 0.0471, training accuracy : 98.56, test loss : 0.2961, test accuracy : 93.86\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.0411, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0360, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0379, accuracy : 98.77\n",
            "iteration : 200, loss : 0.0403, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0403, accuracy : 98.73\n",
            "iteration : 300, loss : 0.0408, accuracy : 98.74\n",
            "iteration : 350, loss : 0.0421, accuracy : 98.70\n",
            "Epoch :  48, training loss : 0.0426, training accuracy : 98.67, test loss : 0.3087, test accuracy : 93.91\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.0414, accuracy : 98.80\n",
            "iteration : 100, loss : 0.0422, accuracy : 98.66\n",
            "iteration : 150, loss : 0.0423, accuracy : 98.66\n",
            "iteration : 200, loss : 0.0419, accuracy : 98.69\n",
            "iteration : 250, loss : 0.0421, accuracy : 98.69\n",
            "iteration : 300, loss : 0.0432, accuracy : 98.62\n",
            "iteration : 350, loss : 0.0426, accuracy : 98.64\n",
            "Epoch :  49, training loss : 0.0430, training accuracy : 98.63, test loss : 0.3280, test accuracy : 93.98\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.0523, accuracy : 98.34\n",
            "iteration : 100, loss : 0.0479, accuracy : 98.54\n",
            "iteration : 150, loss : 0.0438, accuracy : 98.59\n",
            "iteration : 200, loss : 0.0422, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0420, accuracy : 98.70\n",
            "iteration : 300, loss : 0.0415, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0413, accuracy : 98.73\n",
            "Epoch :  50, training loss : 0.0413, training accuracy : 98.74, test loss : 0.3095, test accuracy : 94.08\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.0312, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0311, accuracy : 99.06\n",
            "iteration : 150, loss : 0.0319, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0336, accuracy : 98.96\n",
            "iteration : 250, loss : 0.0354, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0358, accuracy : 98.87\n",
            "iteration : 350, loss : 0.0360, accuracy : 98.88\n",
            "Epoch :  51, training loss : 0.0363, training accuracy : 98.88, test loss : 0.3121, test accuracy : 94.03\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.0357, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0361, accuracy : 98.86\n",
            "iteration : 150, loss : 0.0369, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0371, accuracy : 98.88\n",
            "iteration : 250, loss : 0.0373, accuracy : 98.82\n",
            "iteration : 300, loss : 0.0372, accuracy : 98.81\n",
            "iteration : 350, loss : 0.0378, accuracy : 98.77\n",
            "Epoch :  52, training loss : 0.0378, training accuracy : 98.78, test loss : 0.3265, test accuracy : 93.97\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.0330, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0286, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0332, accuracy : 98.91\n",
            "iteration : 200, loss : 0.0366, accuracy : 98.82\n",
            "iteration : 250, loss : 0.0394, accuracy : 98.75\n",
            "iteration : 300, loss : 0.0385, accuracy : 98.77\n",
            "iteration : 350, loss : 0.0384, accuracy : 98.76\n",
            "Epoch :  53, training loss : 0.0380, training accuracy : 98.78, test loss : 0.3161, test accuracy : 94.22\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.0267, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0287, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0287, accuracy : 99.16\n",
            "iteration : 200, loss : 0.0295, accuracy : 99.12\n",
            "iteration : 250, loss : 0.0319, accuracy : 99.05\n",
            "iteration : 300, loss : 0.0339, accuracy : 99.00\n",
            "iteration : 350, loss : 0.0340, accuracy : 98.99\n",
            "Epoch :  54, training loss : 0.0342, training accuracy : 98.98, test loss : 0.3249, test accuracy : 94.07\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.0344, accuracy : 98.98\n",
            "iteration : 100, loss : 0.0327, accuracy : 98.95\n",
            "iteration : 150, loss : 0.0328, accuracy : 98.92\n",
            "iteration : 200, loss : 0.0335, accuracy : 98.90\n",
            "iteration : 250, loss : 0.0336, accuracy : 98.91\n",
            "iteration : 300, loss : 0.0339, accuracy : 98.93\n",
            "iteration : 350, loss : 0.0338, accuracy : 98.95\n",
            "Epoch :  55, training loss : 0.0338, training accuracy : 98.95, test loss : 0.3219, test accuracy : 94.10\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.0296, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0317, accuracy : 99.00\n",
            "iteration : 150, loss : 0.0308, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0310, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0326, accuracy : 98.99\n",
            "iteration : 300, loss : 0.0324, accuracy : 98.97\n",
            "iteration : 350, loss : 0.0334, accuracy : 98.95\n",
            "Epoch :  56, training loss : 0.0337, training accuracy : 98.94, test loss : 0.3129, test accuracy : 94.22\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.0291, accuracy : 99.06\n",
            "iteration : 100, loss : 0.0282, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0284, accuracy : 99.10\n",
            "iteration : 200, loss : 0.0291, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0315, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0325, accuracy : 98.96\n",
            "iteration : 350, loss : 0.0336, accuracy : 98.92\n",
            "Epoch :  57, training loss : 0.0340, training accuracy : 98.91, test loss : 0.3158, test accuracy : 94.31\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.0312, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0287, accuracy : 99.04\n",
            "iteration : 150, loss : 0.0297, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0301, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0311, accuracy : 98.99\n",
            "iteration : 300, loss : 0.0311, accuracy : 99.01\n",
            "iteration : 350, loss : 0.0316, accuracy : 98.99\n",
            "Epoch :  58, training loss : 0.0317, training accuracy : 98.99, test loss : 0.3338, test accuracy : 93.88\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.0303, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0272, accuracy : 99.03\n",
            "iteration : 150, loss : 0.0274, accuracy : 99.01\n",
            "iteration : 200, loss : 0.0303, accuracy : 98.96\n",
            "iteration : 250, loss : 0.0303, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0299, accuracy : 98.99\n",
            "iteration : 350, loss : 0.0298, accuracy : 98.99\n",
            "Epoch :  59, training loss : 0.0304, training accuracy : 98.96, test loss : 0.3453, test accuracy : 94.07\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.0282, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0261, accuracy : 99.18\n",
            "iteration : 150, loss : 0.0254, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0287, accuracy : 99.10\n",
            "iteration : 250, loss : 0.0306, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0300, accuracy : 99.03\n",
            "iteration : 350, loss : 0.0291, accuracy : 99.06\n",
            "Epoch :  60, training loss : 0.0284, training accuracy : 99.08, test loss : 0.3241, test accuracy : 94.30\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.0245, accuracy : 99.06\n",
            "iteration : 100, loss : 0.0260, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0263, accuracy : 99.13\n",
            "iteration : 200, loss : 0.0254, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0274, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0297, accuracy : 99.04\n",
            "iteration : 350, loss : 0.0297, accuracy : 99.05\n",
            "Epoch :  61, training loss : 0.0294, training accuracy : 99.06, test loss : 0.3322, test accuracy : 94.23\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.0259, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0225, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0232, accuracy : 99.25\n",
            "iteration : 200, loss : 0.0243, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0267, accuracy : 99.15\n",
            "iteration : 300, loss : 0.0288, accuracy : 99.07\n",
            "iteration : 350, loss : 0.0289, accuracy : 99.06\n",
            "Epoch :  62, training loss : 0.0289, training accuracy : 99.05, test loss : 0.3325, test accuracy : 94.20\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.0274, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0265, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0279, accuracy : 99.07\n",
            "iteration : 200, loss : 0.0291, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0288, accuracy : 99.07\n",
            "iteration : 300, loss : 0.0285, accuracy : 99.05\n",
            "iteration : 350, loss : 0.0287, accuracy : 99.04\n",
            "Epoch :  63, training loss : 0.0289, training accuracy : 99.04, test loss : 0.3341, test accuracy : 94.06\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.0235, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0242, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0238, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0244, accuracy : 99.21\n",
            "iteration : 250, loss : 0.0248, accuracy : 99.20\n",
            "iteration : 300, loss : 0.0251, accuracy : 99.20\n",
            "iteration : 350, loss : 0.0251, accuracy : 99.19\n",
            "Epoch :  64, training loss : 0.0257, training accuracy : 99.18, test loss : 0.3346, test accuracy : 94.37\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.0197, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0214, accuracy : 99.36\n",
            "iteration : 150, loss : 0.0240, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0266, accuracy : 99.17\n",
            "iteration : 250, loss : 0.0276, accuracy : 99.13\n",
            "iteration : 300, loss : 0.0278, accuracy : 99.15\n",
            "iteration : 350, loss : 0.0278, accuracy : 99.14\n",
            "Epoch :  65, training loss : 0.0277, training accuracy : 99.13, test loss : 0.3316, test accuracy : 94.26\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.0267, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.16\n",
            "iteration : 150, loss : 0.0247, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0241, accuracy : 99.22\n",
            "iteration : 250, loss : 0.0252, accuracy : 99.17\n",
            "iteration : 300, loss : 0.0260, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0263, accuracy : 99.12\n",
            "Epoch :  66, training loss : 0.0263, training accuracy : 99.11, test loss : 0.3544, test accuracy : 94.05\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.0191, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0199, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0190, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0207, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0213, accuracy : 99.26\n",
            "iteration : 300, loss : 0.0232, accuracy : 99.21\n",
            "iteration : 350, loss : 0.0236, accuracy : 99.20\n",
            "Epoch :  67, training loss : 0.0234, training accuracy : 99.20, test loss : 0.3273, test accuracy : 94.31\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.0243, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0253, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0249, accuracy : 99.21\n",
            "iteration : 200, loss : 0.0243, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0240, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0244, accuracy : 99.19\n",
            "iteration : 350, loss : 0.0244, accuracy : 99.21\n",
            "Epoch :  68, training loss : 0.0249, training accuracy : 99.18, test loss : 0.3605, test accuracy : 94.13\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.0198, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0202, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0192, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0217, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0228, accuracy : 99.29\n",
            "iteration : 300, loss : 0.0229, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0230, accuracy : 99.28\n",
            "Epoch :  69, training loss : 0.0230, training accuracy : 99.28, test loss : 0.3315, test accuracy : 94.33\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.0175, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0180, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0197, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0200, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0207, accuracy : 99.29\n",
            "iteration : 300, loss : 0.0206, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0215, accuracy : 99.25\n",
            "Epoch :  70, training loss : 0.0223, training accuracy : 99.24, test loss : 0.3645, test accuracy : 94.06\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.0225, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0209, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0210, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0228, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0241, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0249, accuracy : 99.21\n",
            "iteration : 350, loss : 0.0249, accuracy : 99.19\n",
            "Epoch :  71, training loss : 0.0251, training accuracy : 99.18, test loss : 0.3752, test accuracy : 93.98\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.0199, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0208, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0196, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0204, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0215, accuracy : 99.30\n",
            "iteration : 300, loss : 0.0212, accuracy : 99.31\n",
            "iteration : 350, loss : 0.0221, accuracy : 99.26\n",
            "Epoch :  72, training loss : 0.0223, training accuracy : 99.26, test loss : 0.3792, test accuracy : 94.35\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.0215, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0192, accuracy : 99.40\n",
            "iteration : 150, loss : 0.0187, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0183, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0185, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0185, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0194, accuracy : 99.35\n",
            "Epoch :  73, training loss : 0.0196, training accuracy : 99.35, test loss : 0.3675, test accuracy : 94.33\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.0215, accuracy : 99.20\n",
            "iteration : 100, loss : 0.0224, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0216, accuracy : 99.29\n",
            "iteration : 200, loss : 0.0206, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0218, accuracy : 99.30\n",
            "iteration : 300, loss : 0.0222, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0219, accuracy : 99.27\n",
            "Epoch :  74, training loss : 0.0222, training accuracy : 99.26, test loss : 0.3611, test accuracy : 94.10\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.0191, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0196, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0227, accuracy : 99.29\n",
            "iteration : 200, loss : 0.0225, accuracy : 99.28\n",
            "iteration : 250, loss : 0.0232, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0236, accuracy : 99.24\n",
            "iteration : 350, loss : 0.0230, accuracy : 99.25\n",
            "Epoch :  75, training loss : 0.0228, training accuracy : 99.25, test loss : 0.3513, test accuracy : 94.36\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.0162, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0188, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0191, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0198, accuracy : 99.35\n",
            "iteration : 250, loss : 0.0204, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0201, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0199, accuracy : 99.34\n",
            "Epoch :  76, training loss : 0.0204, training accuracy : 99.33, test loss : 0.3665, test accuracy : 94.00\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.0197, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0173, accuracy : 99.39\n",
            "iteration : 150, loss : 0.0171, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0170, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0183, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0190, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0206, accuracy : 99.27\n",
            "Epoch :  77, training loss : 0.0206, training accuracy : 99.28, test loss : 0.3580, test accuracy : 94.21\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.0211, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0213, accuracy : 99.31\n",
            "iteration : 150, loss : 0.0222, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0222, accuracy : 99.25\n",
            "iteration : 250, loss : 0.0216, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0210, accuracy : 99.27\n",
            "iteration : 350, loss : 0.0211, accuracy : 99.27\n",
            "Epoch :  78, training loss : 0.0212, training accuracy : 99.27, test loss : 0.3582, test accuracy : 94.42\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.0229, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0235, accuracy : 99.19\n",
            "iteration : 150, loss : 0.0216, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0201, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0200, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0200, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0200, accuracy : 99.29\n",
            "Epoch :  79, training loss : 0.0198, training accuracy : 99.29, test loss : 0.3750, test accuracy : 94.13\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.0198, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0186, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0201, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0213, accuracy : 99.29\n",
            "iteration : 250, loss : 0.0214, accuracy : 99.29\n",
            "iteration : 300, loss : 0.0218, accuracy : 99.27\n",
            "iteration : 350, loss : 0.0226, accuracy : 99.25\n",
            "Epoch :  80, training loss : 0.0228, training accuracy : 99.24, test loss : 0.3517, test accuracy : 94.23\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.0217, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0178, accuracy : 99.36\n",
            "iteration : 150, loss : 0.0169, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0169, accuracy : 99.40\n",
            "iteration : 250, loss : 0.0167, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0171, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0171, accuracy : 99.43\n",
            "Epoch :  81, training loss : 0.0172, training accuracy : 99.43, test loss : 0.3685, test accuracy : 94.40\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.0251, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0234, accuracy : 99.26\n",
            "iteration : 150, loss : 0.0215, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0211, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0209, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0208, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0206, accuracy : 99.28\n",
            "Epoch :  82, training loss : 0.0208, training accuracy : 99.28, test loss : 0.3710, test accuracy : 94.10\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.0154, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0193, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0188, accuracy : 99.34\n",
            "iteration : 200, loss : 0.0184, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0186, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0196, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0197, accuracy : 99.32\n",
            "Epoch :  83, training loss : 0.0201, training accuracy : 99.31, test loss : 0.3717, test accuracy : 94.04\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.0172, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0147, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0143, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0149, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0162, accuracy : 99.45\n",
            "iteration : 300, loss : 0.0163, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0156, accuracy : 99.48\n",
            "Epoch :  84, training loss : 0.0159, training accuracy : 99.46, test loss : 0.3887, test accuracy : 94.37\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0161, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0163, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0165, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0173, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0174, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.41\n",
            "Epoch :  85, training loss : 0.0177, training accuracy : 99.41, test loss : 0.3685, test accuracy : 94.45\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.0141, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0153, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0184, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0197, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0204, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0204, accuracy : 99.36\n",
            "Epoch :  86, training loss : 0.0204, training accuracy : 99.36, test loss : 0.3531, test accuracy : 94.30\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.0141, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0144, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0142, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0137, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0138, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0148, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0150, accuracy : 99.51\n",
            "Epoch :  87, training loss : 0.0153, training accuracy : 99.50, test loss : 0.3660, test accuracy : 94.40\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.0163, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0158, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0143, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0134, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0142, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0155, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0164, accuracy : 99.44\n",
            "Epoch :  88, training loss : 0.0164, training accuracy : 99.44, test loss : 0.3742, test accuracy : 94.28\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.0128, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0129, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0149, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0168, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0172, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0170, accuracy : 99.43\n",
            "Epoch :  89, training loss : 0.0171, training accuracy : 99.43, test loss : 0.3748, test accuracy : 94.35\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.0173, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0163, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0166, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0167, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0183, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0182, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0175, accuracy : 99.42\n",
            "Epoch :  90, training loss : 0.0177, training accuracy : 99.41, test loss : 0.3667, test accuracy : 94.51\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0147, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0171, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0170, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0171, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0171, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0182, accuracy : 99.41\n",
            "Epoch :  91, training loss : 0.0180, training accuracy : 99.41, test loss : 0.3643, test accuracy : 94.35\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.0118, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0115, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0119, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0134, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0145, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0155, accuracy : 99.48\n",
            "Epoch :  92, training loss : 0.0156, training accuracy : 99.48, test loss : 0.3745, test accuracy : 94.14\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.0162, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0158, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0148, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0153, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0150, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0144, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0152, accuracy : 99.49\n",
            "Epoch :  93, training loss : 0.0153, training accuracy : 99.50, test loss : 0.3744, test accuracy : 94.46\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.0092, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0118, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0124, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0125, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0138, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0148, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0153, accuracy : 99.50\n",
            "Epoch :  94, training loss : 0.0153, training accuracy : 99.50, test loss : 0.3670, test accuracy : 94.52\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.0100, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0131, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0148, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0146, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0145, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0154, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0156, accuracy : 99.47\n",
            "Epoch :  95, training loss : 0.0154, training accuracy : 99.47, test loss : 0.3704, test accuracy : 94.41\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0125, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0140, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0140, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0145, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0145, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0139, accuracy : 99.51\n",
            "Epoch :  96, training loss : 0.0142, training accuracy : 99.50, test loss : 0.3895, test accuracy : 94.45\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.0232, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0178, accuracy : 99.39\n",
            "iteration : 150, loss : 0.0166, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0176, accuracy : 99.40\n",
            "iteration : 250, loss : 0.0177, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0178, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0184, accuracy : 99.38\n",
            "Epoch :  97, training loss : 0.0181, training accuracy : 99.39, test loss : 0.3927, test accuracy : 94.00\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.0101, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0131, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0138, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0141, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0137, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0144, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0162, accuracy : 99.45\n",
            "Epoch :  98, training loss : 0.0166, training accuracy : 99.45, test loss : 0.3687, test accuracy : 94.25\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.0154, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0146, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0132, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0127, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0125, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0131, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0132, accuracy : 99.57\n",
            "Epoch :  99, training loss : 0.0131, training accuracy : 99.58, test loss : 0.3656, test accuracy : 94.58\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0114, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0117, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0116, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0130, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0135, accuracy : 99.56\n",
            "Epoch : 100, training loss : 0.0139, training accuracy : 99.55, test loss : 0.3954, test accuracy : 94.14\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.0136, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0151, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0140, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0147, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0147, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0150, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0145, accuracy : 99.55\n",
            "Epoch : 101, training loss : 0.0147, training accuracy : 99.54, test loss : 0.3782, test accuracy : 94.19\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.0108, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0090, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0128, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0134, accuracy : 99.57\n",
            "Epoch : 102, training loss : 0.0135, training accuracy : 99.57, test loss : 0.3859, test accuracy : 94.35\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0137, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0131, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0142, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0142, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0145, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0142, accuracy : 99.53\n",
            "Epoch : 103, training loss : 0.0143, training accuracy : 99.53, test loss : 0.3739, test accuracy : 94.48\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0105, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0122, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0129, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0135, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0138, accuracy : 99.55\n",
            "Epoch : 104, training loss : 0.0139, training accuracy : 99.55, test loss : 0.3743, test accuracy : 94.40\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.0111, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0108, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0114, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0113, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0123, accuracy : 99.58\n",
            "Epoch : 105, training loss : 0.0125, training accuracy : 99.58, test loss : 0.3893, test accuracy : 94.39\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0115, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0111, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0130, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0131, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0126, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0124, accuracy : 99.59\n",
            "Epoch : 106, training loss : 0.0124, training accuracy : 99.58, test loss : 0.3787, test accuracy : 94.40\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0116, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0110, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0112, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0117, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0129, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0137, accuracy : 99.50\n",
            "Epoch : 107, training loss : 0.0139, training accuracy : 99.49, test loss : 0.3784, test accuracy : 94.41\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0147, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0133, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0123, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0119, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0122, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0124, accuracy : 99.58\n",
            "Epoch : 108, training loss : 0.0122, training accuracy : 99.59, test loss : 0.4001, test accuracy : 94.14\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.0175, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0150, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0155, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0154, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0156, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0153, accuracy : 99.51\n",
            "Epoch : 109, training loss : 0.0151, training accuracy : 99.52, test loss : 0.3838, test accuracy : 94.25\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.0133, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0098, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.67\n",
            "Epoch : 110, training loss : 0.0104, training accuracy : 99.67, test loss : 0.3888, test accuracy : 94.40\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.0123, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0118, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0111, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0107, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0122, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0132, accuracy : 99.56\n",
            "Epoch : 111, training loss : 0.0132, training accuracy : 99.56, test loss : 0.3812, test accuracy : 94.50\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.0179, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0153, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0156, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0150, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0143, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0143, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0139, accuracy : 99.57\n",
            "Epoch : 112, training loss : 0.0146, training accuracy : 99.56, test loss : 0.3989, test accuracy : 94.33\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.0137, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0150, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0139, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0136, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0137, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0132, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0125, accuracy : 99.57\n",
            "Epoch : 113, training loss : 0.0127, training accuracy : 99.57, test loss : 0.4040, test accuracy : 94.23\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.0093, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0106, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0101, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0099, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.61\n",
            "Epoch : 114, training loss : 0.0114, training accuracy : 99.60, test loss : 0.4085, test accuracy : 94.35\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0121, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0118, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0119, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0116, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0117, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0118, accuracy : 99.62\n",
            "Epoch : 115, training loss : 0.0117, training accuracy : 99.63, test loss : 0.4040, test accuracy : 94.42\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0099, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0100, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0111, accuracy : 99.60\n",
            "Epoch : 116, training loss : 0.0112, training accuracy : 99.60, test loss : 0.4059, test accuracy : 94.28\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0131, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0139, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0134, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0131, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0128, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0126, accuracy : 99.59\n",
            "Epoch : 117, training loss : 0.0126, training accuracy : 99.59, test loss : 0.3862, test accuracy : 94.46\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.0092, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0088, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0097, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.64\n",
            "Epoch : 118, training loss : 0.0117, training accuracy : 99.63, test loss : 0.3869, test accuracy : 94.18\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0126, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0111, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0116, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0119, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0125, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.60\n",
            "Epoch : 119, training loss : 0.0125, training accuracy : 99.60, test loss : 0.4114, test accuracy : 94.19\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.0101, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0119, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0116, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0103, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.63\n",
            "Epoch : 120, training loss : 0.0118, training accuracy : 99.61, test loss : 0.3980, test accuracy : 94.50\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.0082, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0103, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0097, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0114, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.62\n",
            "Epoch : 121, training loss : 0.0115, training accuracy : 99.61, test loss : 0.4007, test accuracy : 94.10\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.0095, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0103, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0100, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0110, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.65\n",
            "Epoch : 122, training loss : 0.0114, training accuracy : 99.64, test loss : 0.4037, test accuracy : 94.27\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.0141, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0146, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0138, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0135, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0133, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0130, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.59\n",
            "Epoch : 123, training loss : 0.0125, training accuracy : 99.60, test loss : 0.3852, test accuracy : 94.38\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0077, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0078, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0082, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0086, accuracy : 99.71\n",
            "Epoch : 124, training loss : 0.0086, training accuracy : 99.71, test loss : 0.4210, test accuracy : 94.40\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.0132, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0111, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0105, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0106, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0103, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.67\n",
            "Epoch : 125, training loss : 0.0107, training accuracy : 99.66, test loss : 0.4246, test accuracy : 94.11\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.0103, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0100, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0122, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0116, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0108, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.63\n",
            "Epoch : 126, training loss : 0.0112, training accuracy : 99.64, test loss : 0.4092, test accuracy : 94.36\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0072, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0086, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0091, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0093, accuracy : 99.69\n",
            "Epoch : 127, training loss : 0.0093, training accuracy : 99.70, test loss : 0.4053, test accuracy : 94.53\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0090, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0098, accuracy : 99.68\n",
            "Epoch : 128, training loss : 0.0100, training accuracy : 99.68, test loss : 0.4231, test accuracy : 94.36\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0095, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0120, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0123, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0128, accuracy : 99.58\n",
            "Epoch : 129, training loss : 0.0127, training accuracy : 99.59, test loss : 0.3874, test accuracy : 94.51\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.61\n",
            "iteration : 150, loss : 0.0109, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0101, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0105, accuracy : 99.65\n",
            "Epoch : 130, training loss : 0.0103, training accuracy : 99.66, test loss : 0.4195, test accuracy : 94.19\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.0087, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0103, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0104, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0100, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.63\n",
            "Epoch : 131, training loss : 0.0106, training accuracy : 99.63, test loss : 0.4175, test accuracy : 94.26\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.0094, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0113, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0106, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0109, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.65\n",
            "Epoch : 132, training loss : 0.0107, training accuracy : 99.66, test loss : 0.4164, test accuracy : 94.31\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0082, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0103, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0105, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0101, accuracy : 99.67\n",
            "Epoch : 133, training loss : 0.0101, training accuracy : 99.67, test loss : 0.3940, test accuracy : 94.44\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0080, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0117, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0118, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0121, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0122, accuracy : 99.55\n",
            "Epoch : 134, training loss : 0.0127, training accuracy : 99.54, test loss : 0.3938, test accuracy : 94.36\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.0101, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0097, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0097, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0102, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0098, accuracy : 99.69\n",
            "Epoch : 135, training loss : 0.0096, training accuracy : 99.70, test loss : 0.4173, test accuracy : 94.42\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0104, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.64\n",
            "Epoch : 136, training loss : 0.0103, training accuracy : 99.63, test loss : 0.4208, test accuracy : 94.21\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0101, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0104, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0102, accuracy : 99.68\n",
            "Epoch : 137, training loss : 0.0102, training accuracy : 99.67, test loss : 0.4082, test accuracy : 94.38\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.0079, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0110, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0120, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0113, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0107, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.61\n",
            "Epoch : 138, training loss : 0.0111, training accuracy : 99.61, test loss : 0.4369, test accuracy : 94.18\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0140, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.61\n",
            "iteration : 150, loss : 0.0115, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0120, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0117, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0122, accuracy : 99.60\n",
            "Epoch : 139, training loss : 0.0125, training accuracy : 99.60, test loss : 0.3842, test accuracy : 94.22\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0065, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0070, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.76\n",
            "Epoch : 140, training loss : 0.0075, training accuracy : 99.75, test loss : 0.4022, test accuracy : 94.53\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0068, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0068, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0067, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0069, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0070, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0072, accuracy : 99.75\n",
            "Epoch : 141, training loss : 0.0074, training accuracy : 99.75, test loss : 0.4153, test accuracy : 94.53\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0077, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0099, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0098, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0098, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0110, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0111, accuracy : 99.59\n",
            "Epoch : 142, training loss : 0.0111, training accuracy : 99.60, test loss : 0.4123, test accuracy : 94.22\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0110, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0106, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0112, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0115, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0114, accuracy : 99.62\n",
            "Epoch : 143, training loss : 0.0114, training accuracy : 99.63, test loss : 0.3947, test accuracy : 94.57\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0081, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0101, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0090, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0089, accuracy : 99.70\n",
            "Epoch : 144, training loss : 0.0089, training accuracy : 99.69, test loss : 0.4345, test accuracy : 94.22\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0080, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0086, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0105, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0102, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0096, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0096, accuracy : 99.66\n",
            "Epoch : 145, training loss : 0.0099, training accuracy : 99.65, test loss : 0.4225, test accuracy : 94.31\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0104, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0103, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0100, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0103, accuracy : 99.66\n",
            "iteration : 300, loss : 0.0102, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0100, accuracy : 99.66\n",
            "Epoch : 146, training loss : 0.0098, training accuracy : 99.67, test loss : 0.4120, test accuracy : 94.41\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0084, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0081, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0082, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0081, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0081, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0076, accuracy : 99.73\n",
            "Epoch : 147, training loss : 0.0076, training accuracy : 99.73, test loss : 0.4085, test accuracy : 94.50\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0076, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0105, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0110, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0093, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0094, accuracy : 99.72\n",
            "Epoch : 148, training loss : 0.0093, training accuracy : 99.72, test loss : 0.4208, test accuracy : 94.35\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0107, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0089, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0084, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0085, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0083, accuracy : 99.70\n",
            "Epoch : 149, training loss : 0.0084, training accuracy : 99.69, test loss : 0.4342, test accuracy : 94.51\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0101, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0087, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0092, accuracy : 99.67\n",
            "Epoch : 150, training loss : 0.0091, training accuracy : 99.68, test loss : 0.4342, test accuracy : 94.26\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0115, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0117, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0110, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0103, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.66\n",
            "Epoch : 151, training loss : 0.0104, training accuracy : 99.66, test loss : 0.4145, test accuracy : 94.33\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0077, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0068, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0061, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0057, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0059, accuracy : 99.80\n",
            "Epoch : 152, training loss : 0.0061, training accuracy : 99.80, test loss : 0.4202, test accuracy : 94.43\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0063, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.79\n",
            "Epoch : 153, training loss : 0.0065, training accuracy : 99.79, test loss : 0.4367, test accuracy : 94.41\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0077, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0077, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0083, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0098, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0104, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0112, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0114, accuracy : 99.61\n",
            "Epoch : 154, training loss : 0.0111, training accuracy : 99.63, test loss : 0.3971, test accuracy : 94.66\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0093, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0090, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0089, accuracy : 99.72\n",
            "Epoch : 155, training loss : 0.0087, training accuracy : 99.72, test loss : 0.4121, test accuracy : 94.41\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0070, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0080, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0084, accuracy : 99.73\n",
            "Epoch : 156, training loss : 0.0088, training accuracy : 99.73, test loss : 0.4072, test accuracy : 94.40\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0080, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0088, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0095, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0098, accuracy : 99.69\n",
            "Epoch : 157, training loss : 0.0098, training accuracy : 99.69, test loss : 0.4362, test accuracy : 94.24\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0082, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0104, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0097, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0098, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0087, accuracy : 99.73\n",
            "Epoch : 158, training loss : 0.0084, training accuracy : 99.73, test loss : 0.4073, test accuracy : 94.44\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0056, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0074, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0075, accuracy : 99.73\n",
            "Epoch : 159, training loss : 0.0074, training accuracy : 99.73, test loss : 0.4083, test accuracy : 94.30\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0100, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0089, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0092, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0092, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0091, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0097, accuracy : 99.68\n",
            "Epoch : 160, training loss : 0.0099, training accuracy : 99.68, test loss : 0.4300, test accuracy : 94.38\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0079, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0090, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0092, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.71\n",
            "Epoch : 161, training loss : 0.0088, training accuracy : 99.71, test loss : 0.4489, test accuracy : 94.05\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0072, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0075, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.73\n",
            "Epoch : 162, training loss : 0.0077, training accuracy : 99.73, test loss : 0.4490, test accuracy : 94.37\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0087, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0096, accuracy : 99.68\n",
            "Epoch : 163, training loss : 0.0094, training accuracy : 99.68, test loss : 0.4271, test accuracy : 94.33\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0092, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0087, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0085, accuracy : 99.72\n",
            "Epoch : 164, training loss : 0.0083, training accuracy : 99.72, test loss : 0.4120, test accuracy : 94.44\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0065, accuracy : 99.79\n",
            "Epoch : 165, training loss : 0.0066, training accuracy : 99.79, test loss : 0.4172, test accuracy : 94.65\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0031, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.82\n",
            "Epoch : 166, training loss : 0.0061, training accuracy : 99.81, test loss : 0.4613, test accuracy : 93.99\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0161, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0124, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0126, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0121, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0115, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.64\n",
            "Epoch : 167, training loss : 0.0109, training accuracy : 99.65, test loss : 0.4246, test accuracy : 94.53\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0083, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0084, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0078, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0082, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0076, accuracy : 99.73\n",
            "Epoch : 168, training loss : 0.0077, training accuracy : 99.73, test loss : 0.4266, test accuracy : 94.53\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0082, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0077, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0068, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.80\n",
            "Epoch : 169, training loss : 0.0064, training accuracy : 99.80, test loss : 0.4437, test accuracy : 94.38\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0058, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0062, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0075, accuracy : 99.75\n",
            "Epoch : 170, training loss : 0.0077, training accuracy : 99.74, test loss : 0.4234, test accuracy : 94.39\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0140, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0100, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0099, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0093, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.69\n",
            "Epoch : 171, training loss : 0.0089, training accuracy : 99.70, test loss : 0.4164, test accuracy : 94.54\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0061, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0075, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0082, accuracy : 99.71\n",
            "Epoch : 172, training loss : 0.0086, training accuracy : 99.70, test loss : 0.4196, test accuracy : 94.36\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0109, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0069, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0071, accuracy : 99.77\n",
            "Epoch : 173, training loss : 0.0071, training accuracy : 99.77, test loss : 0.4373, test accuracy : 94.45\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0068, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0064, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0070, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0080, accuracy : 99.73\n",
            "Epoch : 174, training loss : 0.0085, training accuracy : 99.72, test loss : 0.4136, test accuracy : 94.29\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0060, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0062, accuracy : 99.80\n",
            "Epoch : 175, training loss : 0.0062, training accuracy : 99.79, test loss : 0.4485, test accuracy : 94.49\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0073, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0064, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.79\n",
            "Epoch : 176, training loss : 0.0066, training accuracy : 99.78, test loss : 0.4382, test accuracy : 94.52\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0054, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0066, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0081, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0086, accuracy : 99.71\n",
            "Epoch : 177, training loss : 0.0086, training accuracy : 99.70, test loss : 0.4255, test accuracy : 94.47\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0075, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0086, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0080, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0075, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0077, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.77\n",
            "Epoch : 178, training loss : 0.0079, training accuracy : 99.76, test loss : 0.4139, test accuracy : 94.46\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.74\n",
            "Epoch : 179, training loss : 0.0075, training accuracy : 99.73, test loss : 0.4226, test accuracy : 94.40\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0087, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0080, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0081, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0074, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0070, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0067, accuracy : 99.79\n",
            "Epoch : 180, training loss : 0.0066, training accuracy : 99.79, test loss : 0.4426, test accuracy : 94.51\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0076, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0078, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0069, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.78\n",
            "Epoch : 181, training loss : 0.0062, training accuracy : 99.78, test loss : 0.5021, test accuracy : 94.10\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0070, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0071, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0062, accuracy : 99.79\n",
            "Epoch : 182, training loss : 0.0061, training accuracy : 99.79, test loss : 0.4218, test accuracy : 94.61\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0065, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0065, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.74\n",
            "Epoch : 183, training loss : 0.0079, training accuracy : 99.74, test loss : 0.4514, test accuracy : 94.42\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0076, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0072, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0071, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0073, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0080, accuracy : 99.71\n",
            "Epoch : 184, training loss : 0.0080, training accuracy : 99.70, test loss : 0.4259, test accuracy : 94.22\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0126, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0093, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0085, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0076, accuracy : 99.75\n",
            "Epoch : 185, training loss : 0.0079, training accuracy : 99.75, test loss : 0.4617, test accuracy : 94.46\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0105, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0086, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0079, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0075, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0077, accuracy : 99.72\n",
            "Epoch : 186, training loss : 0.0076, training accuracy : 99.72, test loss : 0.4279, test accuracy : 94.45\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0066, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0074, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.77\n",
            "Epoch : 187, training loss : 0.0074, training accuracy : 99.76, test loss : 0.4181, test accuracy : 94.48\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.84\n",
            "Epoch : 188, training loss : 0.0046, training accuracy : 99.84, test loss : 0.4346, test accuracy : 94.61\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0065, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0065, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0063, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.79\n",
            "Epoch : 189, training loss : 0.0069, training accuracy : 99.78, test loss : 0.4565, test accuracy : 94.23\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0095, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0086, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0073, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0081, accuracy : 99.74\n",
            "Epoch : 190, training loss : 0.0081, training accuracy : 99.74, test loss : 0.4420, test accuracy : 94.63\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0056, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0076, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0077, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0085, accuracy : 99.68\n",
            "Epoch : 191, training loss : 0.0084, training accuracy : 99.69, test loss : 0.4399, test accuracy : 94.51\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0072, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0061, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.82\n",
            "Epoch : 192, training loss : 0.0055, training accuracy : 99.82, test loss : 0.4580, test accuracy : 94.44\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0076, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0073, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0084, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0076, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0071, accuracy : 99.76\n",
            "Epoch : 193, training loss : 0.0069, training accuracy : 99.77, test loss : 0.4536, test accuracy : 94.52\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0097, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0089, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0084, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0084, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0086, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0083, accuracy : 99.73\n",
            "Epoch : 194, training loss : 0.0082, training accuracy : 99.73, test loss : 0.4266, test accuracy : 94.50\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.86\n",
            "Epoch : 195, training loss : 0.0042, training accuracy : 99.86, test loss : 0.4774, test accuracy : 93.93\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0091, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.70\n",
            "Epoch : 196, training loss : 0.0086, training accuracy : 99.71, test loss : 0.4315, test accuracy : 94.47\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0083, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0082, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.70\n",
            "Epoch : 197, training loss : 0.0087, training accuracy : 99.70, test loss : 0.4298, test accuracy : 94.48\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0078, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0082, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0082, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0077, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0072, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0068, accuracy : 99.76\n",
            "Epoch : 198, training loss : 0.0068, training accuracy : 99.76, test loss : 0.4258, test accuracy : 94.64\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0063, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0067, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0069, accuracy : 99.78\n",
            "Epoch : 199, training loss : 0.0067, training accuracy : 99.79, test loss : 0.4135, test accuracy : 94.58\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.84\n",
            "Epoch : 200, training loss : 0.0055, training accuracy : 99.83, test loss : 0.4423, test accuracy : 94.61\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0070, accuracy : 99.80\n",
            "Epoch : 201, training loss : 0.0069, training accuracy : 99.79, test loss : 0.4451, test accuracy : 94.40\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0083, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0087, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0085, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0084, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0081, accuracy : 99.72\n",
            "Epoch : 202, training loss : 0.0084, training accuracy : 99.72, test loss : 0.4236, test accuracy : 94.57\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0033, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0048, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.80\n",
            "Epoch : 203, training loss : 0.0057, training accuracy : 99.80, test loss : 0.4562, test accuracy : 94.21\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0100, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0082, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0072, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0065, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0062, accuracy : 99.81\n",
            "Epoch : 204, training loss : 0.0061, training accuracy : 99.82, test loss : 0.4532, test accuracy : 94.60\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0068, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0073, accuracy : 99.75\n",
            "Epoch : 205, training loss : 0.0072, training accuracy : 99.75, test loss : 0.4544, test accuracy : 94.32\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0064, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0063, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0064, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.79\n",
            "Epoch : 206, training loss : 0.0062, training accuracy : 99.79, test loss : 0.4289, test accuracy : 94.59\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0071, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0058, accuracy : 99.81\n",
            "Epoch : 207, training loss : 0.0058, training accuracy : 99.81, test loss : 0.4577, test accuracy : 94.25\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0082, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0079, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0064, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.80\n",
            "Epoch : 208, training loss : 0.0055, training accuracy : 99.80, test loss : 0.4652, test accuracy : 94.43\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0074, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0077, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0071, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0074, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0070, accuracy : 99.77\n",
            "Epoch : 209, training loss : 0.0069, training accuracy : 99.78, test loss : 0.4482, test accuracy : 94.55\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0076, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0069, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.77\n",
            "Epoch : 210, training loss : 0.0064, training accuracy : 99.78, test loss : 0.4504, test accuracy : 94.33\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0041, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.86\n",
            "Epoch : 211, training loss : 0.0045, training accuracy : 99.85, test loss : 0.4581, test accuracy : 94.41\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0084, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0088, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.69\n",
            "Epoch : 212, training loss : 0.0094, training accuracy : 99.68, test loss : 0.4833, test accuracy : 94.08\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0087, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0069, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0062, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0062, accuracy : 99.81\n",
            "Epoch : 213, training loss : 0.0060, training accuracy : 99.82, test loss : 0.4330, test accuracy : 94.53\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.94\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.92\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0045, accuracy : 99.87\n",
            "Epoch : 214, training loss : 0.0044, training accuracy : 99.87, test loss : 0.4501, test accuracy : 94.60\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0056, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0073, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0071, accuracy : 99.74\n",
            "Epoch : 215, training loss : 0.0072, training accuracy : 99.74, test loss : 0.4441, test accuracy : 94.67\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0060, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0061, accuracy : 99.78\n",
            "Epoch : 216, training loss : 0.0061, training accuracy : 99.78, test loss : 0.4246, test accuracy : 94.61\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0041, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.85\n",
            "Epoch : 217, training loss : 0.0043, training accuracy : 99.85, test loss : 0.4792, test accuracy : 94.45\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0029, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0044, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0070, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0072, accuracy : 99.77\n",
            "Epoch : 218, training loss : 0.0075, training accuracy : 99.77, test loss : 0.4441, test accuracy : 94.48\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0056, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0068, accuracy : 99.79\n",
            "Epoch : 219, training loss : 0.0068, training accuracy : 99.79, test loss : 0.4233, test accuracy : 94.49\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0057, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0064, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.82\n",
            "Epoch : 220, training loss : 0.0062, training accuracy : 99.82, test loss : 0.4208, test accuracy : 94.68\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0032, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.82\n",
            "Epoch : 221, training loss : 0.0049, training accuracy : 99.81, test loss : 0.4612, test accuracy : 94.37\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0070, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0066, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0064, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0062, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0059, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0064, accuracy : 99.77\n",
            "Epoch : 222, training loss : 0.0065, training accuracy : 99.77, test loss : 0.4447, test accuracy : 94.10\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0042, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0042, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0057, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0060, accuracy : 99.78\n",
            "Epoch : 223, training loss : 0.0061, training accuracy : 99.78, test loss : 0.4583, test accuracy : 94.36\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0062, accuracy : 99.80\n",
            "Epoch : 224, training loss : 0.0061, training accuracy : 99.80, test loss : 0.4615, test accuracy : 94.43\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0065, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0062, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0060, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0059, accuracy : 99.81\n",
            "Epoch : 225, training loss : 0.0061, training accuracy : 99.81, test loss : 0.4621, test accuracy : 94.56\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0061, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0062, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0059, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.83\n",
            "Epoch : 226, training loss : 0.0057, training accuracy : 99.83, test loss : 0.4440, test accuracy : 94.61\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0040, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.80\n",
            "Epoch : 227, training loss : 0.0055, training accuracy : 99.79, test loss : 0.4464, test accuracy : 94.36\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.82\n",
            "Epoch : 228, training loss : 0.0048, training accuracy : 99.83, test loss : 0.4640, test accuracy : 94.62\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0046, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0050, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.82\n",
            "Epoch : 229, training loss : 0.0052, training accuracy : 99.81, test loss : 0.4664, test accuracy : 94.69\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0053, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0055, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0065, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0068, accuracy : 99.76\n",
            "Epoch : 230, training loss : 0.0069, training accuracy : 99.76, test loss : 0.4645, test accuracy : 94.40\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0030, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0033, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0035, accuracy : 99.87\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0056, accuracy : 99.80\n",
            "Epoch : 231, training loss : 0.0054, training accuracy : 99.81, test loss : 0.4498, test accuracy : 94.48\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0057, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.83\n",
            "Epoch : 232, training loss : 0.0052, training accuracy : 99.83, test loss : 0.4492, test accuracy : 94.44\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0043, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0059, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0064, accuracy : 99.81\n",
            "Epoch : 233, training loss : 0.0062, training accuracy : 99.81, test loss : 0.4654, test accuracy : 94.33\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0037, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0034, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0040, accuracy : 99.88\n",
            "Epoch : 234, training loss : 0.0040, training accuracy : 99.88, test loss : 0.4704, test accuracy : 94.69\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0074, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0082, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0076, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0067, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0067, accuracy : 99.76\n",
            "Epoch : 235, training loss : 0.0067, training accuracy : 99.75, test loss : 0.4714, test accuracy : 94.57\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0069, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0065, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0061, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0058, accuracy : 99.79\n",
            "Epoch : 236, training loss : 0.0057, training accuracy : 99.80, test loss : 0.4597, test accuracy : 94.45\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.86\n",
            "Epoch : 237, training loss : 0.0045, training accuracy : 99.85, test loss : 0.4512, test accuracy : 94.62\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0028, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0029, accuracy : 99.92\n",
            "iteration : 300, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0036, accuracy : 99.89\n",
            "Epoch : 238, training loss : 0.0038, training accuracy : 99.89, test loss : 0.4842, test accuracy : 94.40\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0063, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.83\n",
            "Epoch : 239, training loss : 0.0049, training accuracy : 99.83, test loss : 0.4735, test accuracy : 94.48\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.81\n",
            "Epoch : 240, training loss : 0.0053, training accuracy : 99.81, test loss : 0.4673, test accuracy : 94.63\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.84\n",
            "Epoch : 241, training loss : 0.0045, training accuracy : 99.84, test loss : 0.4606, test accuracy : 94.55\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0059, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0068, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0069, accuracy : 99.76\n",
            "Epoch : 242, training loss : 0.0067, training accuracy : 99.77, test loss : 0.4460, test accuracy : 94.52\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0068, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.81\n",
            "Epoch : 243, training loss : 0.0065, training accuracy : 99.81, test loss : 0.4506, test accuracy : 94.61\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0026, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0029, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0028, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0028, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0034, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0034, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0041, accuracy : 99.85\n",
            "Epoch : 244, training loss : 0.0042, training accuracy : 99.85, test loss : 0.4551, test accuracy : 94.56\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0036, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0052, accuracy : 99.83\n",
            "Epoch : 245, training loss : 0.0052, training accuracy : 99.83, test loss : 0.4554, test accuracy : 94.51\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0055, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.82\n",
            "Epoch : 246, training loss : 0.0057, training accuracy : 99.81, test loss : 0.4848, test accuracy : 94.53\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0050, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0046, accuracy : 99.83\n",
            "Epoch : 247, training loss : 0.0046, training accuracy : 99.83, test loss : 0.4727, test accuracy : 94.60\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0044, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.87\n",
            "Epoch : 248, training loss : 0.0047, training accuracy : 99.86, test loss : 0.4594, test accuracy : 94.46\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0043, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0052, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.83\n",
            "Epoch : 249, training loss : 0.0049, training accuracy : 99.84, test loss : 0.4590, test accuracy : 94.40\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0047, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.82\n",
            "Epoch : 250, training loss : 0.0048, training accuracy : 99.82, test loss : 0.4751, test accuracy : 94.49\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0051, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0059, accuracy : 99.81\n",
            "Epoch : 251, training loss : 0.0060, training accuracy : 99.80, test loss : 0.4753, test accuracy : 94.43\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0092, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0085, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0081, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0076, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.78\n",
            "Epoch : 252, training loss : 0.0073, training accuracy : 99.79, test loss : 0.4564, test accuracy : 94.34\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0017, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0030, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0035, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0039, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0038, accuracy : 99.85\n",
            "Epoch : 253, training loss : 0.0039, training accuracy : 99.84, test loss : 0.4544, test accuracy : 94.67\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0041, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.82\n",
            "Epoch : 254, training loss : 0.0050, training accuracy : 99.83, test loss : 0.4464, test accuracy : 94.58\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.82\n",
            "Epoch : 255, training loss : 0.0050, training accuracy : 99.83, test loss : 0.4552, test accuracy : 94.61\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0035, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0059, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0058, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0061, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.79\n",
            "Epoch : 256, training loss : 0.0064, training accuracy : 99.78, test loss : 0.4642, test accuracy : 94.35\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0045, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0035, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0044, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 257, training loss : 0.0046, training accuracy : 99.86, test loss : 0.4458, test accuracy : 94.58\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0022, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0028, accuracy : 99.93\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 258, training loss : 0.0047, training accuracy : 99.85, test loss : 0.4676, test accuracy : 94.51\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0045, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0042, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0037, accuracy : 99.90\n",
            "iteration : 350, loss : 0.0036, accuracy : 99.90\n",
            "Epoch : 259, training loss : 0.0036, training accuracy : 99.90, test loss : 0.4686, test accuracy : 94.71\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0017, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0019, accuracy : 99.92\n",
            "iteration : 150, loss : 0.0025, accuracy : 99.93\n",
            "iteration : 200, loss : 0.0027, accuracy : 99.91\n",
            "iteration : 250, loss : 0.0029, accuracy : 99.91\n",
            "iteration : 300, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 350, loss : 0.0032, accuracy : 99.90\n",
            "Epoch : 260, training loss : 0.0031, training accuracy : 99.90, test loss : 0.4767, test accuracy : 94.71\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0069, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0076, accuracy : 99.79\n",
            "iteration : 150, loss : 0.0076, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0074, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0069, accuracy : 99.77\n",
            "Epoch : 261, training loss : 0.0072, training accuracy : 99.77, test loss : 0.4614, test accuracy : 94.52\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0068, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0058, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0058, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0055, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0057, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0050, accuracy : 99.84\n",
            "Epoch : 262, training loss : 0.0050, training accuracy : 99.84, test loss : 0.4683, test accuracy : 94.38\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0072, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0057, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0053, accuracy : 99.81\n",
            "Epoch : 263, training loss : 0.0052, training accuracy : 99.82, test loss : 0.4728, test accuracy : 94.50\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0039, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.83\n",
            "Epoch : 264, training loss : 0.0053, training accuracy : 99.84, test loss : 0.4412, test accuracy : 94.59\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0025, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0034, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0032, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0043, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0040, accuracy : 99.85\n",
            "Epoch : 265, training loss : 0.0041, training accuracy : 99.86, test loss : 0.4607, test accuracy : 94.53\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0028, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0028, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0026, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0026, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0029, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0031, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0034, accuracy : 99.87\n",
            "Epoch : 266, training loss : 0.0036, training accuracy : 99.87, test loss : 0.5259, test accuracy : 94.41\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0112, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0106, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0088, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0085, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0074, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0063, accuracy : 99.81\n",
            "Epoch : 267, training loss : 0.0062, training accuracy : 99.81, test loss : 0.4835, test accuracy : 94.54\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0057, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0056, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0056, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0060, accuracy : 99.79\n",
            "Epoch : 268, training loss : 0.0059, training accuracy : 99.80, test loss : 0.4697, test accuracy : 94.30\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0078, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0060, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0054, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0039, accuracy : 99.88\n",
            "Epoch : 269, training loss : 0.0039, training accuracy : 99.88, test loss : 0.4692, test accuracy : 94.57\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0058, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0054, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0069, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0068, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0067, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0065, accuracy : 99.80\n",
            "Epoch : 270, training loss : 0.0066, training accuracy : 99.80, test loss : 0.4540, test accuracy : 94.69\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0041, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0037, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0037, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0036, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0037, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0037, accuracy : 99.86\n",
            "Epoch : 271, training loss : 0.0037, training accuracy : 99.86, test loss : 0.4712, test accuracy : 94.62\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0032, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0034, accuracy : 99.89\n",
            "iteration : 250, loss : 0.0033, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.86\n",
            "Epoch : 272, training loss : 0.0044, training accuracy : 99.85, test loss : 0.4692, test accuracy : 94.55\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0060, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0047, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0057, accuracy : 99.83\n",
            "Epoch : 273, training loss : 0.0054, training accuracy : 99.84, test loss : 0.4425, test accuracy : 94.67\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0036, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0036, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0038, accuracy : 99.89\n",
            "iteration : 350, loss : 0.0040, accuracy : 99.89\n",
            "Epoch : 274, training loss : 0.0040, training accuracy : 99.88, test loss : 0.4557, test accuracy : 94.71\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0015, accuracy : 99.97\n",
            "iteration : 100, loss : 0.0020, accuracy : 99.95\n",
            "iteration : 150, loss : 0.0032, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0039, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 275, training loss : 0.0047, training accuracy : 99.85, test loss : 0.4648, test accuracy : 94.59\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0052, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0054, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0054, accuracy : 99.84\n",
            "Epoch : 276, training loss : 0.0055, training accuracy : 99.84, test loss : 0.4752, test accuracy : 94.40\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0050, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.83\n",
            "Epoch : 277, training loss : 0.0053, training accuracy : 99.84, test loss : 0.4409, test accuracy : 94.69\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0038, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0041, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0046, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0037, accuracy : 99.89\n",
            "Epoch : 278, training loss : 0.0037, training accuracy : 99.89, test loss : 0.4787, test accuracy : 94.69\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0030, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0028, accuracy : 99.90\n",
            "iteration : 200, loss : 0.0030, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0035, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0035, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0035, accuracy : 99.88\n",
            "Epoch : 279, training loss : 0.0036, training accuracy : 99.88, test loss : 0.4882, test accuracy : 94.52\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0068, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0056, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0048, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0049, accuracy : 99.83\n",
            "Epoch : 280, training loss : 0.0049, training accuracy : 99.83, test loss : 0.4832, test accuracy : 94.46\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0027, accuracy : 99.94\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0044, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 281, training loss : 0.0048, training accuracy : 99.84, test loss : 0.4533, test accuracy : 94.52\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0039, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0040, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0041, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0041, accuracy : 99.85\n",
            "Epoch : 282, training loss : 0.0041, training accuracy : 99.86, test loss : 0.4667, test accuracy : 94.58\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0053, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0052, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0051, accuracy : 99.86\n",
            "Epoch : 283, training loss : 0.0049, training accuracy : 99.87, test loss : 0.4660, test accuracy : 94.61\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0023, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0027, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0027, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0034, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.84\n",
            "Epoch : 284, training loss : 0.0047, training accuracy : 99.84, test loss : 0.4592, test accuracy : 94.62\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0028, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0024, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0034, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0038, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0037, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0037, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0042, accuracy : 99.83\n",
            "Epoch : 285, training loss : 0.0044, training accuracy : 99.83, test loss : 0.4795, test accuracy : 94.35\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0039, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0041, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0045, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0043, accuracy : 99.84\n",
            "Epoch : 286, training loss : 0.0042, training accuracy : 99.85, test loss : 0.4735, test accuracy : 94.54\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0046, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0035, accuracy : 99.90\n",
            "iteration : 150, loss : 0.0036, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0037, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0035, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 350, loss : 0.0040, accuracy : 99.85\n",
            "Epoch : 287, training loss : 0.0042, training accuracy : 99.85, test loss : 0.4946, test accuracy : 94.41\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0047, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0057, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0053, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0054, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0055, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0057, accuracy : 99.79\n",
            "Epoch : 288, training loss : 0.0059, training accuracy : 99.79, test loss : 0.4771, test accuracy : 94.50\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0048, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0047, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0048, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0046, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0055, accuracy : 99.79\n",
            "Epoch : 289, training loss : 0.0056, training accuracy : 99.80, test loss : 0.4517, test accuracy : 94.43\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0048, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0052, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0041, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0039, accuracy : 99.88\n",
            "Epoch : 290, training loss : 0.0037, training accuracy : 99.88, test loss : 0.4416, test accuracy : 94.68\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0037, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0029, accuracy : 99.91\n",
            "iteration : 150, loss : 0.0023, accuracy : 99.93\n",
            "iteration : 200, loss : 0.0023, accuracy : 99.93\n",
            "iteration : 250, loss : 0.0021, accuracy : 99.93\n",
            "iteration : 300, loss : 0.0021, accuracy : 99.93\n",
            "iteration : 350, loss : 0.0023, accuracy : 99.93\n",
            "Epoch : 291, training loss : 0.0025, training accuracy : 99.92, test loss : 0.4855, test accuracy : 94.42\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0019, accuracy : 99.95\n",
            "iteration : 100, loss : 0.0015, accuracy : 99.96\n",
            "iteration : 150, loss : 0.0020, accuracy : 99.94\n",
            "iteration : 200, loss : 0.0024, accuracy : 99.93\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.89\n",
            "iteration : 300, loss : 0.0046, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0048, accuracy : 99.86\n",
            "Epoch : 292, training loss : 0.0049, training accuracy : 99.86, test loss : 0.4604, test accuracy : 94.43\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0073, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0065, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0062, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0051, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0052, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0049, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 293, training loss : 0.0047, training accuracy : 99.85, test loss : 0.4560, test accuracy : 94.56\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0034, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0036, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0036, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0033, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0030, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0035, accuracy : 99.86\n",
            "Epoch : 294, training loss : 0.0038, training accuracy : 99.86, test loss : 0.4806, test accuracy : 94.65\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0040, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0033, accuracy : 99.87\n",
            "iteration : 150, loss : 0.0025, accuracy : 99.91\n",
            "iteration : 200, loss : 0.0030, accuracy : 99.90\n",
            "iteration : 250, loss : 0.0030, accuracy : 99.90\n",
            "iteration : 300, loss : 0.0034, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0035, accuracy : 99.88\n",
            "Epoch : 295, training loss : 0.0035, training accuracy : 99.88, test loss : 0.4871, test accuracy : 94.49\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0024, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0037, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0033, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0038, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0037, accuracy : 99.88\n",
            "iteration : 300, loss : 0.0038, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0037, accuracy : 99.88\n",
            "Epoch : 296, training loss : 0.0037, training accuracy : 99.88, test loss : 0.5040, test accuracy : 94.47\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0064, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0067, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0063, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0068, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0071, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0066, accuracy : 99.80\n",
            "Epoch : 297, training loss : 0.0067, training accuracy : 99.80, test loss : 0.4785, test accuracy : 94.33\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0059, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0055, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0049, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0047, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0047, accuracy : 99.88\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.86\n",
            "Epoch : 298, training loss : 0.0046, training accuracy : 99.86, test loss : 0.4454, test accuracy : 94.56\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0050, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0038, accuracy : 99.89\n",
            "iteration : 150, loss : 0.0037, accuracy : 99.88\n",
            "iteration : 200, loss : 0.0043, accuracy : 99.87\n",
            "iteration : 250, loss : 0.0053, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0051, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0047, accuracy : 99.85\n",
            "Epoch : 299, training loss : 0.0046, training accuracy : 99.85, test loss : 0.4540, test accuracy : 94.71\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0027, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0033, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0038, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0044, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0043, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0042, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0040, accuracy : 99.87\n",
            "Epoch : 300, training loss : 0.0040, training accuracy : 99.87, test loss : 0.4752, test accuracy : 94.46\n"
          ]
        }
      ],
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 0\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "iUQVIKR-X3v6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.4086923391065177, 95.4940073755378)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7CNz1iabSB21"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : constant LR\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "E9qb9ItHSC5U"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : constant LR\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3eiY3bTlWipW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_loss_list_const = [1.5944857293674293, 0.4869092239677745, 0.3710624326454592, 0.3296906750215101, 0.29884549410039496, 0.27146996971633697, 0.2604337949053382, 0.24417246496532022, 0.23419785655045575, 0.22457996957751147, 0.21173857996457315, 0.20202396321425917, 0.1943116241051414, 0.18745123495052501, 0.1774632519257424, 0.17214223601650902, 0.16552220173886797, 0.15473239550866733, 0.15404348591276948, 0.14213740165398372, 0.13690419029192066, 0.13265049481779578, 0.12643728150322348, 0.1223953123176647, 0.11572830584960256, 0.10872174589453028, 0.10607895330301306, 0.10143619665729645, 0.09846852844464908, 0.09155762590831373, 0.09176684167932689, 0.08320119946962853, 0.07973214024854547, 0.07931727142424037, 0.06983993944351063, 0.07091535234995448, 0.0671700671211713, 0.06470882359558974, 0.061619590856841586, 0.061459206172131346, 0.057121708876198427, 0.053374216585170206, 0.0498615506466044, 0.05073856983490308, 0.04987424655961312, 0.046790412364042994, 0.04711244383266544, 0.04263703334889801, 0.043034394194061555, 0.04126133985860339, 0.03625233271121373, 0.03783284285014904, 0.0379838087573284, 0.03416189216178284, 0.03383261713717981, 0.03370409071676403, 0.033968668481206325, 0.03165312097212274, 0.03038872497441464, 0.02841417055683044, 0.029356409022635036, 0.02889881345480024, 0.028939039615515447, 0.02568263415692672, 0.027656220918051838, 0.026301205131101617, 0.023402816848356156, 0.024891200900357974, 0.022950206206843942, 0.022271349775869285, 0.0250707899006071, 0.022344395426495713, 0.019562920480307198, 0.022191016989943715, 0.02281398802152065, 0.0203801996838415, 0.020550807893653537, 0.02117716848076041, 0.019771566791103416, 0.022764958889409142, 0.017198871523580392, 0.02078044665365311, 0.020109122048629333, 0.01588759385191313, 0.01774437841210012, 0.020397339600326397, 0.01534856222032823, 0.016435430918308794, 0.01711455919842304, 0.01770936636729633, 0.018040290616609177, 0.01564757464108749, 0.015251886521385311, 0.01534154344099917, 0.015427147975282334, 0.01419120683843651, 0.01811075882562171, 0.016642653122089984, 0.01305790530750528, 0.013949767823940267, 0.014656124457392462, 0.013457846312966957, 0.014343252129156173, 0.013850259383991146, 0.012460459422866397, 0.012413865320129272, 0.013889214684304953, 0.01221698466477806, 0.015144580592092777, 0.010421825547786688, 0.01315658220278792, 0.014559990600058128, 0.012665790851758906, 0.01143317199561781, 0.011717887349175301, 0.011158559258571676, 0.012634062059679534, 0.011741106513152792, 0.012515452659958159, 0.011751638104695717, 0.011457235534427129, 0.011438692509360596, 0.012512708038015468, 0.008602326916451698, 0.010682771495822786, 0.011196336384384224, 0.009268063090769713, 0.009998142540840234, 0.012729091897495166, 0.010267185127698763, 0.010566324319321028, 0.010713477439735606, 0.010119398856430022, 0.012654226244056406, 0.009628543746825273, 0.0102562899387879, 0.010169452542220153, 0.011113815507671713, 0.01246779066452764, 0.007518461719419533, 0.007389902458939484, 0.011104748547043264, 0.01135712010794771, 0.008941290669380526, 0.009924649224834431, 0.009838336234147059, 0.007612355080635734, 0.009282812758648675, 0.008423602285464519, 0.009065308554876006, 0.010365516194014629, 0.006115070432431249, 0.006490228478146479, 0.011122480412586441, 0.00872685812082587, 0.008791860558152852, 0.009787340952780856, 0.008418811355189983, 0.007423892029066022, 0.009898100130695444, 0.008828991230531168, 0.007722080036888794, 0.009378696144602659, 0.00829515335842403, 0.006554383687879633, 0.006126663065066873, 0.010910632581322311, 0.007716069375264186, 0.006358072765228986, 0.007697185452836498, 0.008877029283653514, 0.008559577807589543, 0.00707696825589556, 0.00850764569837624, 0.0062317561151193695, 0.0066259507287960455, 0.008618629247509826, 0.007878521594295732, 0.007498281048782551, 0.006598806819186967, 0.006230976371933824, 0.006141664580167378, 0.007923200858393221, 0.008045455975570557, 0.00792612456981016, 0.007601726206049413, 0.007419711852838272, 0.004584900550783639, 0.00685006011318293, 0.008050930358640324, 0.008370204849807444, 0.005507358266772813, 0.006857401374734981, 0.008172955556791895, 0.004244687181667273, 0.008633549227632837, 0.008705449396538356, 0.0067560189635043925, 0.006710546794999935, 0.00550614756518684, 0.0069180619830691474, 0.00835907400430254, 0.005712247417545953, 0.006098340349643973, 0.007154704712266156, 0.0061693518397019, 0.005750073719051822, 0.005536363746094307, 0.006909997050809464, 0.006392437509654407, 0.004530645426771153, 0.00935107147158372, 0.005991038718238898, 0.004413133404240297, 0.007152540924743997, 0.006123794079590131, 0.004265024197029019, 0.007488072243164834, 0.006849384789138319, 0.006199823599835201, 0.004939858833603685, 0.006542191565922548, 0.006134184821023406, 0.006127372573951168, 0.006120713498504846, 0.005705225819374859, 0.0054501006014126396, 0.00476737067947397, 0.005204442173390228, 0.006888031104115721, 0.005438639611901732, 0.005198591940753804, 0.006226397409455738, 0.00398913007948585, 0.006725981141281091, 0.0057350191533619786, 0.004547678196042624, 0.003760278201966155, 0.004862335098237806, 0.0053206211415940214, 0.004511028374038128, 0.006714381331292853, 0.006507630908786355, 0.004150821972775074, 0.005154698122662017, 0.005662354828146746, 0.004606740834664833, 0.004731273036621596, 0.004916682220897782, 0.0048261530498232945, 0.006001339212270287, 0.007263619651073496, 0.0038986454153652154, 0.005036929771996826, 0.005049693082181207, 0.00644224065052205, 0.004578751418381182, 0.004686958738437559, 0.003585059260631736, 0.003101582141557616, 0.007221638428928184, 0.005000114876925785, 0.0051700827908760325, 0.005254589704386773, 0.00406367993565528, 0.0036232792127412505, 0.006171727926178867, 0.005875237415135579, 0.00392443083374783, 0.006561909474143628, 0.0037487958143322068, 0.004443046452993911, 0.005447318903908573, 0.004017829913798502, 0.00472008602112813, 0.0055074588290326305, 0.005279580161203713, 0.0036886348879267314, 0.0035813810519778975, 0.004883374933941502, 0.004846986711028166, 0.004054715485154408, 0.004949977601104352, 0.0046826732979178415, 0.004384392334953691, 0.004187341836835158, 0.004243570542040143, 0.005868256121943751, 0.00557879406272601, 0.0037290168971954394, 0.0024883511193469366, 0.004939023949055762, 0.004748034648033541, 0.0038421762496379727, 0.003486280636854575, 0.0036575751641310756, 0.00670746078328728, 0.004645855993902198, 0.004636629221823842, 0.004005611049817734]\n",
            "train_acc_list_const = [44.03176283748015, 84.49761778718899, 88.61196400211752, 89.87400741132875, 91.01958708311275, 91.79460031762838, 92.2350449973531, 92.80889359449444, 93.03546850185283, 93.37427210164108, 93.80412916887242, 94.0052938062467, 94.38009528851244, 94.6278454208576, 94.88618316569614, 95.01323451561673, 95.1868713605082, 95.66543144520911, 95.57226045526734, 95.91953414505029, 96.01905770248808, 96.16093170989942, 96.35150873478031, 96.54632080465855, 96.74113287453679, 96.86183165696136, 96.96347273689783, 97.10746426680784, 97.21757543673901, 97.32133403917416, 97.3276866066702, 97.63896241397565, 97.72154579142403, 97.7342509264161, 97.88883006881949, 97.87400741132875, 97.96506087877184, 98.09211222869243, 98.24245632609846, 98.17257808364214, 98.28692429857067, 98.34833245103229, 98.44997353096876, 98.49444150344097, 98.49655902593965, 98.54102699841185, 98.55796717840127, 98.67443091582848, 98.63419798835362, 98.74219163578613, 98.87559555320276, 98.77607199576495, 98.78030704076231, 98.9793541556379, 98.94970884065643, 98.9433562731604, 98.90947591318158, 98.98782424563261, 98.96453149814717, 99.08099523557438, 99.05770248808894, 99.05134992059291, 99.0428798305982, 99.1784012705135, 99.13393329804128, 99.1148755955532, 99.20381154049761, 99.18051879301217, 99.27580730545262, 99.23980942297511, 99.1784012705135, 99.25674960296453, 99.3499205929063, 99.26310217046056, 99.2503970354685, 99.32874536791954, 99.28215987294865, 99.27157226045527, 99.29486500794071, 99.24404446797247, 99.42615140285865, 99.2779248279513, 99.31180518793012, 99.46214928533615, 99.40921122286925, 99.36262572789836, 99.49602964531498, 99.43673901535203, 99.42615140285865, 99.40921122286925, 99.41132874536792, 99.47697194282689, 99.49602964531498, 99.49814716781366, 99.47273689782953, 99.5044997353097, 99.39015352038115, 99.45156167284277, 99.57649550026468, 99.55320275277924, 99.5404976177872, 99.5659078877713, 99.53414505029116, 99.54685018528322, 99.58073054526204, 99.5849655902594, 99.49391212281631, 99.58920063525674, 99.51720487030175, 99.66543144520911, 99.5574377977766, 99.56167284277396, 99.56802541026998, 99.60402329274748, 99.62519851773425, 99.6019057702488, 99.5934356802541, 99.62519851773425, 99.60402329274748, 99.61461090524087, 99.61461090524087, 99.63790365272631, 99.6019057702488, 99.70566437268396, 99.66119640021175, 99.64002117522499, 99.69507676019057, 99.68025410269983, 99.58920063525674, 99.6569613552144, 99.62519851773425, 99.6569613552144, 99.66754896770779, 99.54261514028586, 99.69719428268925, 99.63366860772896, 99.67178401270513, 99.60825833774484, 99.6019057702488, 99.75436739015352, 99.75436739015352, 99.59555320275278, 99.62731604023293, 99.6929592376919, 99.65484383271573, 99.66966649020645, 99.7289571201694, 99.71836950767602, 99.69084171519323, 99.67601905770249, 99.66119640021175, 99.79671784012704, 99.78824775013234, 99.62519851773425, 99.71836950767602, 99.72683959767072, 99.68872419269455, 99.73107464266808, 99.73107464266808, 99.67601905770249, 99.70989941768131, 99.73319216516676, 99.68025410269983, 99.7204870301747, 99.78824775013234, 99.80518793012176, 99.64849126521969, 99.73107464266808, 99.79883536262572, 99.74377977766014, 99.69719428268925, 99.69719428268925, 99.77130757014294, 99.71625198517734, 99.7924827951297, 99.784012705135, 99.70354685018528, 99.75860243515088, 99.73319216516676, 99.79036527263102, 99.78189518263632, 99.79036527263102, 99.73530968766543, 99.69719428268925, 99.75013234515616, 99.72472207517205, 99.76071995764956, 99.84330333509793, 99.784012705135, 99.73530968766543, 99.69084171519323, 99.82212811011117, 99.76919004764426, 99.73319216516676, 99.85812599258867, 99.70989941768131, 99.70142932768661, 99.76071995764956, 99.78824775013234, 99.8284806776072, 99.7924827951297, 99.7204870301747, 99.8009528851244, 99.81577554261514, 99.75436739015352, 99.7924827951297, 99.81365802011646, 99.8009528851244, 99.77554261514028, 99.77977766013764, 99.85389094759132, 99.67601905770249, 99.81789306511382, 99.86871360508205, 99.74377977766014, 99.78189518263632, 99.84965590259397, 99.77130757014294, 99.78824775013234, 99.82001058761249, 99.81365802011646, 99.77130757014294, 99.78189518263632, 99.79883536262572, 99.80730545262044, 99.82636315510852, 99.79036527263102, 99.83059820010588, 99.81154049761778, 99.7564849126522, 99.8094229751191, 99.8284806776072, 99.81154049761778, 99.88353626257279, 99.75224986765484, 99.79671784012704, 99.85389094759132, 99.88565378507147, 99.83059820010588, 99.80730545262044, 99.84118581259926, 99.77130757014294, 99.81154049761778, 99.85177342509265, 99.83271572260455, 99.8094229751191, 99.8284806776072, 99.86236103758603, 99.8369507676019, 99.82424563260984, 99.8009528851244, 99.78824775013234, 99.84330333509793, 99.82636315510852, 99.8284806776072, 99.78189518263632, 99.85812599258867, 99.85177342509265, 99.89624139756485, 99.89835892006353, 99.77130757014294, 99.84118581259926, 99.81577554261514, 99.8369507676019, 99.85600847008999, 99.86659608258337, 99.81154049761778, 99.79671784012704, 99.87718369507677, 99.79883536262572, 99.85812599258867, 99.85389094759132, 99.83906829010058, 99.88353626257279, 99.84753838009529, 99.8369507676019, 99.83906829010058, 99.88565378507147, 99.87506617257809, 99.83271572260455, 99.84330333509793, 99.85812599258867, 99.86659608258337, 99.8369507676019, 99.8284806776072, 99.84753838009529, 99.85177342509265, 99.7924827951297, 99.79671784012704, 99.88141874007411, 99.92376919004765, 99.86024351508735, 99.84753838009529, 99.85600847008999, 99.87506617257809, 99.87718369507677, 99.79671784012704, 99.86024351508735, 99.85177342509265, 99.87083112758073]\n",
            "test_loss_list_const = [0.9403148495099124, 0.46971940738605517, 0.34306909633325594, 0.3384028169892582, 0.3164549315823059, 0.31764499748162195, 0.282772644879479, 0.27239831069520876, 0.2788072020618939, 0.2605946859454407, 0.2569786167758353, 0.25151387514436946, 0.23040011815507622, 0.23389688687508597, 0.23323347488893012, 0.22432381940969065, 0.23885389055837603, 0.22965874502837075, 0.23512107754747072, 0.23369459731175618, 0.22183155821745887, 0.2491116562639089, 0.23399925456546686, 0.25359821337841304, 0.23499618255186314, 0.22481991891183106, 0.2324665375966944, 0.23745987874766192, 0.24356824685545528, 0.24952035153503804, 0.23930090873082185, 0.25436064657554325, 0.25367995643732594, 0.26609678615761156, 0.2672480665622096, 0.2746706861893044, 0.25824960023529975, 0.26025373894063863, 0.2791316908072023, 0.26918739248432366, 0.2763673086997633, 0.2997446244436444, 0.2916372886438872, 0.3003798500981693, 0.2806282534501424, 0.2931029381979184, 0.29610180335265457, 0.3087221756942716, 0.32797835285172744, 0.30948590080929445, 0.31205729929292025, 0.3264701991711798, 0.31612680869761345, 0.3249107254029927, 0.3218845636081681, 0.3128845189679779, 0.3158182442261308, 0.3337518404621412, 0.34533743259003935, 0.32409434777447116, 0.3321887091642209, 0.3324712138604738, 0.33405348602864965, 0.3346349073199592, 0.3316200801226146, 0.35435040097446274, 0.3272651647738017, 0.3605334343612377, 0.3315225986252521, 0.36446072681642633, 0.37520757036320135, 0.3792235794586732, 0.3674935271379118, 0.3611337201596767, 0.35125305392213313, 0.3664919300844856, 0.3579704279956572, 0.358177753384499, 0.37499817195074525, 0.3516562903372973, 0.36848902222061275, 0.37098927200570997, 0.37174441270968495, 0.3886868230103716, 0.36849470644751015, 0.3531273581982389, 0.3660184144471571, 0.37421249364520986, 0.3748011891273599, 0.3667490633350669, 0.3643053767691348, 0.37450089440772344, 0.37444961906465535, 0.3669995936729452, 0.37039543743080955, 0.38952702733085437, 0.39265704210208474, 0.3686957350274658, 0.3655954713695774, 0.39540933398529887, 0.37815322328870205, 0.3858970995974161, 0.37387897242682383, 0.37425000457020074, 0.38926156392941874, 0.3786647673787586, 0.37843504400156874, 0.4000659274078869, 0.38383779517721894, 0.38883355182513374, 0.38116612786189746, 0.39886108679952575, 0.4039541971048011, 0.4085461224575399, 0.4040325807553588, 0.40590017215878355, 0.38623136059180196, 0.38693335685221586, 0.41144327609343273, 0.3980357802459313, 0.4007313655798926, 0.40366758222636934, 0.38523547610669745, 0.42097171660804866, 0.4246416387294291, 0.40921079142786126, 0.4052840040850581, 0.4231068941344525, 0.38739459854824576, 0.4194799537094785, 0.41747657055327414, 0.4164134435739149, 0.3939945743985328, 0.393845445202554, 0.4172966992665155, 0.4208380446276244, 0.408174136686641, 0.43686796033608855, 0.3842315211042981, 0.4022488394277353, 0.41533097460427704, 0.4123304054533661, 0.3947430606748836, 0.4344836208585869, 0.42245670334509045, 0.4120090134880122, 0.4084896664774301, 0.4208405370477076, 0.4342218170389898, 0.4342225538966173, 0.414541769489719, 0.42016579197290554, 0.43672865715023934, 0.3970657474470927, 0.4120631425404081, 0.4072342333657777, 0.4361969401840778, 0.40728934426043256, 0.4082725720443562, 0.43000999789721533, 0.4488575828864294, 0.4490402939628956, 0.42708765610358584, 0.4119907574152903, 0.4171651719850214, 0.46127884272559017, 0.424621216852364, 0.42663686249551236, 0.443727373143238, 0.4234333980655042, 0.4163527505554478, 0.41962003758446514, 0.43725314623146666, 0.4135830312939909, 0.4485339165719993, 0.438206387026345, 0.42548432996423513, 0.4139385414218493, 0.422614497990877, 0.44258191624619797, 0.5020544356643679, 0.42181872726217207, 0.45136614169414135, 0.42586187040889817, 0.4617275644605066, 0.42785005136758236, 0.4181436715811929, 0.43456211805745376, 0.456493051905258, 0.4419752404130265, 0.439949865836431, 0.458016514120733, 0.45355927485370023, 0.4266304138993077, 0.4774111132959233, 0.43147677636942733, 0.42984621565533326, 0.4257584124226888, 0.41347907219703, 0.44227188734301165, 0.44509480125270784, 0.42359101505694435, 0.45620652116542937, 0.4532003666402078, 0.4544208156419735, 0.4288948582868804, 0.45772276510365817, 0.4652134206776014, 0.4481767811538542, 0.45044724102926387, 0.45808432965228957, 0.4832801581086481, 0.4330355538119215, 0.450099578350965, 0.44405912482818843, 0.4245691280629413, 0.47918898603130206, 0.4441465862088508, 0.4232571859679678, 0.42079631317699073, 0.46119900758140814, 0.4447093260310152, 0.45832808096619215, 0.4615354106998911, 0.46208279893970955, 0.44396297247651234, 0.44644354116719437, 0.4640259245666219, 0.466397661642701, 0.46452038604811785, 0.4498248620908342, 0.4491576017337103, 0.465446551038208, 0.47035092626716574, 0.4713521593012938, 0.4597488452506927, 0.4512224776633814, 0.48416280633240355, 0.4734927043455708, 0.46729338517887337, 0.46055449181357305, 0.44595291037751617, 0.45059747884606977, 0.4550898942740305, 0.4553583972247354, 0.484807043427638, 0.47273430763972085, 0.45941312947109636, 0.4590342458538419, 0.4751177701332113, 0.47533442642466694, 0.45637929342760175, 0.4544383191900766, 0.4463540438526109, 0.45517611408414427, 0.4642113188975582, 0.44581558580930325, 0.4676268981386195, 0.46861990691874833, 0.47670730448090565, 0.4613685188541079, 0.46828112538502203, 0.47280027833310706, 0.44121663249097764, 0.4606796685479554, 0.5259090004415781, 0.48354477134040175, 0.469667333146265, 0.4692415113266393, 0.45400641179944884, 0.47124824484846756, 0.4692198993460111, 0.44249545135900525, 0.4556816854969571, 0.4647824307538423, 0.47515893393360514, 0.44086019356972445, 0.47874322776481804, 0.48824521492911027, 0.4832109012732319, 0.45331234720138397, 0.46669590011166007, 0.4659509803990231, 0.4592359331253843, 0.4794528412395248, 0.4734935767528619, 0.4946094546649678, 0.47707713397183255, 0.4516702479244593, 0.44164761501893984, 0.48553112554637823, 0.4603754159190929, 0.4560155915005096, 0.48063205852739366, 0.4870615708185177, 0.503982417563926, 0.4785489882890354, 0.44539653152848285, 0.4539821001874539, 0.47518135870204253]\n",
            "test_acc_list_const = [71.6080208973571, 85.50245851259987, 89.67040565457899, 89.58589428395821, 90.46173939766442, 90.5577750460971, 91.50276582667486, 92.0520897357099, 91.96757836508912, 92.47464658881377, 92.6820835894284, 92.88952059004302, 93.56177012907192, 93.4580516287646, 93.66548862937923, 93.80377996312231, 93.34665027658266, 93.71158574062692, 93.5041487400123, 93.57329440688383, 94.06499692685925, 93.33896742470804, 93.72311001843885, 92.94330055316533, 93.68853718500307, 94.18792255685311, 93.96127842655194, 93.91518131530424, 93.82682851874615, 94.03042409342348, 94.1917639827904, 93.77304855562384, 93.9420712968654, 93.68853718500307, 94.08420405654579, 93.77304855562384, 93.91518131530424, 94.14566687154272, 93.93822987092808, 94.14566687154272, 94.06499692685925, 93.73079287031346, 93.8921327596804, 94.07267977873387, 94.01889981561156, 94.22633681622618, 93.86140135218193, 93.90749846342962, 93.97664413030117, 94.07652120467118, 94.03042409342348, 93.97280270436386, 94.21865396435157, 94.07267977873387, 94.09956976029503, 94.21865396435157, 94.30700676090964, 93.88444990780577, 94.06883835279656, 94.30316533497235, 94.23017824216349, 94.19944683466503, 94.05731407498463, 94.36846957590657, 94.26475107559926, 94.04578979717272, 94.31468961278426, 94.13030116779349, 94.3262138905962, 94.06499692685925, 93.98432698217579, 94.34926244622004, 94.3262138905962, 94.10341118623234, 94.35694529809466, 93.99969268592501, 94.21481253841426, 94.41840811309157, 94.13414259373079, 94.23017824216349, 94.39920098340504, 94.10341118623234, 94.0381069452981, 94.36846957590657, 94.44529809465274, 94.29548248309773, 94.39920098340504, 94.2839582052858, 94.34542102028273, 94.51444376152428, 94.34926244622004, 94.1418254456054, 94.45682237246466, 94.5221266133989, 94.41072526121697, 94.45298094652735, 93.9958512599877, 94.24938537185002, 94.57974800245852, 94.1418254456054, 94.18792255685311, 94.34926244622004, 94.48371235402581, 94.39535955746773, 94.39151813153042, 94.39535955746773, 94.41072526121697, 94.1418254456054, 94.24554394591273, 94.39535955746773, 94.49907805777505, 94.3262138905962, 94.22633681622618, 94.34542102028273, 94.42224953902888, 94.27627535341118, 94.46450522433928, 94.1840811309158, 94.1917639827904, 94.50291948371235, 94.09956976029503, 94.27243392747388, 94.38383527965581, 94.40304240934235, 94.10725261216963, 94.36462814996926, 94.52980946527352, 94.36078672403197, 94.50676090964966, 94.1917639827904, 94.25706822372464, 94.31084818684695, 94.43761524277812, 94.36462814996926, 94.42224953902888, 94.21097111247695, 94.38383527965581, 94.18023970497849, 94.21865396435157, 94.5259680393362, 94.53365089121081, 94.21865396435157, 94.56822372464659, 94.21865396435157, 94.31468961278426, 94.41072526121697, 94.50291948371235, 94.35310387215735, 94.51060233558697, 94.26090964966195, 94.3338967424708, 94.4299323909035, 94.40688383527966, 94.66425937307929, 94.41072526121697, 94.39535955746773, 94.23786109403811, 94.44145666871543, 94.30316533497235, 94.3761524277812, 94.04578979717272, 94.36846957590657, 94.3300553165335, 94.44145666871543, 94.64505224339274, 93.99200983405039, 94.53365089121081, 94.52980946527352, 94.3799938537185, 94.38767670559312, 94.54133374308543, 94.36078672403197, 94.44913952059004, 94.28779963122311, 94.48755377996312, 94.51828518746159, 94.4721880762139, 94.45682237246466, 94.39920098340504, 94.51444376152428, 94.10341118623234, 94.61432083589429, 94.41840811309157, 94.21865396435157, 94.46450522433928, 94.44913952059004, 94.4798709280885, 94.61432083589429, 94.22633681622618, 94.63352796558083, 94.51060233558697, 94.44145666871543, 94.5221266133989, 94.50291948371235, 93.93438844499079, 94.4721880762139, 94.48371235402581, 94.63736939151813, 94.5759065765212, 94.61047940995698, 94.39920098340504, 94.56822372464659, 94.21481253841426, 94.59895513214505, 94.32237246465888, 94.59127228027043, 94.25322679778733, 94.4299323909035, 94.54517516902274, 94.3338967424708, 94.40688383527966, 94.08420405654579, 94.5259680393362, 94.59895513214505, 94.6681007990166, 94.61047940995698, 94.44529809465274, 94.48371235402581, 94.48755377996312, 94.67962507682851, 94.36846957590657, 94.09572833435772, 94.35694529809466, 94.43377381684081, 94.55669944683467, 94.61047940995698, 94.36462814996926, 94.6220036877689, 94.69499078057775, 94.39535955746773, 94.47602950215119, 94.43761524277812, 94.3300553165335, 94.68730792870313, 94.56822372464659, 94.45298094652735, 94.61816226183159, 94.40304240934235, 94.48371235402581, 94.62968653964352, 94.54517516902274, 94.51828518746159, 94.60663798401967, 94.55669944683467, 94.50676090964966, 94.52980946527352, 94.59511370620774, 94.46066379840197, 94.40304240934235, 94.49139520590043, 94.4299323909035, 94.34157959434542, 94.6681007990166, 94.58358942839583, 94.61047940995698, 94.34926244622004, 94.57974800245852, 94.50676090964966, 94.71035648432698, 94.70651505838967, 94.5221266133989, 94.3761524277812, 94.50291948371235, 94.59127228027043, 94.52980946527352, 94.41456668715428, 94.53749231714812, 94.29932390903504, 94.5720651505839, 94.68730792870313, 94.61816226183159, 94.55285802089736, 94.6681007990166, 94.70651505838967, 94.59127228027043, 94.40304240934235, 94.69114935464044, 94.68730792870313, 94.5221266133989, 94.45682237246466, 94.51828518746159, 94.57974800245852, 94.61432083589429, 94.6220036877689, 94.34542102028273, 94.53749231714812, 94.40688383527966, 94.49523663183774, 94.42609096496619, 94.6757836508912, 94.42224953902888, 94.43377381684081, 94.56438229870928, 94.65273509526736, 94.49139520590043, 94.4721880762139, 94.3338967424708, 94.56054087277197, 94.70651505838967, 94.45682237246466]\n"
          ]
        }
      ],
      "source": [
        "print(f\"train_loss_list_const = {train_loss_list}\") \n",
        "print(f\"train_acc_list_const = {train_acc_list}\")\n",
        "print(f\"test_loss_list_const = {test_loss_list}\")\n",
        "print(f\"test_acc_list_const = {test_acc_list}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ilmYYfIElcAU"
      },
      "outputs": [],
      "source": [
        "train_loss_list_01 = [2.3849518770770977, 2.2417600981911345, 2.2415069635644516, 2.2409557133186153, 2.2419579268147953, 2.240125378942102, 2.240931454066662, 2.2417839348800785, 2.242252948807507, 2.241471426273749, 2.241231945472035, 2.2413479971691843, 2.241036834432504, 2.2407813569717616, 2.2415969093963706]\n",
        "train_acc_list_01 = [18.56855479089465, 18.617257808364215, 18.746426680783483, 18.60243515087348, 18.61937533086289, 18.886183165696135, 18.60243515087348, 18.598200105876124, 18.604552673372154, 18.740074113287452, 18.731604023292746, 18.814187400741133, 18.848067760719957, 18.82901005823187, 18.752779248279513]\n",
        "test_loss_list_01 = [2.2387831538331273, 2.241503697984359, 2.241926829020182, 2.240557459055209, 2.2406100852816713, 2.252836311564726, 2.2468298743752873, 2.2446437150824305, 2.2425780202828203, 2.240177970306546, 2.243702617346072, 2.2441832843948815, 2.253918958645241, 2.245230858232461, 2.2435202879064224]\n",
        "test_acc_list_01 = [18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 14.27858020897357, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463]\n",
        "train_loss_list_001 = [2.2888378896687414, 2.232192633274771, 1.4893088792236193, 0.5001831288098643, 0.3830500937251218, 0.32867970416539405, 0.29770232389774426, 0.27895135900919354, 0.2627035691970732, 0.24814978033950336, 0.23438045485475198, 0.2243682525668364, 0.21301924496848731, 0.20818865677811266, 0.1958482328166322]\n",
        "train_acc_list_001 = [18.50926416093171, 18.968766543144522, 47.80307040762308, 84.23292747485442, 88.15881418740074, 89.9142403388036, 91.04923239809423, 91.63790365272631, 92.08893594494441, 92.58020116463737, 93.03758602435151, 93.36791953414505, 93.715193223928, 93.84012705134992, 94.35044997353097]\n",
        "test_loss_list_001 = [2.2451091665847627, 2.2302937355695986, 0.684039752711268, 0.4374444575286379, 0.39873581839834943, 0.36398082489476485, 0.3313831433507742, 0.3210300371854329, 0.2847569804346445, 0.29315854806233854, 0.27853756525791157, 0.26896954926789973, 0.2596830692069203, 0.26028463620619446, 0.2491153629460171]\n",
        "test_acc_list_001 = [18.88060848186847, 19.053472649047325, 78.20759065765212, 86.33988936693301, 87.90334972341734, 88.90596189305471, 90.08143822987093, 90.81515058389674, 91.70636140135218, 91.54118008604794, 91.8638598647818, 92.2480024585126, 92.43239090350338, 92.50537799631223, 92.82037492317149]\n",
        "train_loss_list_0001 = [1.8565612323885041, 0.5532636212785715, 0.4003713336094285, 0.3402645644860539, 0.309145948165639, 0.2848975209968523, 0.262982070708501, 0.24855145600026216, 0.2386487888167221, 0.2278864942390098, 0.21327269485164788, 0.20556094100683686, 0.19517452138549268, 0.18913837452608395, 0.18232752032436653]\n",
        "train_acc_list_0001 = [34.7993647432504, 82.35468501852831, 87.65272631021705, 89.51614610905241, 90.6723133933298, 91.51932239280042, 92.26892535733192, 92.59925886712546, 92.97617787188989, 93.34674430915828, 93.69401799894123, 93.97353096876654, 94.31868713605083, 94.53255690841715, 94.71254632080466]\n",
        "test_loss_list_0001 = [1.0861167063315709, 0.46979708385233787, 0.41340532643245714, 0.3364271931350231, 0.3262409484561752, 0.34040282589986043, 0.28803076817854945, 0.2942232322678262, 0.2779932311169949, 0.27514038454083833, 0.2478603608906269, 0.2512748020463714, 0.26197264781769586, 0.2462065773194327, 0.24985540344142446]\n",
        "test_acc_list_0001 = [64.00583896742471, 85.58312845728334, 87.41933005531654, 89.81637984019667, 89.970036877689, 89.99308543331284, 91.59111862323294, 91.35295021511985, 91.82928703134604, 92.03672403196066, 92.76275353411187, 92.83574062692071, 92.90488629379226, 92.98555623847572, 92.87031346035648]\n",
        "train_loss_list_00001 = [1.2440541174192092, 0.4583125376927497, 0.36052036624613815, 0.3151768069603256, 0.2875107263081119, 0.26835051384883196, 0.25187577066948097, 0.23390740957767336, 0.22118305101950317, 0.21190600004299545, 0.20555683115350845, 0.1952596850249018, 0.18612936185546683, 0.18178928815090883, 0.17480877608181986]\n",
        "train_acc_list_00001 = [57.18581259925887, 85.53943885653786, 88.79618845950239, 90.25304393859184, 91.214399152991, 92.01058761249338, 92.36421386977237, 93.09899417681312, 93.43991529910005, 93.80412916887242, 93.86765484383271, 94.25092641609317, 94.500794070937, 94.64478560084702, 94.82477501323451]\n",
        "test_loss_list_00001 = [0.5935118007017117, 0.43328142947718207, 0.3668157114994292, 0.34575528556517526, 0.3590214093964474, 0.3134572241893586, 0.2903973020467104, 0.28062032824199573, 0.26905518266208034, 0.26447016406146917, 0.27132851287138227, 0.26421160440818936, 0.25491809147391836, 0.24584030433028353, 0.2630316608895858]\n",
        "test_acc_list_00001 = [80.93500307314075, 86.63567916410571, 88.97510755992624, 89.3438844499078, 88.99431468961278, 90.58466502765826, 91.54502151198525, 91.56422864167179, 92.02135832821143, 92.25184388444991, 91.97910264290104, 92.37861094038107, 92.72433927473878, 92.96250768285188, 92.59757221880763]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# train_loss_list_001 = [1.5218167792493924, 1.0340943300305083, 0.7934107637633911, 0.6561045421959874, 0.5800367868936862, 0.5100019117132925, 0.4574989944982072, 0.4208847186245476, 0.3881891872079228, 0.35861467820006054, 0.3339165490084944, 0.3111862796849717, 0.2948872684575498, 0.27018505394363557, 0.25883166084940823]\n",
        "# train_acc_list_001 = [44.0975, 63.055, 72.2075, 77.0575, 79.93, 82.43, 84.0575, 85.47, 86.5625, 87.675, 88.48, 89.16, 89.645, 90.5075, 90.9225]\n",
        "# test_loss_list_001 = [1.3665137306044373, 1.0809951907471766, 0.8369579171832604, 0.7580914105041118, 0.6663558483123779, 0.7236429437806334, 0.5709026199352892, 0.5137608224832559, 0.5140901245648348, 0.48385591937016836, 0.47491426897954336, 0.5022718974306614, 0.5243912806993798, 0.42129093069064466, 0.4074777747634091]\n",
        "# test_acc_list_001 = [52.13, 63.83, 71.47, 73.82, 77.34, 76.49, 80.87, 82.32, 82.77, 83.6, 84.17, 83.3, 83.55, 86.28, 86.86]\n",
        "# train_loss_list_01 = [1.8962965864723864, 1.472022865146113, 1.2269341696184664, 1.0348994015885618, 0.9050399440165144, 0.7892040218027255, 0.6947498847120486, 0.6181783355272616, 0.5774767158892208, 0.5502305545936377, 0.5206893583456167, 0.5026034010104097, 0.4774538204311944, 0.4683271567471111, 0.4492297477710742]\n",
        "# train_acc_list_01 = [31.045, 45.415, 55.22, 62.9525, 67.66, 72.095, 75.7825, 78.6325, 80.1775, 80.8325, 82.04, 82.7775, 83.69, 83.865, 84.6225]\n",
        "# test_loss_list_01 = [1.6557437422909314, 1.5662637073782426, 1.1888765285286722, 1.1435910184172136, 0.9932384845576708, 0.7845515208908275, 0.7405012536652481, 0.7023888849004915, 0.6914112718799447, 0.8937227891970284, 0.6744754122027868, 0.7125071339969393, 0.595223272148567, 0.6645651912387414, 0.5624623864511901]\n",
        "# test_acc_list_01 = [38.54, 45.39, 57.61, 59.19, 65.26, 72.6, 74.56, 76.24, 76.7, 71.16, 77.34, 75.59, 80.06, 77.85, 81.28]\n",
        "# train_loss_list_0001 = [1.6901012103016766, 1.282657652045972, 1.0717586383652002, 0.940055356619838, 0.8364242675205389, 0.7547609810821545, 0.6785155514749094, 0.6320573160061821, 0.5852954303875518, 0.5420240767466755, 0.506621429809747, 0.47925190327647393, 0.4483506758563435, 0.4286465794800189, 0.40272510780122717]\n",
        "# train_acc_list_0001 = [37.2, 53.36, 61.4225, 66.4725, 70.2, 73.2275, 76.08, 77.8025, 79.53, 80.905, 82.42, 83.1425, 84.5575, 85.195, 86.005]\n",
        "# test_loss_list_0001 = [1.4665760239468346, 1.2375031430510026, 1.0847761902628066, 1.0535234567485279, 0.8635394460038294, 0.757294207434111, 0.7295623666877988, 0.7312412850464447, 0.7336276015148887, 0.6307676260984396, 0.6266382736495778, 0.6370392079594769, 0.5392829055273081, 0.5410988666588747, 0.5530912065053288]\n",
        "# test_acc_list_0001 = [46.98, 55.76, 61.4, 63.25, 69.13, 73.23, 74.32, 74.64, 73.96, 78.38, 78.25, 78.58, 81.0, 81.49, 81.32]\n",
        "\n",
        "train_loss_list_cut = [1.613359474907287, 1.186605121571416, 0.9613291156558564, 0.8175660327981455, 0.7329503829105974, 0.6675834229198127, 0.6155491586500844, 0.5636878716298186, 0.5260912411319562, 0.5027581219094249, 0.4726218120834698, 0.44723697268543916, 0.43208082353535554, 0.41164722000828946, 0.3974420052652542, 0.3826391176103403, 0.35894755928661115, 0.3507439031863746, 0.34107909901454425, 0.32801356564124173, 0.3124787288542373, 0.31465199137457645, 0.298767308029123, 0.28487236778766584, 0.27646335530966615, 0.27320462827103587, 0.26612033452184053, 0.26308306258993025, 0.2525483318411123, 0.24124891794146822, 0.24217433363389665, 0.23168825182004477, 0.23012469901730076, 0.21693348256162942, 0.21930161923074876, 0.21653421889669217, 0.21434410071125426, 0.20718723166579256, 0.2052558541250305, 0.19716152494041303, 0.1941497164983719, 0.19097941378339792, 0.18447992457939794, 0.18228636479701477, 0.17657887242948667, 0.17477260330043282, 0.1733962548212312, 0.17061091698825168, 0.16732201631219623, 0.1637549052556483, 0.1613706673700779, 0.16051728827075457, 0.15993363755389142, 0.15627589414770993, 0.15532852677158274, 0.15280420149858007, 0.15101519260353174, 0.14880275238341037, 0.13978494027742563, 0.14830744789002803, 0.14331960949463585, 0.13924681502409256, 0.13707038418601114, 0.1308897960062225, 0.13411890785581768, 0.13409768981627002, 0.13344513029217148, 0.1273199183205827, 0.1283481349103367, 0.12471981016924968, 0.1280518789260913, 0.12418704933394639, 0.12167530884139073, 0.12545658500430684, 0.11795977773234105, 0.11789289145423962, 0.12059933257202943, 0.11666790398355491, 0.11807365684558789, 0.11512304616931338, 0.11561917723082125, 0.11491954412323217, 0.11419005636661388, 0.10912806965624944, 0.11002509410198504, 0.11631911551466766, 0.10705401468129394, 0.1092875181331326, 0.10784903160942058, 0.10965043486069186, 0.10153051761511606, 0.1083350477579493, 0.10791226423467501, 0.10326221440665828, 0.10488084647149895, 0.100699621469925, 0.1017464189983595, 0.1026811115919782, 0.10191941613587327, 0.10146693645503384, 0.0997033108370944, 0.09700418662386961, 0.09839744385653221, 0.09619725261109706, 0.0990427272757307, 0.0953601595407096, 0.09479633424157342, 0.09373613773062588, 0.09360281742228486, 0.09450248972224161, 0.0905749333885531, 0.09483718122251499, 0.094004499824188, 0.09207623065731967, 0.0874123571696468, 0.08727556101073282, 0.09273509363444468, 0.08398670141106121, 0.08792692410964935, 0.08606394986732128, 0.08853992108648387, 0.08270674364302105, 0.08553103019539922, 0.08325290489799013, 0.08070171735109613, 0.08357380274028633, 0.0855067997409132, 0.07974059663760585, 0.07902837820208301, 0.07707989068862539, 0.08178071627101768, 0.07777025827025168, 0.0837722305065622, 0.07823536036232599, 0.07849429675731986, 0.08163950307229266, 0.0772132684176151, 0.0742708618630664, 0.07653025378839086, 0.07370652056361635, 0.07428970992767488, 0.066969591103637, 0.07278624822656377, 0.0711755252714022, 0.07434361697005007, 0.0714579665521606, 0.06873778960468194, 0.06797652607099317, 0.06636746319385763, 0.06754642966004035, 0.07100995908232448, 0.07219889727584757, 0.06691185421884631, 0.06648011294047768, 0.06790807842018125, 0.06884961091755583, 0.06649817791454994, 0.06520170254853015, 0.06067689375196116, 0.06284649541583685, 0.058577289441808726, 0.06126952098277859, 0.05914597977369357, 0.06332467258952487, 0.057384233588513474, 0.055878468872473455, 0.0626550735429691, 0.05836306477245241, 0.058223563505294985, 0.05526084442453358, 0.05660795328168633, 0.0533848489447238, 0.05570016410677863, 0.052253410154685806, 0.053411352781418224, 0.05460695667971913, 0.051183629400147417, 0.053861447934287425, 0.05485730031029152, 0.05130477134769146, 0.04791999825934014, 0.0483061204941128, 0.04967576256259895, 0.045770330684848676, 0.04519799896501028, 0.04884168341720161, 0.04494558678963742, 0.04718005882141689, 0.044031581079688506, 0.04476618649699865, 0.043659136812098494, 0.04084714540554145, 0.04083325983599399, 0.04264434854375026, 0.04191593292166297, 0.03981593097694004, 0.043085993813594785, 0.040142026240416705, 0.03974097740119353, 0.037398707843162474, 0.03773854231051268, 0.035977833013172256, 0.037191637455464936, 0.03843252851401631, 0.03596398131787991, 0.03615279636307123, 0.03480513299854038, 0.034845688106557623, 0.03309281391743273, 0.03143419835645075, 0.030938323252438643, 0.031188237175833397, 0.032819104747632485, 0.03191146987844437, 0.03148931583624702, 0.030030576116479815, 0.03224378252050843, 0.03121553240239787, 0.02776832960601063, 0.02880164314531528, 0.029192425741311222, 0.030026258470062107, 0.029695636324269085, 0.027793155418494687, 0.026486221334551828, 0.026275856554293976, 0.025177531725408646, 0.025483246410664278, 0.023590462400574986, 0.025644099438033356, 0.027068594341187146, 0.02301408553766771, 0.02522345676426047, 0.023035302803220865, 0.021253975071444418, 0.02299421675913869, 0.022151207279476424, 0.02280317421388119, 0.022020146825734655, 0.019744288622618865, 0.021754962591507946, 0.02058648679832431, 0.019481258098625193, 0.018746506298904102, 0.02082187346794223, 0.019484504032284973, 0.018540539403487676, 0.018026220258518744, 0.018277281947350635, 0.019090669363150937, 0.018498932709917426, 0.016855266360646357, 0.017087985687076854, 0.01755107378292555, 0.0162497674808287, 0.016687223960023624, 0.01735119057555918, 0.016141063884457057, 0.015963702109648276, 0.01607858874766638, 0.015593683267140184, 0.017362511261907843, 0.015086121411217502, 0.014080422949388076, 0.01572396905130198, 0.01647015061964409, 0.015150130807850569, 0.014522404045821688, 0.015314155997271045, 0.014406249093742798, 0.014474769565128028, 0.014898723138633151, 0.014346211947822056, 0.013767596740370836, 0.012941224452929376, 0.01388948843457376, 0.013728754877591856, 0.012941669198511817, 0.013923830428598122, 0.01301820403204475, 0.013493596776626622, 0.014167216677105608, 0.012224345361983505, 0.013615870727791479, 0.014401413207968916, 0.013055011124228137, 0.013154573026321495, 0.013493608308438295, 0.012507462098944587, 0.012766335765792492, 0.013181844661488367, 0.013105310207699982, 0.014206890060682409, 0.01374277858861005, 0.012588148265660475, 0.013434472534368141, 0.013608296886800576, 0.012433568900153517, 0.012731193249246266, 0.012850580789437429]\n",
        "train_acc_list_cut = [40.9, 57.2025, 65.81, 71.35, 74.33, 76.565, 78.5275, 80.4, 81.7075, 82.5125, 83.585, 84.51, 85.1225, 85.4725, 85.9375, 86.5425, 87.3725, 87.685, 88.0775, 88.5075, 89.195, 88.7475, 89.465, 89.89, 90.23, 90.4575, 90.6025, 90.64, 91.0475, 91.385, 91.485, 91.8375, 91.855, 92.455, 92.2575, 92.455, 92.4025, 92.6625, 92.92, 93.085, 93.19, 93.35, 93.635, 93.515, 93.77, 93.805, 93.8825, 93.9425, 94.15, 94.27, 94.3925, 94.3325, 94.4625, 94.545, 94.6175, 94.6675, 94.66, 94.7825, 95.1875, 94.845, 94.9125, 95.1, 95.2425, 95.45, 95.28, 95.2525, 95.3375, 95.6875, 95.58, 95.74, 95.56, 95.7275, 95.7375, 95.7325, 95.96, 96.0225, 95.8975, 95.9725, 95.9975, 96.035, 95.935, 96.0, 96.1025, 96.3225, 96.3075, 96.0575, 96.32, 96.3125, 96.3075, 96.26, 96.575, 96.3125, 96.3875, 96.495, 96.4125, 96.585, 96.49, 96.455, 96.4925, 96.4725, 96.6375, 96.6125, 96.715, 96.775, 96.675, 96.7725, 96.7825, 96.7825, 96.775, 96.835, 96.9425, 96.72, 96.7375, 96.8475, 97.035, 97.0625, 96.845, 97.2175, 97.06, 97.1575, 97.055, 97.2325, 97.125, 97.2375, 97.315, 97.2125, 97.1925, 97.2775, 97.3325, 97.45, 97.25, 97.395, 97.23, 97.3725, 97.355, 97.2325, 97.4625, 97.4875, 97.4075, 97.545, 97.5525, 97.7425, 97.545, 97.6125, 97.505, 97.635, 97.7125, 97.72, 97.8225, 97.78, 97.6175, 97.5975, 97.86, 97.7825, 97.7725, 97.7525, 97.8075, 97.7875, 97.9775, 97.96, 98.1425, 97.9675, 98.0225, 97.88, 98.1, 98.2375, 97.9275, 97.995, 98.0375, 98.2175, 98.12, 98.26, 98.185, 98.2625, 98.275, 98.23, 98.285, 98.2475, 98.195, 98.36, 98.48, 98.4025, 98.315, 98.4725, 98.5175, 98.4075, 98.5175, 98.46, 98.535, 98.48, 98.6225, 98.6675, 98.72, 98.5975, 98.6575, 98.68, 98.525, 98.7025, 98.715, 98.78, 98.8, 98.845, 98.7975, 98.75, 98.835, 98.8225, 98.8975, 98.9, 98.925, 99.045, 98.995, 99.0175, 99.0025, 98.97, 98.9425, 99.075, 98.945, 98.9975, 99.07, 99.0825, 99.0525, 99.0625, 99.0075, 99.085, 99.2075, 99.19, 99.2475, 99.1775, 99.265, 99.1775, 99.1425, 99.255, 99.21, 99.215, 99.34, 99.245, 99.29, 99.28, 99.3, 99.3725, 99.3175, 99.3475, 99.4, 99.41, 99.2975, 99.4, 99.44, 99.45, 99.4325, 99.425, 99.4025, 99.4775, 99.44, 99.4275, 99.495, 99.4925, 99.4575, 99.485, 99.5075, 99.5, 99.5075, 99.425, 99.535, 99.5825, 99.4725, 99.5025, 99.5375, 99.545, 99.53, 99.58, 99.5525, 99.5425, 99.5775, 99.58, 99.635, 99.605, 99.59, 99.605, 99.6075, 99.65, 99.59, 99.58, 99.6125, 99.575, 99.5775, 99.59, 99.59, 99.585, 99.6075, 99.5975, 99.61, 99.635, 99.5625, 99.6, 99.6175, 99.5925, 99.6075, 99.6225, 99.5925, 99.615]\n",
        "test_loss_list_cut = [1.4791154242769073, 1.1658613644068754, 0.9872214477273482, 0.8314349047745331, 0.9944802085055581, 0.7577941908112055, 0.7259182379215579, 0.6789853301229356, 0.7040932129455518, 0.6710618870167793, 0.5932643009891992, 0.7190747709968422, 0.5915243693544895, 0.5677886009216309, 0.5548223072214972, 0.48938161738311187, 0.5485320487354375, 0.5049594323846358, 0.5024378439293632, 0.5253199633163742, 0.545006987037538, 0.49844539429568036, 0.4618456284456615, 0.47183522431156305, 0.45087659849396233, 0.5046738835075234, 0.4737275925618184, 0.517040255326259, 0.47144785334792316, 0.43287935023066365, 0.4623851370585116, 0.4191177538301371, 0.40598977762687055, 0.39758752437332007, 0.429098657414883, 0.49839673049842254, 0.4335803312214115, 0.4391304107406471, 0.4258851580604722, 0.4272765200349349, 0.41979367031326775, 0.3996517728023891, 0.39644326382799994, 0.4050762924966933, 0.37220431883123856, 0.41266086810751806, 0.4058349334363696, 0.40598829731911046, 0.4262330528301529, 0.4142030260608166, 0.35964680000951016, 0.3874821408262736, 0.3994222947313816, 0.4099999762411359, 0.48654232843767237, 0.40172827545600603, 0.3953292958344085, 0.4276507317642622, 0.37284702835958217, 0.3759660202113888, 0.39387373701681067, 0.3743562951118131, 0.3970871818593786, 0.41597020795828177, 0.3505105532800095, 0.3577760239190693, 0.3745738009486017, 0.36575221789034107, 0.3922628566056867, 0.3804579684628716, 0.3428110468991195, 0.38194986197012887, 0.35429555196550827, 0.361780740603616, 0.3707258914467655, 0.3518540302786646, 0.3650943885875654, 0.3918004062356828, 0.3596177604756778, 0.37847324338140365, 0.40541056722779817, 0.39005918272688417, 0.3564605245107337, 0.36085661019705517, 0.41434526443481445, 0.3446986880860751, 0.3807661754043796, 0.3560769367444364, 0.3401542149389846, 0.33212360739707947, 0.34799107928064804, 0.40249927549422543, 0.3623131317428396, 0.38307136523572705, 0.3425910604905479, 0.3611237068153635, 0.37625472587120684, 0.3761880048845388, 0.3633357567500465, 0.32559320141997516, 0.3548705181743525, 0.37774981407425073, 0.33313915548445305, 0.3662335353938839, 0.34161262233046036, 0.33970868606356125, 0.33961755310810066, 0.35591573598264137, 0.36459438993206505, 0.34952369747282586, 0.3646538981908484, 0.3669853182155875, 0.346222849586342, 0.3648104143293598, 0.35329970227012153, 0.3857896882521955, 0.3397268560491031, 0.3996561256390584, 0.344681443670128, 0.35413239726537393, 0.3316430355174632, 0.38716104249410993, 0.32942197028594683, 0.35217226400405544, 0.37594665285152723, 0.34076649818239335, 0.3449836048898818, 0.3587528810470919, 0.3401296163284326, 0.3270592946983591, 0.32152744599535493, 0.3485121142260636, 0.33158980649483355, 0.35384547361467455, 0.3337541961971718, 0.31837486522861674, 0.32609030859002586, 0.35300223284129856, 0.31153832630643363, 0.331745189390605, 0.31228505301324627, 0.32915962798685966, 0.307383563322357, 0.3250211654584619, 0.32271280979053885, 0.3312963891444327, 0.3233849070494688, 0.3239633294789097, 0.3277184535996823, 0.3104742332538472, 0.32017407990709135, 0.32736679232573207, 0.31899695456782473, 0.29834681513566, 0.3152913172033769, 0.3275818183452268, 0.33088456924203075, 0.3209677238630343, 0.32485942636864096, 0.3357626995708369, 0.3363142457755306, 0.3152533290506918, 0.3161260941171948, 0.3332944193595572, 0.3038376487697227, 0.34032498092591007, 0.31091189195838154, 0.31926076189626623, 0.31000442501110365, 0.3010354260855083, 0.30018055627617657, 0.3127445231510114, 0.30238260064698474, 0.3182603884724122, 0.31473101213385785, 0.29884613908921615, 0.31703367735011667, 0.2913209448886823, 0.2881702622280845, 0.31681924888604807, 0.29637292385855807, 0.2983416555614411, 0.29558402111258686, 0.29196765764227395, 0.3100640736048735, 0.31831720773177813, 0.29677640637264974, 0.30746520772764957, 0.2865563590503946, 0.30253688547807406, 0.28961686959749533, 0.29738174973032144, 0.29680030708071553, 0.29409151684634294, 0.2892140953601161, 0.3005977722851536, 0.2880074861872045, 0.27800113717211955, 0.2859659475993507, 0.2899065754270252, 0.28989569729642023, 0.29461942848902717, 0.27184448862754845, 0.2677759771482854, 0.2699777761214896, 0.3000830202540265, 0.2926526506301723, 0.2815253124395503, 0.2683991437094121, 0.2708052956982504, 0.2659032362737233, 0.29111536092396023, 0.27172746575331386, 0.2721299055633666, 0.27334422420097304, 0.27823255380874945, 0.2846702391022368, 0.26914313680763485, 0.2824036598299878, 0.26155152982926066, 0.28833567049307157, 0.2757935132595557, 0.28530820940114276, 0.26061007040965406, 0.2689024790932861, 0.2690693513502049, 0.2623352914859977, 0.26749783254499676, 0.2726406627629377, 0.2707079539570627, 0.2750299688947352, 0.2752766144237941, 0.26105580568502224, 0.2643242926348614, 0.2672247627302061, 0.27170711321921287, 0.28075962617427486, 0.25020905593528026, 0.24921881190583675, 0.26484523855055436, 0.2614167183637619, 0.27086676525164255, 0.2653467998474459, 0.25267240150442605, 0.2534334840653818, 0.26007933452536786, 0.2619169997640803, 0.2546714186857018, 0.2537678420732293, 0.27184398217669015, 0.24711663049610355, 0.26054487415129624, 0.2526520066246202, 0.2480248349565494, 0.24464330218638045, 0.2543100225208681, 0.2467851509587674, 0.2543815068806274, 0.2550846805112271, 0.24686935845809646, 0.25780645163753363, 0.24647484842357756, 0.2498779943849467, 0.24236657725104802, 0.25234464791756644, 0.23720906674861908, 0.2444623167378993, 0.24947976245533063, 0.24974906840656377, 0.24106483623574052, 0.24051216914306714, 0.24444433122496062, 0.240749250955974, 0.24741494844231424, 0.24492908486082585, 0.24239608881217015, 0.24886162715810764, 0.24737394035239763, 0.24470583080679556, 0.23975468323200563, 0.25973454749659647, 0.24880992423129988, 0.25671024592239644, 0.2394170629072793, 0.2508298989526833, 0.25579182813061946, 0.23148325047915494, 0.2473917234736153, 0.23667787901962858, 0.24764571838741062, 0.23395111053427564, 0.23682133947746664, 0.2453216223777095, 0.2395489558200293, 0.24663213140602352, 0.2379017510934721, 0.24195825148232375, 0.23623901610321638, 0.24765730121090443, 0.24364126761314236]\n",
        "test_acc_list_cut = [47.45, 60.0, 65.86, 71.15, 68.02, 74.04, 74.75, 76.5, 76.38, 77.35, 79.87, 75.79, 80.02, 80.57, 81.27, 83.4, 81.97, 82.87, 83.07, 82.72, 81.44, 83.43, 84.51, 84.41, 84.85, 83.96, 84.21, 82.61, 85.09, 85.42, 85.35, 86.23, 86.96, 86.92, 85.98, 84.21, 85.86, 85.85, 85.91, 86.93, 86.74, 86.97, 87.32, 87.19, 87.67, 86.55, 86.81, 87.37, 86.84, 87.3, 88.46, 87.92, 87.66, 87.19, 85.59, 87.67, 87.69, 86.81, 88.24, 88.24, 87.26, 88.01, 87.61, 87.88, 88.96, 88.76, 88.97, 88.92, 87.96, 88.36, 88.67, 87.88, 88.96, 88.67, 88.46, 88.99, 88.87, 88.21, 88.92, 88.53, 88.17, 88.26, 89.01, 88.57, 87.5, 89.68, 88.57, 88.93, 89.45, 89.58, 89.15, 88.23, 88.64, 88.35, 89.49, 89.34, 89.23, 88.47, 88.78, 89.82, 89.47, 89.04, 89.77, 88.77, 89.85, 89.69, 89.79, 89.18, 89.03, 89.57, 89.04, 89.3, 89.43, 89.17, 89.25, 88.69, 89.73, 88.08, 89.49, 89.59, 90.02, 88.41, 89.94, 89.77, 88.87, 89.91, 89.6, 89.29, 90.01, 90.18, 90.39, 89.52, 90.2, 89.82, 89.98, 90.55, 90.27, 89.84, 90.81, 90.16, 90.58, 90.17, 91.06, 90.49, 90.02, 90.36, 90.15, 90.41, 90.29, 91.05, 90.46, 90.37, 90.3, 91.15, 90.77, 90.49, 90.34, 90.44, 90.56, 90.45, 90.12, 90.36, 91.18, 90.31, 91.04, 90.24, 90.89, 90.56, 91.21, 91.14, 90.88, 90.8, 90.98, 90.69, 90.8, 91.24, 90.76, 91.31, 91.56, 90.68, 90.91, 91.29, 91.61, 91.43, 91.23, 90.98, 91.61, 91.17, 91.7, 91.2, 92.06, 91.39, 91.26, 91.56, 91.42, 91.42, 91.78, 91.94, 91.69, 91.44, 92.0, 91.36, 91.88, 92.07, 92.17, 91.33, 91.74, 91.8, 92.13, 92.09, 92.31, 91.55, 92.01, 92.26, 92.4, 91.8, 91.83, 92.3, 92.02, 92.56, 92.16, 92.24, 91.79, 92.59, 92.35, 92.7, 92.71, 92.45, 92.55, 92.34, 92.02, 92.56, 92.65, 92.35, 92.53, 92.23, 92.47, 92.65, 92.84, 92.75, 92.85, 92.58, 92.62, 92.95, 92.94, 92.61, 92.87, 92.93, 92.82, 92.37, 92.94, 92.82, 92.81, 93.03, 93.0, 92.85, 93.13, 92.88, 92.75, 93.02, 92.82, 93.14, 92.91, 93.07, 92.55, 93.2, 93.19, 93.02, 93.01, 93.03, 93.35, 93.0, 93.14, 92.89, 93.14, 93.2, 93.12, 93.15, 93.17, 93.22, 93.14, 93.1, 93.0, 93.72, 92.96, 92.94, 93.39, 92.96, 93.36, 93.22, 93.51, 93.28, 93.27, 93.3, 93.29, 93.18, 93.24, 93.64, 93.27, 93.29]\n",
        "train_loss_list_wd5e4 = [1.5027774480965952, 1.0276084239490497, 0.7975675012356938, 0.6686166104988549, 0.5772798815474343, 0.5116038408142309, 0.461431055594557, 0.4285996074493701, 0.3872021212459753, 0.36954540337998265, 0.33349453384122146, 0.3191225348760526, 0.29570154136362164, 0.2778870724736692, 0.26335706169041584, 0.2490621807809455, 0.23208605314786443, 0.22031858939522753, 0.20280856089279675, 0.1948807320465295, 0.18225760984058959, 0.17901248411058238, 0.16944760515000493, 0.15725475339034495, 0.15281064094255525, 0.14829571347552747, 0.1351967308396539, 0.1339439639982324, 0.12818257354747373, 0.1161808082328056, 0.11971007128612111, 0.11157218916728474, 0.11184166256969158, 0.1071621371736637, 0.09870970028396041, 0.09892762057221355, 0.0908012348063552, 0.08706318784933596, 0.08129636609492401, 0.0844175164608624, 0.07711375228608378, 0.0727171496318552, 0.07881306375439365, 0.06988748621207456, 0.06977271015187517, 0.07213704411785443, 0.06465452981178467, 0.059860886116259206, 0.06565443985164165, 0.055741984914904966, 0.05937281503273656, 0.054797398726851604, 0.04783525239057339, 0.057667528927778475, 0.0519976342906253, 0.05075258484479195, 0.0479780873997857, 0.051437813953047216, 0.044275228567897514, 0.05218542266202668, 0.047849885717677045, 0.04727344876809861, 0.04353566757275369, 0.05274154159564751, 0.03900954714669778, 0.038132579750217756, 0.0438798694252468, 0.04052391117319655, 0.0444194049285814, 0.03921739171041896, 0.04322019030456059, 0.03981477512677495, 0.04147126957232627, 0.03451747147622295, 0.03873391680645581, 0.03558630044438159, 0.03961444919863448, 0.03973888016243379, 0.03753592373844915, 0.03485113727642943, 0.036234460864811184, 0.03558692057875875, 0.03258687717030747, 0.03053688010224662, 0.03501797043614256, 0.02845712277083137, 0.03135093330991821, 0.03356501266074638, 0.03350056209170019, 0.03376623985473626, 0.0308699009333032, 0.02893257925968272, 0.027391545217150984, 0.0286959023021471, 0.030725791665781942, 0.026020749267517997, 0.03082717279829204, 0.02599965874775173, 0.03394242989184995, 0.03584211132750986, 0.032864981473753815, 0.03261832850271116, 0.03152383655345383, 0.028458409716550725, 0.0237263294655425, 0.02717581400335335, 0.028603503928362084, 0.02477863742285572, 0.026328867512603348, 0.02738554204796283, 0.024270188131818946, 0.02639033293170027, 0.028431153394275915, 0.0267944870994221, 0.022552848333112014, 0.027474652415665147, 0.022996818839450376, 0.02170721744931044, 0.024480172923168005, 0.025365150559651918, 0.02576148252154644, 0.024583259466499946, 0.026209578144188506, 0.022519710091368172, 0.022190634743682446, 0.017703767737207082, 0.019398964044396966, 0.02643031852926833, 0.02380849324953787, 0.02303797679733092, 0.015461873901173318, 0.016369462731820994, 0.023921750444984333, 0.024145581647188375, 0.024439221988923062, 0.021081448698151536, 0.019269814025595806, 0.01770727836546271, 0.015750449192457307, 0.018357921256341586, 0.016851160442084668, 0.016779405207331545, 0.01922887265860749, 0.01491048913590384, 0.017494613310685175, 0.020450935306655357, 0.019253915843045036, 0.01861391122789143, 0.017561238870908397, 0.01683490170342937, 0.012796001498417827, 0.016780197644567552, 0.014076976357370067, 0.0157260496032457, 0.016004126569547784, 0.017361198594353307, 0.01750588047760018, 0.014100992143118439, 0.012314881729229857, 0.008250951137283621, 0.013473853790245878, 0.010151084203766985, 0.009842516266005918, 0.010620841719269657, 0.009969146477769667, 0.011355855877097613, 0.007406337388096372, 0.008986727931512931, 0.009986997959159387, 0.009125781858848712, 0.007472109663621567, 0.008835015864224551, 0.004292508458737998, 0.005812553818874692, 0.00677099364538924, 0.0067322269050826946, 0.004625315853274061, 0.005215383676132158, 0.006344635733596671, 0.005187604376992669, 0.003839952308653345, 0.003965519353213271, 0.002669242659610467, 0.002117635275329502, 0.002775355486739308, 0.00265736014845546, 0.0022718249013076145, 0.0020042899910497447, 0.0016117119838441976, 0.0014573458997613063, 0.001228732236187917, 0.0012267742422409356, 0.0014500122589130586, 0.0014915024765362493, 0.0012808567260394986, 0.0011165388441382767, 0.0012906825464864531, 0.001357362313477245, 0.0012554813951131897, 0.0011106275753623928, 0.0011766462907799112, 0.0011652772541354281, 0.0014127067940387006, 0.0012243740158691145, 0.0011723753179798421, 0.0012344509843341149, 0.0012428820435963451, 0.0011997493186641259, 0.001336276902967451, 0.0011452813391029217, 0.0011089205116661378, 0.001165861148571673, 0.0012621701129313452, 0.0011624022613475902, 0.0012330106737020725, 0.0011809163343030425, 0.001101714661181235, 0.0011684441624340205, 0.0011685377142329615, 0.0012322912058129478, 0.0011872104088293787, 0.001143343054307119, 0.0011744529526597394, 0.001203257890125088, 0.00115107328663714, 0.0011390011661653273, 0.0012292457904782706, 0.0011242386569215443, 0.0011883258320072802, 0.0011454876748467097, 0.001156560716423066, 0.0011528850767014627, 0.001165562277152456, 0.0011236844305395365, 0.0011447015948914967, 0.0012182490931773268, 0.0011218772364628559, 0.001162500955319157, 0.0011594401151403047, 0.001198594366161587, 0.0011824657069370388, 0.001172395464897263, 0.0012272381267998333, 0.001153445542647173, 0.0012023703809961462, 0.0011459320135334262, 0.0012018852982485589, 0.0011949562754023809, 0.0011886567682230149, 0.0011700176551697043, 0.0011815206049539792, 0.0011973182879798947, 0.0012122210003242206, 0.0011632622707971392, 0.0011874910569734894, 0.0011854940763534828, 0.0011964635884451765, 0.0011862511147771734, 0.001194577806401617, 0.0012562638589407142, 0.0012219592303293534, 0.0011857493995159221, 0.0011942158280217204, 0.0011750017043601828, 0.001190489213080547, 0.0011951297786780082, 0.0011906062601321635, 0.0011730845402048442, 0.0012332784984939206, 0.0012139175384760664, 0.0011773592329021698, 0.0012054313082075394, 0.0011924652807851926, 0.0012579563214404942, 0.001199958441122926, 0.0012173140406632386, 0.0012009810876122083, 0.001210526931255806, 0.0011779946152689143, 0.0011719051533164427, 0.001181355180083432, 0.0011896568855580191, 0.0012324937115670345, 0.0011941396765940534, 0.0011866521274376386, 0.0011868935034494287, 0.0011929624456207093, 0.0012042453618922506, 0.001203572630105665, 0.001169494700063163, 0.0012129276695946893, 0.0012054597970452337, 0.0012071873559750402, 0.0011781047958621797, 0.0011885599540055584, 0.0011778895794518126, 0.0011938168243624079, 0.0011808463227607833, 0.0012280858834437763, 0.001201700613523027]\n",
        "train_acc_list_wd5e4 = [44.9675, 63.1875, 71.8725, 76.5, 79.885, 82.1125, 83.93, 85.2275, 86.5525, 87.015, 88.4975, 88.8775, 89.675, 90.22, 90.8925, 91.28, 91.83, 92.26, 92.8925, 93.18, 93.5725, 93.6975, 94.0725, 94.48, 94.7275, 94.8025, 95.185, 95.255, 95.445, 96.0025, 95.7275, 96.06, 96.1225, 96.2175, 96.5175, 96.4475, 96.815, 96.9275, 97.2, 97.03, 97.3525, 97.5325, 97.2775, 97.6, 97.6325, 97.4725, 97.8175, 97.9575, 97.6775, 98.13, 97.9825, 98.115, 98.465, 98.025, 98.2125, 98.315, 98.395, 98.265, 98.535, 98.2375, 98.3425, 98.4175, 98.59, 98.165, 98.775, 98.7575, 98.4825, 98.6875, 98.425, 98.76, 98.555, 98.6875, 98.565, 98.86, 98.715, 98.8725, 98.665, 98.645, 98.765, 98.8425, 98.845, 98.8475, 98.95, 99.0225, 98.8175, 99.0825, 98.97, 98.91, 98.885, 98.87, 98.975, 99.0475, 99.1425, 99.065, 99.0175, 99.1325, 98.9975, 99.205, 98.9175, 98.85, 98.9625, 98.9, 99.02, 99.0675, 99.295, 99.1675, 99.08, 99.225, 99.1425, 99.1, 99.2525, 99.155, 99.0975, 99.165, 99.3, 99.1275, 99.3225, 99.3125, 99.245, 99.1725, 99.1575, 99.2175, 99.1825, 99.32, 99.29, 99.48, 99.44, 99.17, 99.25, 99.32, 99.5675, 99.5, 99.275, 99.2075, 99.2025, 99.3425, 99.4, 99.5125, 99.5125, 99.495, 99.4975, 99.4925, 99.4125, 99.565, 99.465, 99.365, 99.4025, 99.4375, 99.445, 99.4825, 99.645, 99.4825, 99.62, 99.5275, 99.53, 99.43, 99.485, 99.61, 99.6675, 99.785, 99.5775, 99.705, 99.725, 99.6975, 99.69, 99.705, 99.8125, 99.765, 99.725, 99.77, 99.81, 99.785, 99.92, 99.8575, 99.8125, 99.82, 99.9025, 99.885, 99.84, 99.88, 99.935, 99.93, 99.965, 99.9775, 99.9475, 99.9625, 99.955, 99.98, 99.9875, 99.9925, 100.0, 99.995, 99.985, 99.9925, 99.9975, 100.0, 99.99, 99.9925, 99.9975, 99.9975, 99.995, 99.9925, 99.9925, 99.995, 99.9975, 99.9975, 99.9975, 99.9975, 99.99, 100.0, 100.0, 100.0, 99.9925, 100.0, 99.995, 100.0, 100.0, 99.9975, 99.9975, 99.995, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 99.9975, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 99.9975, 100.0, 99.9975, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
        "test_loss_list_wd5e4 = [1.2136726432208773, 0.9851384208172183, 0.8816822442827346, 0.9342003608051734, 0.7205261165582681, 0.7135021192363545, 0.6295640034766137, 0.6222952057289172, 0.5453219817409033, 0.6333636732041081, 0.5167707955535454, 0.5517654562298255, 0.4426909998247895, 0.41966562520099593, 0.5765619779689403, 0.43953118633620347, 0.3954082934916774, 0.399195172552821, 0.3910487050874324, 0.38879678803908674, 0.4299736600133437, 0.38120186121403415, 0.4164479757788815, 0.3728801941947092, 0.42476696719097184, 0.395566382551495, 0.39108878091166294, 0.390310141670553, 0.4194243029703068, 0.3851487408333187, 0.43473647101015983, 0.3949222198770016, 0.43027733530424817, 0.46044455280032337, 0.4314384918801392, 0.402132544525062, 0.4030761315098292, 0.4495672079958493, 0.39670435857923725, 0.3508086872251728, 0.43815861830982983, 0.4294034951849829, 0.44646918698202204, 0.4012679084192348, 0.3856019262648836, 0.38729274404954306, 0.41598545373240603, 0.4074918993666202, 0.3850251690873617, 0.36644049479237084, 0.4081271881166893, 0.3885994453596163, 0.3874774380952497, 0.38053672147702566, 0.46447082222262515, 0.3927165820628782, 0.38685785233974457, 0.3861157114181337, 0.37443527598169785, 0.3745674545629115, 0.42922606705864774, 0.3947119893906992, 0.4066375641128685, 0.4015842970413498, 0.36048868430566183, 0.35823154091080534, 0.36034491677072983, 0.3813755610321142, 0.35023685667333726, 0.3729325658347033, 0.3793654409767706, 0.3758758772400361, 0.386005234491976, 0.3741716125720664, 0.38768857264820533, 0.4233778007800066, 0.3564016471935224, 0.37897610098500795, 0.40855275943309444, 0.37117944054211244, 0.4877219747138929, 0.32491220892230166, 0.3434305104273784, 0.3840078609653666, 0.3549162511584125, 0.3900517902419537, 0.4008802462227737, 0.37963688882845864, 0.41286554225260697, 0.3738790906116932, 0.39308812431519546, 0.40336327028425434, 0.3903807069681868, 0.3731421724527697, 0.36139429652992683, 0.36114918450011485, 0.3433755166545699, 0.33576442904864684, 0.38036759027951883, 0.389648184368882, 0.3580984584515608, 0.38816530191445653, 0.35682666716696343, 0.3467950366343124, 0.3625389872283875, 0.35996044842125496, 0.3454359777366059, 0.3356767878690852, 0.37700900737243365, 0.35454515490350846, 0.33726465683194656, 0.37725700768111625, 0.37593507747861404, 0.3295244945189621, 0.3762158106399488, 0.38346752543238144, 0.3287717970677569, 0.3532322215128548, 0.3622320907402642, 0.3615872695853439, 0.3733052865047998, 0.33632627308745927, 0.3659961585757099, 0.34503319652020176, 0.3413062731299219, 0.3364775993778736, 0.3670855132462103, 0.34390894257569615, 0.35701036585282675, 0.33068783996225914, 0.3194702649606934, 0.35311298242098166, 0.35698866938488394, 0.35237887407405466, 0.38091013099573834, 0.3747274860551086, 0.3419938689168495, 0.3380954069427297, 0.36431473842527295, 0.37605810872738876, 0.3960524099537089, 0.33081852295730685, 0.36062877370586877, 0.3480613913528527, 0.37270384611962715, 0.34597224412085137, 0.3434583605090274, 0.34766560349660586, 0.3527573428199261, 0.3291342975218085, 0.3357704000759728, 0.3537662517798098, 0.35035382720488534, 0.3609619809291031, 0.36449365525306027, 0.38716287873213806, 0.36449907418293287, 0.3543851946162272, 0.3296925408553474, 0.33976211521444444, 0.36568557687952546, 0.3350432302944268, 0.35040959508358677, 0.33734601575739775, 0.35390251950372625, 0.325316196189651, 0.3150268595995782, 0.34807058489775355, 0.34181052182294147, 0.30128647698254524, 0.3265819955098478, 0.3273799891901922, 0.3255926795964, 0.3264697471374198, 0.31853930699297145, 0.3249078211905081, 0.30419793300613573, 0.31979161029375053, 0.33804947467921653, 0.30469325590360014, 0.32201941085012653, 0.32308190551739707, 0.3179449903531165, 0.29340258193544194, 0.30763082790978347, 0.3099403906660744, 0.3032598841133751, 0.30206969182325316, 0.28878650833157044, 0.298862298172486, 0.2903934564394287, 0.2809895239298857, 0.2981501730936992, 0.2927116846925096, 0.2824138938626157, 0.28358515229406234, 0.2825708462844921, 0.29461616579490374, 0.2836763092988654, 0.28226735923863666, 0.2746177826878391, 0.27155414657502236, 0.2885775946174996, 0.27818745195488387, 0.27747765494675575, 0.28425973069064225, 0.27348717754777474, 0.27310062048933176, 0.2786915644626074, 0.2682647790429713, 0.27626944993492925, 0.28479830703780623, 0.27027927178748046, 0.2733822947344448, 0.2701373880233946, 0.2778020071549506, 0.2666197860542732, 0.26562596329405336, 0.2745143292070944, 0.27233143329997606, 0.26666624231051794, 0.2696480587407758, 0.2745003360736219, 0.27249939272863954, 0.27024260172738307, 0.2686373513144783, 0.27092717569085617, 0.2656354494392872, 0.26311222001721585, 0.26525769873133187, 0.26492436928085134, 0.2699424793731563, 0.2719281587419631, 0.27291557249389115, 0.2715623058095763, 0.26247314769255964, 0.25891310165200054, 0.27840734131728545, 0.27305792734215534, 0.2638986749645275, 0.26656815414375895, 0.2615214324544502, 0.2578285336494446, 0.27850422602665575, 0.2716222282829164, 0.2712610442238518, 0.2711411705594274, 0.26657556308598457, 0.265236472498767, 0.2611617881663238, 0.27041642509306535, 0.25506745703235456, 0.2576812288806408, 0.2628553329577929, 0.2742771508195732, 0.26676151354478883, 0.26692218276896057, 0.27390673850910574, 0.2696215535360801, 0.2623596463022353, 0.2678805201585534, 0.2617303827140905, 0.2673761573254685, 0.2679485789016832, 0.26076228487529335, 0.26153764751138564, 0.2555880120283441, 0.2642273437939113, 0.25984900718248344, 0.26822723297378687, 0.2618812537645992, 0.26794138192376005, 0.2702119716832155, 0.27756816747633717, 0.2612695039450368, 0.2614873691072947, 0.2663590583148636, 0.25799272407459306, 0.2680966036417816, 0.27000968761836425, 0.2693696271015119, 0.25710588410685337, 0.26840231454447855, 0.2647050721924516, 0.2639507615019249, 0.27469288246541085, 0.2502636852898175, 0.26998129741677757, 0.2636978205623506, 0.2510043778279914, 0.26851412955718706, 0.2697306358833102, 0.26649072681423985, 0.2556019111336032, 0.2660581706256806, 0.27122338370809074, 0.2661679719067827, 0.2716638084170939, 0.2666107994280284, 0.25817724770005746]\n",
        "test_acc_list_wd5e4 = [57.49, 65.32, 68.85, 69.25, 75.11, 76.47, 79.04, 78.91, 81.38, 79.78, 83.31, 82.27, 85.39, 86.04, 81.19, 85.57, 86.87, 86.8, 87.39, 87.89, 86.55, 87.98, 87.07, 88.42, 87.53, 87.54, 88.73, 88.01, 87.66, 88.41, 87.62, 88.47, 87.7, 87.31, 87.93, 88.33, 88.35, 87.92, 88.93, 89.82, 88.12, 87.97, 88.15, 89.02, 89.51, 89.51, 88.97, 88.91, 89.73, 90.22, 89.19, 89.69, 89.73, 89.78, 87.74, 89.45, 89.75, 89.88, 89.95, 90.06, 88.65, 89.62, 89.26, 89.31, 90.25, 90.69, 90.2, 90.24, 90.68, 90.65, 90.07, 89.97, 90.12, 90.12, 89.86, 88.49, 90.59, 90.09, 89.83, 90.61, 87.36, 91.49, 90.93, 90.05, 90.69, 89.85, 89.84, 90.3, 89.98, 90.2, 90.1, 89.68, 89.81, 90.13, 90.46, 90.27, 91.08, 91.19, 90.42, 89.93, 90.76, 90.14, 90.89, 90.66, 90.76, 91.03, 91.41, 90.93, 90.67, 90.6, 91.03, 90.24, 90.46, 91.57, 90.73, 90.03, 91.58, 90.88, 90.92, 90.68, 90.39, 91.06, 90.78, 91.26, 91.24, 91.34, 90.73, 90.8, 90.85, 91.48, 91.91, 91.43, 90.71, 90.95, 90.31, 90.52, 91.39, 91.55, 90.75, 90.81, 90.03, 91.94, 90.96, 91.39, 90.87, 91.48, 91.35, 91.49, 91.32, 91.69, 92.11, 91.03, 91.27, 91.16, 91.05, 90.7, 91.13, 91.46, 91.82, 91.81, 91.49, 91.99, 91.79, 91.73, 91.2, 91.93, 92.24, 91.43, 91.47, 92.96, 92.19, 92.09, 92.48, 92.02, 92.34, 92.2, 92.68, 92.39, 91.92, 92.75, 92.45, 92.44, 92.63, 93.15, 92.81, 92.81, 92.69, 92.93, 93.25, 92.81, 93.06, 93.3, 93.25, 93.18, 93.24, 93.27, 93.09, 92.97, 93.34, 93.48, 93.5, 93.69, 92.96, 93.63, 93.47, 93.2, 93.34, 93.33, 93.45, 93.63, 93.43, 93.45, 93.43, 93.28, 93.34, 93.46, 93.25, 93.5, 93.43, 93.59, 93.52, 93.6, 93.48, 93.48, 93.64, 93.39, 93.43, 93.53, 93.86, 93.52, 93.62, 93.52, 93.53, 93.39, 93.49, 93.6, 93.69, 93.23, 93.55, 93.52, 93.32, 93.61, 93.58, 93.53, 93.55, 93.62, 93.58, 93.59, 93.54, 93.48, 93.56, 93.71, 93.59, 93.5, 93.49, 93.58, 93.58, 93.58, 93.57, 93.58, 93.4, 93.52, 93.66, 93.54, 93.58, 93.84, 93.78, 93.32, 93.76, 93.65, 93.72, 93.33, 93.67, 93.51, 93.87, 93.69, 93.5, 93.81, 93.66, 93.74, 93.39, 93.78, 93.7, 93.6, 93.74, 93.79, 93.85, 93.54, 93.78, 93.83, 93.51, 93.47, 93.65, 93.63, 93.65, 93.52, 93.58, 93.48, 93.61, 93.69]\n",
        "train_loss_list_wd1e2 = [1.5167798984545868, 1.027339467034934, 0.8097865730047987, 0.719615878769384, 0.658255658305872, 0.615522557935014, 0.5982994875207115, 0.5846719085789336, 0.5773727887164289, 0.5627464520664641, 0.5591158762145728, 0.5449530482292175, 0.5340716247550976, 0.5340199959925569, 0.5181504983109788, 0.5176402507498622, 0.5080099205810803, 0.49660853408396055, 0.49530376631992695, 0.4915612660848295, 0.4967052273856946, 0.49324704406741327, 0.49331052558490646, 0.48590693801355817, 0.4841031406443721, 0.48545389605787237, 0.4772332070734554, 0.47386771064406386, 0.47569251241394506, 0.4783006565639386, 0.4702882940967243, 0.47064500009289945, 0.47294196457908555, 0.45738911800110305, 0.4716209914928046, 0.4719133836011917, 0.4696217343068351, 0.45638318650257853, 0.46723246879090136, 0.4585420537870913, 0.45578641851489154, 0.465418814565427, 0.4614397684415689, 0.45631032715590236, 0.4486660618370714, 0.4534833081804525, 0.45102608156280394, 0.4530490816782077, 0.45389505011585957, 0.45014332449093414, 0.4494515047096216, 0.450234164349949, 0.4440193151037533, 0.44662567982658413, 0.44600409516892114, 0.4423091958125178, 0.44665925180950106, 0.4360342098120302, 0.43951448350668715, 0.44023644362394804, 0.4460902175964258, 0.43867638192999475, 0.4361597361465612, 0.433228444367552, 0.43528716051921296, 0.4372344275061696, 0.4323961653838904, 0.43725441934201664, 0.43545952991555675, 0.42910580998792436, 0.43043560903674116, 0.43121091731059286, 0.42305412998024267, 0.42636741055086397, 0.4268225592831834, 0.42100020314748293, 0.41761867097391486, 0.4185335077702428, 0.42368483900452575, 0.41616832524442826, 0.41972924578494536, 0.410687822789049, 0.4157286212562372, 0.4089399065358189, 0.40666280982022085, 0.4132289454198112, 0.40205181121064454, 0.40999404700419395, 0.4086824649819932, 0.40080371218177074, 0.4015931721788626, 0.4063145552580349, 0.4066784158110999, 0.40118428522024674, 0.4009731699483463, 0.3969333416547257, 0.39313930387314133, 0.39997971186432213, 0.3931039036653293, 0.38969586989559685, 0.3916486538351534, 0.38877204070076016, 0.38579398788773595, 0.3810211131557489, 0.3882365185803118, 0.38886873219340756, 0.3846093181984874, 0.38404670505287547, 0.3830401858392234, 0.3767934866701833, 0.37473060204959907, 0.37977902538860187, 0.37377259620843223, 0.38002860750824496, 0.36324887853651383, 0.3618837351235338, 0.36737791051308566, 0.3669434030311176, 0.3633396982099302, 0.36406448107367506, 0.3577735069365547, 0.353955817536805, 0.3599519186888259, 0.35612359438270047, 0.35251178142552175, 0.35027061312343366, 0.3470285452497653, 0.3470985202934034, 0.3467607243468586, 0.33784743396047584, 0.34203178333207823, 0.3446982724312395, 0.3408664211678429, 0.3342146307420426, 0.33859410405920715, 0.3276973366737366, 0.33356578295794537, 0.33072001608415913, 0.32800227998735043, 0.32374877627855675, 0.32380120865643597, 0.323726001829385, 0.3161135405397263, 0.32177544154298193, 0.3120566315639514, 0.31642273844431, 0.308783637401395, 0.32058241168340557, 0.31076529688728505, 0.3038082039489533, 0.2967669633678354, 0.30553271728582654, 0.29508178397870294, 0.2989721709070876, 0.2943779766654816, 0.29553699583862536, 0.28911615298769344, 0.29165190915330147, 0.29362286717746966, 0.28382395979124136, 0.2868227225999101, 0.2803722090138414, 0.27424085373505236, 0.28322451868758036, 0.2771561364777172, 0.2667812052816629, 0.2687817446578044, 0.26123926229179856, 0.2610385168474703, 0.2666228299799819, 0.2595565085784315, 0.25669008474380445, 0.2462938559798006, 0.2528521096982514, 0.24889454671654837, 0.2559791601504, 0.24216721735347194, 0.2481234513056545, 0.2393635288356973, 0.23972950325891995, 0.23211169509461133, 0.23631990731905061, 0.2304189305621595, 0.22999336732366976, 0.2226063944994451, 0.22054641979475753, 0.21703065643771388, 0.21686043295140464, 0.21233341681024137, 0.21059229460577614, 0.20723726490911204, 0.20685803997345245, 0.20887376510868438, 0.1978470858531638, 0.2038871747569535, 0.19181662209974693, 0.19145933929057166, 0.1935519773167924, 0.19051632768334673, 0.18543817138614746, 0.17970925828995415, 0.18026368427105224, 0.17701446067410917, 0.17569915164773836, 0.16774458888049323, 0.17213442190862693, 0.1631373402100211, 0.16456650209407836, 0.16205769175062545, 0.1562980950569002, 0.1515140765891098, 0.15663365386545466, 0.15088544290857955, 0.14032981417382867, 0.14692522343783715, 0.14212213777981628, 0.1392525550894463, 0.13808176975947217, 0.12566476896071968, 0.13361202100642955, 0.1256047902753749, 0.1272383151462855, 0.11795103549957275, 0.11710383291966237, 0.11266100632782561, 0.11401164303191554, 0.106656764119197, 0.10556819721961173, 0.10439886295566925, 0.10726791070387386, 0.09460471522884246, 0.0962810536590628, 0.09549028507341592, 0.09219917137259112, 0.0865226152379768, 0.08575040581651008, 0.08219950793745419, 0.07716210633992387, 0.07188480715163219, 0.08065827630650692, 0.06764008765355847, 0.06986326664186324, 0.07430443038551, 0.06353453783609997, 0.06618601354523398, 0.057043209338721375, 0.051298089039782745, 0.054104091200870445, 0.04759680214352882, 0.053709484345187394, 0.046893905104396824, 0.043303748533224905, 0.04337597922228586, 0.0364828174844527, 0.035764870218956434, 0.03372384082918731, 0.03319300545718723, 0.03330101266193885, 0.03186584125978116, 0.027693336871199715, 0.026877068286648573, 0.026169767811561166, 0.0256406356053897, 0.025631253985456005, 0.024362976106401448, 0.023579948727553264, 0.02450076937556457, 0.023537636362611296, 0.02355496631786465, 0.023737981790504136, 0.022986546093330215, 0.023408689955695748, 0.023427624023332, 0.02295914088813261, 0.023163542187156768, 0.023356065636102003, 0.023212027953217584, 0.02309029637434231, 0.022946245367296586, 0.022692412935411587, 0.02339219701842378, 0.023122945366004784, 0.023124087244843522, 0.02343619195893169, 0.022982527528660365, 0.023213101919895163, 0.02317301971248735, 0.022974888207956245, 0.023228626388806503, 0.02310882277263049, 0.023166305644396014, 0.022929255026407518, 0.02320044601377778, 0.02337465347787633, 0.02341003445224069, 0.022785558921698566, 0.023203995150213423, 0.023422795285384494, 0.023028960588355414, 0.023342699217148863]\n",
        "train_acc_list_wd1e2 = [44.33, 63.2525, 71.6725, 75.2775, 77.445, 79.1175, 80.16, 80.8725, 80.74, 81.4475, 81.53, 82.1375, 82.65, 82.6, 82.96, 83.1775, 83.415, 83.7225, 83.915, 84.0275, 83.7325, 83.8675, 83.855, 84.1725, 84.09, 84.1675, 84.56, 84.5075, 84.6425, 84.4775, 84.74, 84.7025, 84.6975, 85.115, 84.7575, 84.705, 84.82, 85.3025, 84.7525, 85.0175, 85.3575, 84.9425, 85.02, 85.27, 85.5525, 85.28, 85.3825, 85.275, 85.2875, 85.52, 85.4475, 85.18, 85.4925, 85.525, 85.6125, 85.75, 85.4725, 85.7925, 85.8875, 85.5825, 85.7075, 85.8275, 86.0, 86.11, 85.95, 85.9425, 85.93, 85.89, 85.92, 86.29, 86.0525, 86.085, 86.41, 86.3275, 86.2675, 86.5075, 86.5475, 86.4525, 86.245, 86.705, 86.37, 86.7625, 86.705, 86.8875, 86.9225, 86.7075, 87.0625, 86.7475, 87.0175, 87.1175, 87.105, 86.8725, 87.09, 87.08, 87.1425, 87.405, 87.4725, 87.09, 87.4975, 87.53, 87.6025, 87.6175, 87.61, 87.685, 87.5425, 87.6175, 87.635, 87.8375, 87.78, 87.9375, 88.0275, 87.7925, 88.2425, 87.6675, 88.5325, 88.5275, 88.3, 88.4075, 88.42, 88.435, 88.6525, 88.795, 88.54, 88.7475, 88.7525, 88.77, 88.8375, 88.9225, 88.9325, 89.41, 89.1775, 89.045, 89.1575, 89.53, 89.3225, 89.855, 89.5225, 89.5275, 89.67, 89.66, 89.7775, 89.81, 90.0825, 89.9125, 90.125, 90.0025, 90.1125, 89.94, 90.265, 90.4675, 90.795, 90.4, 90.8425, 90.7075, 90.79, 90.6825, 91.02, 91.01, 90.935, 91.225, 90.995, 91.265, 91.47, 91.1175, 91.3675, 91.805, 91.6375, 91.945, 92.08, 91.76, 91.995, 92.1175, 92.48, 92.1875, 92.3625, 92.13, 92.595, 92.445, 92.71, 92.67, 93.005, 92.7375, 92.9675, 93.0725, 93.2, 93.335, 93.5175, 93.475, 93.62, 93.7325, 93.7275, 93.7925, 93.66, 94.175, 93.9525, 94.44, 94.3725, 94.22, 94.4575, 94.51, 94.8425, 94.695, 94.77, 94.87, 95.23, 94.9475, 95.2325, 95.23, 95.28, 95.5575, 95.725, 95.595, 95.7125, 96.0975, 95.9275, 95.985, 96.155, 96.23, 96.7125, 96.4, 96.5725, 96.5825, 96.8275, 96.9275, 97.0525, 96.9475, 97.2575, 97.295, 97.4575, 97.2975, 97.66, 97.6175, 97.5925, 97.81, 98.01, 97.9825, 98.08, 98.2975, 98.5225, 98.105, 98.595, 98.47, 98.3475, 98.6925, 98.59, 99.0025, 99.1525, 99.095, 99.27, 99.0025, 99.2475, 99.4225, 99.43, 99.6275, 99.635, 99.6975, 99.715, 99.7175, 99.7575, 99.885, 99.895, 99.9175, 99.925, 99.9225, 99.9675, 99.99, 99.955, 99.9825, 99.985, 99.98, 99.9875, 99.985, 99.9875, 99.995, 99.9925, 99.995, 99.9975, 99.9975, 99.9975, 99.9975, 99.995, 99.9975, 99.9975, 99.9975, 99.9925, 99.995, 99.995, 99.9975, 99.9925, 99.9975, 99.9975, 99.9925, 99.9925, 99.9975, 100.0, 99.99, 99.9975, 99.9975, 99.9925, 99.995]\n",
        "test_loss_list_wd1e2 = [1.3725743022146104, 1.1929742784439763, 0.9703254548809196, 0.8823626479016075, 0.7912342133401316, 0.7744240081762965, 0.9433422028263913, 0.698423402218879, 0.6534588306765013, 0.8720321919344649, 0.9694693926014478, 0.7136559109144573, 1.0214628554597687, 0.8677132740805421, 0.7085553053059156, 0.6858387778076944, 0.8705045671402654, 0.7853590740433222, 0.6123643494859526, 0.8236432565918451, 0.639409672987612, 0.7271271928956237, 0.6982333724257312, 0.8511822268932681, 0.8309068830707406, 0.8227003050755851, 0.7763343317599236, 1.2531145765811582, 0.7305624854715564, 0.7311744048625608, 0.9035976242415512, 0.6800222517568854, 0.5853997695295117, 0.7430345084093795, 0.8785871493665478, 1.108451329454591, 0.6588784128050261, 0.7044716858411137, 0.6049494954604137, 0.6143702955185613, 0.7461452868920339, 0.6528882425797137, 0.8694826870024959, 0.5893921456005, 0.6651792013192479, 0.6350515420678295, 0.6344684543488901, 0.6994746138777914, 0.5725251994555509, 0.6624086785920059, 0.9637637417527694, 0.8048486015464686, 0.6040883234030083, 0.7363305929340894, 0.6205344230313844, 0.6153643851793265, 0.7306718335875982, 0.6487590738489658, 0.7805856805813464, 0.6067438793333271, 0.6581050104732755, 0.6229219368741482, 0.5560055741026432, 0.6660232819333861, 0.7725009058095231, 0.6505244955231871, 0.5968554544297955, 0.7541794965538797, 0.6514349609990663, 0.7917481374137009, 0.6845550778545911, 0.7952898657774623, 0.6756763978849484, 0.7763081943687005, 0.8057050523878653, 0.7995553084566623, 0.6522585726991484, 0.5840945632397374, 0.6991102891632274, 0.6438415571104122, 0.5860964950126938, 0.635802041126203, 0.9848449245283876, 0.7002730075317093, 0.7162983300565164, 0.637317259100419, 1.0616072612472727, 0.5819362533997886, 0.6480541802659819, 0.7091145688974405, 0.532154082497464, 0.7218736040441296, 1.2941748115080822, 0.5242264851739135, 0.5975038005581385, 0.5866859227041655, 0.7909847196144394, 0.5758454048935371, 0.613415573971181, 0.6326792579662951, 0.6269164990775192, 0.5572914639605752, 0.5726913866362994, 0.6187993967080418, 0.6928256652023219, 0.5965810301183145, 0.6376756267457069, 0.5003311121011083, 0.5492256042323534, 0.601341659509683, 0.68496092286291, 0.5872152938118463, 0.5926056911673727, 0.5051948205579685, 0.5469717281528667, 0.7418177361729779, 0.7266407209106639, 0.7612239318557933, 0.4831244179719611, 0.6568866824801964, 0.7060362685330307, 0.693530101564866, 0.5567308860489085, 0.7726584718197207, 0.7781753985187675, 0.761501886422121, 0.6479162256928939, 0.7326241222363484, 0.5910413627383075, 0.7026853682119635, 0.7172112155564224, 0.6026144582259504, 0.5826918271523488, 0.5626278546037553, 0.49854521132722684, 0.6062672617314737, 0.5234693847125089, 0.5272691702540917, 0.6541247133967243, 0.42634944221641446, 0.638887640041641, 0.49163430740561664, 0.8650434718856329, 0.6591806264617776, 0.5614948665039449, 0.5188067898720126, 0.583303042982198, 0.5776442757135705, 0.7945110775247405, 0.4383143160162093, 0.5127309507961515, 0.48725567663772196, 0.5552749682830859, 0.5127951763098753, 0.5552909845792795, 0.6396092316017875, 0.6915487266039546, 0.5918025759202016, 0.6109406363360489, 0.49616090745865543, 0.4317665135935892, 0.5233679349663891, 0.46711384986020343, 0.5448861080634443, 0.705641579401644, 0.48266834171512457, 0.4491133007067668, 0.6736038417755803, 0.41231273925757106, 0.4804556973372834, 0.49244492793384986, 0.5156307778780973, 0.5698083575013317, 0.4579203664502011, 0.5245823471606532, 0.4540955895864511, 0.4462763463394551, 0.47661840538435346, 0.3877083959081505, 0.5359865875938271, 0.47837658622596835, 0.5976610659044, 0.4057789820281765, 0.4568231581132623, 0.5033613733852966, 0.4880643072007578, 0.421935570013674, 0.3864406771674941, 0.3924215535951566, 0.4215288765822785, 0.3856588805778117, 0.39591050393219235, 0.3960021594657174, 0.4162447214881076, 0.4317766793921024, 0.43716210089152374, 0.3752851269290417, 0.3684254226428044, 0.4046018751738947, 0.4570601512736912, 0.42681945275656785, 0.3557315610254867, 0.38338644142392314, 0.45626604632486273, 0.48571689672107937, 0.4053226277420792, 0.38042039244989806, 0.40194732573213454, 0.3663549528846258, 0.40040300088592723, 0.33492799480504626, 0.34661694832994966, 0.3733026383421089, 0.31432308200039444, 0.35983454105974755, 0.3449418092075783, 0.38464838134337076, 0.34258481332018403, 0.3416274746384802, 0.3719708046203927, 0.33954131207134153, 0.3303550594969641, 0.3256681533176688, 0.3663260734911206, 0.305223887479758, 0.3600897785229019, 0.359975767286518, 0.3134645165144643, 0.31159543180013005, 0.2859196728920635, 0.3370714645031132, 0.30697164241271685, 0.3175534585231467, 0.31721039870871776, 0.3941425471743451, 0.34323022165630435, 0.2986761056169679, 0.2926558808812612, 0.298173848963991, 0.29151669446426104, 0.3160338721509221, 0.2907146306920655, 0.29263262648748445, 0.27918298995193047, 0.2787177439166021, 0.27893129094869273, 0.27565936725350876, 0.2982580420337146, 0.32113587752550465, 0.28794968552604505, 0.2873940949764433, 0.2720843562031094, 0.255550967647305, 0.24216599573817435, 0.2502805740584301, 0.2626812101542195, 0.2440197715842271, 0.24417917901956582, 0.2448149986470802, 0.2319961440148233, 0.22655500405574147, 0.2284358123058005, 0.2258316146422036, 0.22443584173540526, 0.21244820348824126, 0.21419980961687957, 0.20566311539917054, 0.20899182690095297, 0.20759060931733891, 0.20997956381002558, 0.2037774644131902, 0.20164045950845827, 0.20583347683843178, 0.209160113353518, 0.2061407360000701, 0.1988746996827518, 0.2016011231307742, 0.20003076869098446, 0.20136920108070858, 0.20208889264849167, 0.19937377427763578, 0.20227546367464186, 0.19644954659139055, 0.2079336524386949, 0.19830586704649503, 0.2019951339763931, 0.2011260839202736, 0.1998056554341618, 0.20407455923813808, 0.20218830621695216, 0.20711066649307178, 0.20228053129549267, 0.1982952310126039, 0.1989742641018916, 0.20098775002775313, 0.19563924860727938, 0.20572516359860385, 0.19656268205446534, 0.19923276595677, 0.1936172448287282]\n",
        "test_acc_list_wd1e2 = [52.13, 58.0, 66.47, 68.48, 71.93, 73.38, 68.66, 76.35, 78.12, 70.27, 67.31, 76.04, 66.26, 71.09, 76.74, 77.08, 70.56, 74.21, 79.8, 73.72, 78.4, 76.32, 76.58, 71.8, 72.87, 73.02, 74.38, 58.91, 74.56, 75.75, 69.39, 78.21, 80.51, 75.74, 71.26, 65.74, 77.96, 76.81, 80.1, 79.77, 74.87, 77.76, 72.43, 80.66, 78.24, 79.29, 79.0, 77.18, 81.23, 77.41, 70.82, 74.14, 79.96, 75.72, 79.75, 80.63, 76.93, 78.66, 73.71, 79.9, 78.5, 79.77, 82.0, 78.63, 74.67, 77.94, 80.15, 75.57, 78.33, 74.1, 77.43, 74.34, 78.18, 75.35, 74.2, 73.88, 78.85, 80.44, 76.92, 78.07, 80.31, 78.92, 68.42, 77.39, 76.56, 79.35, 67.33, 80.82, 78.55, 77.35, 82.3, 77.25, 61.02, 82.77, 79.87, 81.04, 73.94, 81.59, 79.69, 78.21, 79.9, 81.84, 81.13, 80.0, 77.16, 80.57, 79.35, 83.74, 81.48, 80.41, 78.15, 80.5, 80.9, 83.64, 82.31, 75.42, 76.71, 75.99, 84.67, 79.01, 76.87, 77.9, 81.65, 74.49, 73.92, 75.98, 78.22, 76.52, 80.6, 77.42, 76.86, 80.4, 80.66, 81.63, 84.08, 80.38, 83.18, 82.73, 78.42, 86.07, 78.92, 84.8, 74.23, 78.45, 82.33, 83.06, 80.78, 81.68, 75.94, 85.72, 83.08, 84.02, 81.86, 83.38, 81.98, 79.64, 79.1, 80.78, 80.36, 83.85, 86.0, 82.75, 85.08, 82.98, 77.55, 84.19, 85.7, 77.95, 86.42, 84.35, 83.96, 82.9, 81.53, 85.4, 83.22, 85.75, 85.89, 84.67, 87.57, 83.16, 85.08, 81.54, 87.43, 85.54, 83.96, 84.48, 86.54, 87.69, 87.78, 86.26, 87.73, 87.31, 87.28, 86.94, 86.55, 86.06, 88.29, 88.32, 87.19, 85.53, 86.85, 89.14, 88.15, 85.86, 84.75, 87.47, 88.24, 87.6, 88.28, 87.78, 89.53, 89.36, 88.74, 90.36, 88.87, 89.5, 87.99, 89.19, 89.71, 88.6, 89.51, 90.06, 90.36, 88.83, 90.54, 89.04, 89.2, 90.63, 90.62, 91.59, 90.18, 90.87, 90.09, 90.93, 88.63, 89.71, 91.42, 91.72, 91.15, 91.61, 90.94, 91.63, 91.49, 92.14, 92.34, 92.5, 92.22, 91.41, 90.91, 92.27, 92.05, 92.77, 93.17, 93.48, 93.25, 92.79, 93.44, 93.49, 93.54, 93.83, 93.98, 93.84, 94.0, 94.24, 94.31, 94.42, 94.67, 94.34, 94.28, 94.61, 94.58, 94.66, 94.53, 94.5, 94.65, 94.62, 94.71, 94.73, 94.69, 94.61, 94.78, 94.59, 94.83, 94.68, 94.68, 94.76, 94.76, 94.77, 94.61, 94.75, 94.64, 94.51, 94.78, 94.81, 94.72, 94.81, 94.72, 94.92, 94.66, 94.96]\n",
        "train_loss_list_300const = [1.5072595094339536, 1.0231038059670323, 0.7837633473423723, 0.6642771163306678, 0.5742667439265754, 0.5122711343315843, 0.4693711296723673, 0.42363618176204326, 0.38468483099922207, 0.3563146945386649, 0.32989333312922775, 0.3088601354402475, 0.28732080238695723, 0.2773427833050204, 0.255741254994854, 0.2355493075502947, 0.22032112675829055, 0.20569888635660513, 0.19303267375348857, 0.18455393645710078, 0.1760997775430306, 0.1610161360388937, 0.15303532645915643, 0.14379756043132502, 0.13295495837998275, 0.12522198601414603, 0.12198948780425821, 0.10830174501949606, 0.1100872532187845, 0.1019453627054398, 0.10001775390876178, 0.0945176584318804, 0.09056858165552631, 0.08527834246905086, 0.07428395165243563, 0.073358194616894, 0.0680886913840763, 0.06761494047416094, 0.06709703737006972, 0.05811852629716023, 0.05694502567985473, 0.054410113292499285, 0.048847372924831635, 0.05006321483610061, 0.04818616545470521, 0.04477997207657074, 0.04385583504540518, 0.044935471408021524, 0.04291497548661245, 0.03747522448019955, 0.03720155920232304, 0.03781872331246603, 0.03907512170084106, 0.03808571144822426, 0.0337266662997155, 0.03255701906303652, 0.02515740589887356, 0.024542150663780255, 0.02793147737166276, 0.02593546934959928, 0.02561265908809492, 0.027039424086304994, 0.02195272859767639, 0.022481467933937037, 0.023805265842651288, 0.019805631663177174, 0.020343415882660094, 0.022202910110800743, 0.021919588551681024, 0.025648440866278835, 0.024350768256822405, 0.02123939903258099, 0.01514708600285948, 0.013058545996253482, 0.016306638918867626, 0.01685327520953289, 0.016574702121484014, 0.016640731977996603, 0.01489181877258933, 0.015860306949298176, 0.014099853219945703, 0.013995581963445438, 0.016321878165777286, 0.013422961495113753, 0.010429043487289373, 0.015911166273230783, 0.011775680992100685, 0.014917363881686935, 0.018574517294552517, 0.012787643851754746, 0.014939473080523192, 0.01176600590211186, 0.012982209259662748, 0.008858771788844534, 0.009820060386200072, 0.009505460298391053, 0.011240429890194399, 0.010675670454989353, 0.010158317344160542, 0.006973291840534449, 0.006880928409012844, 0.005811905768775168, 0.007408292713742807, 0.007451252999362914, 0.010354434164527717, 0.01092628369954178, 0.012062771040645908, 0.007832180905609038, 0.010386954512029251, 0.010358608426182522, 0.00795716502255174, 0.0063932851670936985, 0.006447436372195698, 0.004587642372207018, 0.007233876957458231, 0.007980881170325053, 0.010550779263485601, 0.012090449684582942, 0.009726078081678914, 0.00813099715300611, 0.006632155526443176, 0.00597371292983393, 0.004589174477389631, 0.003584162614903578, 0.005510094879151764, 0.007690454193111211, 0.006682628281595362, 0.005014813251454181, 0.005320725576812532, 0.007166650853292368, 0.007646939404855167, 0.0068286928395223144, 0.008263683283372191, 0.007336255545465137, 0.007655256824561777, 0.007321817508736698, 0.00431125137086996, 0.003226296991330126, 0.004760576777890367, 0.0063299430897483565, 0.004795740673675748, 0.0033784159969536777, 0.0035024787487254625, 0.005299788615376221, 0.003612780912293712, 0.0032062702902157835, 0.004884716233539565, 0.005524865538374664, 0.0067742976795348445, 0.00882896750121604, 0.005893147790118473, 0.00848282681003614, 0.007084418307778205, 0.0041311039470466075, 0.003654746470935981, 0.0033575089104424215, 0.0021545471597365076, 0.003389105463073925, 0.004264044310640164, 0.004854566317398292, 0.0023904619371291056, 0.002829549484107492, 0.002053513441469564, 0.002324710492194019, 0.0021624633618179965, 0.00212872851480438, 0.0023564651491906493, 0.003220773860256611, 0.002329243483711235, 0.0029083483295660978, 0.001973968918878853, 0.00209470545457433, 0.0020779970437868503, 0.0028365065618229167, 0.003357957487961407, 0.0030591108087931595, 0.002803605341979417, 0.0023873494445828206, 0.0023525519992746557, 0.002546498180350455, 0.0016468311933363184, 0.004213925738022168, 0.003313322557223379, 0.003231858396061877, 0.002629125782985243, 0.0029141006389791914, 0.003536610023096286, 0.004142602702752726, 0.005321672680347896, 0.005746499752380254, 0.0031256999963618884, 0.0032810929011951122, 0.0043974452997825285, 0.005239838635538117, 0.004797415686162696, 0.0032062243664227707, 0.003533768229311691, 0.0029152913773755726, 0.0041634554793835415, 0.0021060470147562338, 0.005407320675629354, 0.006459823155465985, 0.004450791821244248, 0.004911506875814143, 0.003201270101376409, 0.0062131848153570805, 0.003254528715726523, 0.003693553237162894, 0.00231178058878833, 0.002128929863538825, 0.0034310214809528308, 0.0021012531412071865, 0.0037686275719023775, 0.0026402074050687757, 0.0015023963813803039, 0.0016959904317550368, 0.0023601827229728445, 0.0016248808494453031, 0.001744074698825616, 0.0017098960009427912, 0.001745167300260726, 0.001709676564420346, 0.0014354304178802375, 0.0017766182493312826, 0.0017726632421619605, 0.005174195695061486, 0.00404562306744882, 0.0019462628384505156, 0.0021156957895648327, 0.003270384036677395, 0.004819947180601103, 0.004801801557994203, 0.003892041585816488, 0.003026418740098223, 0.0016867717583173738, 0.00243665445885507, 0.001924227454584904, 0.0032565365431517818, 0.0025015898762403913, 0.004080375838660543, 0.0034671720829839744, 0.002242059777764631, 0.00174158359747937, 0.0023047459024574768, 0.0018280270336568435, 0.003222634576515772, 0.002665815578360469, 0.002171202788719592, 0.004139460606425585, 0.005407126804135614, 0.003192231660575842, 0.0023893227233684547, 0.0017126825603354112, 0.0012934664354602478, 0.0024260497488599003, 0.0017768104415345862, 0.001707909157712362, 0.0025417737575718987, 0.0026618135311662788, 0.0013963878702470945, 0.001963627433890095, 0.0015268292372885813, 0.0010931521974983068, 0.0014422832873635296, 0.0014909466068760181, 0.0013153712134022825, 0.0010111237624123271, 0.0009827128673119666, 0.0005205708235784471, 0.0008319928409817137, 0.0007095568490024797, 0.00037128912167129697, 0.0006035467810239169, 0.0011062782132314122, 0.000675376708713361, 0.0006885260622773773, 0.00101789951790438, 0.0006217252514125575, 0.0007492425983336682, 0.0021772955004840527, 0.0016886424110349782, 0.0013806177742215066, 0.001364799225246333, 0.0013202676723717748, 0.0013149851532095538, 0.0010957021063876863, 0.0007910217270622839, 0.0011311705605573214, 0.000875898118397272, 0.0015793223269400525, 0.0013224604617411992, 0.0022564116427801696, 0.0026849516309372387, 0.0020301068258375223, 0.0006494859589330384, 0.0005210057817731313, 0.000972889763034617, 0.0010060088542230168, 0.0009481032836110347, 0.001107927744007216]\n",
        "train_acc_list_300const = [44.7525, 63.5, 72.3975, 76.605, 79.975, 82.355, 83.73, 85.3525, 86.61, 87.5825, 88.51, 89.1375, 90.01, 90.3875, 91.105, 91.8175, 92.1325, 92.8725, 93.125, 93.5025, 93.6875, 94.285, 94.705, 94.91, 95.275, 95.5425, 95.6975, 96.1225, 96.08, 96.3625, 96.4425, 96.6725, 96.75, 96.98, 97.385, 97.405, 97.635, 97.55, 97.615, 97.935, 97.9225, 98.115, 98.2575, 98.245, 98.31, 98.4675, 98.44, 98.46, 98.4925, 98.7, 98.6225, 98.6575, 98.6, 98.63, 98.765, 98.835, 99.135, 99.13, 99.0825, 99.0975, 99.1425, 99.085, 99.25, 99.21, 99.1575, 99.3575, 99.285, 99.2325, 99.23, 99.12, 99.155, 99.3075, 99.505, 99.57, 99.4, 99.43, 99.425, 99.4425, 99.495, 99.4975, 99.4875, 99.5125, 99.4175, 99.5525, 99.6225, 99.455, 99.59, 99.475, 99.355, 99.5525, 99.4975, 99.6075, 99.545, 99.7325, 99.655, 99.6675, 99.655, 99.63, 99.65, 99.7875, 99.8125, 99.82, 99.7425, 99.7475, 99.66, 99.6325, 99.55, 99.74, 99.685, 99.6475, 99.7425, 99.7775, 99.77, 99.86, 99.7625, 99.7475, 99.6, 99.5925, 99.68, 99.7275, 99.775, 99.8, 99.8475, 99.895, 99.82, 99.7325, 99.7725, 99.8375, 99.8025, 99.7625, 99.725, 99.7525, 99.705, 99.7675, 99.735, 99.72, 99.8675, 99.8975, 99.845, 99.7875, 99.8475, 99.8725, 99.8825, 99.8425, 99.865, 99.905, 99.8525, 99.805, 99.7725, 99.695, 99.8, 99.7325, 99.76, 99.8675, 99.8725, 99.9125, 99.9425, 99.9, 99.86, 99.83, 99.935, 99.8975, 99.9425, 99.9225, 99.9325, 99.9325, 99.9275, 99.8825, 99.9275, 99.8975, 99.9425, 99.92, 99.94, 99.8975, 99.895, 99.89, 99.9075, 99.9225, 99.9175, 99.91, 99.9525, 99.855, 99.885, 99.88, 99.915, 99.9, 99.8775, 99.8475, 99.8175, 99.7875, 99.905, 99.8825, 99.8775, 99.8225, 99.8675, 99.875, 99.89, 99.9025, 99.8725, 99.9275, 99.825, 99.77, 99.86, 99.815, 99.905, 99.78, 99.8975, 99.8875, 99.925, 99.9225, 99.8875, 99.9375, 99.8775, 99.91, 99.96, 99.9425, 99.9175, 99.9525, 99.945, 99.9425, 99.945, 99.935, 99.96, 99.9375, 99.9275, 99.8525, 99.8525, 99.945, 99.935, 99.8825, 99.8275, 99.825, 99.8725, 99.8875, 99.9425, 99.9125, 99.925, 99.89, 99.9175, 99.8725, 99.8975, 99.925, 99.9475, 99.9325, 99.93, 99.8975, 99.88, 99.925, 99.8575, 99.8025, 99.895, 99.9175, 99.9475, 99.955, 99.9225, 99.94, 99.9325, 99.92, 99.8875, 99.9625, 99.925, 99.95, 99.965, 99.955, 99.9625, 99.9675, 99.9725, 99.97, 99.9925, 99.9725, 99.9825, 99.9925, 99.9825, 99.9625, 99.9875, 99.9725, 99.9625, 99.985, 99.975, 99.9375, 99.9475, 99.9525, 99.9625, 99.965, 99.97, 99.97, 99.9775, 99.9675, 99.9775, 99.9525, 99.95, 99.9225, 99.905, 99.945, 99.985, 99.985, 99.9675, 99.98, 99.9725, 99.95]\n",
        "test_loss_list_300const = [1.3109248876571655, 0.9655934632578983, 0.8187996443313889, 0.689740173047102, 0.6772998514809186, 0.6437552631655826, 0.5553377157525171, 0.5520454826234262, 0.5120236824584913, 0.5249090968053552, 0.47987785124326054, 0.45728976138030425, 0.4652932938895648, 0.49931400976603546, 0.5139825019655349, 0.46358482577378235, 0.5389019496078733, 0.4064606526229955, 0.4374082180895383, 0.41569147004356866, 0.42417793334284914, 0.43346760355973546, 0.4166975364654879, 0.3910007622045807, 0.4190592248983021, 0.4890744652174696, 0.4130742766811878, 0.44733505535729323, 0.3960334837813921, 0.4205396575263784, 0.4506959509623202, 0.45510448968108697, 0.4337734672464902, 0.4140701989961576, 0.44726073873948446, 0.43756397483469567, 0.4081488196985631, 0.4404116733164727, 0.4069605218836024, 0.4365550541802298, 0.4524993513581119, 0.4810156839180596, 0.4475790313150309, 0.43388396841061266, 0.4388974361781833, 0.4250442619565167, 0.4161135846678215, 0.4599215022370785, 0.4951787300502198, 0.5083512184740622, 0.4711726530443264, 0.5019661293754095, 0.42351165593047685, 0.4634768115946009, 0.472742423415184, 0.4849290226267863, 0.4477674431061443, 0.48274370330043986, 0.4825481849757931, 0.4942950615777245, 0.5367305686202231, 0.4768067407834379, 0.4528346563441844, 0.49772302598892887, 0.43751826391944404, 0.4484408743019345, 0.5320610732217378, 0.47107771444547025, 0.4548007876058168, 0.48090932169292544, 0.47415826705437675, 0.44294909170911284, 0.408969871039632, 0.4602267291349701, 0.4937338570627985, 0.5272085793033431, 0.5100355125680754, 0.46345121894456165, 0.4728180213442332, 0.48767814587188674, 0.4734298460468461, 0.5140062019794802, 0.48423475234568875, 0.4522600043800813, 0.4885755015324943, 0.4898544026515152, 0.48496218667000157, 0.5125900219324269, 0.4700437818146959, 0.45802032928678055, 0.49178935455370554, 0.48068239813364005, 0.4772832633196553, 0.45595825010839897, 0.4741470496865767, 0.508940930796575, 0.47970542651188525, 0.5067461744139466, 0.47261701335635364, 0.4504903811442701, 0.4822925106634068, 0.4916953529360928, 0.4976545244078093, 0.5076064607010612, 0.512105936019481, 0.5002134619634363, 0.4895322826466983, 0.46702025679847864, 0.5068643896640102, 0.5748100429773331, 0.4924963390148139, 0.49676062770282164, 0.5234567566385752, 0.49199416045146654, 0.5275697438400003, 0.5010442611160157, 0.49138097276416004, 0.5251847050989731, 0.5231706929169123, 0.4994612190919586, 0.475140321481077, 0.4973598703553405, 0.46391360763507555, 0.4987307276718224, 0.5342934448507768, 0.490154218258737, 0.5101715593209749, 0.5079800128182278, 0.4876681245580504, 0.5047554036107245, 0.4933491258681575, 0.5017587872622888, 0.5059225696928894, 0.4965471655507631, 0.5347162768999233, 0.5293986288995682, 0.48946061645504796, 0.4993787820957884, 0.5046113696468028, 0.5203786311270315, 0.5163537305367144, 0.4821115889692608, 0.554601405617557, 0.5177212712130969, 0.5428616695011719, 0.5283017215094988, 0.5123567226566846, 0.5303934999281847, 0.5816303280335439, 0.5290483341941351, 0.5672795996069908, 0.579266353309909, 0.5199603319545335, 0.4895725855721703, 0.5205931299472157, 0.5044250222323816, 0.5172668089029155, 0.48294846157107174, 0.5366536256255983, 0.5250458570220803, 0.5189918728568886, 0.5009268411918532, 0.5045282197526738, 0.5237355313346356, 0.511520393098457, 0.525018301002587, 0.510031249515618, 0.5295086546411997, 0.4966339695302746, 0.52833208498321, 0.49447107315063477, 0.5136264913444277, 0.5089094453031504, 0.5464415133376664, 0.5187408701528476, 0.5190243262656128, 0.5516869642689258, 0.5397132114519047, 0.5154607352576678, 0.5146083532820774, 0.5073600661339639, 0.5701480022148241, 0.5735466538728038, 0.5491804146691214, 0.5422320174454134, 0.5446482123453406, 0.5426935783669918, 0.5710887145015258, 0.5220382987887044, 0.5417104106915148, 0.5754493603223487, 0.533619965744924, 0.5762778842185117, 0.5338592627380467, 0.5573781338296359, 0.5375887101775483, 0.5312822029183183, 0.5451974290647085, 0.5270344963933848, 0.5519347873669637, 0.5583887022884586, 0.5747697632524031, 0.53732075910025, 0.544713698799097, 0.54281858609447, 0.5719384611407413, 0.5202449362111997, 0.5084290615742719, 0.5141259058366848, 0.5029577503475962, 0.5330851511864723, 0.5383445264040669, 0.5445871983147874, 0.5439003353254704, 0.5215786695480347, 0.5109690418349037, 0.530744302310521, 0.49466055395859704, 0.5258592778368841, 0.5579611717522899, 0.5201689512292041, 0.5321517630091196, 0.5553653376011909, 0.5559994182254695, 0.5492764958475209, 0.5437155616811559, 0.5355881104552294, 0.5277321902634222, 0.539707180819934, 0.5771049804325346, 0.5852305162933809, 0.5669170301171798, 0.5253017652261106, 0.5338942474579509, 0.5554900363653521, 0.5405757495119602, 0.5658218766315074, 0.5613152086734772, 0.5465129902468452, 0.5272626559945601, 0.5295005417134189, 0.5047299167777919, 0.49300466383559793, 0.5492192791609825, 0.553901323977905, 0.5375248759607726, 0.5450990347168113, 0.5740641949674751, 0.6136820884067801, 0.549185292064389, 0.5533378690103942, 0.5597253417289709, 0.5323915726776365, 0.5313113266720048, 0.5539356519149828, 0.5392684795056717, 0.5539341314306742, 0.5655378125890901, 0.5571187838346143, 0.5493852450877805, 0.541150254162052, 0.5641609287903279, 0.5385742417619198, 0.5644041978860204, 0.5555187009180649, 0.5374931981669197, 0.5337770737801926, 0.5295022774157645, 0.5276252516085589, 0.526966481646405, 0.5239434323356121, 0.5202406580689587, 0.5424046025057382, 0.5546739329642887, 0.5492268700765658, 0.5330030725726599, 0.5302840090250667, 0.559327697074866, 0.5229975948982601, 0.5579739920323408, 0.5483722508519511, 0.5500919239807732, 0.5255736416276497, 0.5679198548763613, 0.5679364106323146, 0.5342161246115649, 0.5500478484208071, 0.5313761402157289, 0.5704804831479169, 0.5673190015780775, 0.5638997756604907, 0.5896313563932346, 0.5433095578528657, 0.5565140822642967, 0.5584272151883645, 0.5537656983242759, 0.548184348058097, 0.5406601837352861, 0.555926272952104, 0.5514698358653467]\n",
        "test_acc_list_300const = [53.98, 65.75, 72.1, 76.14, 77.31, 77.93, 81.02, 81.31, 82.75, 82.79, 83.69, 84.8, 85.35, 83.73, 83.79, 85.27, 84.08, 86.95, 86.6, 86.76, 87.11, 87.3, 88.07, 88.24, 87.9, 86.96, 88.42, 88.0, 88.7, 88.74, 87.84, 87.77, 88.76, 88.95, 88.48, 88.71, 89.77, 88.58, 89.26, 89.81, 89.22, 88.58, 89.44, 89.89, 89.57, 89.96, 89.93, 89.62, 88.5, 89.02, 89.32, 89.09, 90.25, 89.9, 90.15, 89.83, 90.56, 89.5, 90.0, 89.84, 89.55, 90.01, 90.27, 89.89, 90.63, 90.74, 89.52, 90.6, 90.0, 90.07, 90.5, 91.04, 91.19, 90.84, 90.5, 89.92, 90.31, 90.59, 90.56, 90.44, 90.87, 90.43, 90.47, 91.12, 90.58, 90.7, 90.83, 90.28, 91.18, 91.08, 90.76, 90.93, 91.05, 91.44, 91.19, 90.73, 91.14, 90.84, 91.04, 91.48, 90.9, 91.52, 91.12, 91.39, 90.99, 90.96, 91.09, 91.44, 91.0, 90.32, 91.25, 91.42, 91.16, 91.08, 90.75, 91.24, 91.32, 90.74, 90.9, 91.13, 91.6, 91.41, 91.7, 91.22, 90.61, 91.46, 90.95, 91.5, 91.36, 91.43, 91.4, 90.91, 91.13, 91.28, 91.29, 91.08, 91.73, 91.34, 91.19, 91.15, 91.59, 91.75, 91.13, 91.5, 91.47, 91.33, 91.42, 91.52, 90.83, 91.11, 91.09, 90.95, 91.6, 91.74, 91.37, 91.74, 91.61, 91.83, 90.89, 91.51, 91.79, 91.72, 92.0, 91.68, 91.82, 91.75, 91.83, 91.66, 91.79, 91.57, 91.82, 91.86, 91.98, 91.55, 91.74, 91.64, 91.27, 91.43, 91.88, 91.69, 92.03, 91.23, 91.2, 91.33, 91.81, 91.53, 91.45, 91.15, 91.51, 90.91, 91.23, 91.95, 91.46, 91.29, 91.48, 91.44, 91.69, 91.54, 91.51, 91.46, 91.42, 91.05, 91.18, 91.19, 91.56, 91.29, 91.73, 91.76, 91.56, 91.49, 91.62, 91.53, 91.7, 91.61, 91.97, 91.83, 92.04, 91.91, 91.65, 91.62, 91.6, 91.81, 91.53, 91.51, 91.84, 91.74, 91.75, 91.74, 91.9, 91.56, 91.06, 91.42, 91.51, 91.83, 91.39, 91.63, 91.73, 91.66, 91.55, 91.7, 91.4, 91.93, 92.04, 91.7, 91.47, 91.45, 91.64, 91.32, 90.81, 91.54, 91.31, 91.47, 92.03, 91.83, 91.86, 91.87, 91.91, 91.64, 91.84, 91.63, 91.8, 91.87, 92.0, 91.71, 91.79, 91.94, 92.08, 91.91, 92.31, 92.23, 92.29, 92.38, 92.07, 92.0, 91.81, 92.24, 92.44, 92.16, 92.14, 91.99, 91.69, 91.77, 92.03, 91.96, 91.84, 92.05, 91.96, 91.89, 92.32, 91.83, 91.85, 91.69, 92.02, 91.96, 91.93, 92.11, 91.92, 92.17, 91.77, 92.18]\n",
        "train_loss_list_300cosine = [1.5071465310197287, 1.0219155532864337, 0.7846573638839843, 0.6613810150958479, 0.5711964959153732, 0.5125089824770968, 0.4690145238900718, 0.4279545375142996, 0.38901165142036476, 0.3574826151799089, 0.32995736313323243, 0.3070905926033331, 0.29151374520585177, 0.27276346844415694, 0.2528251954637016, 0.2403833875164818, 0.21865639683251945, 0.21172414217798854, 0.1913816854833795, 0.18111336490692803, 0.1749152649943821, 0.16864872572663875, 0.15266839607645527, 0.1438077868649754, 0.131705386099962, 0.12894651846002086, 0.12342969215097138, 0.11197239956773888, 0.10989574266198915, 0.10079097377058988, 0.08904699315302098, 0.09436483237856684, 0.08817791208898583, 0.08477092711939313, 0.08056267116147393, 0.07277504891490404, 0.07563522219146117, 0.07128996417795222, 0.057986204402324874, 0.06231818245218013, 0.05127317121716591, 0.05075172336099628, 0.05728455614774657, 0.05022484378014414, 0.047201051832007145, 0.04299503102446326, 0.04491299623028396, 0.03698193365499711, 0.03486268078765502, 0.03927968754864539, 0.03724308135345007, 0.03970887712738551, 0.03285741158493292, 0.028173236158518744, 0.028217624198978605, 0.029066849599247234, 0.028810474211156678, 0.026399405389732587, 0.025763797607963174, 0.022101726805115827, 0.0206387186558328, 0.023366123397712605, 0.019345413963012873, 0.024243142143205797, 0.02154286678515214, 0.01816358644301042, 0.021265280448992292, 0.015330405338271595, 0.017995145478354284, 0.014342598256486924, 0.015467149512258362, 0.016130026668673175, 0.013699235769971377, 0.011888031450363573, 0.010789586022736494, 0.010900068600833514, 0.009051817458605739, 0.010178615377595523, 0.011202018590738473, 0.014404841935349116, 0.01132253416424867, 0.010159075095953521, 0.008117529459464283, 0.009489624014945505, 0.010137052083415494, 0.008486186185057242, 0.006968282311358831, 0.007533594105837527, 0.008950318053382235, 0.008889065595822946, 0.007539918537286241, 0.0057594785649967666, 0.006437454009137117, 0.005282934063498991, 0.004347972699934572, 0.005166493265034323, 0.004321007760212431, 0.00471395535047535, 0.0032943137242381028, 0.004311655514717282, 0.003059743971269304, 0.0039670848353404575, 0.0030527470893090434, 0.003868208768115135, 0.0034133147897016363, 0.004668654355293675, 0.0059490043992545235, 0.0041872456292248, 0.0038983581991906308, 0.0037588840379914716, 0.003916245875395213, 0.0034797456036773718, 0.0029193548814658767, 0.0028640743580595858, 0.0023959889678485742, 0.0022046099687237837, 0.00213721157060805, 0.001905863403416865, 0.0018298605383502951, 0.0019732695092532088, 0.0012731390455790247, 0.0014643412322186396, 0.0014584561287045482, 0.0013225552577550437, 0.0012107872296070626, 0.0009283569928718647, 0.0009475817591004316, 0.0009087429049996629, 0.0012826446498267045, 0.0011837309814490989, 0.0007089067522449444, 0.0007332131970385923, 0.0012466143751911653, 0.0009211793556768187, 0.000789917121722373, 0.0007207691697882189, 0.0007975770474887959, 0.0006758498559978029, 0.0007351151072098131, 0.0010837652537124284, 0.000724128287233449, 0.0005810306201093711, 0.00047435187363775613, 0.0004135937659276342, 0.0007063415897789764, 0.000588239943727893, 0.0005826207090225168, 0.0003262833067903336, 0.00038663124462745416, 0.0004899335792227164, 0.00045023311883942547, 0.00032505322259691985, 0.0004240722920772437, 0.000434831311982059, 0.0005759489223388422, 0.0003049708239195287, 0.00034293277186921384, 0.00026971405763318425, 0.00031280760296981017, 0.0007162304362590164, 0.0005976533334854375, 0.00044388456952353377, 0.00032143614894833265, 0.0003041571174924992, 0.00035453209945005774, 0.00023628432779605936, 0.00022958813212190958, 0.00018880277622619602, 0.00021482868581873839, 0.0001753957263578302, 0.00041403284841927493, 0.00021361697475566445, 0.0002520289759267065, 0.00019671708842346002, 0.00019921282705291114, 0.00030435736312682684, 0.00029258786672637244, 0.00027827393721283804, 0.0002795331743087422, 0.00024584021705562536, 0.00020301578391222058, 0.00015932558792188886, 0.00035799621748254593, 0.00028251744383299383, 0.00021141435477205867, 0.00022243324366817674, 0.00020670883468984464, 0.00021545639445687408, 0.00016235478455645368, 0.00022787957680421982, 0.00016665564254608194, 0.0001477420311949244, 0.00018142510538173126, 0.00034612401611176406, 0.00015685088123029036, 0.00019157101443428062, 0.00019864877790692532, 0.00020180520096545342, 0.00012184150966041865, 0.00010955794329265264, 0.0002272417015756974, 0.00012062133251651577, 0.00024990162183020963, 0.00019493304157317115, 0.00011319477053125949, 0.0001146485327846252, 0.0001126736392463569, 0.00013831783792320434, 0.0001353729226037266, 0.0001630674412422994, 0.0001457879433627867, 0.00012657002445250193, 0.00020756496892448712, 0.0001576078423556483, 0.00016432452238864054, 0.00013326239761994157, 0.000126308732199333, 0.00011329932245867901, 9.373737983756487e-05, 0.00012264910064071485, 9.72924513225386e-05, 9.851183427198616e-05, 0.00014160107466290727, 0.00019542356384294292, 0.00010866884136099055, 0.00012103338623344784, 0.00010338596340528902, 0.00012947641171964864, 0.00016585586701756884, 0.00012378471780928916, 0.00012914159678432927, 8.873532887934217e-05, 0.00012277717895865415, 0.00010324342793536724, 0.00011429319050957435, 0.00014108263730566994, 0.00011344413554959498, 0.0001314912391754855, 0.00014366292956832224, 0.00010670398796356165, 9.395798729230545e-05, 9.690826291712445e-05, 8.27918457593515e-05, 7.49523732641198e-05, 0.000116572610751607, 0.00010861793663089236, 0.00010167052199556757, 0.0001307339418356521, 0.00010273090034757616, 0.00012697175096751327, 0.00012768910120719, 7.014640353397078e-05, 9.454710011857551e-05, 0.00011274885802570828, 0.000126904926870751, 9.48965783575255e-05, 9.193459106337249e-05, 8.93238301681306e-05, 0.00010053854063013211, 0.00010620467252830129, 0.00014532459377346714, 8.020320023135105e-05, 7.178885715230178e-05, 0.00011935103254135713, 9.011012685912722e-05, 8.608197714011839e-05, 9.837138899922847e-05, 0.00010366790116276302, 8.538921241570249e-05, 0.0001675107899030607, 7.792623628388836e-05, 6.928247518943407e-05, 8.20774587304151e-05, 0.00011248917587745207, 7.720731971381645e-05, 6.833156445040754e-05, 0.00010231887682267187, 9.609293597030557e-05, 7.292958810775106e-05, 0.00010774314482761675, 9.255914947628301e-05, 0.0001237650751620048, 9.64451171900979e-05, 0.0001169090761506665, 0.0001072614105620238, 9.686175892860217e-05, 0.00011957684801773822, 0.00011583886435356528, 0.0001275175088925282, 6.546866618231805e-05, 9.038513419941541e-05, 0.00010006681310687346, 9.007086580128472e-05, 0.00022282703456296465, 8.881661869528004e-05, 8.626907432722472e-05, 9.961434439616733e-05, 0.0001082291823644854, 0.00010509098167338485, 7.50494904148657e-05]\n",
        "train_acc_list_300cosine = [44.7275, 63.7, 72.3575, 76.9475, 80.11, 82.24, 83.6775, 85.0225, 86.4575, 87.51, 88.3775, 89.1875, 89.8275, 90.4725, 91.27, 91.6125, 92.195, 92.6575, 93.2125, 93.6125, 93.7775, 93.95, 94.65, 94.79, 95.3675, 95.5025, 95.58, 96.0475, 96.075, 96.4175, 96.79, 96.66, 96.8425, 97.0175, 97.1425, 97.4075, 97.345, 97.375, 97.945, 97.73, 98.185, 98.1775, 98.0075, 98.175, 98.36, 98.465, 98.4525, 98.72, 98.815, 98.635, 98.6625, 98.58, 98.825, 99.075, 99.0425, 98.985, 98.965, 99.1025, 99.0875, 99.26, 99.2775, 99.1925, 99.3375, 99.1475, 99.2425, 99.425, 99.2375, 99.475, 99.38, 99.505, 99.465, 99.4675, 99.5625, 99.605, 99.64, 99.6425, 99.71, 99.66, 99.625, 99.4775, 99.625, 99.6725, 99.745, 99.7, 99.67, 99.7325, 99.785, 99.7425, 99.675, 99.6725, 99.7625, 99.83, 99.7925, 99.8025, 99.8675, 99.8425, 99.8625, 99.85, 99.905, 99.8775, 99.9, 99.8825, 99.895, 99.8625, 99.9025, 99.8625, 99.7975, 99.8725, 99.87, 99.89, 99.88, 99.87, 99.9125, 99.915, 99.9375, 99.9475, 99.94, 99.945, 99.94, 99.9425, 99.9775, 99.96, 99.9575, 99.965, 99.9725, 99.975, 99.98, 99.9825, 99.9675, 99.965, 99.9925, 99.985, 99.9675, 99.975, 99.9725, 99.975, 99.98, 99.985, 99.9875, 99.9725, 99.9825, 99.9925, 99.9925, 99.9975, 99.98, 99.99, 99.9875, 99.9975, 99.995, 99.985, 99.99, 100.0, 99.9925, 99.9875, 99.9825, 100.0, 99.9975, 99.9975, 99.9925, 99.9825, 99.9875, 99.99, 99.9975, 99.9975, 99.9925, 100.0, 99.9975, 99.9975, 100.0, 100.0, 99.985, 99.9975, 99.9975, 99.995, 99.9975, 99.9975, 99.9975, 99.9975, 99.9925, 99.995, 99.9975, 100.0, 99.9875, 99.9925, 100.0, 99.995, 100.0, 99.995, 100.0, 99.9925, 100.0, 100.0, 99.9975, 99.99, 100.0, 99.9975, 99.995, 99.995, 100.0, 100.0, 99.9925, 100.0, 99.995, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 99.9975, 99.995, 99.9975, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9925, 100.0, 100.0, 100.0, 99.9975, 99.9975, 100.0, 99.9975, 100.0, 99.9975, 100.0, 99.9975, 100.0, 99.9975, 99.9975, 99.9975, 100.0, 100.0, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.995, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 99.9975, 99.9975, 100.0, 100.0, 100.0, 100.0, 99.9975, 99.9975, 100.0, 100.0, 99.9975, 100.0, 99.9925, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
        "test_loss_list_300cosine = [1.3360588429849358, 1.0448881915852994, 0.8597957295707509, 0.6716653414164917, 0.6908234833162042, 0.5932456066336813, 0.5461285729197007, 0.5097410641139066, 0.5401990885221506, 0.5333668193485164, 0.4556268532819386, 0.4484600973657415, 0.4620721623112884, 0.4327629621647581, 0.47597266639335245, 0.48141292925876905, 0.49725772666780255, 0.40901502231253856, 0.4323547872938687, 0.4539363452150852, 0.4590581006641629, 0.40523579641233515, 0.4201337242428261, 0.418421223005162, 0.41465936016432847, 0.43959930557993393, 0.3880773431892636, 0.397792113732688, 0.4114573869524123, 0.3806772181127645, 0.40738119260419775, 0.41724546894996983, 0.3961485907246795, 0.40635761147058463, 0.41718924026700516, 0.4624185173571864, 0.4003025793199298, 0.4194344795202907, 0.43098732470711576, 0.42882050819034817, 0.4735664082101629, 0.4482132949029343, 0.4824381698913212, 0.489483314035814, 0.42275018914590906, 0.46854421854773654, 0.46698710643037966, 0.45736848336609104, 0.48327861801732946, 0.42990076523038406, 0.4146351450228993, 0.4377061692596991, 0.4491257427991191, 0.4464895281987854, 0.5250044391502308, 0.4606155977596211, 0.48777404917946343, 0.46350483313391483, 0.45923378441152696, 0.47999939944925185, 0.4535947281725799, 0.45670295101177844, 0.4861161712604233, 0.5072417436521265, 0.45164442854591563, 0.4723219115145599, 0.465178365948834, 0.48657907800206657, 0.4763214318435403, 0.4628955114491378, 0.4531608254094667, 0.4590076025150999, 0.48202957307236105, 0.47070856392383575, 0.4524376158472858, 0.5099793840435487, 0.4644981126619291, 0.4989790280785742, 0.4910671897704088, 0.5239907608756537, 0.4774412533527688, 0.4662430659879612, 0.47549027996727183, 0.49829778188391577, 0.4727332616531396, 0.48873594582458085, 0.4645166827153556, 0.4946923785949055, 0.5139467286158211, 0.504527775924417, 0.4836999862631665, 0.4878289546770386, 0.49920391819522353, 0.48420851479602767, 0.4741327323113816, 0.4969299920374834, 0.492643963309783, 0.4669509222613105, 0.5004429721002337, 0.5008861348976062, 0.48978376954416686, 0.48975364958183676, 0.4894806569135642, 0.4939129582688778, 0.5057695177914221, 0.483693698161765, 0.49143673158899137, 0.508361778493169, 0.49146506284611136, 0.5179333079464828, 0.4985507955845398, 0.48249870953680596, 0.4882995514552804, 0.48072621575261976, 0.4871305170692975, 0.495542240859587, 0.502074130728275, 0.4848601612486417, 0.46474564952563635, 0.45505768362479876, 0.480818674534182, 0.48904268066339857, 0.48706334348343594, 0.5078222957215731, 0.48777740590180024, 0.48671044654484036, 0.4677445718759223, 0.4843851542925533, 0.47715435280830043, 0.4916676714827743, 0.48873102721534195, 0.4853220989432516, 0.495768607794484, 0.48384790348855755, 0.494273115373865, 0.520792075538937, 0.48109641991838625, 0.4784795025104209, 0.5150823619546769, 0.49928266564501994, 0.508841446871999, 0.4865191377039197, 0.49814116068278685, 0.4979177566268776, 0.509591865388653, 0.5287647111506402, 0.5030483018748367, 0.47968817134446734, 0.5014746417350406, 0.5026743098904815, 0.5065745726416383, 0.4991808756242824, 0.49776733959022956, 0.4859552224980125, 0.505850138354905, 0.507838894294787, 0.4913963109632082, 0.4985096752643585, 0.5035310268779344, 0.49438839943348606, 0.5127879598095447, 0.49104869393985484, 0.4965999275445938, 0.500093090760557, 0.5078368173747123, 0.49182856611058684, 0.4928507318225088, 0.5030448830957654, 0.4847751912436908, 0.5120558350146571, 0.4888622204336939, 0.49651806179103974, 0.49612243831912173, 0.4962354428783248, 0.513516125611112, 0.49529338731795924, 0.4980339683309386, 0.5014159933109826, 0.49898972069915337, 0.507745862573008, 0.52068718268147, 0.5003404364555697, 0.525249100938628, 0.5042423366368571, 0.5168220468714267, 0.5160682231565065, 0.5124318371467953, 0.5218248301291768, 0.4837766302914559, 0.5026627190505402, 0.5024134409201296, 0.512225415887712, 0.5186735810358313, 0.5077592483426951, 0.4826306046187123, 0.512151868848861, 0.5008148086976402, 0.4929636383924303, 0.4970756280648557, 0.5172987021977389, 0.4966934053580972, 0.5119075952451441, 0.4916618470149704, 0.5191661170389079, 0.5202390191298497, 0.5187712847432003, 0.5312336112690877, 0.5149811487409133, 0.5015346613488619, 0.4897740053225167, 0.5123436318922646, 0.4950245019001297, 0.5100116333629512, 0.517671759935874, 0.49631667703012877, 0.5063869094924082, 0.5044590200804457, 0.5116969626161116, 0.5092341914018498, 0.5033747812237921, 0.5193329711880865, 0.5127834038266653, 0.5309269350918033, 0.49558807419070716, 0.513400316049781, 0.5085111194396321, 0.510913455599471, 0.4971718424105946, 0.5032308197851423, 0.5266274438251423, 0.49809465491319005, 0.5013407922432392, 0.4893049780703798, 0.5038147557385361, 0.5071697546334206, 0.5146695088736618, 0.49518201302123976, 0.5041418398105646, 0.5182757000379925, 0.5224867431046087, 0.4978565979230253, 0.507878503939019, 0.49611272823206987, 0.5108789631837531, 0.5073691453737549, 0.5223453206163419, 0.5107548187805128, 0.5007183399004272, 0.5212629056429561, 0.49115426876122437, 0.5297577135925051, 0.5027483824310424, 0.5041316181798524, 0.4906521471618097, 0.493595665768732, 0.502438643687888, 0.4948954885896248, 0.4982064822806588, 0.5073866029328937, 0.5064849340462987, 0.5071891489662702, 0.5056039866390107, 0.5256269764673861, 0.4950309816417815, 0.5119404613594466, 0.4975051161231874, 0.4961673260866841, 0.5196594443125061, 0.4737729119914996, 0.5019824165332166, 0.4959078891367852, 0.5141448318203793, 0.49820214887208575, 0.5068159803182264, 0.4858330869221989, 0.496248337852804, 0.4940428052899204, 0.503951388069346, 0.49354483621029915, 0.4873908001788055, 0.5022429193876967, 0.50107072425794, 0.49141834751714636, 0.50169266326518, 0.4897840660584124, 0.5037467987099781, 0.501852917708928, 0.5113212121061131, 0.5046284511873994, 0.4889931109132646, 0.507928684731073, 0.4926022017681146, 0.5067250309865686, 0.510816790823695, 0.5253809748948375, 0.5128096621247786, 0.4980110616623601, 0.5035155990832969, 0.5060317163603215, 0.49634331974047646]\n",
        "test_acc_list_300cosine = [52.55, 63.48, 70.79, 76.77, 77.07, 79.66, 81.7, 82.61, 82.36, 83.15, 84.72, 85.19, 85.13, 85.7, 85.08, 84.65, 84.64, 87.17, 86.56, 86.12, 86.83, 87.48, 87.37, 87.5, 88.14, 87.33, 89.04, 88.78, 88.32, 89.27, 88.82, 88.61, 89.75, 88.78, 89.11, 88.37, 89.51, 89.38, 88.67, 89.45, 88.48, 89.32, 89.16, 88.72, 89.82, 89.02, 88.77, 89.55, 89.58, 90.05, 90.18, 90.23, 89.78, 90.12, 89.23, 89.82, 89.9, 89.95, 89.87, 90.25, 90.38, 90.39, 90.25, 89.66, 90.88, 90.49, 90.4, 90.26, 90.13, 91.0, 90.89, 90.98, 90.93, 90.94, 91.42, 90.54, 91.11, 90.89, 90.63, 90.02, 91.17, 90.81, 91.24, 90.97, 91.14, 91.35, 91.47, 91.32, 90.83, 90.87, 91.23, 90.95, 91.06, 91.39, 91.41, 90.98, 91.19, 91.65, 91.43, 90.97, 90.93, 91.36, 91.17, 91.47, 91.56, 91.59, 91.49, 90.95, 91.39, 91.46, 91.51, 91.64, 91.6, 91.65, 91.39, 91.51, 91.58, 92.1, 91.79, 91.97, 91.75, 91.74, 91.57, 91.96, 91.91, 92.0, 92.15, 92.02, 91.96, 91.79, 91.77, 92.07, 91.99, 92.41, 92.06, 91.7, 92.06, 92.22, 91.85, 91.93, 92.2, 92.1, 92.04, 91.91, 91.85, 91.94, 91.89, 92.15, 92.18, 91.94, 92.03, 92.19, 92.17, 92.45, 92.43, 91.98, 92.28, 92.25, 92.4, 92.24, 92.02, 92.17, 92.01, 92.24, 92.28, 92.14, 92.37, 92.14, 92.25, 92.3, 92.22, 92.12, 92.18, 92.11, 92.29, 92.04, 92.02, 92.2, 92.37, 92.34, 92.25, 92.27, 91.84, 92.13, 92.18, 91.97, 92.19, 91.85, 91.98, 92.21, 92.11, 92.25, 92.17, 92.13, 92.4, 92.37, 92.21, 92.18, 92.44, 92.29, 92.36, 92.11, 92.35, 92.51, 91.96, 92.18, 92.12, 92.41, 92.3, 92.57, 92.18, 92.45, 92.11, 92.13, 92.52, 92.18, 92.1, 92.1, 92.24, 92.59, 92.23, 92.27, 92.27, 92.48, 92.31, 92.38, 92.04, 92.33, 92.33, 92.15, 92.27, 92.48, 92.52, 92.61, 92.2, 92.3, 92.31, 92.55, 92.25, 92.12, 92.24, 92.12, 92.47, 92.29, 92.22, 92.34, 92.4, 92.34, 92.23, 92.56, 92.08, 92.09, 92.33, 92.22, 92.44, 92.34, 92.41, 92.41, 92.2, 92.35, 92.42, 92.43, 91.96, 92.21, 92.27, 92.41, 92.21, 92.36, 92.66, 92.32, 92.33, 92.37, 92.2, 92.16, 92.31, 92.34, 92.35, 92.43, 92.49, 92.47, 92.15, 92.23, 92.61, 92.26, 92.76, 92.44, 92.32, 92.18, 92.38, 92.38, 92.28, 92.54, 92.5, 92.4, 92.13, 92.37, 92.31, 92.37, 92.12, 92.62]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ubm3Y5CSnZ0D",
        "outputId": "4a06cd0a-11fc-421f-a89e-36d4ba3bb09b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_acc_list_01)), train_acc_list_01, 'b')\n",
        "plt.plot(range(len(train_acc_list_001)), train_acc_list_001, 'r')\n",
        "plt.plot(range(len(train_acc_list_0001)), train_acc_list_0001, 'g')\n",
        "\n",
        "plt.plot(range(len(test_acc_list_01)), test_acc_list_01, color='b', linestyle='--')\n",
        "plt.plot(range(len(test_acc_list_001)), test_acc_list_001,color='r', linestyle='--')\n",
        "plt.plot(range(len(test_acc_list_0001)), test_acc_list_0001, color='g', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Combined accuracy\")\n",
        "plt.legend(['train with lr = 1e-1', 'train with lr = 1e-2','train with lr = 1e-3',\n",
        "            'test with lr = 1e-1','test with lr = 1e-2', 'test with lr = 1e-3'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "QnOTaNoesBEb",
        "outputId": "a9d14de7-2779-4113-826f-b39d684e7a75"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_loss_list_01)), train_loss_list_01, 'b')\n",
        "plt.plot(range(len(train_loss_list_001)), train_loss_list_001, 'r')\n",
        "plt.plot(range(len(train_loss_list_0001)), train_loss_list_0001, 'g')\n",
        "\n",
        "plt.plot(range(len(test_loss_list_01)), test_loss_list_01, color='b', linestyle='--')\n",
        "plt.plot(range(len(test_loss_list_001)), test_loss_list_001,color='r', linestyle='--')\n",
        "plt.plot(range(len(test_loss_list_0001)), test_loss_list_0001, color='g', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Combined loss\")\n",
        "plt.legend(['train with lr = 1e-1', 'train with lr = 1e-2','train with lr = 1e-3',\n",
        "            'test with lr = 1e-1','test with lr = 1e-2', 'test with lr = 1e-3'])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "2d35a857fdeedecb30594b1c7eb95a8c0480700735195f416faf3d51f501baa5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
