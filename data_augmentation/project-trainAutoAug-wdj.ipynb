{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "75eb9200e74b4078bd8e8e09a491b8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd0903d776cf44cab69229fbffb6ec2b",
              "IPY_MODEL_5914d62d1ef9442598e9cc900843613b",
              "IPY_MODEL_e7b4a8d1bac34d28b2a6ad0132130b9e"
            ],
            "layout": "IPY_MODEL_1f207da1f7ed4abd97dcdeca163cf629"
          }
        },
        "bd0903d776cf44cab69229fbffb6ec2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e431e4aa5854581897368c85b90e446",
            "placeholder": "​",
            "style": "IPY_MODEL_e07d2e319e3b46d6bf81625e40cd4968",
            "value": "100%"
          }
        },
        "5914d62d1ef9442598e9cc900843613b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce3145d3030f422c8712b33a37f18258",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35ad9df8dc23478eb7fd6f4872edd6bd",
            "value": 182040794
          }
        },
        "e7b4a8d1bac34d28b2a6ad0132130b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60b2713c3a304ff898c64b6acef8764a",
            "placeholder": "​",
            "style": "IPY_MODEL_9c367d63be624821bf33043d47157c98",
            "value": " 182040794/182040794 [00:02&lt;00:00, 107525993.25it/s]"
          }
        },
        "1f207da1f7ed4abd97dcdeca163cf629": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e431e4aa5854581897368c85b90e446": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e07d2e319e3b46d6bf81625e40cd4968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce3145d3030f422c8712b33a37f18258": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35ad9df8dc23478eb7fd6f4872edd6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60b2713c3a304ff898c64b6acef8764a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c367d63be624821bf33043d47157c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "621798e5c7a94d6c88056e55e39d8a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13f12ebb293746d6a4c26028400b20b6",
              "IPY_MODEL_3422efda4b4c4f0899ce3fd29aeb9366",
              "IPY_MODEL_ccc04cd6ec22424b911e8b5fb03e4945"
            ],
            "layout": "IPY_MODEL_b76bbeb58add499698b313f7e6e52e05"
          }
        },
        "13f12ebb293746d6a4c26028400b20b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a9fdf30c0334c95b9f7a0c6a8eaeca9",
            "placeholder": "​",
            "style": "IPY_MODEL_a8f7ee6088b64b60a3feea8a0f418d31",
            "value": "100%"
          }
        },
        "3422efda4b4c4f0899ce3fd29aeb9366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16b8d466ac734a0eb4bbdc556731e4be",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_848797e922614d87a0b042129fe37602",
            "value": 64275384
          }
        },
        "ccc04cd6ec22424b911e8b5fb03e4945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e2d20821a144c9cb7d3b5747f7afde3",
            "placeholder": "​",
            "style": "IPY_MODEL_3107253601a3425fa818051daf26278d",
            "value": " 64275384/64275384 [00:00&lt;00:00, 93889388.69it/s]"
          }
        },
        "b76bbeb58add499698b313f7e6e52e05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a9fdf30c0334c95b9f7a0c6a8eaeca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f7ee6088b64b60a3feea8a0f418d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16b8d466ac734a0eb4bbdc556731e4be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "848797e922614d87a0b042129fe37602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e2d20821a144c9cb7d3b5747f7afde3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3107253601a3425fa818051daf26278d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bHuJUs3ir2g2"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.SVHN),\n",
        "    transforms.ToTensor(),\n",
        "    \n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "75eb9200e74b4078bd8e8e09a491b8c0",
            "bd0903d776cf44cab69229fbffb6ec2b",
            "5914d62d1ef9442598e9cc900843613b",
            "e7b4a8d1bac34d28b2a6ad0132130b9e",
            "1f207da1f7ed4abd97dcdeca163cf629",
            "3e431e4aa5854581897368c85b90e446",
            "e07d2e319e3b46d6bf81625e40cd4968",
            "ce3145d3030f422c8712b33a37f18258",
            "35ad9df8dc23478eb7fd6f4872edd6bd",
            "60b2713c3a304ff898c64b6acef8764a",
            "9c367d63be624821bf33043d47157c98",
            "621798e5c7a94d6c88056e55e39d8a7c",
            "13f12ebb293746d6a4c26028400b20b6",
            "3422efda4b4c4f0899ce3fd29aeb9366",
            "ccc04cd6ec22424b911e8b5fb03e4945",
            "b76bbeb58add499698b313f7e6e52e05",
            "7a9fdf30c0334c95b9f7a0c6a8eaeca9",
            "a8f7ee6088b64b60a3feea8a0f418d31",
            "16b8d466ac734a0eb4bbdc556731e4be",
            "848797e922614d87a0b042129fe37602",
            "7e2d20821a144c9cb7d3b5747f7afde3",
            "3107253601a3425fa818051daf26278d"
          ]
        },
        "id": "Ow5ZNogRuEpq",
        "outputId": "72982738-e633-41ed-c553-38bba6207505"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75eb9200e74b4078bd8e8e09a491b8c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "621798e5c7a94d6c88056e55e39d8a7c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))\n",
        "print(trainset[4][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExjAMaRquGPp",
        "outputId": "86b8e9dc-c4f5-4bc8-f9c4-5af47486501d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "metadata": {
        "id": "PoGC4Zo7uIX9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "metadata": {
        "id": "HLTkfp-wuKXN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x4c_HGo6uM22"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # This is the \"stem\"\n",
        "        # For CIFAR (32x32 images), it does not perform downsampling\n",
        "        # It should downsample for ImageNet\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "metadata": {
        "id": "KAh57cWQuOmQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "I8CXMbsguP81",
        "outputId": "fad37cd9-6d0d-4165-d3e4-e0fe0d587f3c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSHmDHpkuRe5",
        "outputId": "e81c02c3-274e-4f0b-f4fc-8ce58be1d4d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFogi3EguTDh",
        "outputId": "be0d4a46-94d4-4594-c52b-2fae841c48e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3374, accuracy : 17.53\n",
            "iteration : 100, loss : 2.2384, accuracy : 20.60\n",
            "iteration : 150, loss : 2.0340, accuracy : 28.31\n",
            "iteration : 200, loss : 1.7671, accuracy : 38.18\n",
            "iteration : 250, loss : 1.5559, accuracy : 45.89\n",
            "iteration : 300, loss : 1.3921, accuracy : 51.91\n",
            "iteration : 350, loss : 1.2714, accuracy : 56.38\n",
            "Epoch :   1, training loss : 1.2340, training accuracy : 57.70, test loss : 0.8547, test accuracy : 72.76\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.4817, accuracy : 84.75\n",
            "iteration : 100, loss : 0.4713, accuracy : 85.08\n",
            "iteration : 150, loss : 0.4692, accuracy : 85.32\n",
            "iteration : 200, loss : 0.4663, accuracy : 85.49\n",
            "iteration : 250, loss : 0.4629, accuracy : 85.53\n",
            "iteration : 300, loss : 0.4582, accuracy : 85.74\n",
            "iteration : 350, loss : 0.4550, accuracy : 85.81\n",
            "Epoch :   2, training loss : 0.4539, training accuracy : 85.87, test loss : 0.6585, test accuracy : 79.31\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.4070, accuracy : 87.56\n",
            "iteration : 100, loss : 0.4147, accuracy : 87.33\n",
            "iteration : 150, loss : 0.4144, accuracy : 87.36\n",
            "iteration : 200, loss : 0.4085, accuracy : 87.39\n",
            "iteration : 250, loss : 0.4029, accuracy : 87.55\n",
            "iteration : 300, loss : 0.4028, accuracy : 87.55\n",
            "iteration : 350, loss : 0.3995, accuracy : 87.69\n",
            "Epoch :   3, training loss : 0.3989, training accuracy : 87.69, test loss : 0.4442, test accuracy : 86.37\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3800, accuracy : 88.17\n",
            "iteration : 100, loss : 0.3738, accuracy : 88.56\n",
            "iteration : 150, loss : 0.3733, accuracy : 88.67\n",
            "iteration : 200, loss : 0.3730, accuracy : 88.66\n",
            "iteration : 250, loss : 0.3736, accuracy : 88.54\n",
            "iteration : 300, loss : 0.3708, accuracy : 88.62\n",
            "iteration : 350, loss : 0.3705, accuracy : 88.70\n",
            "Epoch :   4, training loss : 0.3695, training accuracy : 88.76, test loss : 0.3682, test accuracy : 88.90\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.3472, accuracy : 89.78\n",
            "iteration : 100, loss : 0.3548, accuracy : 89.38\n",
            "iteration : 150, loss : 0.3568, accuracy : 89.19\n",
            "iteration : 200, loss : 0.3628, accuracy : 89.04\n",
            "iteration : 250, loss : 0.3617, accuracy : 89.06\n",
            "iteration : 300, loss : 0.3623, accuracy : 89.09\n",
            "iteration : 350, loss : 0.3583, accuracy : 89.16\n",
            "Epoch :   5, training loss : 0.3572, training accuracy : 89.19, test loss : 0.4941, test accuracy : 84.18\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.3349, accuracy : 89.73\n",
            "iteration : 100, loss : 0.3429, accuracy : 89.59\n",
            "iteration : 150, loss : 0.3416, accuracy : 89.70\n",
            "iteration : 200, loss : 0.3398, accuracy : 89.77\n",
            "iteration : 250, loss : 0.3401, accuracy : 89.72\n",
            "iteration : 300, loss : 0.3414, accuracy : 89.77\n",
            "iteration : 350, loss : 0.3367, accuracy : 89.96\n",
            "Epoch :   6, training loss : 0.3365, training accuracy : 89.96, test loss : 0.3691, test accuracy : 88.76\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.3046, accuracy : 90.84\n",
            "iteration : 100, loss : 0.3284, accuracy : 90.24\n",
            "iteration : 150, loss : 0.3245, accuracy : 90.36\n",
            "iteration : 200, loss : 0.3270, accuracy : 90.26\n",
            "iteration : 250, loss : 0.3255, accuracy : 90.33\n",
            "iteration : 300, loss : 0.3220, accuracy : 90.45\n",
            "iteration : 350, loss : 0.3249, accuracy : 90.38\n",
            "Epoch :   7, training loss : 0.3232, training accuracy : 90.43, test loss : 0.3590, test accuracy : 89.15\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.3226, accuracy : 90.36\n",
            "iteration : 100, loss : 0.3229, accuracy : 90.38\n",
            "iteration : 150, loss : 0.3229, accuracy : 90.23\n",
            "iteration : 200, loss : 0.3147, accuracy : 90.52\n",
            "iteration : 250, loss : 0.3171, accuracy : 90.48\n",
            "iteration : 300, loss : 0.3165, accuracy : 90.53\n",
            "iteration : 350, loss : 0.3133, accuracy : 90.63\n",
            "Epoch :   8, training loss : 0.3139, training accuracy : 90.62, test loss : 0.3806, test accuracy : 88.55\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.3209, accuracy : 90.50\n",
            "iteration : 100, loss : 0.3091, accuracy : 90.89\n",
            "iteration : 150, loss : 0.3074, accuracy : 90.98\n",
            "iteration : 200, loss : 0.3075, accuracy : 91.01\n",
            "iteration : 250, loss : 0.3043, accuracy : 91.13\n",
            "iteration : 300, loss : 0.3045, accuracy : 91.07\n",
            "iteration : 350, loss : 0.3033, accuracy : 91.05\n",
            "Epoch :   9, training loss : 0.3027, training accuracy : 91.05, test loss : 0.3423, test accuracy : 89.67\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2723, accuracy : 91.56\n",
            "iteration : 100, loss : 0.2883, accuracy : 91.36\n",
            "iteration : 150, loss : 0.2858, accuracy : 91.42\n",
            "iteration : 200, loss : 0.2833, accuracy : 91.45\n",
            "iteration : 250, loss : 0.2834, accuracy : 91.50\n",
            "iteration : 300, loss : 0.2840, accuracy : 91.48\n",
            "iteration : 350, loss : 0.2866, accuracy : 91.40\n",
            "Epoch :  10, training loss : 0.2875, training accuracy : 91.41, test loss : 0.3357, test accuracy : 90.24\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.2923, accuracy : 91.34\n",
            "iteration : 100, loss : 0.2832, accuracy : 91.44\n",
            "iteration : 150, loss : 0.2767, accuracy : 91.59\n",
            "iteration : 200, loss : 0.2810, accuracy : 91.45\n",
            "iteration : 250, loss : 0.2795, accuracy : 91.56\n",
            "iteration : 300, loss : 0.2789, accuracy : 91.59\n",
            "iteration : 350, loss : 0.2777, accuracy : 91.68\n",
            "Epoch :  11, training loss : 0.2782, training accuracy : 91.68, test loss : 0.3110, test accuracy : 90.95\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.2682, accuracy : 91.98\n",
            "iteration : 100, loss : 0.2760, accuracy : 91.80\n",
            "iteration : 150, loss : 0.2747, accuracy : 91.78\n",
            "iteration : 200, loss : 0.2732, accuracy : 91.86\n",
            "iteration : 250, loss : 0.2726, accuracy : 91.85\n",
            "iteration : 300, loss : 0.2701, accuracy : 91.95\n",
            "iteration : 350, loss : 0.2681, accuracy : 91.96\n",
            "Epoch :  12, training loss : 0.2686, training accuracy : 91.92, test loss : 0.2993, test accuracy : 90.96\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.2524, accuracy : 92.88\n",
            "iteration : 100, loss : 0.2491, accuracy : 92.68\n",
            "iteration : 150, loss : 0.2512, accuracy : 92.52\n",
            "iteration : 200, loss : 0.2539, accuracy : 92.50\n",
            "iteration : 250, loss : 0.2546, accuracy : 92.48\n",
            "iteration : 300, loss : 0.2548, accuracy : 92.49\n",
            "iteration : 350, loss : 0.2571, accuracy : 92.44\n",
            "Epoch :  13, training loss : 0.2591, training accuracy : 92.38, test loss : 0.3139, test accuracy : 90.68\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.2435, accuracy : 92.48\n",
            "iteration : 100, loss : 0.2469, accuracy : 92.43\n",
            "iteration : 150, loss : 0.2464, accuracy : 92.51\n",
            "iteration : 200, loss : 0.2483, accuracy : 92.57\n",
            "iteration : 250, loss : 0.2503, accuracy : 92.58\n",
            "iteration : 300, loss : 0.2552, accuracy : 92.46\n",
            "iteration : 350, loss : 0.2533, accuracy : 92.51\n",
            "Epoch :  14, training loss : 0.2540, training accuracy : 92.51, test loss : 0.2749, test accuracy : 91.91\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.2371, accuracy : 92.89\n",
            "iteration : 100, loss : 0.2430, accuracy : 92.66\n",
            "iteration : 150, loss : 0.2513, accuracy : 92.52\n",
            "iteration : 200, loss : 0.2472, accuracy : 92.72\n",
            "iteration : 250, loss : 0.2473, accuracy : 92.75\n",
            "iteration : 300, loss : 0.2466, accuracy : 92.83\n",
            "iteration : 350, loss : 0.2473, accuracy : 92.80\n",
            "Epoch :  15, training loss : 0.2486, training accuracy : 92.76, test loss : 0.2926, test accuracy : 91.58\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.2623, accuracy : 92.05\n",
            "iteration : 100, loss : 0.2422, accuracy : 92.73\n",
            "iteration : 150, loss : 0.2414, accuracy : 92.79\n",
            "iteration : 200, loss : 0.2495, accuracy : 92.61\n",
            "iteration : 250, loss : 0.2500, accuracy : 92.67\n",
            "iteration : 300, loss : 0.2491, accuracy : 92.71\n",
            "iteration : 350, loss : 0.2454, accuracy : 92.79\n",
            "Epoch :  16, training loss : 0.2440, training accuracy : 92.84, test loss : 0.2924, test accuracy : 91.43\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.2179, accuracy : 93.55\n",
            "iteration : 100, loss : 0.2217, accuracy : 93.35\n",
            "iteration : 150, loss : 0.2326, accuracy : 93.03\n",
            "iteration : 200, loss : 0.2335, accuracy : 93.02\n",
            "iteration : 250, loss : 0.2358, accuracy : 92.97\n",
            "iteration : 300, loss : 0.2398, accuracy : 92.93\n",
            "iteration : 350, loss : 0.2386, accuracy : 92.96\n",
            "Epoch :  17, training loss : 0.2392, training accuracy : 92.96, test loss : 0.2915, test accuracy : 91.60\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.2290, accuracy : 93.55\n",
            "iteration : 100, loss : 0.2287, accuracy : 93.46\n",
            "iteration : 150, loss : 0.2379, accuracy : 93.10\n",
            "iteration : 200, loss : 0.2351, accuracy : 93.15\n",
            "iteration : 250, loss : 0.2316, accuracy : 93.19\n",
            "iteration : 300, loss : 0.2301, accuracy : 93.21\n",
            "iteration : 350, loss : 0.2329, accuracy : 93.15\n",
            "Epoch :  18, training loss : 0.2337, training accuracy : 93.15, test loss : 0.3100, test accuracy : 91.09\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.2033, accuracy : 93.69\n",
            "iteration : 100, loss : 0.2083, accuracy : 93.86\n",
            "iteration : 150, loss : 0.2172, accuracy : 93.68\n",
            "iteration : 200, loss : 0.2215, accuracy : 93.48\n",
            "iteration : 250, loss : 0.2266, accuracy : 93.39\n",
            "iteration : 300, loss : 0.2273, accuracy : 93.39\n",
            "iteration : 350, loss : 0.2280, accuracy : 93.35\n",
            "Epoch :  19, training loss : 0.2279, training accuracy : 93.35, test loss : 0.2777, test accuracy : 91.74\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.2189, accuracy : 93.72\n",
            "iteration : 100, loss : 0.2223, accuracy : 93.66\n",
            "iteration : 150, loss : 0.2191, accuracy : 93.70\n",
            "iteration : 200, loss : 0.2250, accuracy : 93.45\n",
            "iteration : 250, loss : 0.2206, accuracy : 93.57\n",
            "iteration : 300, loss : 0.2209, accuracy : 93.52\n",
            "iteration : 350, loss : 0.2232, accuracy : 93.46\n",
            "Epoch :  20, training loss : 0.2250, training accuracy : 93.40, test loss : 0.2716, test accuracy : 92.13\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.2001, accuracy : 94.22\n",
            "iteration : 100, loss : 0.2028, accuracy : 94.25\n",
            "iteration : 150, loss : 0.2074, accuracy : 93.97\n",
            "iteration : 200, loss : 0.2171, accuracy : 93.68\n",
            "iteration : 250, loss : 0.2210, accuracy : 93.61\n",
            "iteration : 300, loss : 0.2188, accuracy : 93.67\n",
            "iteration : 350, loss : 0.2201, accuracy : 93.62\n",
            "Epoch :  21, training loss : 0.2189, training accuracy : 93.64, test loss : 0.2659, test accuracy : 92.30\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.1954, accuracy : 94.30\n",
            "iteration : 100, loss : 0.2081, accuracy : 94.07\n",
            "iteration : 150, loss : 0.2089, accuracy : 93.96\n",
            "iteration : 200, loss : 0.2099, accuracy : 93.91\n",
            "iteration : 250, loss : 0.2101, accuracy : 93.92\n",
            "iteration : 300, loss : 0.2142, accuracy : 93.83\n",
            "iteration : 350, loss : 0.2144, accuracy : 93.81\n",
            "Epoch :  22, training loss : 0.2159, training accuracy : 93.79, test loss : 0.2805, test accuracy : 91.75\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.2190, accuracy : 93.42\n",
            "iteration : 100, loss : 0.2153, accuracy : 93.44\n",
            "iteration : 150, loss : 0.2124, accuracy : 93.71\n",
            "iteration : 200, loss : 0.2123, accuracy : 93.79\n",
            "iteration : 250, loss : 0.2123, accuracy : 93.77\n",
            "iteration : 300, loss : 0.2128, accuracy : 93.73\n",
            "iteration : 350, loss : 0.2124, accuracy : 93.72\n",
            "Epoch :  23, training loss : 0.2125, training accuracy : 93.68, test loss : 0.2707, test accuracy : 92.26\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.2025, accuracy : 94.44\n",
            "iteration : 100, loss : 0.2070, accuracy : 93.99\n",
            "iteration : 150, loss : 0.2049, accuracy : 94.06\n",
            "iteration : 200, loss : 0.2058, accuracy : 94.04\n",
            "iteration : 250, loss : 0.2082, accuracy : 94.04\n",
            "iteration : 300, loss : 0.2085, accuracy : 93.99\n",
            "iteration : 350, loss : 0.2092, accuracy : 93.95\n",
            "Epoch :  24, training loss : 0.2103, training accuracy : 93.92, test loss : 0.2468, test accuracy : 92.84\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.1819, accuracy : 94.59\n",
            "iteration : 100, loss : 0.1898, accuracy : 94.49\n",
            "iteration : 150, loss : 0.1915, accuracy : 94.45\n",
            "iteration : 200, loss : 0.1966, accuracy : 94.28\n",
            "iteration : 250, loss : 0.2002, accuracy : 94.12\n",
            "iteration : 300, loss : 0.2005, accuracy : 94.12\n",
            "iteration : 350, loss : 0.2029, accuracy : 94.05\n",
            "Epoch :  25, training loss : 0.2057, training accuracy : 93.98, test loss : 0.2648, test accuracy : 92.40\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.1933, accuracy : 94.28\n",
            "iteration : 100, loss : 0.2008, accuracy : 94.12\n",
            "iteration : 150, loss : 0.2021, accuracy : 94.13\n",
            "iteration : 200, loss : 0.2021, accuracy : 94.15\n",
            "iteration : 250, loss : 0.2050, accuracy : 94.09\n",
            "iteration : 300, loss : 0.2032, accuracy : 94.09\n",
            "iteration : 350, loss : 0.2030, accuracy : 94.04\n",
            "Epoch :  26, training loss : 0.2033, training accuracy : 94.03, test loss : 0.2729, test accuracy : 92.08\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.1912, accuracy : 94.34\n",
            "iteration : 100, loss : 0.1997, accuracy : 94.06\n",
            "iteration : 150, loss : 0.2001, accuracy : 94.16\n",
            "iteration : 200, loss : 0.2015, accuracy : 94.15\n",
            "iteration : 250, loss : 0.2016, accuracy : 94.11\n",
            "iteration : 300, loss : 0.2023, accuracy : 94.10\n",
            "iteration : 350, loss : 0.2007, accuracy : 94.15\n",
            "Epoch :  27, training loss : 0.2007, training accuracy : 94.15, test loss : 0.2413, test accuracy : 93.09\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.1791, accuracy : 94.95\n",
            "iteration : 100, loss : 0.1865, accuracy : 94.73\n",
            "iteration : 150, loss : 0.1912, accuracy : 94.50\n",
            "iteration : 200, loss : 0.1973, accuracy : 94.37\n",
            "iteration : 250, loss : 0.1956, accuracy : 94.41\n",
            "iteration : 300, loss : 0.1966, accuracy : 94.42\n",
            "iteration : 350, loss : 0.1978, accuracy : 94.36\n",
            "Epoch :  28, training loss : 0.1987, training accuracy : 94.35, test loss : 0.2661, test accuracy : 92.47\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.1884, accuracy : 94.30\n",
            "iteration : 100, loss : 0.1875, accuracy : 94.40\n",
            "iteration : 150, loss : 0.1898, accuracy : 94.32\n",
            "iteration : 200, loss : 0.1910, accuracy : 94.30\n",
            "iteration : 250, loss : 0.1966, accuracy : 94.22\n",
            "iteration : 300, loss : 0.1959, accuracy : 94.31\n",
            "iteration : 350, loss : 0.1967, accuracy : 94.27\n",
            "Epoch :  29, training loss : 0.1969, training accuracy : 94.26, test loss : 0.2648, test accuracy : 92.49\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.1917, accuracy : 94.59\n",
            "iteration : 100, loss : 0.1959, accuracy : 94.44\n",
            "iteration : 150, loss : 0.1941, accuracy : 94.56\n",
            "iteration : 200, loss : 0.1916, accuracy : 94.57\n",
            "iteration : 250, loss : 0.1917, accuracy : 94.51\n",
            "iteration : 300, loss : 0.1934, accuracy : 94.48\n",
            "iteration : 350, loss : 0.1959, accuracy : 94.40\n",
            "Epoch :  30, training loss : 0.1957, training accuracy : 94.39, test loss : 0.2645, test accuracy : 92.47\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.1900, accuracy : 94.31\n",
            "iteration : 100, loss : 0.1864, accuracy : 94.45\n",
            "iteration : 150, loss : 0.1864, accuracy : 94.55\n",
            "iteration : 200, loss : 0.1877, accuracy : 94.53\n",
            "iteration : 250, loss : 0.1935, accuracy : 94.35\n",
            "iteration : 300, loss : 0.1946, accuracy : 94.28\n",
            "iteration : 350, loss : 0.1951, accuracy : 94.27\n",
            "Epoch :  31, training loss : 0.1952, training accuracy : 94.24, test loss : 0.2507, test accuracy : 92.93\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.1752, accuracy : 94.98\n",
            "iteration : 100, loss : 0.1815, accuracy : 94.88\n",
            "iteration : 150, loss : 0.1911, accuracy : 94.52\n",
            "iteration : 200, loss : 0.1918, accuracy : 94.49\n",
            "iteration : 250, loss : 0.1903, accuracy : 94.54\n",
            "iteration : 300, loss : 0.1930, accuracy : 94.46\n",
            "iteration : 350, loss : 0.1915, accuracy : 94.52\n",
            "Epoch :  32, training loss : 0.1929, training accuracy : 94.47, test loss : 0.2605, test accuracy : 92.42\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.1816, accuracy : 94.84\n",
            "iteration : 100, loss : 0.1835, accuracy : 94.80\n",
            "iteration : 150, loss : 0.1881, accuracy : 94.60\n",
            "iteration : 200, loss : 0.1872, accuracy : 94.67\n",
            "iteration : 250, loss : 0.1875, accuracy : 94.63\n",
            "iteration : 300, loss : 0.1890, accuracy : 94.61\n",
            "iteration : 350, loss : 0.1878, accuracy : 94.60\n",
            "Epoch :  33, training loss : 0.1878, training accuracy : 94.60, test loss : 0.2517, test accuracy : 92.87\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.1833, accuracy : 94.69\n",
            "iteration : 100, loss : 0.1746, accuracy : 94.89\n",
            "iteration : 150, loss : 0.1817, accuracy : 94.73\n",
            "iteration : 200, loss : 0.1842, accuracy : 94.61\n",
            "iteration : 250, loss : 0.1856, accuracy : 94.63\n",
            "iteration : 300, loss : 0.1864, accuracy : 94.60\n",
            "iteration : 350, loss : 0.1883, accuracy : 94.56\n",
            "Epoch :  34, training loss : 0.1892, training accuracy : 94.55, test loss : 0.2407, test accuracy : 93.24\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.1579, accuracy : 95.17\n",
            "iteration : 100, loss : 0.1695, accuracy : 94.96\n",
            "iteration : 150, loss : 0.1745, accuracy : 94.94\n",
            "iteration : 200, loss : 0.1747, accuracy : 94.89\n",
            "iteration : 250, loss : 0.1779, accuracy : 94.84\n",
            "iteration : 300, loss : 0.1800, accuracy : 94.79\n",
            "iteration : 350, loss : 0.1827, accuracy : 94.70\n",
            "Epoch :  35, training loss : 0.1829, training accuracy : 94.69, test loss : 0.2411, test accuracy : 93.35\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.1857, accuracy : 94.56\n",
            "iteration : 100, loss : 0.1821, accuracy : 94.69\n",
            "iteration : 150, loss : 0.1807, accuracy : 94.70\n",
            "iteration : 200, loss : 0.1793, accuracy : 94.78\n",
            "iteration : 250, loss : 0.1799, accuracy : 94.72\n",
            "iteration : 300, loss : 0.1821, accuracy : 94.64\n",
            "iteration : 350, loss : 0.1838, accuracy : 94.61\n",
            "Epoch :  36, training loss : 0.1839, training accuracy : 94.60, test loss : 0.2387, test accuracy : 93.17\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.1750, accuracy : 94.89\n",
            "iteration : 100, loss : 0.1751, accuracy : 94.84\n",
            "iteration : 150, loss : 0.1758, accuracy : 94.83\n",
            "iteration : 200, loss : 0.1811, accuracy : 94.72\n",
            "iteration : 250, loss : 0.1820, accuracy : 94.67\n",
            "iteration : 300, loss : 0.1833, accuracy : 94.63\n",
            "iteration : 350, loss : 0.1840, accuracy : 94.62\n",
            "Epoch :  37, training loss : 0.1843, training accuracy : 94.61, test loss : 0.2425, test accuracy : 93.21\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.1720, accuracy : 95.08\n",
            "iteration : 100, loss : 0.1779, accuracy : 94.88\n",
            "iteration : 150, loss : 0.1798, accuracy : 94.81\n",
            "iteration : 200, loss : 0.1793, accuracy : 94.82\n",
            "iteration : 250, loss : 0.1803, accuracy : 94.75\n",
            "iteration : 300, loss : 0.1838, accuracy : 94.64\n",
            "iteration : 350, loss : 0.1833, accuracy : 94.66\n",
            "Epoch :  38, training loss : 0.1830, training accuracy : 94.67, test loss : 0.2394, test accuracy : 93.28\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.1752, accuracy : 94.91\n",
            "iteration : 100, loss : 0.1715, accuracy : 94.96\n",
            "iteration : 150, loss : 0.1711, accuracy : 95.02\n",
            "iteration : 200, loss : 0.1711, accuracy : 94.95\n",
            "iteration : 250, loss : 0.1725, accuracy : 94.90\n",
            "iteration : 300, loss : 0.1748, accuracy : 94.82\n",
            "iteration : 350, loss : 0.1784, accuracy : 94.69\n",
            "Epoch :  39, training loss : 0.1799, training accuracy : 94.66, test loss : 0.2456, test accuracy : 93.12\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.1795, accuracy : 94.80\n",
            "iteration : 100, loss : 0.1812, accuracy : 94.66\n",
            "iteration : 150, loss : 0.1804, accuracy : 94.65\n",
            "iteration : 200, loss : 0.1812, accuracy : 94.69\n",
            "iteration : 250, loss : 0.1798, accuracy : 94.74\n",
            "iteration : 300, loss : 0.1827, accuracy : 94.69\n",
            "iteration : 350, loss : 0.1815, accuracy : 94.73\n",
            "Epoch :  40, training loss : 0.1813, training accuracy : 94.72, test loss : 0.2464, test accuracy : 93.09\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.1593, accuracy : 95.25\n",
            "iteration : 100, loss : 0.1699, accuracy : 94.97\n",
            "iteration : 150, loss : 0.1789, accuracy : 94.67\n",
            "iteration : 200, loss : 0.1790, accuracy : 94.75\n",
            "iteration : 250, loss : 0.1780, accuracy : 94.78\n",
            "iteration : 300, loss : 0.1788, accuracy : 94.78\n",
            "iteration : 350, loss : 0.1787, accuracy : 94.78\n",
            "Epoch :  41, training loss : 0.1785, training accuracy : 94.79, test loss : 0.2441, test accuracy : 93.13\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.1710, accuracy : 94.83\n",
            "iteration : 100, loss : 0.1675, accuracy : 95.01\n",
            "iteration : 150, loss : 0.1695, accuracy : 95.05\n",
            "iteration : 200, loss : 0.1710, accuracy : 95.04\n",
            "iteration : 250, loss : 0.1731, accuracy : 94.98\n",
            "iteration : 300, loss : 0.1724, accuracy : 94.95\n",
            "iteration : 350, loss : 0.1736, accuracy : 94.92\n",
            "Epoch :  42, training loss : 0.1739, training accuracy : 94.90, test loss : 0.2694, test accuracy : 92.53\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.1696, accuracy : 95.03\n",
            "iteration : 100, loss : 0.1727, accuracy : 94.87\n",
            "iteration : 150, loss : 0.1737, accuracy : 94.92\n",
            "iteration : 200, loss : 0.1764, accuracy : 94.82\n",
            "iteration : 250, loss : 0.1764, accuracy : 94.83\n",
            "iteration : 300, loss : 0.1772, accuracy : 94.77\n",
            "iteration : 350, loss : 0.1757, accuracy : 94.80\n",
            "Epoch :  43, training loss : 0.1762, training accuracy : 94.81, test loss : 0.2634, test accuracy : 92.77\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.1503, accuracy : 95.45\n",
            "iteration : 100, loss : 0.1555, accuracy : 95.31\n",
            "iteration : 150, loss : 0.1614, accuracy : 95.26\n",
            "iteration : 200, loss : 0.1682, accuracy : 95.02\n",
            "iteration : 250, loss : 0.1695, accuracy : 95.00\n",
            "iteration : 300, loss : 0.1738, accuracy : 94.89\n",
            "iteration : 350, loss : 0.1751, accuracy : 94.92\n",
            "Epoch :  44, training loss : 0.1758, training accuracy : 94.89, test loss : 0.2529, test accuracy : 92.75\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.1727, accuracy : 95.28\n",
            "iteration : 100, loss : 0.1736, accuracy : 95.02\n",
            "iteration : 150, loss : 0.1755, accuracy : 94.92\n",
            "iteration : 200, loss : 0.1753, accuracy : 94.91\n",
            "iteration : 250, loss : 0.1730, accuracy : 94.90\n",
            "iteration : 300, loss : 0.1714, accuracy : 94.95\n",
            "iteration : 350, loss : 0.1714, accuracy : 94.96\n",
            "Epoch :  45, training loss : 0.1712, training accuracy : 94.97, test loss : 0.2382, test accuracy : 93.26\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.1793, accuracy : 94.83\n",
            "iteration : 100, loss : 0.1758, accuracy : 95.01\n",
            "iteration : 150, loss : 0.1772, accuracy : 94.88\n",
            "iteration : 200, loss : 0.1725, accuracy : 94.98\n",
            "iteration : 250, loss : 0.1727, accuracy : 94.96\n",
            "iteration : 300, loss : 0.1735, accuracy : 94.96\n",
            "iteration : 350, loss : 0.1753, accuracy : 94.90\n",
            "Epoch :  46, training loss : 0.1757, training accuracy : 94.89, test loss : 0.2391, test accuracy : 93.36\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.1634, accuracy : 95.34\n",
            "iteration : 100, loss : 0.1607, accuracy : 95.47\n",
            "iteration : 150, loss : 0.1692, accuracy : 95.30\n",
            "iteration : 200, loss : 0.1684, accuracy : 95.23\n",
            "iteration : 250, loss : 0.1690, accuracy : 95.17\n",
            "iteration : 300, loss : 0.1695, accuracy : 95.15\n",
            "iteration : 350, loss : 0.1706, accuracy : 95.11\n",
            "Epoch :  47, training loss : 0.1718, training accuracy : 95.06, test loss : 0.2521, test accuracy : 92.96\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.1790, accuracy : 94.77\n",
            "iteration : 100, loss : 0.1671, accuracy : 94.98\n",
            "iteration : 150, loss : 0.1671, accuracy : 94.99\n",
            "iteration : 200, loss : 0.1684, accuracy : 94.96\n",
            "iteration : 250, loss : 0.1716, accuracy : 94.96\n",
            "iteration : 300, loss : 0.1730, accuracy : 94.95\n",
            "iteration : 350, loss : 0.1748, accuracy : 94.96\n",
            "Epoch :  48, training loss : 0.1739, training accuracy : 94.97, test loss : 0.2428, test accuracy : 93.02\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.1480, accuracy : 95.95\n",
            "iteration : 100, loss : 0.1555, accuracy : 95.69\n",
            "iteration : 150, loss : 0.1574, accuracy : 95.58\n",
            "iteration : 200, loss : 0.1628, accuracy : 95.40\n",
            "iteration : 250, loss : 0.1652, accuracy : 95.34\n",
            "iteration : 300, loss : 0.1654, accuracy : 95.33\n",
            "iteration : 350, loss : 0.1666, accuracy : 95.27\n",
            "Epoch :  49, training loss : 0.1669, training accuracy : 95.24, test loss : 0.2521, test accuracy : 93.00\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.1634, accuracy : 95.48\n",
            "iteration : 100, loss : 0.1631, accuracy : 95.26\n",
            "iteration : 150, loss : 0.1637, accuracy : 95.19\n",
            "iteration : 200, loss : 0.1652, accuracy : 95.14\n",
            "iteration : 250, loss : 0.1670, accuracy : 95.10\n",
            "iteration : 300, loss : 0.1687, accuracy : 95.05\n",
            "iteration : 350, loss : 0.1716, accuracy : 94.97\n",
            "Epoch :  50, training loss : 0.1723, training accuracy : 94.94, test loss : 0.2451, test accuracy : 92.86\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.1603, accuracy : 95.58\n",
            "iteration : 100, loss : 0.1528, accuracy : 95.63\n",
            "iteration : 150, loss : 0.1599, accuracy : 95.43\n",
            "iteration : 200, loss : 0.1595, accuracy : 95.39\n",
            "iteration : 250, loss : 0.1602, accuracy : 95.36\n",
            "iteration : 300, loss : 0.1668, accuracy : 95.19\n",
            "iteration : 350, loss : 0.1671, accuracy : 95.20\n",
            "Epoch :  51, training loss : 0.1674, training accuracy : 95.19, test loss : 0.2500, test accuracy : 92.78\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.1665, accuracy : 95.03\n",
            "iteration : 100, loss : 0.1583, accuracy : 95.27\n",
            "iteration : 150, loss : 0.1571, accuracy : 95.27\n",
            "iteration : 200, loss : 0.1587, accuracy : 95.33\n",
            "iteration : 250, loss : 0.1601, accuracy : 95.33\n",
            "iteration : 300, loss : 0.1610, accuracy : 95.28\n",
            "iteration : 350, loss : 0.1668, accuracy : 95.14\n",
            "Epoch :  52, training loss : 0.1674, training accuracy : 95.11, test loss : 0.2431, test accuracy : 93.15\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.1533, accuracy : 95.84\n",
            "iteration : 100, loss : 0.1570, accuracy : 95.73\n",
            "iteration : 150, loss : 0.1557, accuracy : 95.62\n",
            "iteration : 200, loss : 0.1572, accuracy : 95.56\n",
            "iteration : 250, loss : 0.1631, accuracy : 95.34\n",
            "iteration : 300, loss : 0.1657, accuracy : 95.26\n",
            "iteration : 350, loss : 0.1652, accuracy : 95.26\n",
            "Epoch :  53, training loss : 0.1645, training accuracy : 95.26, test loss : 0.2341, test accuracy : 93.43\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.1661, accuracy : 95.27\n",
            "iteration : 100, loss : 0.1662, accuracy : 95.31\n",
            "iteration : 150, loss : 0.1634, accuracy : 95.37\n",
            "iteration : 200, loss : 0.1612, accuracy : 95.42\n",
            "iteration : 250, loss : 0.1605, accuracy : 95.39\n",
            "iteration : 300, loss : 0.1621, accuracy : 95.35\n",
            "iteration : 350, loss : 0.1645, accuracy : 95.25\n",
            "Epoch :  54, training loss : 0.1657, training accuracy : 95.19, test loss : 0.2221, test accuracy : 93.84\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.1483, accuracy : 95.77\n",
            "iteration : 100, loss : 0.1571, accuracy : 95.57\n",
            "iteration : 150, loss : 0.1659, accuracy : 95.38\n",
            "iteration : 200, loss : 0.1660, accuracy : 95.27\n",
            "iteration : 250, loss : 0.1635, accuracy : 95.36\n",
            "iteration : 300, loss : 0.1652, accuracy : 95.28\n",
            "iteration : 350, loss : 0.1637, accuracy : 95.29\n",
            "Epoch :  55, training loss : 0.1635, training accuracy : 95.27, test loss : 0.2472, test accuracy : 93.05\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.1502, accuracy : 95.55\n",
            "iteration : 100, loss : 0.1481, accuracy : 95.62\n",
            "iteration : 150, loss : 0.1541, accuracy : 95.38\n",
            "iteration : 200, loss : 0.1585, accuracy : 95.25\n",
            "iteration : 250, loss : 0.1594, accuracy : 95.26\n",
            "iteration : 300, loss : 0.1627, accuracy : 95.20\n",
            "iteration : 350, loss : 0.1622, accuracy : 95.21\n",
            "Epoch :  56, training loss : 0.1624, training accuracy : 95.20, test loss : 0.2361, test accuracy : 93.24\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.1551, accuracy : 95.52\n",
            "iteration : 100, loss : 0.1548, accuracy : 95.59\n",
            "iteration : 150, loss : 0.1617, accuracy : 95.25\n",
            "iteration : 200, loss : 0.1602, accuracy : 95.30\n",
            "iteration : 250, loss : 0.1621, accuracy : 95.22\n",
            "iteration : 300, loss : 0.1616, accuracy : 95.24\n",
            "iteration : 350, loss : 0.1619, accuracy : 95.29\n",
            "Epoch :  57, training loss : 0.1614, training accuracy : 95.30, test loss : 0.2380, test accuracy : 93.46\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.1517, accuracy : 95.34\n",
            "iteration : 100, loss : 0.1493, accuracy : 95.45\n",
            "iteration : 150, loss : 0.1481, accuracy : 95.62\n",
            "iteration : 200, loss : 0.1539, accuracy : 95.41\n",
            "iteration : 250, loss : 0.1582, accuracy : 95.33\n",
            "iteration : 300, loss : 0.1580, accuracy : 95.31\n",
            "iteration : 350, loss : 0.1609, accuracy : 95.25\n",
            "Epoch :  58, training loss : 0.1613, training accuracy : 95.24, test loss : 0.2389, test accuracy : 93.39\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.1509, accuracy : 95.91\n",
            "iteration : 100, loss : 0.1416, accuracy : 96.02\n",
            "iteration : 150, loss : 0.1491, accuracy : 95.79\n",
            "iteration : 200, loss : 0.1526, accuracy : 95.61\n",
            "iteration : 250, loss : 0.1570, accuracy : 95.52\n",
            "iteration : 300, loss : 0.1584, accuracy : 95.48\n",
            "iteration : 350, loss : 0.1607, accuracy : 95.40\n",
            "Epoch :  59, training loss : 0.1612, training accuracy : 95.36, test loss : 0.2358, test accuracy : 93.30\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.1532, accuracy : 95.67\n",
            "iteration : 100, loss : 0.1523, accuracy : 95.60\n",
            "iteration : 150, loss : 0.1501, accuracy : 95.64\n",
            "iteration : 200, loss : 0.1545, accuracy : 95.55\n",
            "iteration : 250, loss : 0.1550, accuracy : 95.54\n",
            "iteration : 300, loss : 0.1554, accuracy : 95.50\n",
            "iteration : 350, loss : 0.1581, accuracy : 95.44\n",
            "Epoch :  60, training loss : 0.1591, training accuracy : 95.40, test loss : 0.2408, test accuracy : 93.14\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.1574, accuracy : 95.45\n",
            "iteration : 100, loss : 0.1603, accuracy : 95.35\n",
            "iteration : 150, loss : 0.1583, accuracy : 95.45\n",
            "iteration : 200, loss : 0.1585, accuracy : 95.45\n",
            "iteration : 250, loss : 0.1584, accuracy : 95.44\n",
            "iteration : 300, loss : 0.1604, accuracy : 95.30\n",
            "iteration : 350, loss : 0.1592, accuracy : 95.34\n",
            "Epoch :  61, training loss : 0.1600, training accuracy : 95.30, test loss : 0.2557, test accuracy : 92.91\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.1480, accuracy : 95.59\n",
            "iteration : 100, loss : 0.1454, accuracy : 95.57\n",
            "iteration : 150, loss : 0.1460, accuracy : 95.59\n",
            "iteration : 200, loss : 0.1525, accuracy : 95.54\n",
            "iteration : 250, loss : 0.1578, accuracy : 95.41\n",
            "iteration : 300, loss : 0.1583, accuracy : 95.38\n",
            "iteration : 350, loss : 0.1617, accuracy : 95.30\n",
            "Epoch :  62, training loss : 0.1610, training accuracy : 95.28, test loss : 0.2338, test accuracy : 93.53\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.1577, accuracy : 95.28\n",
            "iteration : 100, loss : 0.1532, accuracy : 95.49\n",
            "iteration : 150, loss : 0.1516, accuracy : 95.51\n",
            "iteration : 200, loss : 0.1535, accuracy : 95.49\n",
            "iteration : 250, loss : 0.1551, accuracy : 95.47\n",
            "iteration : 300, loss : 0.1560, accuracy : 95.42\n",
            "iteration : 350, loss : 0.1568, accuracy : 95.40\n",
            "Epoch :  63, training loss : 0.1576, training accuracy : 95.37, test loss : 0.2347, test accuracy : 93.46\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.1472, accuracy : 95.83\n",
            "iteration : 100, loss : 0.1441, accuracy : 95.79\n",
            "iteration : 150, loss : 0.1508, accuracy : 95.65\n",
            "iteration : 200, loss : 0.1534, accuracy : 95.51\n",
            "iteration : 250, loss : 0.1566, accuracy : 95.41\n",
            "iteration : 300, loss : 0.1555, accuracy : 95.38\n",
            "iteration : 350, loss : 0.1549, accuracy : 95.44\n",
            "Epoch :  64, training loss : 0.1540, training accuracy : 95.47, test loss : 0.2314, test accuracy : 93.54\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.1453, accuracy : 95.77\n",
            "iteration : 100, loss : 0.1557, accuracy : 95.45\n",
            "iteration : 150, loss : 0.1480, accuracy : 95.71\n",
            "iteration : 200, loss : 0.1538, accuracy : 95.56\n",
            "iteration : 250, loss : 0.1557, accuracy : 95.48\n",
            "iteration : 300, loss : 0.1570, accuracy : 95.41\n",
            "iteration : 350, loss : 0.1571, accuracy : 95.40\n",
            "Epoch :  65, training loss : 0.1577, training accuracy : 95.39, test loss : 0.2443, test accuracy : 93.24\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.1535, accuracy : 95.86\n",
            "iteration : 100, loss : 0.1528, accuracy : 95.79\n",
            "iteration : 150, loss : 0.1497, accuracy : 95.68\n",
            "iteration : 200, loss : 0.1493, accuracy : 95.68\n",
            "iteration : 250, loss : 0.1542, accuracy : 95.53\n",
            "iteration : 300, loss : 0.1544, accuracy : 95.61\n",
            "iteration : 350, loss : 0.1549, accuracy : 95.58\n",
            "Epoch :  66, training loss : 0.1553, training accuracy : 95.56, test loss : 0.2439, test accuracy : 93.17\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.1423, accuracy : 95.83\n",
            "iteration : 100, loss : 0.1464, accuracy : 95.86\n",
            "iteration : 150, loss : 0.1455, accuracy : 95.90\n",
            "iteration : 200, loss : 0.1458, accuracy : 95.81\n",
            "iteration : 250, loss : 0.1520, accuracy : 95.60\n",
            "iteration : 300, loss : 0.1540, accuracy : 95.57\n",
            "iteration : 350, loss : 0.1563, accuracy : 95.49\n",
            "Epoch :  67, training loss : 0.1562, training accuracy : 95.47, test loss : 0.2297, test accuracy : 93.67\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.1527, accuracy : 95.31\n",
            "iteration : 100, loss : 0.1536, accuracy : 95.37\n",
            "iteration : 150, loss : 0.1554, accuracy : 95.36\n",
            "iteration : 200, loss : 0.1524, accuracy : 95.50\n",
            "iteration : 250, loss : 0.1541, accuracy : 95.45\n",
            "iteration : 300, loss : 0.1585, accuracy : 95.32\n",
            "iteration : 350, loss : 0.1589, accuracy : 95.30\n",
            "Epoch :  68, training loss : 0.1592, training accuracy : 95.29, test loss : 0.2495, test accuracy : 93.15\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.1693, accuracy : 95.33\n",
            "iteration : 100, loss : 0.1531, accuracy : 95.63\n",
            "iteration : 150, loss : 0.1560, accuracy : 95.42\n",
            "iteration : 200, loss : 0.1549, accuracy : 95.46\n",
            "iteration : 250, loss : 0.1567, accuracy : 95.45\n",
            "iteration : 300, loss : 0.1580, accuracy : 95.43\n",
            "iteration : 350, loss : 0.1574, accuracy : 95.45\n",
            "Epoch :  69, training loss : 0.1570, training accuracy : 95.47, test loss : 0.2391, test accuracy : 93.37\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.1432, accuracy : 95.84\n",
            "iteration : 100, loss : 0.1473, accuracy : 95.66\n",
            "iteration : 150, loss : 0.1435, accuracy : 95.76\n",
            "iteration : 200, loss : 0.1447, accuracy : 95.72\n",
            "iteration : 250, loss : 0.1472, accuracy : 95.68\n",
            "iteration : 300, loss : 0.1480, accuracy : 95.67\n",
            "iteration : 350, loss : 0.1501, accuracy : 95.64\n",
            "Epoch :  70, training loss : 0.1516, training accuracy : 95.61, test loss : 0.2546, test accuracy : 92.82\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.1406, accuracy : 96.00\n",
            "iteration : 100, loss : 0.1449, accuracy : 95.88\n",
            "iteration : 150, loss : 0.1464, accuracy : 95.78\n",
            "iteration : 200, loss : 0.1489, accuracy : 95.73\n",
            "iteration : 250, loss : 0.1526, accuracy : 95.62\n",
            "iteration : 300, loss : 0.1534, accuracy : 95.61\n",
            "iteration : 350, loss : 0.1545, accuracy : 95.57\n",
            "Epoch :  71, training loss : 0.1543, training accuracy : 95.56, test loss : 0.2380, test accuracy : 93.39\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.1338, accuracy : 95.97\n",
            "iteration : 100, loss : 0.1400, accuracy : 95.86\n",
            "iteration : 150, loss : 0.1430, accuracy : 95.75\n",
            "iteration : 200, loss : 0.1457, accuracy : 95.70\n",
            "iteration : 250, loss : 0.1443, accuracy : 95.70\n",
            "iteration : 300, loss : 0.1469, accuracy : 95.71\n",
            "iteration : 350, loss : 0.1490, accuracy : 95.63\n",
            "Epoch :  72, training loss : 0.1490, training accuracy : 95.59, test loss : 0.2321, test accuracy : 93.57\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.1310, accuracy : 96.20\n",
            "iteration : 100, loss : 0.1465, accuracy : 95.75\n",
            "iteration : 150, loss : 0.1498, accuracy : 95.66\n",
            "iteration : 200, loss : 0.1509, accuracy : 95.54\n",
            "iteration : 250, loss : 0.1528, accuracy : 95.50\n",
            "iteration : 300, loss : 0.1537, accuracy : 95.48\n",
            "iteration : 350, loss : 0.1533, accuracy : 95.48\n",
            "Epoch :  73, training loss : 0.1534, training accuracy : 95.47, test loss : 0.2523, test accuracy : 92.79\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.1474, accuracy : 95.92\n",
            "iteration : 100, loss : 0.1506, accuracy : 95.70\n",
            "iteration : 150, loss : 0.1509, accuracy : 95.58\n",
            "iteration : 200, loss : 0.1493, accuracy : 95.64\n",
            "iteration : 250, loss : 0.1471, accuracy : 95.69\n",
            "iteration : 300, loss : 0.1498, accuracy : 95.58\n",
            "iteration : 350, loss : 0.1501, accuracy : 95.61\n",
            "Epoch :  74, training loss : 0.1494, training accuracy : 95.62, test loss : 0.2409, test accuracy : 93.22\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.1385, accuracy : 96.02\n",
            "iteration : 100, loss : 0.1488, accuracy : 95.73\n",
            "iteration : 150, loss : 0.1513, accuracy : 95.62\n",
            "iteration : 200, loss : 0.1485, accuracy : 95.75\n",
            "iteration : 250, loss : 0.1481, accuracy : 95.76\n",
            "iteration : 300, loss : 0.1498, accuracy : 95.64\n",
            "iteration : 350, loss : 0.1494, accuracy : 95.69\n",
            "Epoch :  75, training loss : 0.1491, training accuracy : 95.68, test loss : 0.2353, test accuracy : 93.46\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.1341, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1404, accuracy : 95.77\n",
            "iteration : 150, loss : 0.1462, accuracy : 95.69\n",
            "iteration : 200, loss : 0.1464, accuracy : 95.77\n",
            "iteration : 250, loss : 0.1451, accuracy : 95.83\n",
            "iteration : 300, loss : 0.1463, accuracy : 95.76\n",
            "iteration : 350, loss : 0.1493, accuracy : 95.71\n",
            "Epoch :  76, training loss : 0.1496, training accuracy : 95.73, test loss : 0.2258, test accuracy : 93.80\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.1366, accuracy : 96.33\n",
            "iteration : 100, loss : 0.1422, accuracy : 96.03\n",
            "iteration : 150, loss : 0.1459, accuracy : 95.83\n",
            "iteration : 200, loss : 0.1475, accuracy : 95.71\n",
            "iteration : 250, loss : 0.1469, accuracy : 95.78\n",
            "iteration : 300, loss : 0.1468, accuracy : 95.73\n",
            "iteration : 350, loss : 0.1475, accuracy : 95.72\n",
            "Epoch :  77, training loss : 0.1488, training accuracy : 95.68, test loss : 0.2264, test accuracy : 93.83\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.1402, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1448, accuracy : 95.88\n",
            "iteration : 150, loss : 0.1468, accuracy : 95.71\n",
            "iteration : 200, loss : 0.1525, accuracy : 95.59\n",
            "iteration : 250, loss : 0.1496, accuracy : 95.71\n",
            "iteration : 300, loss : 0.1498, accuracy : 95.71\n",
            "iteration : 350, loss : 0.1506, accuracy : 95.70\n",
            "Epoch :  78, training loss : 0.1509, training accuracy : 95.70, test loss : 0.2365, test accuracy : 93.49\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.1470, accuracy : 95.83\n",
            "iteration : 100, loss : 0.1411, accuracy : 95.92\n",
            "iteration : 150, loss : 0.1437, accuracy : 95.84\n",
            "iteration : 200, loss : 0.1443, accuracy : 95.73\n",
            "iteration : 250, loss : 0.1427, accuracy : 95.77\n",
            "iteration : 300, loss : 0.1461, accuracy : 95.67\n",
            "iteration : 350, loss : 0.1490, accuracy : 95.60\n",
            "Epoch :  79, training loss : 0.1486, training accuracy : 95.62, test loss : 0.2343, test accuracy : 93.32\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.1180, accuracy : 96.69\n",
            "iteration : 100, loss : 0.1227, accuracy : 96.46\n",
            "iteration : 150, loss : 0.1274, accuracy : 96.29\n",
            "iteration : 200, loss : 0.1325, accuracy : 96.18\n",
            "iteration : 250, loss : 0.1357, accuracy : 96.08\n",
            "iteration : 300, loss : 0.1366, accuracy : 96.03\n",
            "iteration : 350, loss : 0.1402, accuracy : 95.93\n",
            "Epoch :  80, training loss : 0.1423, training accuracy : 95.88, test loss : 0.2312, test accuracy : 93.70\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.1417, accuracy : 95.89\n",
            "iteration : 100, loss : 0.1350, accuracy : 96.14\n",
            "iteration : 150, loss : 0.1388, accuracy : 95.97\n",
            "iteration : 200, loss : 0.1434, accuracy : 95.90\n",
            "iteration : 250, loss : 0.1446, accuracy : 95.86\n",
            "iteration : 300, loss : 0.1468, accuracy : 95.78\n",
            "iteration : 350, loss : 0.1462, accuracy : 95.78\n",
            "Epoch :  81, training loss : 0.1457, training accuracy : 95.79, test loss : 0.2435, test accuracy : 93.47\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.1254, accuracy : 96.31\n",
            "iteration : 100, loss : 0.1246, accuracy : 96.26\n",
            "iteration : 150, loss : 0.1251, accuracy : 96.29\n",
            "iteration : 200, loss : 0.1291, accuracy : 96.22\n",
            "iteration : 250, loss : 0.1335, accuracy : 96.16\n",
            "iteration : 300, loss : 0.1363, accuracy : 96.04\n",
            "iteration : 350, loss : 0.1406, accuracy : 95.93\n",
            "Epoch :  82, training loss : 0.1402, training accuracy : 95.95, test loss : 0.2479, test accuracy : 93.11\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.1296, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1273, accuracy : 96.37\n",
            "iteration : 150, loss : 0.1295, accuracy : 96.23\n",
            "iteration : 200, loss : 0.1340, accuracy : 96.15\n",
            "iteration : 250, loss : 0.1384, accuracy : 96.01\n",
            "iteration : 300, loss : 0.1409, accuracy : 95.95\n",
            "iteration : 350, loss : 0.1434, accuracy : 95.83\n",
            "Epoch :  83, training loss : 0.1441, training accuracy : 95.81, test loss : 0.2303, test accuracy : 93.70\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.1227, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1315, accuracy : 96.12\n",
            "iteration : 150, loss : 0.1414, accuracy : 95.85\n",
            "iteration : 200, loss : 0.1399, accuracy : 95.86\n",
            "iteration : 250, loss : 0.1414, accuracy : 95.88\n",
            "iteration : 300, loss : 0.1420, accuracy : 95.86\n",
            "iteration : 350, loss : 0.1433, accuracy : 95.80\n",
            "Epoch :  84, training loss : 0.1435, training accuracy : 95.78, test loss : 0.2307, test accuracy : 93.64\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.1453, accuracy : 95.89\n",
            "iteration : 100, loss : 0.1395, accuracy : 95.97\n",
            "iteration : 150, loss : 0.1350, accuracy : 96.01\n",
            "iteration : 200, loss : 0.1388, accuracy : 95.91\n",
            "iteration : 250, loss : 0.1402, accuracy : 95.86\n",
            "iteration : 300, loss : 0.1427, accuracy : 95.80\n",
            "iteration : 350, loss : 0.1433, accuracy : 95.80\n",
            "Epoch :  85, training loss : 0.1429, training accuracy : 95.81, test loss : 0.2417, test accuracy : 93.39\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.1396, accuracy : 95.95\n",
            "iteration : 100, loss : 0.1404, accuracy : 96.01\n",
            "iteration : 150, loss : 0.1368, accuracy : 96.03\n",
            "iteration : 200, loss : 0.1382, accuracy : 95.97\n",
            "iteration : 250, loss : 0.1389, accuracy : 95.99\n",
            "iteration : 300, loss : 0.1370, accuracy : 96.05\n",
            "iteration : 350, loss : 0.1380, accuracy : 95.98\n",
            "Epoch :  86, training loss : 0.1382, training accuracy : 95.98, test loss : 0.2479, test accuracy : 93.26\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.1265, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1324, accuracy : 96.03\n",
            "iteration : 150, loss : 0.1360, accuracy : 95.93\n",
            "iteration : 200, loss : 0.1393, accuracy : 95.85\n",
            "iteration : 250, loss : 0.1391, accuracy : 95.87\n",
            "iteration : 300, loss : 0.1392, accuracy : 95.90\n",
            "iteration : 350, loss : 0.1413, accuracy : 95.83\n",
            "Epoch :  87, training loss : 0.1414, training accuracy : 95.83, test loss : 0.2356, test accuracy : 93.42\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.1337, accuracy : 95.94\n",
            "iteration : 100, loss : 0.1342, accuracy : 96.01\n",
            "iteration : 150, loss : 0.1357, accuracy : 95.94\n",
            "iteration : 200, loss : 0.1394, accuracy : 95.87\n",
            "iteration : 250, loss : 0.1408, accuracy : 95.84\n",
            "iteration : 300, loss : 0.1400, accuracy : 95.84\n",
            "iteration : 350, loss : 0.1399, accuracy : 95.81\n",
            "Epoch :  88, training loss : 0.1405, training accuracy : 95.81, test loss : 0.2424, test accuracy : 93.31\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.1223, accuracy : 96.36\n",
            "iteration : 100, loss : 0.1243, accuracy : 96.35\n",
            "iteration : 150, loss : 0.1284, accuracy : 96.22\n",
            "iteration : 200, loss : 0.1323, accuracy : 96.05\n",
            "iteration : 250, loss : 0.1355, accuracy : 95.97\n",
            "iteration : 300, loss : 0.1375, accuracy : 95.91\n",
            "iteration : 350, loss : 0.1416, accuracy : 95.82\n",
            "Epoch :  89, training loss : 0.1418, training accuracy : 95.80, test loss : 0.2316, test accuracy : 93.75\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.1248, accuracy : 96.53\n",
            "iteration : 100, loss : 0.1274, accuracy : 96.37\n",
            "iteration : 150, loss : 0.1320, accuracy : 96.25\n",
            "iteration : 200, loss : 0.1325, accuracy : 96.20\n",
            "iteration : 250, loss : 0.1348, accuracy : 96.11\n",
            "iteration : 300, loss : 0.1367, accuracy : 96.09\n",
            "iteration : 350, loss : 0.1369, accuracy : 96.07\n",
            "Epoch :  90, training loss : 0.1366, training accuracy : 96.07, test loss : 0.2281, test accuracy : 93.81\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.1262, accuracy : 96.19\n",
            "iteration : 100, loss : 0.1280, accuracy : 96.23\n",
            "iteration : 150, loss : 0.1256, accuracy : 96.37\n",
            "iteration : 200, loss : 0.1270, accuracy : 96.32\n",
            "iteration : 250, loss : 0.1297, accuracy : 96.21\n",
            "iteration : 300, loss : 0.1321, accuracy : 96.15\n",
            "iteration : 350, loss : 0.1369, accuracy : 96.01\n",
            "Epoch :  91, training loss : 0.1371, training accuracy : 95.98, test loss : 0.2360, test accuracy : 93.58\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.1171, accuracy : 96.50\n",
            "iteration : 100, loss : 0.1231, accuracy : 96.35\n",
            "iteration : 150, loss : 0.1289, accuracy : 96.14\n",
            "iteration : 200, loss : 0.1329, accuracy : 96.00\n",
            "iteration : 250, loss : 0.1323, accuracy : 96.05\n",
            "iteration : 300, loss : 0.1358, accuracy : 95.97\n",
            "iteration : 350, loss : 0.1360, accuracy : 96.00\n",
            "Epoch :  92, training loss : 0.1365, training accuracy : 95.99, test loss : 0.2292, test accuracy : 93.61\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.1204, accuracy : 96.36\n",
            "iteration : 100, loss : 0.1297, accuracy : 96.22\n",
            "iteration : 150, loss : 0.1335, accuracy : 96.11\n",
            "iteration : 200, loss : 0.1401, accuracy : 95.93\n",
            "iteration : 250, loss : 0.1405, accuracy : 95.92\n",
            "iteration : 300, loss : 0.1393, accuracy : 95.94\n",
            "iteration : 350, loss : 0.1398, accuracy : 95.94\n",
            "Epoch :  93, training loss : 0.1403, training accuracy : 95.95, test loss : 0.2359, test accuracy : 93.46\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.1297, accuracy : 96.14\n",
            "iteration : 100, loss : 0.1286, accuracy : 96.23\n",
            "iteration : 150, loss : 0.1303, accuracy : 96.18\n",
            "iteration : 200, loss : 0.1298, accuracy : 96.17\n",
            "iteration : 250, loss : 0.1306, accuracy : 96.15\n",
            "iteration : 300, loss : 0.1323, accuracy : 96.12\n",
            "iteration : 350, loss : 0.1332, accuracy : 96.07\n",
            "Epoch :  94, training loss : 0.1343, training accuracy : 96.04, test loss : 0.2405, test accuracy : 93.40\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.1234, accuracy : 96.66\n",
            "iteration : 100, loss : 0.1202, accuracy : 96.63\n",
            "iteration : 150, loss : 0.1186, accuracy : 96.67\n",
            "iteration : 200, loss : 0.1210, accuracy : 96.59\n",
            "iteration : 250, loss : 0.1227, accuracy : 96.45\n",
            "iteration : 300, loss : 0.1286, accuracy : 96.25\n",
            "iteration : 350, loss : 0.1323, accuracy : 96.09\n",
            "Epoch :  95, training loss : 0.1325, training accuracy : 96.08, test loss : 0.2432, test accuracy : 93.22\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.1157, accuracy : 96.75\n",
            "iteration : 100, loss : 0.1172, accuracy : 96.62\n",
            "iteration : 150, loss : 0.1245, accuracy : 96.28\n",
            "iteration : 200, loss : 0.1307, accuracy : 96.14\n",
            "iteration : 250, loss : 0.1299, accuracy : 96.10\n",
            "iteration : 300, loss : 0.1313, accuracy : 96.09\n",
            "iteration : 350, loss : 0.1317, accuracy : 96.07\n",
            "Epoch :  96, training loss : 0.1351, training accuracy : 95.98, test loss : 0.2453, test accuracy : 93.11\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.1336, accuracy : 96.20\n",
            "iteration : 100, loss : 0.1336, accuracy : 96.11\n",
            "iteration : 150, loss : 0.1324, accuracy : 96.23\n",
            "iteration : 200, loss : 0.1328, accuracy : 96.16\n",
            "iteration : 250, loss : 0.1356, accuracy : 96.08\n",
            "iteration : 300, loss : 0.1335, accuracy : 96.13\n",
            "iteration : 350, loss : 0.1334, accuracy : 96.11\n",
            "Epoch :  97, training loss : 0.1344, training accuracy : 96.10, test loss : 0.2396, test accuracy : 93.49\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.1403, accuracy : 95.78\n",
            "iteration : 100, loss : 0.1373, accuracy : 95.91\n",
            "iteration : 150, loss : 0.1332, accuracy : 96.03\n",
            "iteration : 200, loss : 0.1332, accuracy : 96.05\n",
            "iteration : 250, loss : 0.1319, accuracy : 96.14\n",
            "iteration : 300, loss : 0.1317, accuracy : 96.15\n",
            "iteration : 350, loss : 0.1331, accuracy : 96.09\n",
            "Epoch :  98, training loss : 0.1339, training accuracy : 96.04, test loss : 0.2583, test accuracy : 92.97\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.1310, accuracy : 96.28\n",
            "iteration : 100, loss : 0.1292, accuracy : 96.16\n",
            "iteration : 150, loss : 0.1317, accuracy : 96.04\n",
            "iteration : 200, loss : 0.1324, accuracy : 96.05\n",
            "iteration : 250, loss : 0.1356, accuracy : 95.95\n",
            "iteration : 300, loss : 0.1356, accuracy : 95.93\n",
            "iteration : 350, loss : 0.1354, accuracy : 95.94\n",
            "Epoch :  99, training loss : 0.1353, training accuracy : 95.92, test loss : 0.2396, test accuracy : 93.53\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.1262, accuracy : 96.19\n",
            "iteration : 100, loss : 0.1321, accuracy : 96.11\n",
            "iteration : 150, loss : 0.1285, accuracy : 96.20\n",
            "iteration : 200, loss : 0.1328, accuracy : 96.04\n",
            "iteration : 250, loss : 0.1316, accuracy : 96.12\n",
            "iteration : 300, loss : 0.1321, accuracy : 96.12\n",
            "iteration : 350, loss : 0.1323, accuracy : 96.08\n",
            "Epoch : 100, training loss : 0.1319, training accuracy : 96.11, test loss : 0.2340, test accuracy : 93.48\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.1241, accuracy : 96.25\n",
            "iteration : 100, loss : 0.1200, accuracy : 96.41\n",
            "iteration : 150, loss : 0.1217, accuracy : 96.43\n",
            "iteration : 200, loss : 0.1234, accuracy : 96.39\n",
            "iteration : 250, loss : 0.1242, accuracy : 96.35\n",
            "iteration : 300, loss : 0.1257, accuracy : 96.30\n",
            "iteration : 350, loss : 0.1274, accuracy : 96.25\n",
            "Epoch : 101, training loss : 0.1286, training accuracy : 96.22, test loss : 0.2342, test accuracy : 93.63\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.1211, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1190, accuracy : 96.48\n",
            "iteration : 150, loss : 0.1217, accuracy : 96.40\n",
            "iteration : 200, loss : 0.1235, accuracy : 96.38\n",
            "iteration : 250, loss : 0.1287, accuracy : 96.26\n",
            "iteration : 300, loss : 0.1308, accuracy : 96.17\n",
            "iteration : 350, loss : 0.1307, accuracy : 96.15\n",
            "Epoch : 102, training loss : 0.1311, training accuracy : 96.13, test loss : 0.2400, test accuracy : 93.47\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.1224, accuracy : 96.48\n",
            "iteration : 100, loss : 0.1186, accuracy : 96.60\n",
            "iteration : 150, loss : 0.1199, accuracy : 96.51\n",
            "iteration : 200, loss : 0.1263, accuracy : 96.34\n",
            "iteration : 250, loss : 0.1285, accuracy : 96.27\n",
            "iteration : 300, loss : 0.1294, accuracy : 96.24\n",
            "iteration : 350, loss : 0.1290, accuracy : 96.27\n",
            "Epoch : 103, training loss : 0.1290, training accuracy : 96.26, test loss : 0.2439, test accuracy : 93.35\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.1397, accuracy : 95.80\n",
            "iteration : 100, loss : 0.1349, accuracy : 95.97\n",
            "iteration : 150, loss : 0.1271, accuracy : 96.25\n",
            "iteration : 200, loss : 0.1234, accuracy : 96.41\n",
            "iteration : 250, loss : 0.1244, accuracy : 96.39\n",
            "iteration : 300, loss : 0.1250, accuracy : 96.34\n",
            "iteration : 350, loss : 0.1274, accuracy : 96.27\n",
            "Epoch : 104, training loss : 0.1281, training accuracy : 96.25, test loss : 0.2674, test accuracy : 92.67\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.1204, accuracy : 96.52\n",
            "iteration : 100, loss : 0.1146, accuracy : 96.56\n",
            "iteration : 150, loss : 0.1207, accuracy : 96.45\n",
            "iteration : 200, loss : 0.1230, accuracy : 96.38\n",
            "iteration : 250, loss : 0.1213, accuracy : 96.42\n",
            "iteration : 300, loss : 0.1230, accuracy : 96.38\n",
            "iteration : 350, loss : 0.1244, accuracy : 96.35\n",
            "Epoch : 105, training loss : 0.1247, training accuracy : 96.36, test loss : 0.2269, test accuracy : 93.82\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.1202, accuracy : 96.58\n",
            "iteration : 100, loss : 0.1143, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1164, accuracy : 96.52\n",
            "iteration : 200, loss : 0.1183, accuracy : 96.48\n",
            "iteration : 250, loss : 0.1209, accuracy : 96.37\n",
            "iteration : 300, loss : 0.1227, accuracy : 96.36\n",
            "iteration : 350, loss : 0.1251, accuracy : 96.27\n",
            "Epoch : 106, training loss : 0.1258, training accuracy : 96.23, test loss : 0.2338, test accuracy : 93.80\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.1195, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1216, accuracy : 96.20\n",
            "iteration : 150, loss : 0.1206, accuracy : 96.28\n",
            "iteration : 200, loss : 0.1225, accuracy : 96.25\n",
            "iteration : 250, loss : 0.1239, accuracy : 96.22\n",
            "iteration : 300, loss : 0.1282, accuracy : 96.11\n",
            "iteration : 350, loss : 0.1312, accuracy : 96.00\n",
            "Epoch : 107, training loss : 0.1312, training accuracy : 96.01, test loss : 0.2334, test accuracy : 93.73\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.1207, accuracy : 96.50\n",
            "iteration : 100, loss : 0.1134, accuracy : 96.70\n",
            "iteration : 150, loss : 0.1204, accuracy : 96.50\n",
            "iteration : 200, loss : 0.1212, accuracy : 96.49\n",
            "iteration : 250, loss : 0.1227, accuracy : 96.44\n",
            "iteration : 300, loss : 0.1226, accuracy : 96.41\n",
            "iteration : 350, loss : 0.1234, accuracy : 96.38\n",
            "Epoch : 108, training loss : 0.1237, training accuracy : 96.38, test loss : 0.2259, test accuracy : 93.96\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.1069, accuracy : 96.66\n",
            "iteration : 100, loss : 0.1175, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1146, accuracy : 96.58\n",
            "iteration : 200, loss : 0.1173, accuracy : 96.52\n",
            "iteration : 250, loss : 0.1173, accuracy : 96.54\n",
            "iteration : 300, loss : 0.1197, accuracy : 96.45\n",
            "iteration : 350, loss : 0.1220, accuracy : 96.37\n",
            "Epoch : 109, training loss : 0.1237, training accuracy : 96.32, test loss : 0.2409, test accuracy : 93.46\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.1090, accuracy : 96.81\n",
            "iteration : 100, loss : 0.1138, accuracy : 96.66\n",
            "iteration : 150, loss : 0.1198, accuracy : 96.58\n",
            "iteration : 200, loss : 0.1256, accuracy : 96.37\n",
            "iteration : 250, loss : 0.1260, accuracy : 96.33\n",
            "iteration : 300, loss : 0.1257, accuracy : 96.33\n",
            "iteration : 350, loss : 0.1267, accuracy : 96.32\n",
            "Epoch : 110, training loss : 0.1271, training accuracy : 96.30, test loss : 0.2504, test accuracy : 93.21\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.1164, accuracy : 96.44\n",
            "iteration : 100, loss : 0.1156, accuracy : 96.52\n",
            "iteration : 150, loss : 0.1197, accuracy : 96.44\n",
            "iteration : 200, loss : 0.1204, accuracy : 96.43\n",
            "iteration : 250, loss : 0.1214, accuracy : 96.38\n",
            "iteration : 300, loss : 0.1215, accuracy : 96.38\n",
            "iteration : 350, loss : 0.1231, accuracy : 96.35\n",
            "Epoch : 111, training loss : 0.1229, training accuracy : 96.37, test loss : 0.2399, test accuracy : 93.51\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.1253, accuracy : 95.95\n",
            "iteration : 100, loss : 0.1219, accuracy : 96.30\n",
            "iteration : 150, loss : 0.1165, accuracy : 96.51\n",
            "iteration : 200, loss : 0.1174, accuracy : 96.49\n",
            "iteration : 250, loss : 0.1196, accuracy : 96.41\n",
            "iteration : 300, loss : 0.1190, accuracy : 96.41\n",
            "iteration : 350, loss : 0.1193, accuracy : 96.43\n",
            "Epoch : 112, training loss : 0.1203, training accuracy : 96.42, test loss : 0.2468, test accuracy : 93.34\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.1188, accuracy : 96.19\n",
            "iteration : 100, loss : 0.1162, accuracy : 96.28\n",
            "iteration : 150, loss : 0.1232, accuracy : 96.22\n",
            "iteration : 200, loss : 0.1247, accuracy : 96.22\n",
            "iteration : 250, loss : 0.1232, accuracy : 96.25\n",
            "iteration : 300, loss : 0.1240, accuracy : 96.23\n",
            "iteration : 350, loss : 0.1251, accuracy : 96.24\n",
            "Epoch : 113, training loss : 0.1257, training accuracy : 96.24, test loss : 0.2418, test accuracy : 93.39\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.1124, accuracy : 96.97\n",
            "iteration : 100, loss : 0.1118, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1144, accuracy : 96.59\n",
            "iteration : 200, loss : 0.1157, accuracy : 96.53\n",
            "iteration : 250, loss : 0.1171, accuracy : 96.49\n",
            "iteration : 300, loss : 0.1171, accuracy : 96.52\n",
            "iteration : 350, loss : 0.1193, accuracy : 96.42\n",
            "Epoch : 114, training loss : 0.1191, training accuracy : 96.43, test loss : 0.2318, test accuracy : 93.70\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0985, accuracy : 97.20\n",
            "iteration : 100, loss : 0.1030, accuracy : 97.08\n",
            "iteration : 150, loss : 0.1100, accuracy : 96.91\n",
            "iteration : 200, loss : 0.1099, accuracy : 96.87\n",
            "iteration : 250, loss : 0.1140, accuracy : 96.78\n",
            "iteration : 300, loss : 0.1133, accuracy : 96.80\n",
            "iteration : 350, loss : 0.1150, accuracy : 96.77\n",
            "Epoch : 115, training loss : 0.1162, training accuracy : 96.74, test loss : 0.2468, test accuracy : 93.45\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0982, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1090, accuracy : 96.65\n",
            "iteration : 150, loss : 0.1118, accuracy : 96.58\n",
            "iteration : 200, loss : 0.1179, accuracy : 96.44\n",
            "iteration : 250, loss : 0.1178, accuracy : 96.44\n",
            "iteration : 300, loss : 0.1190, accuracy : 96.41\n",
            "iteration : 350, loss : 0.1210, accuracy : 96.37\n",
            "Epoch : 116, training loss : 0.1204, training accuracy : 96.38, test loss : 0.2287, test accuracy : 93.85\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.1220, accuracy : 96.33\n",
            "iteration : 100, loss : 0.1176, accuracy : 96.63\n",
            "iteration : 150, loss : 0.1195, accuracy : 96.55\n",
            "iteration : 200, loss : 0.1212, accuracy : 96.45\n",
            "iteration : 250, loss : 0.1211, accuracy : 96.47\n",
            "iteration : 300, loss : 0.1195, accuracy : 96.51\n",
            "iteration : 350, loss : 0.1194, accuracy : 96.52\n",
            "Epoch : 117, training loss : 0.1193, training accuracy : 96.52, test loss : 0.2391, test accuracy : 93.61\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.1108, accuracy : 96.56\n",
            "iteration : 100, loss : 0.1156, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1159, accuracy : 96.45\n",
            "iteration : 200, loss : 0.1180, accuracy : 96.41\n",
            "iteration : 250, loss : 0.1184, accuracy : 96.39\n",
            "iteration : 300, loss : 0.1186, accuracy : 96.40\n",
            "iteration : 350, loss : 0.1191, accuracy : 96.40\n",
            "Epoch : 118, training loss : 0.1204, training accuracy : 96.37, test loss : 0.2333, test accuracy : 93.66\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.1200, accuracy : 96.25\n",
            "iteration : 100, loss : 0.1130, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1178, accuracy : 96.42\n",
            "iteration : 200, loss : 0.1176, accuracy : 96.48\n",
            "iteration : 250, loss : 0.1192, accuracy : 96.50\n",
            "iteration : 300, loss : 0.1184, accuracy : 96.50\n",
            "iteration : 350, loss : 0.1190, accuracy : 96.50\n",
            "Epoch : 119, training loss : 0.1188, training accuracy : 96.50, test loss : 0.2317, test accuracy : 93.76\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.1075, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1141, accuracy : 96.62\n",
            "iteration : 150, loss : 0.1152, accuracy : 96.60\n",
            "iteration : 200, loss : 0.1141, accuracy : 96.66\n",
            "iteration : 250, loss : 0.1138, accuracy : 96.64\n",
            "iteration : 300, loss : 0.1142, accuracy : 96.58\n",
            "iteration : 350, loss : 0.1156, accuracy : 96.53\n",
            "Epoch : 120, training loss : 0.1160, training accuracy : 96.51, test loss : 0.2456, test accuracy : 93.50\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.1172, accuracy : 96.64\n",
            "iteration : 100, loss : 0.1117, accuracy : 96.84\n",
            "iteration : 150, loss : 0.1091, accuracy : 96.85\n",
            "iteration : 200, loss : 0.1096, accuracy : 96.75\n",
            "iteration : 250, loss : 0.1097, accuracy : 96.69\n",
            "iteration : 300, loss : 0.1115, accuracy : 96.66\n",
            "iteration : 350, loss : 0.1140, accuracy : 96.60\n",
            "Epoch : 121, training loss : 0.1141, training accuracy : 96.60, test loss : 0.2354, test accuracy : 93.60\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.1062, accuracy : 96.94\n",
            "iteration : 100, loss : 0.1132, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1145, accuracy : 96.66\n",
            "iteration : 200, loss : 0.1112, accuracy : 96.75\n",
            "iteration : 250, loss : 0.1117, accuracy : 96.72\n",
            "iteration : 300, loss : 0.1119, accuracy : 96.74\n",
            "iteration : 350, loss : 0.1138, accuracy : 96.67\n",
            "Epoch : 122, training loss : 0.1143, training accuracy : 96.67, test loss : 0.2366, test accuracy : 93.72\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.1140, accuracy : 96.64\n",
            "iteration : 100, loss : 0.1133, accuracy : 96.72\n",
            "iteration : 150, loss : 0.1142, accuracy : 96.68\n",
            "iteration : 200, loss : 0.1141, accuracy : 96.68\n",
            "iteration : 250, loss : 0.1178, accuracy : 96.53\n",
            "iteration : 300, loss : 0.1171, accuracy : 96.55\n",
            "iteration : 350, loss : 0.1180, accuracy : 96.50\n",
            "Epoch : 123, training loss : 0.1185, training accuracy : 96.49, test loss : 0.2380, test accuracy : 93.53\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.1158, accuracy : 96.45\n",
            "iteration : 100, loss : 0.1114, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1135, accuracy : 96.64\n",
            "iteration : 200, loss : 0.1104, accuracy : 96.71\n",
            "iteration : 250, loss : 0.1125, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1142, accuracy : 96.62\n",
            "iteration : 350, loss : 0.1139, accuracy : 96.60\n",
            "Epoch : 124, training loss : 0.1151, training accuracy : 96.57, test loss : 0.2439, test accuracy : 93.52\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.1174, accuracy : 96.67\n",
            "iteration : 100, loss : 0.1155, accuracy : 96.66\n",
            "iteration : 150, loss : 0.1158, accuracy : 96.65\n",
            "iteration : 200, loss : 0.1122, accuracy : 96.74\n",
            "iteration : 250, loss : 0.1121, accuracy : 96.69\n",
            "iteration : 300, loss : 0.1128, accuracy : 96.65\n",
            "iteration : 350, loss : 0.1141, accuracy : 96.61\n",
            "Epoch : 125, training loss : 0.1153, training accuracy : 96.57, test loss : 0.2382, test accuracy : 93.83\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.1096, accuracy : 96.84\n",
            "iteration : 100, loss : 0.1021, accuracy : 97.06\n",
            "iteration : 150, loss : 0.1018, accuracy : 97.04\n",
            "iteration : 200, loss : 0.1052, accuracy : 96.95\n",
            "iteration : 250, loss : 0.1051, accuracy : 96.92\n",
            "iteration : 300, loss : 0.1079, accuracy : 96.85\n",
            "iteration : 350, loss : 0.1082, accuracy : 96.84\n",
            "Epoch : 126, training loss : 0.1089, training accuracy : 96.79, test loss : 0.2447, test accuracy : 93.56\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0994, accuracy : 96.97\n",
            "iteration : 100, loss : 0.1041, accuracy : 96.85\n",
            "iteration : 150, loss : 0.1060, accuracy : 96.81\n",
            "iteration : 200, loss : 0.1095, accuracy : 96.73\n",
            "iteration : 250, loss : 0.1100, accuracy : 96.75\n",
            "iteration : 300, loss : 0.1117, accuracy : 96.70\n",
            "iteration : 350, loss : 0.1129, accuracy : 96.68\n",
            "Epoch : 127, training loss : 0.1122, training accuracy : 96.72, test loss : 0.2389, test accuracy : 93.72\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0966, accuracy : 97.03\n",
            "iteration : 100, loss : 0.1039, accuracy : 96.76\n",
            "iteration : 150, loss : 0.1101, accuracy : 96.58\n",
            "iteration : 200, loss : 0.1102, accuracy : 96.55\n",
            "iteration : 250, loss : 0.1120, accuracy : 96.56\n",
            "iteration : 300, loss : 0.1118, accuracy : 96.56\n",
            "iteration : 350, loss : 0.1121, accuracy : 96.56\n",
            "Epoch : 128, training loss : 0.1130, training accuracy : 96.53, test loss : 0.2434, test accuracy : 93.55\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.1006, accuracy : 97.09\n",
            "iteration : 100, loss : 0.1052, accuracy : 96.99\n",
            "iteration : 150, loss : 0.1041, accuracy : 96.91\n",
            "iteration : 200, loss : 0.1017, accuracy : 97.03\n",
            "iteration : 250, loss : 0.1034, accuracy : 96.97\n",
            "iteration : 300, loss : 0.1048, accuracy : 96.96\n",
            "iteration : 350, loss : 0.1065, accuracy : 96.92\n",
            "Epoch : 129, training loss : 0.1082, training accuracy : 96.87, test loss : 0.2389, test accuracy : 93.56\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.1065, accuracy : 96.77\n",
            "iteration : 100, loss : 0.1041, accuracy : 97.00\n",
            "iteration : 150, loss : 0.1042, accuracy : 96.97\n",
            "iteration : 200, loss : 0.1035, accuracy : 96.99\n",
            "iteration : 250, loss : 0.1055, accuracy : 96.97\n",
            "iteration : 300, loss : 0.1053, accuracy : 96.94\n",
            "iteration : 350, loss : 0.1067, accuracy : 96.90\n",
            "Epoch : 130, training loss : 0.1079, training accuracy : 96.88, test loss : 0.2468, test accuracy : 93.59\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.1058, accuracy : 96.78\n",
            "iteration : 100, loss : 0.1012, accuracy : 96.96\n",
            "iteration : 150, loss : 0.1017, accuracy : 96.95\n",
            "iteration : 200, loss : 0.1065, accuracy : 96.83\n",
            "iteration : 250, loss : 0.1082, accuracy : 96.79\n",
            "iteration : 300, loss : 0.1096, accuracy : 96.76\n",
            "iteration : 350, loss : 0.1091, accuracy : 96.76\n",
            "Epoch : 131, training loss : 0.1098, training accuracy : 96.75, test loss : 0.2382, test accuracy : 93.62\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.1066, accuracy : 96.84\n",
            "iteration : 100, loss : 0.1080, accuracy : 96.83\n",
            "iteration : 150, loss : 0.1053, accuracy : 96.82\n",
            "iteration : 200, loss : 0.1025, accuracy : 96.89\n",
            "iteration : 250, loss : 0.1050, accuracy : 96.81\n",
            "iteration : 300, loss : 0.1052, accuracy : 96.81\n",
            "iteration : 350, loss : 0.1073, accuracy : 96.75\n",
            "Epoch : 132, training loss : 0.1072, training accuracy : 96.77, test loss : 0.2438, test accuracy : 93.55\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.1094, accuracy : 96.75\n",
            "iteration : 100, loss : 0.1093, accuracy : 96.71\n",
            "iteration : 150, loss : 0.1052, accuracy : 96.90\n",
            "iteration : 200, loss : 0.1079, accuracy : 96.82\n",
            "iteration : 250, loss : 0.1094, accuracy : 96.78\n",
            "iteration : 300, loss : 0.1109, accuracy : 96.73\n",
            "iteration : 350, loss : 0.1102, accuracy : 96.75\n",
            "Epoch : 133, training loss : 0.1109, training accuracy : 96.74, test loss : 0.2340, test accuracy : 93.89\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0912, accuracy : 97.33\n",
            "iteration : 100, loss : 0.1042, accuracy : 96.89\n",
            "iteration : 150, loss : 0.1015, accuracy : 97.03\n",
            "iteration : 200, loss : 0.1002, accuracy : 97.05\n",
            "iteration : 250, loss : 0.1005, accuracy : 97.00\n",
            "iteration : 300, loss : 0.1029, accuracy : 96.92\n",
            "iteration : 350, loss : 0.1033, accuracy : 96.93\n",
            "Epoch : 134, training loss : 0.1037, training accuracy : 96.94, test loss : 0.2259, test accuracy : 94.16\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.1037, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1062, accuracy : 96.98\n",
            "iteration : 150, loss : 0.1091, accuracy : 96.89\n",
            "iteration : 200, loss : 0.1094, accuracy : 96.80\n",
            "iteration : 250, loss : 0.1073, accuracy : 96.88\n",
            "iteration : 300, loss : 0.1072, accuracy : 96.90\n",
            "iteration : 350, loss : 0.1055, accuracy : 96.94\n",
            "Epoch : 135, training loss : 0.1054, training accuracy : 96.94, test loss : 0.2346, test accuracy : 93.73\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0914, accuracy : 97.12\n",
            "iteration : 100, loss : 0.0979, accuracy : 96.91\n",
            "iteration : 150, loss : 0.0985, accuracy : 97.01\n",
            "iteration : 200, loss : 0.1031, accuracy : 96.93\n",
            "iteration : 250, loss : 0.1037, accuracy : 96.94\n",
            "iteration : 300, loss : 0.1061, accuracy : 96.85\n",
            "iteration : 350, loss : 0.1072, accuracy : 96.83\n",
            "Epoch : 136, training loss : 0.1073, training accuracy : 96.84, test loss : 0.2277, test accuracy : 94.06\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0992, accuracy : 96.92\n",
            "iteration : 100, loss : 0.0974, accuracy : 97.12\n",
            "iteration : 150, loss : 0.1015, accuracy : 96.93\n",
            "iteration : 200, loss : 0.1041, accuracy : 96.88\n",
            "iteration : 250, loss : 0.1060, accuracy : 96.85\n",
            "iteration : 300, loss : 0.1043, accuracy : 96.91\n",
            "iteration : 350, loss : 0.1048, accuracy : 96.88\n",
            "Epoch : 137, training loss : 0.1047, training accuracy : 96.88, test loss : 0.2241, test accuracy : 94.15\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.1045, accuracy : 97.05\n",
            "iteration : 100, loss : 0.0999, accuracy : 97.14\n",
            "iteration : 150, loss : 0.0981, accuracy : 97.13\n",
            "iteration : 200, loss : 0.0982, accuracy : 97.14\n",
            "iteration : 250, loss : 0.1001, accuracy : 97.10\n",
            "iteration : 300, loss : 0.1057, accuracy : 96.91\n",
            "iteration : 350, loss : 0.1052, accuracy : 96.92\n",
            "Epoch : 138, training loss : 0.1052, training accuracy : 96.91, test loss : 0.2388, test accuracy : 93.62\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0939, accuracy : 97.19\n",
            "iteration : 100, loss : 0.0957, accuracy : 97.09\n",
            "iteration : 150, loss : 0.0987, accuracy : 97.06\n",
            "iteration : 200, loss : 0.1017, accuracy : 96.96\n",
            "iteration : 250, loss : 0.1038, accuracy : 96.90\n",
            "iteration : 300, loss : 0.1035, accuracy : 96.89\n",
            "iteration : 350, loss : 0.1035, accuracy : 96.91\n",
            "Epoch : 139, training loss : 0.1034, training accuracy : 96.92, test loss : 0.2331, test accuracy : 94.03\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.1022, accuracy : 96.97\n",
            "iteration : 100, loss : 0.1009, accuracy : 97.10\n",
            "iteration : 150, loss : 0.0979, accuracy : 97.19\n",
            "iteration : 200, loss : 0.0995, accuracy : 97.16\n",
            "iteration : 250, loss : 0.1023, accuracy : 97.07\n",
            "iteration : 300, loss : 0.1023, accuracy : 97.08\n",
            "iteration : 350, loss : 0.1006, accuracy : 97.12\n",
            "Epoch : 140, training loss : 0.1005, training accuracy : 97.14, test loss : 0.2375, test accuracy : 93.88\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0751, accuracy : 97.69\n",
            "iteration : 100, loss : 0.0865, accuracy : 97.51\n",
            "iteration : 150, loss : 0.0882, accuracy : 97.40\n",
            "iteration : 200, loss : 0.0924, accuracy : 97.17\n",
            "iteration : 250, loss : 0.0982, accuracy : 97.03\n",
            "iteration : 300, loss : 0.0995, accuracy : 96.98\n",
            "iteration : 350, loss : 0.1029, accuracy : 96.87\n",
            "Epoch : 141, training loss : 0.1023, training accuracy : 96.89, test loss : 0.2394, test accuracy : 93.68\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0985, accuracy : 97.20\n",
            "iteration : 100, loss : 0.0960, accuracy : 97.17\n",
            "iteration : 150, loss : 0.0945, accuracy : 97.24\n",
            "iteration : 200, loss : 0.0949, accuracy : 97.15\n",
            "iteration : 250, loss : 0.0959, accuracy : 97.10\n",
            "iteration : 300, loss : 0.0998, accuracy : 97.01\n",
            "iteration : 350, loss : 0.1006, accuracy : 96.98\n",
            "Epoch : 142, training loss : 0.1010, training accuracy : 96.99, test loss : 0.2320, test accuracy : 93.68\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0840, accuracy : 97.36\n",
            "iteration : 100, loss : 0.0887, accuracy : 97.31\n",
            "iteration : 150, loss : 0.0916, accuracy : 97.21\n",
            "iteration : 200, loss : 0.0948, accuracy : 97.20\n",
            "iteration : 250, loss : 0.0971, accuracy : 97.14\n",
            "iteration : 300, loss : 0.0967, accuracy : 97.11\n",
            "iteration : 350, loss : 0.0978, accuracy : 97.09\n",
            "Epoch : 143, training loss : 0.0982, training accuracy : 97.06, test loss : 0.2249, test accuracy : 94.19\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0862, accuracy : 97.55\n",
            "iteration : 100, loss : 0.0842, accuracy : 97.51\n",
            "iteration : 150, loss : 0.0867, accuracy : 97.46\n",
            "iteration : 200, loss : 0.0881, accuracy : 97.39\n",
            "iteration : 250, loss : 0.0917, accuracy : 97.28\n",
            "iteration : 300, loss : 0.0929, accuracy : 97.23\n",
            "iteration : 350, loss : 0.0949, accuracy : 97.17\n",
            "Epoch : 144, training loss : 0.0964, training accuracy : 97.14, test loss : 0.2320, test accuracy : 93.93\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0910, accuracy : 97.05\n",
            "iteration : 100, loss : 0.0966, accuracy : 97.04\n",
            "iteration : 150, loss : 0.0975, accuracy : 97.04\n",
            "iteration : 200, loss : 0.0987, accuracy : 97.02\n",
            "iteration : 250, loss : 0.0995, accuracy : 96.97\n",
            "iteration : 300, loss : 0.0989, accuracy : 96.98\n",
            "iteration : 350, loss : 0.0990, accuracy : 96.99\n",
            "Epoch : 145, training loss : 0.0988, training accuracy : 97.00, test loss : 0.2462, test accuracy : 93.57\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0897, accuracy : 97.23\n",
            "iteration : 100, loss : 0.0898, accuracy : 97.20\n",
            "iteration : 150, loss : 0.0933, accuracy : 97.15\n",
            "iteration : 200, loss : 0.0953, accuracy : 97.12\n",
            "iteration : 250, loss : 0.0952, accuracy : 97.16\n",
            "iteration : 300, loss : 0.0975, accuracy : 97.08\n",
            "iteration : 350, loss : 0.0990, accuracy : 97.05\n",
            "Epoch : 146, training loss : 0.0991, training accuracy : 97.04, test loss : 0.2447, test accuracy : 93.63\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0874, accuracy : 97.33\n",
            "iteration : 100, loss : 0.0851, accuracy : 97.36\n",
            "iteration : 150, loss : 0.0878, accuracy : 97.36\n",
            "iteration : 200, loss : 0.0901, accuracy : 97.28\n",
            "iteration : 250, loss : 0.0934, accuracy : 97.18\n",
            "iteration : 300, loss : 0.0965, accuracy : 97.12\n",
            "iteration : 350, loss : 0.0966, accuracy : 97.08\n",
            "Epoch : 147, training loss : 0.0964, training accuracy : 97.09, test loss : 0.2182, test accuracy : 94.16\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0936, accuracy : 97.23\n",
            "iteration : 100, loss : 0.0920, accuracy : 97.20\n",
            "iteration : 150, loss : 0.0895, accuracy : 97.33\n",
            "iteration : 200, loss : 0.0900, accuracy : 97.36\n",
            "iteration : 250, loss : 0.0917, accuracy : 97.30\n",
            "iteration : 300, loss : 0.0932, accuracy : 97.30\n",
            "iteration : 350, loss : 0.0938, accuracy : 97.24\n",
            "Epoch : 148, training loss : 0.0939, training accuracy : 97.23, test loss : 0.2358, test accuracy : 93.70\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0832, accuracy : 97.75\n",
            "iteration : 100, loss : 0.0881, accuracy : 97.48\n",
            "iteration : 150, loss : 0.0878, accuracy : 97.46\n",
            "iteration : 200, loss : 0.0908, accuracy : 97.37\n",
            "iteration : 250, loss : 0.0938, accuracy : 97.29\n",
            "iteration : 300, loss : 0.0967, accuracy : 97.22\n",
            "iteration : 350, loss : 0.0971, accuracy : 97.19\n",
            "Epoch : 149, training loss : 0.0966, training accuracy : 97.21, test loss : 0.2346, test accuracy : 93.85\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0869, accuracy : 97.50\n",
            "iteration : 100, loss : 0.0922, accuracy : 97.34\n",
            "iteration : 150, loss : 0.0889, accuracy : 97.45\n",
            "iteration : 200, loss : 0.0886, accuracy : 97.40\n",
            "iteration : 250, loss : 0.0895, accuracy : 97.35\n",
            "iteration : 300, loss : 0.0907, accuracy : 97.31\n",
            "iteration : 350, loss : 0.0921, accuracy : 97.28\n",
            "Epoch : 150, training loss : 0.0929, training accuracy : 97.25, test loss : 0.2343, test accuracy : 93.92\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0926, accuracy : 97.17\n",
            "iteration : 100, loss : 0.0936, accuracy : 97.16\n",
            "iteration : 150, loss : 0.0959, accuracy : 97.12\n",
            "iteration : 200, loss : 0.0956, accuracy : 97.17\n",
            "iteration : 250, loss : 0.0960, accuracy : 97.14\n",
            "iteration : 300, loss : 0.0972, accuracy : 97.09\n",
            "iteration : 350, loss : 0.0973, accuracy : 97.11\n",
            "Epoch : 151, training loss : 0.0973, training accuracy : 97.10, test loss : 0.2312, test accuracy : 93.95\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0975, accuracy : 97.06\n",
            "iteration : 100, loss : 0.0912, accuracy : 97.28\n",
            "iteration : 150, loss : 0.0947, accuracy : 97.15\n",
            "iteration : 200, loss : 0.0923, accuracy : 97.20\n",
            "iteration : 250, loss : 0.0931, accuracy : 97.21\n",
            "iteration : 300, loss : 0.0925, accuracy : 97.24\n",
            "iteration : 350, loss : 0.0939, accuracy : 97.23\n",
            "Epoch : 152, training loss : 0.0939, training accuracy : 97.23, test loss : 0.2332, test accuracy : 93.83\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0832, accuracy : 97.64\n",
            "iteration : 100, loss : 0.0902, accuracy : 97.44\n",
            "iteration : 150, loss : 0.0865, accuracy : 97.52\n",
            "iteration : 200, loss : 0.0880, accuracy : 97.45\n",
            "iteration : 250, loss : 0.0914, accuracy : 97.33\n",
            "iteration : 300, loss : 0.0923, accuracy : 97.29\n",
            "iteration : 350, loss : 0.0916, accuracy : 97.30\n",
            "Epoch : 153, training loss : 0.0911, training accuracy : 97.31, test loss : 0.2409, test accuracy : 93.81\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0869, accuracy : 97.48\n",
            "iteration : 100, loss : 0.0858, accuracy : 97.51\n",
            "iteration : 150, loss : 0.0886, accuracy : 97.40\n",
            "iteration : 200, loss : 0.0890, accuracy : 97.38\n",
            "iteration : 250, loss : 0.0885, accuracy : 97.37\n",
            "iteration : 300, loss : 0.0901, accuracy : 97.30\n",
            "iteration : 350, loss : 0.0922, accuracy : 97.29\n",
            "Epoch : 154, training loss : 0.0927, training accuracy : 97.26, test loss : 0.2354, test accuracy : 93.93\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0891, accuracy : 97.27\n",
            "iteration : 100, loss : 0.0882, accuracy : 97.27\n",
            "iteration : 150, loss : 0.0870, accuracy : 97.35\n",
            "iteration : 200, loss : 0.0877, accuracy : 97.38\n",
            "iteration : 250, loss : 0.0892, accuracy : 97.33\n",
            "iteration : 300, loss : 0.0881, accuracy : 97.35\n",
            "iteration : 350, loss : 0.0898, accuracy : 97.32\n",
            "Epoch : 155, training loss : 0.0910, training accuracy : 97.28, test loss : 0.2391, test accuracy : 93.87\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0895, accuracy : 97.41\n",
            "iteration : 100, loss : 0.0882, accuracy : 97.47\n",
            "iteration : 150, loss : 0.0866, accuracy : 97.55\n",
            "iteration : 200, loss : 0.0861, accuracy : 97.56\n",
            "iteration : 250, loss : 0.0874, accuracy : 97.46\n",
            "iteration : 300, loss : 0.0885, accuracy : 97.43\n",
            "iteration : 350, loss : 0.0890, accuracy : 97.41\n",
            "Epoch : 156, training loss : 0.0896, training accuracy : 97.40, test loss : 0.2414, test accuracy : 93.68\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0949, accuracy : 97.17\n",
            "iteration : 100, loss : 0.0887, accuracy : 97.43\n",
            "iteration : 150, loss : 0.0900, accuracy : 97.41\n",
            "iteration : 200, loss : 0.0919, accuracy : 97.36\n",
            "iteration : 250, loss : 0.0938, accuracy : 97.23\n",
            "iteration : 300, loss : 0.0917, accuracy : 97.26\n",
            "iteration : 350, loss : 0.0920, accuracy : 97.24\n",
            "Epoch : 157, training loss : 0.0918, training accuracy : 97.25, test loss : 0.2401, test accuracy : 93.71\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0957, accuracy : 97.05\n",
            "iteration : 100, loss : 0.0853, accuracy : 97.41\n",
            "iteration : 150, loss : 0.0883, accuracy : 97.26\n",
            "iteration : 200, loss : 0.0882, accuracy : 97.30\n",
            "iteration : 250, loss : 0.0908, accuracy : 97.31\n",
            "iteration : 300, loss : 0.0899, accuracy : 97.32\n",
            "iteration : 350, loss : 0.0903, accuracy : 97.33\n",
            "Epoch : 158, training loss : 0.0894, training accuracy : 97.35, test loss : 0.2303, test accuracy : 94.12\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0805, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0874, accuracy : 97.44\n",
            "iteration : 150, loss : 0.0846, accuracy : 97.50\n",
            "iteration : 200, loss : 0.0887, accuracy : 97.41\n",
            "iteration : 250, loss : 0.0882, accuracy : 97.42\n",
            "iteration : 300, loss : 0.0888, accuracy : 97.38\n",
            "iteration : 350, loss : 0.0901, accuracy : 97.34\n",
            "Epoch : 159, training loss : 0.0899, training accuracy : 97.34, test loss : 0.2359, test accuracy : 93.98\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0766, accuracy : 97.80\n",
            "iteration : 100, loss : 0.0793, accuracy : 97.70\n",
            "iteration : 150, loss : 0.0834, accuracy : 97.56\n",
            "iteration : 200, loss : 0.0820, accuracy : 97.62\n",
            "iteration : 250, loss : 0.0831, accuracy : 97.57\n",
            "iteration : 300, loss : 0.0848, accuracy : 97.56\n",
            "iteration : 350, loss : 0.0849, accuracy : 97.55\n",
            "Epoch : 160, training loss : 0.0842, training accuracy : 97.57, test loss : 0.2345, test accuracy : 94.06\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0737, accuracy : 97.83\n",
            "iteration : 100, loss : 0.0778, accuracy : 97.69\n",
            "iteration : 150, loss : 0.0789, accuracy : 97.58\n",
            "iteration : 200, loss : 0.0810, accuracy : 97.54\n",
            "iteration : 250, loss : 0.0838, accuracy : 97.41\n",
            "iteration : 300, loss : 0.0856, accuracy : 97.40\n",
            "iteration : 350, loss : 0.0841, accuracy : 97.46\n",
            "Epoch : 161, training loss : 0.0840, training accuracy : 97.44, test loss : 0.2357, test accuracy : 94.03\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0760, accuracy : 97.73\n",
            "iteration : 100, loss : 0.0839, accuracy : 97.40\n",
            "iteration : 150, loss : 0.0859, accuracy : 97.38\n",
            "iteration : 200, loss : 0.0853, accuracy : 97.38\n",
            "iteration : 250, loss : 0.0845, accuracy : 97.41\n",
            "iteration : 300, loss : 0.0869, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0882, accuracy : 97.34\n",
            "Epoch : 162, training loss : 0.0887, training accuracy : 97.34, test loss : 0.2268, test accuracy : 94.08\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0876, accuracy : 97.39\n",
            "iteration : 100, loss : 0.0789, accuracy : 97.59\n",
            "iteration : 150, loss : 0.0803, accuracy : 97.60\n",
            "iteration : 200, loss : 0.0782, accuracy : 97.68\n",
            "iteration : 250, loss : 0.0803, accuracy : 97.61\n",
            "iteration : 300, loss : 0.0827, accuracy : 97.56\n",
            "iteration : 350, loss : 0.0831, accuracy : 97.51\n",
            "Epoch : 163, training loss : 0.0837, training accuracy : 97.49, test loss : 0.2379, test accuracy : 93.92\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0691, accuracy : 98.00\n",
            "iteration : 100, loss : 0.0761, accuracy : 97.73\n",
            "iteration : 150, loss : 0.0771, accuracy : 97.75\n",
            "iteration : 200, loss : 0.0765, accuracy : 97.80\n",
            "iteration : 250, loss : 0.0794, accuracy : 97.68\n",
            "iteration : 300, loss : 0.0802, accuracy : 97.64\n",
            "iteration : 350, loss : 0.0820, accuracy : 97.60\n",
            "Epoch : 164, training loss : 0.0826, training accuracy : 97.58, test loss : 0.2423, test accuracy : 93.73\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0747, accuracy : 97.78\n",
            "iteration : 100, loss : 0.0757, accuracy : 97.68\n",
            "iteration : 150, loss : 0.0797, accuracy : 97.61\n",
            "iteration : 200, loss : 0.0810, accuracy : 97.55\n",
            "iteration : 250, loss : 0.0831, accuracy : 97.53\n",
            "iteration : 300, loss : 0.0839, accuracy : 97.46\n",
            "iteration : 350, loss : 0.0838, accuracy : 97.48\n",
            "Epoch : 165, training loss : 0.0840, training accuracy : 97.46, test loss : 0.2341, test accuracy : 93.89\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0723, accuracy : 97.88\n",
            "iteration : 100, loss : 0.0806, accuracy : 97.56\n",
            "iteration : 150, loss : 0.0830, accuracy : 97.47\n",
            "iteration : 200, loss : 0.0837, accuracy : 97.44\n",
            "iteration : 250, loss : 0.0820, accuracy : 97.50\n",
            "iteration : 300, loss : 0.0816, accuracy : 97.52\n",
            "iteration : 350, loss : 0.0816, accuracy : 97.53\n",
            "Epoch : 166, training loss : 0.0823, training accuracy : 97.51, test loss : 0.2457, test accuracy : 93.92\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0790, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0799, accuracy : 97.61\n",
            "iteration : 150, loss : 0.0764, accuracy : 97.73\n",
            "iteration : 200, loss : 0.0768, accuracy : 97.72\n",
            "iteration : 250, loss : 0.0758, accuracy : 97.77\n",
            "iteration : 300, loss : 0.0778, accuracy : 97.71\n",
            "iteration : 350, loss : 0.0797, accuracy : 97.65\n",
            "Epoch : 167, training loss : 0.0796, training accuracy : 97.66, test loss : 0.2344, test accuracy : 93.95\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0704, accuracy : 97.95\n",
            "iteration : 100, loss : 0.0686, accuracy : 97.96\n",
            "iteration : 150, loss : 0.0721, accuracy : 97.92\n",
            "iteration : 200, loss : 0.0730, accuracy : 97.88\n",
            "iteration : 250, loss : 0.0758, accuracy : 97.72\n",
            "iteration : 300, loss : 0.0769, accuracy : 97.69\n",
            "iteration : 350, loss : 0.0784, accuracy : 97.62\n",
            "Epoch : 168, training loss : 0.0784, training accuracy : 97.62, test loss : 0.2461, test accuracy : 93.77\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0746, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0804, accuracy : 97.47\n",
            "iteration : 150, loss : 0.0803, accuracy : 97.52\n",
            "iteration : 200, loss : 0.0837, accuracy : 97.42\n",
            "iteration : 250, loss : 0.0838, accuracy : 97.42\n",
            "iteration : 300, loss : 0.0839, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0841, accuracy : 97.40\n",
            "Epoch : 169, training loss : 0.0844, training accuracy : 97.40, test loss : 0.2308, test accuracy : 94.05\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0630, accuracy : 98.14\n",
            "iteration : 100, loss : 0.0727, accuracy : 97.91\n",
            "iteration : 150, loss : 0.0759, accuracy : 97.81\n",
            "iteration : 200, loss : 0.0763, accuracy : 97.84\n",
            "iteration : 250, loss : 0.0760, accuracy : 97.81\n",
            "iteration : 300, loss : 0.0759, accuracy : 97.79\n",
            "iteration : 350, loss : 0.0758, accuracy : 97.77\n",
            "Epoch : 170, training loss : 0.0762, training accuracy : 97.78, test loss : 0.2422, test accuracy : 93.84\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0766, accuracy : 97.55\n",
            "iteration : 100, loss : 0.0744, accuracy : 97.72\n",
            "iteration : 150, loss : 0.0712, accuracy : 97.88\n",
            "iteration : 200, loss : 0.0728, accuracy : 97.83\n",
            "iteration : 250, loss : 0.0740, accuracy : 97.78\n",
            "iteration : 300, loss : 0.0752, accuracy : 97.76\n",
            "iteration : 350, loss : 0.0772, accuracy : 97.68\n",
            "Epoch : 171, training loss : 0.0776, training accuracy : 97.66, test loss : 0.2452, test accuracy : 94.05\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0667, accuracy : 97.75\n",
            "iteration : 100, loss : 0.0778, accuracy : 97.54\n",
            "iteration : 150, loss : 0.0797, accuracy : 97.56\n",
            "iteration : 200, loss : 0.0794, accuracy : 97.57\n",
            "iteration : 250, loss : 0.0784, accuracy : 97.58\n",
            "iteration : 300, loss : 0.0795, accuracy : 97.57\n",
            "iteration : 350, loss : 0.0790, accuracy : 97.59\n",
            "Epoch : 172, training loss : 0.0784, training accuracy : 97.62, test loss : 0.2325, test accuracy : 94.24\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0658, accuracy : 98.20\n",
            "iteration : 100, loss : 0.0669, accuracy : 98.10\n",
            "iteration : 150, loss : 0.0707, accuracy : 97.93\n",
            "iteration : 200, loss : 0.0727, accuracy : 97.82\n",
            "iteration : 250, loss : 0.0766, accuracy : 97.72\n",
            "iteration : 300, loss : 0.0766, accuracy : 97.76\n",
            "iteration : 350, loss : 0.0772, accuracy : 97.75\n",
            "Epoch : 173, training loss : 0.0768, training accuracy : 97.76, test loss : 0.2376, test accuracy : 94.09\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0748, accuracy : 97.75\n",
            "iteration : 100, loss : 0.0754, accuracy : 97.68\n",
            "iteration : 150, loss : 0.0732, accuracy : 97.79\n",
            "iteration : 200, loss : 0.0742, accuracy : 97.76\n",
            "iteration : 250, loss : 0.0737, accuracy : 97.75\n",
            "iteration : 300, loss : 0.0756, accuracy : 97.72\n",
            "iteration : 350, loss : 0.0760, accuracy : 97.69\n",
            "Epoch : 174, training loss : 0.0760, training accuracy : 97.69, test loss : 0.2352, test accuracy : 94.06\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0642, accuracy : 98.06\n",
            "iteration : 100, loss : 0.0741, accuracy : 97.74\n",
            "iteration : 150, loss : 0.0725, accuracy : 97.81\n",
            "iteration : 200, loss : 0.0721, accuracy : 97.80\n",
            "iteration : 250, loss : 0.0705, accuracy : 97.86\n",
            "iteration : 300, loss : 0.0716, accuracy : 97.82\n",
            "iteration : 350, loss : 0.0731, accuracy : 97.75\n",
            "Epoch : 175, training loss : 0.0725, training accuracy : 97.77, test loss : 0.2402, test accuracy : 93.94\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0679, accuracy : 98.14\n",
            "iteration : 100, loss : 0.0722, accuracy : 97.91\n",
            "iteration : 150, loss : 0.0728, accuracy : 97.88\n",
            "iteration : 200, loss : 0.0744, accuracy : 97.87\n",
            "iteration : 250, loss : 0.0736, accuracy : 97.88\n",
            "iteration : 300, loss : 0.0735, accuracy : 97.85\n",
            "iteration : 350, loss : 0.0740, accuracy : 97.83\n",
            "Epoch : 176, training loss : 0.0739, training accuracy : 97.83, test loss : 0.2464, test accuracy : 93.83\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0758, accuracy : 97.86\n",
            "iteration : 100, loss : 0.0718, accuracy : 97.96\n",
            "iteration : 150, loss : 0.0715, accuracy : 97.95\n",
            "iteration : 200, loss : 0.0753, accuracy : 97.84\n",
            "iteration : 250, loss : 0.0739, accuracy : 97.85\n",
            "iteration : 300, loss : 0.0736, accuracy : 97.84\n",
            "iteration : 350, loss : 0.0730, accuracy : 97.84\n",
            "Epoch : 177, training loss : 0.0731, training accuracy : 97.83, test loss : 0.2417, test accuracy : 94.02\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0635, accuracy : 98.09\n",
            "iteration : 100, loss : 0.0707, accuracy : 97.92\n",
            "iteration : 150, loss : 0.0737, accuracy : 97.86\n",
            "iteration : 200, loss : 0.0719, accuracy : 97.85\n",
            "iteration : 250, loss : 0.0719, accuracy : 97.84\n",
            "iteration : 300, loss : 0.0724, accuracy : 97.83\n",
            "iteration : 350, loss : 0.0724, accuracy : 97.83\n",
            "Epoch : 178, training loss : 0.0724, training accuracy : 97.83, test loss : 0.2378, test accuracy : 94.14\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0704, accuracy : 97.81\n",
            "iteration : 100, loss : 0.0731, accuracy : 97.75\n",
            "iteration : 150, loss : 0.0742, accuracy : 97.74\n",
            "iteration : 200, loss : 0.0731, accuracy : 97.74\n",
            "iteration : 250, loss : 0.0749, accuracy : 97.74\n",
            "iteration : 300, loss : 0.0750, accuracy : 97.72\n",
            "iteration : 350, loss : 0.0734, accuracy : 97.76\n",
            "Epoch : 179, training loss : 0.0739, training accuracy : 97.74, test loss : 0.2321, test accuracy : 94.32\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0707, accuracy : 98.08\n",
            "iteration : 100, loss : 0.0671, accuracy : 98.02\n",
            "iteration : 150, loss : 0.0689, accuracy : 97.99\n",
            "iteration : 200, loss : 0.0699, accuracy : 97.90\n",
            "iteration : 250, loss : 0.0703, accuracy : 97.91\n",
            "iteration : 300, loss : 0.0722, accuracy : 97.84\n",
            "iteration : 350, loss : 0.0729, accuracy : 97.78\n",
            "Epoch : 180, training loss : 0.0734, training accuracy : 97.76, test loss : 0.2319, test accuracy : 94.21\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0736, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0703, accuracy : 97.90\n",
            "iteration : 150, loss : 0.0692, accuracy : 97.97\n",
            "iteration : 200, loss : 0.0718, accuracy : 97.92\n",
            "iteration : 250, loss : 0.0719, accuracy : 97.88\n",
            "iteration : 300, loss : 0.0700, accuracy : 97.93\n",
            "iteration : 350, loss : 0.0704, accuracy : 97.90\n",
            "Epoch : 181, training loss : 0.0711, training accuracy : 97.89, test loss : 0.2367, test accuracy : 94.19\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0646, accuracy : 97.98\n",
            "iteration : 100, loss : 0.0638, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0688, accuracy : 97.94\n",
            "iteration : 200, loss : 0.0685, accuracy : 97.98\n",
            "iteration : 250, loss : 0.0684, accuracy : 97.98\n",
            "iteration : 300, loss : 0.0694, accuracy : 97.96\n",
            "iteration : 350, loss : 0.0691, accuracy : 97.96\n",
            "Epoch : 182, training loss : 0.0690, training accuracy : 97.97, test loss : 0.2339, test accuracy : 93.98\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0617, accuracy : 98.06\n",
            "iteration : 100, loss : 0.0656, accuracy : 97.98\n",
            "iteration : 150, loss : 0.0628, accuracy : 98.09\n",
            "iteration : 200, loss : 0.0634, accuracy : 98.09\n",
            "iteration : 250, loss : 0.0641, accuracy : 98.07\n",
            "iteration : 300, loss : 0.0651, accuracy : 98.06\n",
            "iteration : 350, loss : 0.0680, accuracy : 97.98\n",
            "Epoch : 183, training loss : 0.0682, training accuracy : 97.96, test loss : 0.2358, test accuracy : 94.24\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0656, accuracy : 98.16\n",
            "iteration : 100, loss : 0.0661, accuracy : 98.12\n",
            "iteration : 150, loss : 0.0663, accuracy : 98.06\n",
            "iteration : 200, loss : 0.0680, accuracy : 98.05\n",
            "iteration : 250, loss : 0.0691, accuracy : 98.03\n",
            "iteration : 300, loss : 0.0696, accuracy : 98.03\n",
            "iteration : 350, loss : 0.0709, accuracy : 97.98\n",
            "Epoch : 184, training loss : 0.0701, training accuracy : 98.00, test loss : 0.2355, test accuracy : 94.16\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0727, accuracy : 97.94\n",
            "iteration : 100, loss : 0.0702, accuracy : 97.95\n",
            "iteration : 150, loss : 0.0700, accuracy : 97.95\n",
            "iteration : 200, loss : 0.0677, accuracy : 98.01\n",
            "iteration : 250, loss : 0.0667, accuracy : 98.06\n",
            "iteration : 300, loss : 0.0685, accuracy : 97.98\n",
            "iteration : 350, loss : 0.0683, accuracy : 97.98\n",
            "Epoch : 185, training loss : 0.0682, training accuracy : 97.99, test loss : 0.2413, test accuracy : 94.23\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0609, accuracy : 98.19\n",
            "iteration : 100, loss : 0.0693, accuracy : 97.94\n",
            "iteration : 150, loss : 0.0717, accuracy : 97.85\n",
            "iteration : 200, loss : 0.0706, accuracy : 97.89\n",
            "iteration : 250, loss : 0.0691, accuracy : 97.94\n",
            "iteration : 300, loss : 0.0680, accuracy : 97.98\n",
            "iteration : 350, loss : 0.0671, accuracy : 98.00\n",
            "Epoch : 186, training loss : 0.0673, training accuracy : 97.98, test loss : 0.2363, test accuracy : 94.22\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0545, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0625, accuracy : 98.13\n",
            "iteration : 150, loss : 0.0641, accuracy : 98.08\n",
            "iteration : 200, loss : 0.0640, accuracy : 98.10\n",
            "iteration : 250, loss : 0.0638, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0656, accuracy : 98.05\n",
            "iteration : 350, loss : 0.0655, accuracy : 98.06\n",
            "Epoch : 187, training loss : 0.0658, training accuracy : 98.06, test loss : 0.2450, test accuracy : 94.01\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0570, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0589, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0608, accuracy : 98.24\n",
            "iteration : 200, loss : 0.0609, accuracy : 98.22\n",
            "iteration : 250, loss : 0.0629, accuracy : 98.17\n",
            "iteration : 300, loss : 0.0653, accuracy : 98.11\n",
            "iteration : 350, loss : 0.0655, accuracy : 98.07\n",
            "Epoch : 188, training loss : 0.0656, training accuracy : 98.05, test loss : 0.2349, test accuracy : 94.23\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0653, accuracy : 97.91\n",
            "iteration : 100, loss : 0.0622, accuracy : 98.02\n",
            "iteration : 150, loss : 0.0594, accuracy : 98.20\n",
            "iteration : 200, loss : 0.0605, accuracy : 98.18\n",
            "iteration : 250, loss : 0.0616, accuracy : 98.13\n",
            "iteration : 300, loss : 0.0642, accuracy : 98.03\n",
            "iteration : 350, loss : 0.0644, accuracy : 98.04\n",
            "Epoch : 189, training loss : 0.0640, training accuracy : 98.05, test loss : 0.2451, test accuracy : 94.13\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0469, accuracy : 98.59\n",
            "iteration : 100, loss : 0.0511, accuracy : 98.47\n",
            "iteration : 150, loss : 0.0547, accuracy : 98.32\n",
            "iteration : 200, loss : 0.0574, accuracy : 98.23\n",
            "iteration : 250, loss : 0.0578, accuracy : 98.22\n",
            "iteration : 300, loss : 0.0615, accuracy : 98.10\n",
            "iteration : 350, loss : 0.0627, accuracy : 98.08\n",
            "Epoch : 190, training loss : 0.0633, training accuracy : 98.07, test loss : 0.2408, test accuracy : 94.10\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0699, accuracy : 97.84\n",
            "iteration : 100, loss : 0.0669, accuracy : 97.93\n",
            "iteration : 150, loss : 0.0654, accuracy : 98.01\n",
            "iteration : 200, loss : 0.0652, accuracy : 98.04\n",
            "iteration : 250, loss : 0.0646, accuracy : 98.06\n",
            "iteration : 300, loss : 0.0642, accuracy : 98.09\n",
            "iteration : 350, loss : 0.0652, accuracy : 98.05\n",
            "Epoch : 191, training loss : 0.0660, training accuracy : 98.02, test loss : 0.2395, test accuracy : 94.20\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0624, accuracy : 98.02\n",
            "iteration : 100, loss : 0.0641, accuracy : 97.99\n",
            "iteration : 150, loss : 0.0626, accuracy : 98.09\n",
            "iteration : 200, loss : 0.0633, accuracy : 98.10\n",
            "iteration : 250, loss : 0.0642, accuracy : 97.99\n",
            "iteration : 300, loss : 0.0643, accuracy : 98.02\n",
            "iteration : 350, loss : 0.0632, accuracy : 98.05\n",
            "Epoch : 192, training loss : 0.0630, training accuracy : 98.06, test loss : 0.2314, test accuracy : 94.28\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0516, accuracy : 98.61\n",
            "iteration : 100, loss : 0.0558, accuracy : 98.40\n",
            "iteration : 150, loss : 0.0579, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0599, accuracy : 98.24\n",
            "iteration : 250, loss : 0.0595, accuracy : 98.23\n",
            "iteration : 300, loss : 0.0597, accuracy : 98.23\n",
            "iteration : 350, loss : 0.0600, accuracy : 98.24\n",
            "Epoch : 193, training loss : 0.0601, training accuracy : 98.23, test loss : 0.2340, test accuracy : 94.37\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0580, accuracy : 98.42\n",
            "iteration : 100, loss : 0.0558, accuracy : 98.47\n",
            "iteration : 150, loss : 0.0554, accuracy : 98.45\n",
            "iteration : 200, loss : 0.0578, accuracy : 98.35\n",
            "iteration : 250, loss : 0.0572, accuracy : 98.34\n",
            "iteration : 300, loss : 0.0575, accuracy : 98.32\n",
            "iteration : 350, loss : 0.0587, accuracy : 98.29\n",
            "Epoch : 194, training loss : 0.0590, training accuracy : 98.26, test loss : 0.2489, test accuracy : 94.00\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0521, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0610, accuracy : 98.17\n",
            "iteration : 150, loss : 0.0583, accuracy : 98.23\n",
            "iteration : 200, loss : 0.0605, accuracy : 98.17\n",
            "iteration : 250, loss : 0.0601, accuracy : 98.12\n",
            "iteration : 300, loss : 0.0601, accuracy : 98.16\n",
            "iteration : 350, loss : 0.0602, accuracy : 98.16\n",
            "Epoch : 195, training loss : 0.0601, training accuracy : 98.16, test loss : 0.2467, test accuracy : 94.07\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0550, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0586, accuracy : 98.27\n",
            "iteration : 150, loss : 0.0583, accuracy : 98.26\n",
            "iteration : 200, loss : 0.0612, accuracy : 98.18\n",
            "iteration : 250, loss : 0.0618, accuracy : 98.17\n",
            "iteration : 300, loss : 0.0615, accuracy : 98.19\n",
            "iteration : 350, loss : 0.0601, accuracy : 98.24\n",
            "Epoch : 196, training loss : 0.0608, training accuracy : 98.23, test loss : 0.2451, test accuracy : 94.02\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0643, accuracy : 98.25\n",
            "iteration : 100, loss : 0.0629, accuracy : 98.27\n",
            "iteration : 150, loss : 0.0611, accuracy : 98.26\n",
            "iteration : 200, loss : 0.0593, accuracy : 98.27\n",
            "iteration : 250, loss : 0.0599, accuracy : 98.25\n",
            "iteration : 300, loss : 0.0600, accuracy : 98.25\n",
            "iteration : 350, loss : 0.0597, accuracy : 98.26\n",
            "Epoch : 197, training loss : 0.0599, training accuracy : 98.25, test loss : 0.2420, test accuracy : 94.25\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0586, accuracy : 98.14\n",
            "iteration : 100, loss : 0.0578, accuracy : 98.24\n",
            "iteration : 150, loss : 0.0609, accuracy : 98.14\n",
            "iteration : 200, loss : 0.0577, accuracy : 98.22\n",
            "iteration : 250, loss : 0.0580, accuracy : 98.25\n",
            "iteration : 300, loss : 0.0574, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0569, accuracy : 98.28\n",
            "Epoch : 198, training loss : 0.0564, training accuracy : 98.31, test loss : 0.2356, test accuracy : 94.31\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0517, accuracy : 98.30\n",
            "iteration : 100, loss : 0.0510, accuracy : 98.43\n",
            "iteration : 150, loss : 0.0536, accuracy : 98.41\n",
            "iteration : 200, loss : 0.0533, accuracy : 98.39\n",
            "iteration : 250, loss : 0.0560, accuracy : 98.33\n",
            "iteration : 300, loss : 0.0579, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0570, accuracy : 98.30\n",
            "Epoch : 199, training loss : 0.0577, training accuracy : 98.28, test loss : 0.2422, test accuracy : 94.11\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0551, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0541, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0554, accuracy : 98.36\n",
            "iteration : 200, loss : 0.0583, accuracy : 98.27\n",
            "iteration : 250, loss : 0.0567, accuracy : 98.32\n",
            "iteration : 300, loss : 0.0562, accuracy : 98.31\n",
            "iteration : 350, loss : 0.0562, accuracy : 98.32\n",
            "Epoch : 200, training loss : 0.0566, training accuracy : 98.30, test loss : 0.2413, test accuracy : 94.24\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0582, accuracy : 98.16\n",
            "iteration : 100, loss : 0.0578, accuracy : 98.14\n",
            "iteration : 150, loss : 0.0557, accuracy : 98.23\n",
            "iteration : 200, loss : 0.0554, accuracy : 98.26\n",
            "iteration : 250, loss : 0.0556, accuracy : 98.26\n",
            "iteration : 300, loss : 0.0552, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0538, accuracy : 98.35\n",
            "Epoch : 201, training loss : 0.0536, training accuracy : 98.36, test loss : 0.2454, test accuracy : 94.20\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0546, accuracy : 98.34\n",
            "iteration : 100, loss : 0.0571, accuracy : 98.30\n",
            "iteration : 150, loss : 0.0540, accuracy : 98.39\n",
            "iteration : 200, loss : 0.0532, accuracy : 98.39\n",
            "iteration : 250, loss : 0.0534, accuracy : 98.41\n",
            "iteration : 300, loss : 0.0541, accuracy : 98.39\n",
            "iteration : 350, loss : 0.0548, accuracy : 98.37\n",
            "Epoch : 202, training loss : 0.0550, training accuracy : 98.35, test loss : 0.2416, test accuracy : 94.26\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0408, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0473, accuracy : 98.71\n",
            "iteration : 150, loss : 0.0501, accuracy : 98.59\n",
            "iteration : 200, loss : 0.0528, accuracy : 98.45\n",
            "iteration : 250, loss : 0.0530, accuracy : 98.42\n",
            "iteration : 300, loss : 0.0543, accuracy : 98.39\n",
            "iteration : 350, loss : 0.0538, accuracy : 98.39\n",
            "Epoch : 203, training loss : 0.0531, training accuracy : 98.41, test loss : 0.2447, test accuracy : 94.20\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0484, accuracy : 98.52\n",
            "iteration : 100, loss : 0.0524, accuracy : 98.45\n",
            "iteration : 150, loss : 0.0530, accuracy : 98.40\n",
            "iteration : 200, loss : 0.0536, accuracy : 98.36\n",
            "iteration : 250, loss : 0.0539, accuracy : 98.35\n",
            "iteration : 300, loss : 0.0541, accuracy : 98.38\n",
            "iteration : 350, loss : 0.0549, accuracy : 98.34\n",
            "Epoch : 204, training loss : 0.0554, training accuracy : 98.33, test loss : 0.2393, test accuracy : 94.20\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0581, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0536, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0575, accuracy : 98.30\n",
            "iteration : 200, loss : 0.0551, accuracy : 98.36\n",
            "iteration : 250, loss : 0.0549, accuracy : 98.37\n",
            "iteration : 300, loss : 0.0544, accuracy : 98.36\n",
            "iteration : 350, loss : 0.0550, accuracy : 98.36\n",
            "Epoch : 205, training loss : 0.0549, training accuracy : 98.37, test loss : 0.2390, test accuracy : 94.28\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0517, accuracy : 98.36\n",
            "iteration : 100, loss : 0.0512, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0484, accuracy : 98.57\n",
            "iteration : 200, loss : 0.0504, accuracy : 98.50\n",
            "iteration : 250, loss : 0.0500, accuracy : 98.51\n",
            "iteration : 300, loss : 0.0509, accuracy : 98.47\n",
            "iteration : 350, loss : 0.0508, accuracy : 98.46\n",
            "Epoch : 206, training loss : 0.0509, training accuracy : 98.46, test loss : 0.2456, test accuracy : 94.42\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0526, accuracy : 98.47\n",
            "iteration : 100, loss : 0.0500, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0481, accuracy : 98.52\n",
            "iteration : 200, loss : 0.0499, accuracy : 98.47\n",
            "iteration : 250, loss : 0.0505, accuracy : 98.45\n",
            "iteration : 300, loss : 0.0513, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0524, accuracy : 98.38\n",
            "Epoch : 207, training loss : 0.0525, training accuracy : 98.37, test loss : 0.2389, test accuracy : 94.23\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0442, accuracy : 98.80\n",
            "iteration : 100, loss : 0.0485, accuracy : 98.58\n",
            "iteration : 150, loss : 0.0482, accuracy : 98.55\n",
            "iteration : 200, loss : 0.0484, accuracy : 98.53\n",
            "iteration : 250, loss : 0.0490, accuracy : 98.50\n",
            "iteration : 300, loss : 0.0495, accuracy : 98.46\n",
            "iteration : 350, loss : 0.0508, accuracy : 98.42\n",
            "Epoch : 208, training loss : 0.0505, training accuracy : 98.43, test loss : 0.2360, test accuracy : 94.31\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0371, accuracy : 98.83\n",
            "iteration : 100, loss : 0.0413, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0447, accuracy : 98.71\n",
            "iteration : 200, loss : 0.0477, accuracy : 98.65\n",
            "iteration : 250, loss : 0.0483, accuracy : 98.62\n",
            "iteration : 300, loss : 0.0498, accuracy : 98.56\n",
            "iteration : 350, loss : 0.0514, accuracy : 98.49\n",
            "Epoch : 209, training loss : 0.0519, training accuracy : 98.47, test loss : 0.2299, test accuracy : 94.45\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0533, accuracy : 98.42\n",
            "iteration : 100, loss : 0.0539, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0537, accuracy : 98.39\n",
            "iteration : 200, loss : 0.0533, accuracy : 98.44\n",
            "iteration : 250, loss : 0.0526, accuracy : 98.44\n",
            "iteration : 300, loss : 0.0507, accuracy : 98.49\n",
            "iteration : 350, loss : 0.0523, accuracy : 98.46\n",
            "Epoch : 210, training loss : 0.0527, training accuracy : 98.45, test loss : 0.2470, test accuracy : 94.15\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0438, accuracy : 98.70\n",
            "iteration : 100, loss : 0.0438, accuracy : 98.66\n",
            "iteration : 150, loss : 0.0439, accuracy : 98.66\n",
            "iteration : 200, loss : 0.0457, accuracy : 98.60\n",
            "iteration : 250, loss : 0.0455, accuracy : 98.60\n",
            "iteration : 300, loss : 0.0467, accuracy : 98.58\n",
            "iteration : 350, loss : 0.0467, accuracy : 98.56\n",
            "Epoch : 211, training loss : 0.0467, training accuracy : 98.55, test loss : 0.2438, test accuracy : 94.11\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0414, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0443, accuracy : 98.65\n",
            "iteration : 150, loss : 0.0456, accuracy : 98.65\n",
            "iteration : 200, loss : 0.0475, accuracy : 98.57\n",
            "iteration : 250, loss : 0.0500, accuracy : 98.51\n",
            "iteration : 300, loss : 0.0503, accuracy : 98.50\n",
            "iteration : 350, loss : 0.0514, accuracy : 98.45\n",
            "Epoch : 212, training loss : 0.0513, training accuracy : 98.45, test loss : 0.2420, test accuracy : 94.30\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0470, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0472, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0477, accuracy : 98.60\n",
            "iteration : 200, loss : 0.0485, accuracy : 98.58\n",
            "iteration : 250, loss : 0.0489, accuracy : 98.55\n",
            "iteration : 300, loss : 0.0482, accuracy : 98.56\n",
            "iteration : 350, loss : 0.0489, accuracy : 98.54\n",
            "Epoch : 213, training loss : 0.0497, training accuracy : 98.50, test loss : 0.2438, test accuracy : 94.20\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0464, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0428, accuracy : 98.82\n",
            "iteration : 150, loss : 0.0432, accuracy : 98.79\n",
            "iteration : 200, loss : 0.0446, accuracy : 98.73\n",
            "iteration : 250, loss : 0.0458, accuracy : 98.67\n",
            "iteration : 300, loss : 0.0462, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0462, accuracy : 98.67\n",
            "Epoch : 214, training loss : 0.0466, training accuracy : 98.66, test loss : 0.2362, test accuracy : 94.51\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0441, accuracy : 98.70\n",
            "iteration : 100, loss : 0.0429, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0425, accuracy : 98.74\n",
            "iteration : 200, loss : 0.0431, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0436, accuracy : 98.68\n",
            "iteration : 300, loss : 0.0445, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0447, accuracy : 98.69\n",
            "Epoch : 215, training loss : 0.0450, training accuracy : 98.66, test loss : 0.2412, test accuracy : 94.51\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0465, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0490, accuracy : 98.60\n",
            "iteration : 150, loss : 0.0485, accuracy : 98.60\n",
            "iteration : 200, loss : 0.0478, accuracy : 98.64\n",
            "iteration : 250, loss : 0.0482, accuracy : 98.63\n",
            "iteration : 300, loss : 0.0484, accuracy : 98.62\n",
            "iteration : 350, loss : 0.0486, accuracy : 98.58\n",
            "Epoch : 216, training loss : 0.0487, training accuracy : 98.59, test loss : 0.2395, test accuracy : 94.33\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0414, accuracy : 98.72\n",
            "iteration : 100, loss : 0.0407, accuracy : 98.80\n",
            "iteration : 150, loss : 0.0389, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0376, accuracy : 98.87\n",
            "iteration : 250, loss : 0.0397, accuracy : 98.80\n",
            "iteration : 300, loss : 0.0407, accuracy : 98.77\n",
            "iteration : 350, loss : 0.0416, accuracy : 98.73\n",
            "Epoch : 217, training loss : 0.0420, training accuracy : 98.72, test loss : 0.2458, test accuracy : 94.31\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0374, accuracy : 98.88\n",
            "iteration : 100, loss : 0.0417, accuracy : 98.78\n",
            "iteration : 150, loss : 0.0452, accuracy : 98.67\n",
            "iteration : 200, loss : 0.0450, accuracy : 98.67\n",
            "iteration : 250, loss : 0.0457, accuracy : 98.67\n",
            "iteration : 300, loss : 0.0466, accuracy : 98.64\n",
            "iteration : 350, loss : 0.0460, accuracy : 98.66\n",
            "Epoch : 218, training loss : 0.0455, training accuracy : 98.68, test loss : 0.2466, test accuracy : 94.33\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0422, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0417, accuracy : 98.75\n",
            "iteration : 150, loss : 0.0425, accuracy : 98.70\n",
            "iteration : 200, loss : 0.0420, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0425, accuracy : 98.70\n",
            "iteration : 300, loss : 0.0432, accuracy : 98.70\n",
            "iteration : 350, loss : 0.0440, accuracy : 98.68\n",
            "Epoch : 219, training loss : 0.0445, training accuracy : 98.66, test loss : 0.2372, test accuracy : 94.54\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0386, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0409, accuracy : 98.67\n",
            "iteration : 150, loss : 0.0436, accuracy : 98.56\n",
            "iteration : 200, loss : 0.0436, accuracy : 98.62\n",
            "iteration : 250, loss : 0.0437, accuracy : 98.65\n",
            "iteration : 300, loss : 0.0432, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0429, accuracy : 98.69\n",
            "Epoch : 220, training loss : 0.0430, training accuracy : 98.69, test loss : 0.2379, test accuracy : 94.43\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0398, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0394, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0409, accuracy : 98.76\n",
            "iteration : 200, loss : 0.0404, accuracy : 98.80\n",
            "iteration : 250, loss : 0.0414, accuracy : 98.75\n",
            "iteration : 300, loss : 0.0415, accuracy : 98.75\n",
            "iteration : 350, loss : 0.0419, accuracy : 98.74\n",
            "Epoch : 221, training loss : 0.0425, training accuracy : 98.74, test loss : 0.2483, test accuracy : 94.30\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0405, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0450, accuracy : 98.63\n",
            "iteration : 150, loss : 0.0454, accuracy : 98.59\n",
            "iteration : 200, loss : 0.0428, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0417, accuracy : 98.72\n",
            "iteration : 300, loss : 0.0427, accuracy : 98.69\n",
            "iteration : 350, loss : 0.0433, accuracy : 98.67\n",
            "Epoch : 222, training loss : 0.0432, training accuracy : 98.69, test loss : 0.2406, test accuracy : 94.43\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0380, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0354, accuracy : 98.89\n",
            "iteration : 150, loss : 0.0373, accuracy : 98.81\n",
            "iteration : 200, loss : 0.0392, accuracy : 98.78\n",
            "iteration : 250, loss : 0.0397, accuracy : 98.77\n",
            "iteration : 300, loss : 0.0402, accuracy : 98.74\n",
            "iteration : 350, loss : 0.0397, accuracy : 98.75\n",
            "Epoch : 223, training loss : 0.0392, training accuracy : 98.77, test loss : 0.2505, test accuracy : 94.39\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0419, accuracy : 98.66\n",
            "iteration : 100, loss : 0.0420, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0409, accuracy : 98.80\n",
            "iteration : 200, loss : 0.0407, accuracy : 98.81\n",
            "iteration : 250, loss : 0.0404, accuracy : 98.84\n",
            "iteration : 300, loss : 0.0408, accuracy : 98.82\n",
            "iteration : 350, loss : 0.0410, accuracy : 98.81\n",
            "Epoch : 224, training loss : 0.0413, training accuracy : 98.81, test loss : 0.2461, test accuracy : 94.50\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0395, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0425, accuracy : 98.79\n",
            "iteration : 150, loss : 0.0434, accuracy : 98.70\n",
            "iteration : 200, loss : 0.0428, accuracy : 98.70\n",
            "iteration : 250, loss : 0.0423, accuracy : 98.72\n",
            "iteration : 300, loss : 0.0420, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0419, accuracy : 98.71\n",
            "Epoch : 225, training loss : 0.0420, training accuracy : 98.70, test loss : 0.2412, test accuracy : 94.42\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0424, accuracy : 98.72\n",
            "iteration : 100, loss : 0.0380, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0379, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0375, accuracy : 98.91\n",
            "iteration : 250, loss : 0.0385, accuracy : 98.87\n",
            "iteration : 300, loss : 0.0392, accuracy : 98.85\n",
            "iteration : 350, loss : 0.0383, accuracy : 98.86\n",
            "Epoch : 226, training loss : 0.0382, training accuracy : 98.86, test loss : 0.2454, test accuracy : 94.31\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0386, accuracy : 98.80\n",
            "iteration : 100, loss : 0.0392, accuracy : 98.79\n",
            "iteration : 150, loss : 0.0377, accuracy : 98.89\n",
            "iteration : 200, loss : 0.0371, accuracy : 98.90\n",
            "iteration : 250, loss : 0.0374, accuracy : 98.91\n",
            "iteration : 300, loss : 0.0377, accuracy : 98.89\n",
            "iteration : 350, loss : 0.0391, accuracy : 98.83\n",
            "Epoch : 227, training loss : 0.0389, training accuracy : 98.84, test loss : 0.2459, test accuracy : 94.40\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0368, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0376, accuracy : 98.89\n",
            "iteration : 150, loss : 0.0368, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0388, accuracy : 98.79\n",
            "iteration : 250, loss : 0.0376, accuracy : 98.82\n",
            "iteration : 300, loss : 0.0380, accuracy : 98.82\n",
            "iteration : 350, loss : 0.0372, accuracy : 98.85\n",
            "Epoch : 228, training loss : 0.0380, training accuracy : 98.84, test loss : 0.2437, test accuracy : 94.43\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0316, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0327, accuracy : 99.03\n",
            "iteration : 150, loss : 0.0373, accuracy : 98.88\n",
            "iteration : 200, loss : 0.0375, accuracy : 98.86\n",
            "iteration : 250, loss : 0.0370, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0378, accuracy : 98.85\n",
            "iteration : 350, loss : 0.0381, accuracy : 98.85\n",
            "Epoch : 229, training loss : 0.0378, training accuracy : 98.86, test loss : 0.2433, test accuracy : 94.39\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0397, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0394, accuracy : 98.72\n",
            "iteration : 150, loss : 0.0362, accuracy : 98.84\n",
            "iteration : 200, loss : 0.0367, accuracy : 98.85\n",
            "iteration : 250, loss : 0.0376, accuracy : 98.81\n",
            "iteration : 300, loss : 0.0375, accuracy : 98.81\n",
            "iteration : 350, loss : 0.0376, accuracy : 98.80\n",
            "Epoch : 230, training loss : 0.0375, training accuracy : 98.81, test loss : 0.2412, test accuracy : 94.39\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0371, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0370, accuracy : 98.97\n",
            "iteration : 150, loss : 0.0372, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0366, accuracy : 98.95\n",
            "iteration : 250, loss : 0.0358, accuracy : 98.97\n",
            "iteration : 300, loss : 0.0359, accuracy : 98.98\n",
            "iteration : 350, loss : 0.0358, accuracy : 98.98\n",
            "Epoch : 231, training loss : 0.0358, training accuracy : 98.98, test loss : 0.2422, test accuracy : 94.56\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0369, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0358, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0330, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0346, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0350, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0341, accuracy : 99.02\n",
            "iteration : 350, loss : 0.0346, accuracy : 99.01\n",
            "Epoch : 232, training loss : 0.0348, training accuracy : 99.00, test loss : 0.2416, test accuracy : 94.51\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0389, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0373, accuracy : 98.87\n",
            "iteration : 150, loss : 0.0381, accuracy : 98.81\n",
            "iteration : 200, loss : 0.0382, accuracy : 98.85\n",
            "iteration : 250, loss : 0.0380, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0385, accuracy : 98.87\n",
            "iteration : 350, loss : 0.0375, accuracy : 98.90\n",
            "Epoch : 233, training loss : 0.0372, training accuracy : 98.90, test loss : 0.2413, test accuracy : 94.62\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0360, accuracy : 98.78\n",
            "iteration : 100, loss : 0.0376, accuracy : 98.78\n",
            "iteration : 150, loss : 0.0358, accuracy : 98.86\n",
            "iteration : 200, loss : 0.0353, accuracy : 98.92\n",
            "iteration : 250, loss : 0.0355, accuracy : 98.93\n",
            "iteration : 300, loss : 0.0349, accuracy : 98.94\n",
            "iteration : 350, loss : 0.0354, accuracy : 98.94\n",
            "Epoch : 234, training loss : 0.0350, training accuracy : 98.96, test loss : 0.2379, test accuracy : 94.64\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0300, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0318, accuracy : 99.07\n",
            "iteration : 150, loss : 0.0322, accuracy : 99.07\n",
            "iteration : 200, loss : 0.0325, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0322, accuracy : 99.05\n",
            "iteration : 300, loss : 0.0325, accuracy : 99.04\n",
            "iteration : 350, loss : 0.0321, accuracy : 99.05\n",
            "Epoch : 235, training loss : 0.0320, training accuracy : 99.05, test loss : 0.2414, test accuracy : 94.48\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0354, accuracy : 98.88\n",
            "iteration : 100, loss : 0.0344, accuracy : 98.98\n",
            "iteration : 150, loss : 0.0335, accuracy : 98.99\n",
            "iteration : 200, loss : 0.0340, accuracy : 98.98\n",
            "iteration : 250, loss : 0.0356, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0365, accuracy : 98.91\n",
            "iteration : 350, loss : 0.0359, accuracy : 98.92\n",
            "Epoch : 236, training loss : 0.0363, training accuracy : 98.91, test loss : 0.2404, test accuracy : 94.54\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0306, accuracy : 99.08\n",
            "iteration : 100, loss : 0.0312, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0335, accuracy : 99.03\n",
            "iteration : 200, loss : 0.0343, accuracy : 98.98\n",
            "iteration : 250, loss : 0.0346, accuracy : 98.98\n",
            "iteration : 300, loss : 0.0339, accuracy : 99.02\n",
            "iteration : 350, loss : 0.0336, accuracy : 99.03\n",
            "Epoch : 237, training loss : 0.0336, training accuracy : 99.04, test loss : 0.2413, test accuracy : 94.47\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0345, accuracy : 99.06\n",
            "iteration : 100, loss : 0.0339, accuracy : 99.04\n",
            "iteration : 150, loss : 0.0339, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0317, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0319, accuracy : 99.07\n",
            "iteration : 300, loss : 0.0326, accuracy : 99.04\n",
            "iteration : 350, loss : 0.0338, accuracy : 99.02\n",
            "Epoch : 238, training loss : 0.0338, training accuracy : 99.01, test loss : 0.2424, test accuracy : 94.42\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0298, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0310, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0322, accuracy : 99.03\n",
            "iteration : 200, loss : 0.0327, accuracy : 98.98\n",
            "iteration : 250, loss : 0.0331, accuracy : 98.98\n",
            "iteration : 300, loss : 0.0336, accuracy : 98.96\n",
            "iteration : 350, loss : 0.0327, accuracy : 99.00\n",
            "Epoch : 239, training loss : 0.0327, training accuracy : 99.00, test loss : 0.2417, test accuracy : 94.63\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0317, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0313, accuracy : 99.07\n",
            "iteration : 150, loss : 0.0320, accuracy : 99.08\n",
            "iteration : 200, loss : 0.0315, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0314, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0309, accuracy : 99.13\n",
            "iteration : 350, loss : 0.0307, accuracy : 99.13\n",
            "Epoch : 240, training loss : 0.0303, training accuracy : 99.15, test loss : 0.2440, test accuracy : 94.50\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0295, accuracy : 99.20\n",
            "iteration : 100, loss : 0.0300, accuracy : 99.16\n",
            "iteration : 150, loss : 0.0296, accuracy : 99.15\n",
            "iteration : 200, loss : 0.0284, accuracy : 99.21\n",
            "iteration : 250, loss : 0.0283, accuracy : 99.19\n",
            "iteration : 300, loss : 0.0287, accuracy : 99.17\n",
            "iteration : 350, loss : 0.0296, accuracy : 99.14\n",
            "Epoch : 241, training loss : 0.0295, training accuracy : 99.14, test loss : 0.2455, test accuracy : 94.55\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0278, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0277, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0283, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0296, accuracy : 99.15\n",
            "iteration : 250, loss : 0.0300, accuracy : 99.14\n",
            "iteration : 300, loss : 0.0310, accuracy : 99.09\n",
            "iteration : 350, loss : 0.0315, accuracy : 99.07\n",
            "Epoch : 242, training loss : 0.0315, training accuracy : 99.07, test loss : 0.2450, test accuracy : 94.63\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0252, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0288, accuracy : 99.15\n",
            "iteration : 150, loss : 0.0318, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0297, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0292, accuracy : 99.10\n",
            "iteration : 300, loss : 0.0288, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0290, accuracy : 99.11\n",
            "Epoch : 243, training loss : 0.0291, training accuracy : 99.11, test loss : 0.2389, test accuracy : 94.66\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0222, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0257, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0269, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0275, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0285, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0286, accuracy : 99.13\n",
            "iteration : 350, loss : 0.0295, accuracy : 99.10\n",
            "Epoch : 244, training loss : 0.0292, training accuracy : 99.11, test loss : 0.2488, test accuracy : 94.43\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0270, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0283, accuracy : 99.19\n",
            "iteration : 150, loss : 0.0278, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0294, accuracy : 99.13\n",
            "iteration : 250, loss : 0.0291, accuracy : 99.13\n",
            "iteration : 300, loss : 0.0293, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0287, accuracy : 99.17\n",
            "Epoch : 245, training loss : 0.0288, training accuracy : 99.16, test loss : 0.2421, test accuracy : 94.65\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0269, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0293, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0290, accuracy : 99.14\n",
            "iteration : 200, loss : 0.0291, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0285, accuracy : 99.18\n",
            "iteration : 300, loss : 0.0295, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0293, accuracy : 99.15\n",
            "Epoch : 246, training loss : 0.0295, training accuracy : 99.15, test loss : 0.2488, test accuracy : 94.58\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0314, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0287, accuracy : 99.17\n",
            "iteration : 150, loss : 0.0277, accuracy : 99.19\n",
            "iteration : 200, loss : 0.0268, accuracy : 99.20\n",
            "iteration : 250, loss : 0.0267, accuracy : 99.20\n",
            "iteration : 300, loss : 0.0268, accuracy : 99.20\n",
            "iteration : 350, loss : 0.0272, accuracy : 99.20\n",
            "Epoch : 247, training loss : 0.0270, training accuracy : 99.20, test loss : 0.2477, test accuracy : 94.47\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0222, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0219, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0234, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0243, accuracy : 99.24\n",
            "iteration : 250, loss : 0.0259, accuracy : 99.21\n",
            "iteration : 300, loss : 0.0263, accuracy : 99.20\n",
            "iteration : 350, loss : 0.0269, accuracy : 99.19\n",
            "Epoch : 248, training loss : 0.0273, training accuracy : 99.17, test loss : 0.2403, test accuracy : 94.59\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0273, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0292, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0283, accuracy : 99.22\n",
            "iteration : 200, loss : 0.0280, accuracy : 99.25\n",
            "iteration : 250, loss : 0.0270, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0263, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0259, accuracy : 99.28\n",
            "Epoch : 249, training loss : 0.0258, training accuracy : 99.28, test loss : 0.2464, test accuracy : 94.55\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0270, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0244, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0252, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0259, accuracy : 99.28\n",
            "iteration : 250, loss : 0.0262, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0268, accuracy : 99.22\n",
            "iteration : 350, loss : 0.0272, accuracy : 99.21\n",
            "Epoch : 250, training loss : 0.0268, training accuracy : 99.21, test loss : 0.2420, test accuracy : 94.66\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0218, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0255, accuracy : 99.24\n",
            "iteration : 150, loss : 0.0256, accuracy : 99.25\n",
            "iteration : 200, loss : 0.0257, accuracy : 99.25\n",
            "iteration : 250, loss : 0.0263, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0259, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0262, accuracy : 99.25\n",
            "Epoch : 251, training loss : 0.0264, training accuracy : 99.23, test loss : 0.2470, test accuracy : 94.72\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0257, accuracy : 99.19\n",
            "iteration : 100, loss : 0.0238, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0249, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0246, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0256, accuracy : 99.21\n",
            "iteration : 300, loss : 0.0251, accuracy : 99.23\n",
            "iteration : 350, loss : 0.0251, accuracy : 99.23\n",
            "Epoch : 252, training loss : 0.0261, training accuracy : 99.21, test loss : 0.2464, test accuracy : 94.70\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0237, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0269, accuracy : 99.26\n",
            "iteration : 150, loss : 0.0265, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0259, accuracy : 99.25\n",
            "iteration : 250, loss : 0.0253, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0256, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0254, accuracy : 99.27\n",
            "Epoch : 253, training loss : 0.0248, training accuracy : 99.29, test loss : 0.2441, test accuracy : 94.59\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0261, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0255, accuracy : 99.27\n",
            "iteration : 150, loss : 0.0244, accuracy : 99.30\n",
            "iteration : 200, loss : 0.0239, accuracy : 99.29\n",
            "iteration : 250, loss : 0.0239, accuracy : 99.31\n",
            "iteration : 300, loss : 0.0240, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0243, accuracy : 99.30\n",
            "Epoch : 254, training loss : 0.0245, training accuracy : 99.29, test loss : 0.2428, test accuracy : 94.67\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0240, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0222, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0239, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0247, accuracy : 99.28\n",
            "iteration : 250, loss : 0.0238, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0233, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0235, accuracy : 99.33\n",
            "Epoch : 255, training loss : 0.0239, training accuracy : 99.32, test loss : 0.2525, test accuracy : 94.54\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0235, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0234, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0255, accuracy : 99.22\n",
            "iteration : 200, loss : 0.0246, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0249, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0241, accuracy : 99.30\n",
            "iteration : 350, loss : 0.0246, accuracy : 99.29\n",
            "Epoch : 256, training loss : 0.0244, training accuracy : 99.30, test loss : 0.2452, test accuracy : 94.80\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0271, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0239, accuracy : 99.32\n",
            "iteration : 150, loss : 0.0237, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0230, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0245, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0248, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0247, accuracy : 99.30\n",
            "Epoch : 257, training loss : 0.0246, training accuracy : 99.30, test loss : 0.2505, test accuracy : 94.50\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0254, accuracy : 99.28\n",
            "iteration : 100, loss : 0.0229, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0217, accuracy : 99.34\n",
            "iteration : 200, loss : 0.0219, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0217, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0219, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0220, accuracy : 99.35\n",
            "Epoch : 258, training loss : 0.0218, training accuracy : 99.35, test loss : 0.2452, test accuracy : 94.74\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0214, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0208, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0201, accuracy : 99.37\n",
            "iteration : 200, loss : 0.0210, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0219, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0210, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0219, accuracy : 99.37\n",
            "Epoch : 259, training loss : 0.0221, training accuracy : 99.36, test loss : 0.2511, test accuracy : 94.63\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0194, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0202, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0219, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0221, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0228, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0231, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0233, accuracy : 99.34\n",
            "Epoch : 260, training loss : 0.0240, training accuracy : 99.32, test loss : 0.2491, test accuracy : 94.58\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0240, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0207, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0211, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0215, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0222, accuracy : 99.37\n",
            "iteration : 300, loss : 0.0221, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0211, accuracy : 99.40\n",
            "Epoch : 261, training loss : 0.0213, training accuracy : 99.39, test loss : 0.2447, test accuracy : 94.87\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0165, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0203, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0208, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0207, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0207, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0206, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0205, accuracy : 99.44\n",
            "Epoch : 262, training loss : 0.0207, training accuracy : 99.41, test loss : 0.2462, test accuracy : 94.65\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0244, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0229, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0221, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0213, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0219, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0210, accuracy : 99.40\n",
            "iteration : 350, loss : 0.0206, accuracy : 99.40\n",
            "Epoch : 263, training loss : 0.0208, training accuracy : 99.40, test loss : 0.2518, test accuracy : 94.71\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0234, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.26\n",
            "iteration : 150, loss : 0.0234, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0234, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0236, accuracy : 99.31\n",
            "iteration : 300, loss : 0.0240, accuracy : 99.31\n",
            "iteration : 350, loss : 0.0238, accuracy : 99.33\n",
            "Epoch : 264, training loss : 0.0238, training accuracy : 99.33, test loss : 0.2413, test accuracy : 94.65\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0179, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0170, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0173, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0179, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0186, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0192, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0194, accuracy : 99.46\n",
            "Epoch : 265, training loss : 0.0196, training accuracy : 99.45, test loss : 0.2461, test accuracy : 94.67\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0209, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0222, accuracy : 99.36\n",
            "iteration : 150, loss : 0.0227, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0217, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0224, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0223, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0217, accuracy : 99.38\n",
            "Epoch : 266, training loss : 0.0219, training accuracy : 99.38, test loss : 0.2393, test accuracy : 94.76\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0225, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0215, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0195, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0203, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0203, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0205, accuracy : 99.40\n",
            "iteration : 350, loss : 0.0195, accuracy : 99.44\n",
            "Epoch : 267, training loss : 0.0194, training accuracy : 99.45, test loss : 0.2400, test accuracy : 94.80\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0197, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0229, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0233, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0235, accuracy : 99.33\n",
            "iteration : 250, loss : 0.0229, accuracy : 99.35\n",
            "iteration : 300, loss : 0.0224, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0224, accuracy : 99.37\n",
            "Epoch : 268, training loss : 0.0220, training accuracy : 99.38, test loss : 0.2383, test accuracy : 94.84\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0196, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0203, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0213, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0208, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0208, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0206, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0210, accuracy : 99.40\n",
            "Epoch : 269, training loss : 0.0206, training accuracy : 99.42, test loss : 0.2426, test accuracy : 94.76\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0167, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0207, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0218, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0215, accuracy : 99.38\n",
            "iteration : 250, loss : 0.0213, accuracy : 99.40\n",
            "iteration : 300, loss : 0.0218, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0210, accuracy : 99.40\n",
            "Epoch : 270, training loss : 0.0212, training accuracy : 99.39, test loss : 0.2428, test accuracy : 94.73\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0219, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0192, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0198, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0203, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0204, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0194, accuracy : 99.44\n",
            "iteration : 350, loss : 0.0196, accuracy : 99.42\n",
            "Epoch : 271, training loss : 0.0194, training accuracy : 99.42, test loss : 0.2423, test accuracy : 94.77\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0206, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0195, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0194, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0192, accuracy : 99.47\n",
            "iteration : 250, loss : 0.0195, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0206, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0202, accuracy : 99.43\n",
            "Epoch : 272, training loss : 0.0202, training accuracy : 99.43, test loss : 0.2425, test accuracy : 94.73\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0175, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0181, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0186, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0188, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0192, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0194, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0193, accuracy : 99.46\n",
            "Epoch : 273, training loss : 0.0189, training accuracy : 99.48, test loss : 0.2432, test accuracy : 94.80\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0198, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0211, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0207, accuracy : 99.37\n",
            "iteration : 200, loss : 0.0203, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0192, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0194, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0192, accuracy : 99.43\n",
            "Epoch : 274, training loss : 0.0193, training accuracy : 99.43, test loss : 0.2498, test accuracy : 94.73\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0193, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0205, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0201, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0194, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0190, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0189, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0189, accuracy : 99.48\n",
            "Epoch : 275, training loss : 0.0191, training accuracy : 99.47, test loss : 0.2421, test accuracy : 94.77\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0191, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0187, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0190, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0185, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0184, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0182, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0187, accuracy : 99.51\n",
            "Epoch : 276, training loss : 0.0188, training accuracy : 99.51, test loss : 0.2455, test accuracy : 94.80\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0162, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0192, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0190, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0188, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0181, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0178, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0177, accuracy : 99.51\n",
            "Epoch : 277, training loss : 0.0176, training accuracy : 99.51, test loss : 0.2454, test accuracy : 94.78\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0206, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0188, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0187, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0177, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0186, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0181, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.51\n",
            "Epoch : 278, training loss : 0.0178, training accuracy : 99.50, test loss : 0.2537, test accuracy : 94.67\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0168, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0170, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0175, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0173, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0170, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.52\n",
            "Epoch : 279, training loss : 0.0178, training accuracy : 99.51, test loss : 0.2499, test accuracy : 94.70\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0160, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0167, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0179, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0181, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0178, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0180, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.49\n",
            "Epoch : 280, training loss : 0.0177, training accuracy : 99.48, test loss : 0.2442, test accuracy : 94.92\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0156, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0172, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0172, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0162, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0158, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0157, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0160, accuracy : 99.57\n",
            "Epoch : 281, training loss : 0.0160, training accuracy : 99.56, test loss : 0.2476, test accuracy : 94.89\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0162, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0158, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0176, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0173, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0169, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0170, accuracy : 99.54\n",
            "Epoch : 282, training loss : 0.0170, training accuracy : 99.54, test loss : 0.2491, test accuracy : 94.83\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0146, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0151, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0150, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0156, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0159, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0165, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0166, accuracy : 99.57\n",
            "Epoch : 283, training loss : 0.0164, training accuracy : 99.57, test loss : 0.2496, test accuracy : 94.72\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0174, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0175, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0171, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0165, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0169, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0167, accuracy : 99.58\n",
            "Epoch : 284, training loss : 0.0172, training accuracy : 99.56, test loss : 0.2487, test accuracy : 94.74\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0162, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0161, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0167, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0165, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0165, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0163, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0160, accuracy : 99.53\n",
            "Epoch : 285, training loss : 0.0164, training accuracy : 99.53, test loss : 0.2396, test accuracy : 94.95\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0146, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0189, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0182, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0191, accuracy : 99.47\n",
            "iteration : 250, loss : 0.0184, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0181, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0183, accuracy : 99.49\n",
            "Epoch : 286, training loss : 0.0181, training accuracy : 99.50, test loss : 0.2488, test accuracy : 94.66\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0169, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0179, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0190, accuracy : 99.50\n",
            "iteration : 200, loss : 0.0187, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0182, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0184, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0179, accuracy : 99.51\n",
            "Epoch : 287, training loss : 0.0178, training accuracy : 99.51, test loss : 0.2503, test accuracy : 94.81\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0221, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0188, accuracy : 99.50\n",
            "iteration : 150, loss : 0.0177, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0171, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0179, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0174, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0171, accuracy : 99.56\n",
            "Epoch : 288, training loss : 0.0170, training accuracy : 99.56, test loss : 0.2454, test accuracy : 94.89\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0173, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0168, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0181, accuracy : 99.50\n",
            "iteration : 200, loss : 0.0182, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0177, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0174, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0173, accuracy : 99.52\n",
            "Epoch : 289, training loss : 0.0172, training accuracy : 99.53, test loss : 0.2475, test accuracy : 94.76\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0166, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0163, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0165, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0166, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0166, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0165, accuracy : 99.55\n",
            "Epoch : 290, training loss : 0.0166, training accuracy : 99.55, test loss : 0.2444, test accuracy : 94.84\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0172, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0159, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0156, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0148, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0155, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0156, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0160, accuracy : 99.58\n",
            "Epoch : 291, training loss : 0.0158, training accuracy : 99.58, test loss : 0.2459, test accuracy : 94.80\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0192, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0176, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0173, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0173, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0181, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0180, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0177, accuracy : 99.53\n",
            "Epoch : 292, training loss : 0.0184, training accuracy : 99.52, test loss : 0.2439, test accuracy : 94.83\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0195, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0191, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0183, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0181, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0188, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0186, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0184, accuracy : 99.52\n",
            "Epoch : 293, training loss : 0.0181, training accuracy : 99.53, test loss : 0.2482, test accuracy : 94.79\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0158, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0174, accuracy : 99.50\n",
            "iteration : 200, loss : 0.0177, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0171, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0173, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0175, accuracy : 99.53\n",
            "Epoch : 294, training loss : 0.0175, training accuracy : 99.53, test loss : 0.2409, test accuracy : 94.93\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0152, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0165, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0172, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0178, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0176, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0171, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0171, accuracy : 99.52\n",
            "Epoch : 295, training loss : 0.0170, training accuracy : 99.52, test loss : 0.2496, test accuracy : 94.77\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0163, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0157, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0171, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0167, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0168, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0167, accuracy : 99.56\n",
            "Epoch : 296, training loss : 0.0165, training accuracy : 99.57, test loss : 0.2468, test accuracy : 94.74\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0150, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0158, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0166, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0171, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0168, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0168, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0165, accuracy : 99.57\n",
            "Epoch : 297, training loss : 0.0167, training accuracy : 99.56, test loss : 0.2454, test accuracy : 94.83\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0185, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0171, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0180, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0180, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0177, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0181, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0184, accuracy : 99.52\n",
            "Epoch : 298, training loss : 0.0187, training accuracy : 99.52, test loss : 0.2442, test accuracy : 94.86\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0160, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0154, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0151, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0160, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0164, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0161, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0161, accuracy : 99.54\n",
            "Epoch : 299, training loss : 0.0164, training accuracy : 99.53, test loss : 0.2486, test accuracy : 94.68\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0178, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0189, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0182, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0170, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0162, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0157, accuracy : 99.58\n",
            "Epoch : 300, training loss : 0.0159, training accuracy : 99.57, test loss : 0.2471, test accuracy : 94.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V59bF--4uUZz",
        "outputId": "cd61e257-0d5a-4001-a712-e246ed9c8fab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.16236956734392866, 96.49661954517516)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : AutoAugment\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "J0T9g8oYuVvQ",
        "outputId": "22a5c481-27a1-4c45-96d8-cdf11a8cc4ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : AutoAugment\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "zw1016HZuXjZ",
        "outputId": "95b7dc1a-ad78-49f4-8549-db39e96ba523"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_auto = {train_loss_list}\") \n",
        "print(f\"train_acc_list_auto = {train_acc_list}\")\n",
        "print(f\"test_loss_list_auto = {test_loss_list}\")\n",
        "print(f\"test_acc_list_auto = {test_acc_list}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwZSFAgeuZ1W",
        "outputId": "506ba40e-dd9a-48b4-9008-533930c3c777"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_auto = [1.233994613171916, 0.4539226504360757, 0.39892785723616436, 0.36951946796278967, 0.3572437209165516, 0.3364686794720368, 0.3232357050304813, 0.3139158589891625, 0.3026524600459308, 0.2874533208566629, 0.2781618708961701, 0.2686093915607225, 0.2590999076481111, 0.2540124244685095, 0.24861641480589947, 0.24399951269956138, 0.23923811831367695, 0.23367213850543106, 0.22791126531960196, 0.22499188158893327, 0.2189200812683196, 0.21588786823680084, 0.2124546934878277, 0.21025256699501338, 0.20572112148329819, 0.20327270222792457, 0.20072129237538755, 0.19866017976064024, 0.1968877356711442, 0.19574071886901287, 0.1951820781923891, 0.19288266742940194, 0.18784783969363222, 0.18916413984526464, 0.1828709624929803, 0.18385730885716312, 0.1843048237102626, 0.18300333889680825, 0.17988054518938712, 0.181258858744368, 0.178542331658647, 0.17391858212548866, 0.1761636258706168, 0.17584359522171944, 0.1711982707808496, 0.17569120711871603, 0.17175773992569143, 0.173915388350322, 0.16686817361571923, 0.17225952138543776, 0.16737438036699281, 0.1673657556839267, 0.16454098701759728, 0.16573736306814965, 0.1634644942918444, 0.1624382801535653, 0.16135445509741947, 0.16131306556747535, 0.16117559352177915, 0.15914395086083633, 0.1600172953392432, 0.16101780488865, 0.15760242929909288, 0.15396898807730616, 0.15771295737000662, 0.15534843960751685, 0.1561997470756372, 0.1591509728664709, 0.15701023427653443, 0.15164943112105858, 0.15431425330761647, 0.14900664126041135, 0.153352343944232, 0.1493505515198559, 0.14911859873732738, 0.14959679384460942, 0.14878295808222883, 0.15090822231599954, 0.14857992124024447, 0.1422798229043238, 0.14566040609466027, 0.14016860803251022, 0.14411792125764902, 0.1434947282459516, 0.14285532939361362, 0.13819237505678886, 0.1414163154436321, 0.14048225328144504, 0.14184128016556505, 0.13658087189766127, 0.1371091021209713, 0.13647958469705854, 0.14030104450033448, 0.1342820566864356, 0.132460017448801, 0.13510010811452297, 0.13437728684264308, 0.13391815061330148, 0.13526999323974617, 0.1319109741446933, 0.12862556951680804, 0.1311150672050511, 0.12895138073601536, 0.1280926277502604, 0.1246583734444648, 0.1258313457928295, 0.13119039074252775, 0.12371570407476044, 0.12365666381146527, 0.127088230446506, 0.12291975324938129, 0.12026552173503369, 0.12566383413687793, 0.11908095294665191, 0.11616113810701584, 0.12035970375709094, 0.11933369983522711, 0.12039770268226865, 0.11876794354621634, 0.11601000496244365, 0.11410852625643012, 0.1143225629872019, 0.11847472557833363, 0.11505588743492436, 0.11526977998232292, 0.10887061650069749, 0.11221126501314685, 0.1130289217270406, 0.10819881202193699, 0.10785190677800315, 0.10976265016993694, 0.10722535189372415, 0.11091778569710933, 0.1037355175993953, 0.10539038995720022, 0.10725624183934879, 0.10473110441492017, 0.10524187781431568, 0.10344582266580606, 0.10046376290965855, 0.10230541636102244, 0.10104834323188638, 0.09822337343016775, 0.09643481571358556, 0.09879150071501894, 0.09910423679120818, 0.09642951638213179, 0.09388373310609562, 0.09664719413996227, 0.09291944237360301, 0.09731551465454706, 0.09393294235100834, 0.0910643270961072, 0.09272648085686087, 0.09104305285162881, 0.08955065546138338, 0.0918019543374134, 0.0893830211510059, 0.08985447149400708, 0.08418837443579665, 0.08402341892968025, 0.08872269753487856, 0.08367900030440108, 0.08263780010818708, 0.08400570419503421, 0.08226117769352143, 0.07964462621849243, 0.07838223139732713, 0.08443687432180576, 0.07623482352440678, 0.0776083698258975, 0.07838072785779551, 0.0768121676771904, 0.07604568784546642, 0.0725143459350436, 0.07393465473551333, 0.07306322548538446, 0.07240707697952545, 0.07385282045316083, 0.07343979802507615, 0.07113057270969156, 0.06896691102797019, 0.06815409484296674, 0.07009156708804977, 0.0681532467273758, 0.06728222967456107, 0.06581138744748381, 0.06558232954362543, 0.06402831394629184, 0.06333761556619187, 0.06604849166542211, 0.0629559245281083, 0.06010610925356749, 0.058980082528953384, 0.06005086182922044, 0.060824787476400376, 0.05992534698989131, 0.05643098939306085, 0.057716956475704184, 0.05659668708124216, 0.053585430908806846, 0.055033171598655665, 0.05309754107929867, 0.055382789058363455, 0.05485604374056182, 0.05092235739269675, 0.05250531846346954, 0.05053107007873575, 0.0519121534665567, 0.05268361157520616, 0.0467233537332859, 0.05134408576742097, 0.04966532168808951, 0.04662036005569102, 0.0450139503640837, 0.04866801698750193, 0.04202018051501616, 0.04554983728052922, 0.04452912419968787, 0.04304213310670118, 0.04245415255982621, 0.043207022256715105, 0.03915923756804711, 0.04125189706607441, 0.04201531324972245, 0.038168357637885016, 0.03893532040768725, 0.03796733750174261, 0.03782938735188568, 0.037471614144540574, 0.03584066027936937, 0.03475043938310973, 0.0372414168192523, 0.0349701477995844, 0.032000547232671685, 0.036324127803135126, 0.03360092840889383, 0.03376660571619111, 0.03266529707985155, 0.03031955579255372, 0.029453861048233096, 0.03147298792379638, 0.02912828978151083, 0.02919105650051964, 0.028830317497697627, 0.029499432632982287, 0.026986113937475735, 0.02733181188448375, 0.025803588209598046, 0.02681095107315931, 0.026404689203034126, 0.026050992537829974, 0.024809241716038224, 0.024545829161385774, 0.023867841801785316, 0.02437650318747149, 0.024598442087124094, 0.02184943623618705, 0.02212401709032212, 0.024020484405087666, 0.02130729546814622, 0.020725422284217566, 0.02076827016183324, 0.02384621110337163, 0.01958747187485479, 0.021904331655620137, 0.019382887802573884, 0.02201010962111527, 0.02058606615812495, 0.021179698650891764, 0.019447557968882527, 0.020201372229405816, 0.01894510081315509, 0.019270095492140225, 0.019144803218715156, 0.018824441063149674, 0.01760245234608337, 0.017825258844961136, 0.01776745569889704, 0.017674809900367364, 0.015994397650768117, 0.01700054095782905, 0.016447655360661145, 0.017156456882351137, 0.01641642231447496, 0.018059637108855556, 0.017820714128034588, 0.017028371335592797, 0.017178229659197596, 0.016597093391827834, 0.015798507185733616, 0.018355573935441793, 0.018139768882747993, 0.01748620302146326, 0.017001588801414197, 0.016481924058458084, 0.016723642519140127, 0.018721227355233235, 0.01641849956732637, 0.015901307702231093]\n",
            "train_acc_list_auto = [57.69825304393859, 85.86977236633139, 87.68872419269455, 88.7580730545262, 89.19428268925357, 89.96294335627316, 90.42668078348332, 90.6193753308629, 91.05134992059291, 91.40921122286925, 91.68448914769719, 91.92376919004765, 92.38327157226045, 92.51032292218105, 92.7580730545262, 92.83642138697724, 92.9571201694018, 93.145579671784, 93.35097935415564, 93.40391741662255, 93.63684489147697, 93.787188988883, 93.67707781895183, 93.92059290629963, 93.98200105876126, 94.0307040762308, 94.1492853361567, 94.34621492853361, 94.2572789835892, 94.38644785600847, 94.23822128110112, 94.46903123345686, 94.59820010587613, 94.54526204340921, 94.68925357331922, 94.60455267337215, 94.61302276336686, 94.66807834833246, 94.65749073583906, 94.71889888830069, 94.78877713075701, 94.89677077818952, 94.80783483324511, 94.89253573319216, 94.97300158814187, 94.88618316569614, 95.06405505558496, 94.9708840656432, 95.24404446797247, 94.94123875066173, 95.18898888300689, 95.11275807305452, 95.25886712546321, 95.18898888300689, 95.26521969295923, 95.19745897300159, 95.30121757543674, 95.23980942297511, 95.36262572789836, 95.39650608787719, 95.29910005293806, 95.28427739544733, 95.36897829539438, 95.47273689782953, 95.39015352038115, 95.56379036527264, 95.47061937533087, 95.29062996294336, 95.47485442032821, 95.60614081524616, 95.55532027527792, 95.58920063525674, 95.47061937533087, 95.62308099523557, 95.68237162519851, 95.7289571201694, 95.68025410269983, 95.69719428268925, 95.61672842773955, 95.87718369507677, 95.78824775013234, 95.94706193753309, 95.81365802011646, 95.784012705135, 95.80518793012176, 95.98094229751192, 95.8284806776072, 95.80518793012176, 95.79671784012704, 96.06564319745897, 95.98305982001058, 95.98729486500794, 95.95341450502912, 96.04235044997353, 96.07834833245103, 95.97882477501324, 96.10164107993647, 96.04235044997353, 95.92165166754897, 96.11222869242985, 96.22445738485972, 96.13128639491795, 96.25622022233986, 96.24563260984648, 96.35574377977765, 96.23080995235574, 96.01482265749074, 96.3790365272631, 96.32186341979883, 96.30068819481207, 96.36633139227104, 96.42350449973532, 96.23928004235044, 96.42773954473266, 96.73901535203811, 96.37691900476443, 96.52302805717311, 96.3705664372684, 96.49973530968767, 96.51244044467973, 96.60349391212281, 96.67125463208046, 96.48914769719428, 96.57173107464267, 96.56749602964531, 96.78771836950767, 96.71995764955003, 96.53361566966649, 96.87030174695606, 96.88300688194812, 96.74748544203283, 96.76866066701959, 96.74113287453679, 96.9380624669137, 96.94229751191106, 96.8406564319746, 96.87877183695076, 96.91476971942826, 96.91688724192694, 97.1371095817893, 96.88935944944416, 96.98676548438327, 97.05876124933827, 97.13922710428798, 97.00370566437269, 97.03970354685019, 97.0926416093171, 97.23451561672843, 97.21334039174167, 97.25145579671783, 97.10322922181048, 97.23451561672843, 97.31286394917946, 97.2641609317099, 97.27898358920064, 97.39544732662785, 97.24510322922181, 97.35097935415564, 97.34250926416094, 97.571201694018, 97.43779777660137, 97.34250926416094, 97.4907358390683, 97.58178930651138, 97.46109052408681, 97.51402858655374, 97.66225516146109, 97.62202223398624, 97.39544732662785, 97.7787188988883, 97.65802011646373, 97.61990471148756, 97.76177871889888, 97.68554790894653, 97.77236633139228, 97.82530439385918, 97.82953943885654, 97.83377448385389, 97.74483853890948, 97.76389624139756, 97.88671254632081, 97.9692959237692, 97.96294335627316, 97.99682371625198, 97.99258867125464, 97.98411858125992, 98.05611434621493, 98.05187930121758, 98.05399682371625, 98.06670195870831, 98.01588141874008, 98.06034939121228, 98.22763366860772, 98.26363155108523, 98.1577554261514, 98.23186871360508, 98.25304393859184, 98.31233456855479, 98.28268925357332, 98.30174695606141, 98.356802541027, 98.35256749602965, 98.40762308099524, 98.32715722604553, 98.36739015352038, 98.4647961884595, 98.37374272101641, 98.43303335097936, 98.47114875595553, 98.45420857596612, 98.54737956590789, 98.45209105346744, 98.500794070937, 98.6553732133404, 98.65749073583906, 98.59396506087877, 98.72313393329804, 98.67866596082584, 98.65960825833774, 98.68713605082054, 98.7358390682901, 98.68925357331922, 98.77183695076761, 98.81418740074113, 98.69772366331392, 98.85865537321334, 98.8438327157226, 98.83748014822658, 98.86077289571202, 98.80571731074643, 98.9793541556379, 99.00052938062467, 98.90312334568554, 98.96029645314981, 99.04923239809423, 98.91371095817892, 99.03652726310217, 99.00899947061937, 99.00052938062467, 99.14663843303335, 99.13605082053996, 99.070407623081, 99.1064055055585, 99.11275807305452, 99.16357861302276, 99.1508734780307, 99.20169401799895, 99.17416622551615, 99.28427739544733, 99.214399152991, 99.23133933298041, 99.214399152991, 99.29486500794071, 99.29062996294336, 99.31815775542616, 99.29698253043938, 99.30333509793542, 99.3499205929063, 99.36050820539968, 99.31604023292748, 99.38803599788248, 99.41344626786659, 99.40074113287454, 99.33298041291688, 99.45367919534145, 99.37533086289042, 99.44732662784543, 99.37533086289042, 99.41979883536263, 99.3859184753838, 99.4219163578613, 99.43038644785601, 99.47697194282689, 99.42826892535733, 99.47485442032821, 99.50661725780836, 99.50661725780836, 99.50026469031233, 99.51085230280572, 99.48332451032292, 99.56379036527264, 99.53838009528852, 99.57437797776602, 99.55955532027528, 99.52779248279514, 99.50238221281101, 99.50661725780836, 99.55955532027528, 99.52779248279514, 99.54685018528322, 99.57861302276336, 99.51508734780307, 99.52567496029646, 99.52567496029646, 99.51932239280042, 99.56802541026998, 99.55955532027528, 99.51720487030175, 99.53414505029116, 99.57014293276866]\n",
            "test_loss_list_auto = [0.854697944048573, 0.6584666724882874, 0.44419189148089466, 0.368239817227803, 0.4940592580858399, 0.3690771728607954, 0.3589504133982986, 0.38058222209413844, 0.34228865349409626, 0.335723892000376, 0.3109845759383604, 0.29928872741612733, 0.31392319905845556, 0.27485897220378996, 0.29255017404462774, 0.29240670237763255, 0.29151450864532413, 0.31001000624953534, 0.27774785678176317, 0.2716419890376867, 0.26590198022769945, 0.2805121213416843, 0.2706572686632474, 0.2468469972703971, 0.26480546842018765, 0.2728810971054961, 0.2413128849995487, 0.2660955771365586, 0.2647931940634461, 0.2645452043370289, 0.25069177731433334, 0.2604605190309824, 0.2517391454574524, 0.24074612459277406, 0.24114863044929272, 0.23866420425474644, 0.2425372660817469, 0.23938925549680112, 0.2456162325015255, 0.24643426432329066, 0.2441049038487322, 0.2694204639205161, 0.2634314925720294, 0.25292548164725304, 0.23815971517971918, 0.23908892777912757, 0.25206760996404814, 0.24280147584995218, 0.2520727869488445, 0.24506068306372447, 0.24997088634500317, 0.2430651504546404, 0.23414121728901768, 0.22213738378794753, 0.247228968632864, 0.2360824038658072, 0.23799972842429198, 0.23885504494183787, 0.2358027821151065, 0.24082540213039108, 0.2557054047710171, 0.23378647274027267, 0.23469267157362958, 0.23138651949371777, 0.24428480978616895, 0.24390636994412132, 0.2296548501095351, 0.2494744443718125, 0.23908411262228207, 0.25455430325339823, 0.23804409528041587, 0.23210003986662509, 0.2522754323687039, 0.24094025425466836, 0.2353047978352098, 0.22578220312282735, 0.22642643833715542, 0.23648040128104827, 0.234295295368807, 0.23118343532961957, 0.243452065167766, 0.2479072526535567, 0.23031508502568684, 0.2307430071865811, 0.24165107517996254, 0.24791739840863966, 0.2356352105225418, 0.24240106997974947, 0.23156126701802598, 0.2280945602950512, 0.23597690470370591, 0.22922540408577405, 0.23591952800166374, 0.2404799498617649, 0.2431817608063712, 0.2453049985947562, 0.23956574037598044, 0.2583299451964159, 0.2395639850338008, 0.23400587027928993, 0.23422996013187894, 0.23995153374020375, 0.2439286811825107, 0.2674359942183775, 0.22688387820095407, 0.23382534689324744, 0.23335401259143562, 0.22591980135835268, 0.24091479602251567, 0.25041070262737136, 0.23992513426963022, 0.246772261190356, 0.2418343970077295, 0.23178920693987726, 0.24677032612118066, 0.2287094014532426, 0.23909340462848253, 0.23326664954862175, 0.23168033502046384, 0.2456203530743426, 0.235371302448067, 0.23657854840013326, 0.23801417038867287, 0.24392543954080811, 0.23823796327718916, 0.24473854583487206, 0.2388810735999369, 0.24342903593445525, 0.23894497259136507, 0.24675732284930407, 0.23822559116809977, 0.24382309390998938, 0.233994769516821, 0.2258786432828535, 0.23459827412358103, 0.2277108756277491, 0.22413930849300004, 0.23876028571862215, 0.2331210555435688, 0.23749786916681948, 0.23940081419605835, 0.23199230918259012, 0.2248807309432795, 0.2320321638356237, 0.24623563007323765, 0.2446849833370424, 0.21816706947763176, 0.23581119864156433, 0.23456720739383907, 0.23432794914526098, 0.23122229463621682, 0.23320124464511288, 0.2409158118945711, 0.23544718715928348, 0.23912205432048617, 0.24141824647199875, 0.24010024325666474, 0.23027214853494776, 0.23587687485212205, 0.23447171204230366, 0.23573052238526881, 0.22679930932673753, 0.23788673353984074, 0.2422654035811623, 0.23406771814231486, 0.24568954926422415, 0.23444239714858578, 0.24608345859337086, 0.23077788804749064, 0.24224108709570238, 0.24521961490459301, 0.23251463422624796, 0.23764923413959788, 0.23524608099650518, 0.24019971548342237, 0.24640236853384503, 0.2417017009072736, 0.23782137799642833, 0.2321096704633651, 0.23188522421554023, 0.2366727679310476, 0.23386354640345364, 0.23581221812934267, 0.2355241840340051, 0.24134659725150057, 0.23631648153212725, 0.24496783960756718, 0.23486665660040637, 0.24513840322912323, 0.24078461396343567, 0.23950668804201425, 0.23140845239600716, 0.23399199486034467, 0.24893675214957958, 0.24671873864809088, 0.2451156622127575, 0.24195303742353821, 0.23557983760667198, 0.2421512438678274, 0.2412950030948017, 0.24542047667737102, 0.24159109972271264, 0.24468982922753282, 0.23934249762518733, 0.23897204303420058, 0.24556210668136677, 0.2388775425725708, 0.23601807960692575, 0.22987533620029105, 0.246987152406398, 0.24375060487392486, 0.24201714165289612, 0.24381259923764304, 0.23624742922245287, 0.2411629831162738, 0.23951337353655083, 0.24577452823081436, 0.2465735137937408, 0.23719464980211913, 0.2379434087185883, 0.24825760760508916, 0.24059011924135335, 0.2505442435287085, 0.24614961039932334, 0.24116307924337246, 0.24537831190608295, 0.24592128061853788, 0.2437269031161479, 0.24327192477443638, 0.2411747233686494, 0.24220603216877756, 0.2415566837743801, 0.24126598647996492, 0.23794463349908007, 0.2413742526744803, 0.24037200650748083, 0.2412653976467018, 0.24241722687421477, 0.2416582004368013, 0.24397952188098548, 0.24545666710564903, 0.24501819806356057, 0.238878321870431, 0.248782909119173, 0.24211586062230317, 0.24884491410179466, 0.24774682367512701, 0.2402858989331506, 0.2464034580169063, 0.2420361806255053, 0.24698023514493422, 0.24638258141702882, 0.2440691111819344, 0.24283416516275383, 0.25251583178874615, 0.24522095931438254, 0.2505062839162408, 0.24517181726610837, 0.2511264611327765, 0.24910575721193762, 0.24469954134238994, 0.2462432504467228, 0.2518011958374843, 0.24128046771511436, 0.24614255446210212, 0.23931376788509534, 0.23999540297789315, 0.23831663229594044, 0.24262406715356252, 0.24283856312360833, 0.24234587159555623, 0.24248098625856288, 0.2431600397525757, 0.2497850547763793, 0.2420718543523667, 0.2455297116664987, 0.2453975499078047, 0.25367831595826384, 0.24988036927328827, 0.2441944194884569, 0.2475952035382244, 0.24913023439619472, 0.2496498769934417, 0.2486914823233497, 0.2396043237463078, 0.24878114394332265, 0.250280501229652, 0.24539401237944178, 0.24746752302984104, 0.24443048573391257, 0.24588232408916832, 0.24387701142433227, 0.24820849407172085, 0.24091276705411135, 0.24957285682195982, 0.2468200375179888, 0.2454140442880053, 0.24415877538130565, 0.24864615638758622, 0.2471419099864422]\n",
            "test_acc_list_auto = [72.76044867854948, 79.30623847572218, 86.36677934849416, 88.89827904118009, 84.18484941610326, 88.76382913337432, 89.1479717271051, 88.55255070682237, 89.66656422864168, 90.23509526736325, 90.95344191763982, 90.95728334357713, 90.68070067609096, 91.9061155500922, 91.58343577135832, 91.4259373079287, 91.60264290104487, 91.08789182544561, 91.74093423478796, 92.12891825445605, 92.2979409956976, 91.75245851259987, 92.26336816226183, 92.83574062692071, 92.39781807006761, 92.07513829133374, 93.09311616472034, 92.47464658881377, 92.490012292563, 92.46696373693915, 92.9279348494161, 92.42086662569146, 92.87031346035648, 93.23909035033805, 93.34665027658266, 93.1661032575292, 93.21220036877689, 93.28134603564843, 93.1238475722188, 93.08927473878303, 93.13153042409343, 92.52842655193608, 92.76659496004918, 92.74738783036263, 93.25829748002458, 93.3581745543946, 92.96250768285188, 93.02397049784881, 93.00092194222495, 92.85878918254456, 92.7819606637984, 93.14689612784265, 93.42732022126613, 93.8421942224954, 93.05470190534726, 93.23909035033805, 93.4580516287646, 93.38890596189306, 93.30055316533497, 93.13921327596803, 92.91256914566686, 93.53488014751076, 93.4618930547019, 93.53872157344806, 93.24293177627536, 93.1737861094038, 93.66548862937923, 93.14689612784265, 93.36585740626921, 92.81653349723418, 93.38506453595575, 93.57329440688383, 92.79348494161033, 93.21988322065151, 93.4580516287646, 93.79609711124769, 93.82682851874615, 93.49262446220037, 93.3159188690842, 93.700061462815, 93.46573448063921, 93.10848186846958, 93.70390288875231, 93.63859864781807, 93.38506453595575, 93.26213890596189, 93.41963736939152, 93.31207744314689, 93.75384142593731, 93.80762138905962, 93.58481868469576, 93.6078672403196, 93.4618930547019, 93.40043023970497, 93.2160417947142, 93.10848186846958, 93.49262446220037, 92.96634910878919, 93.53103872157345, 93.48110018438844, 93.63475722188076, 93.46957590657652, 93.35433312845728, 92.67440073755378, 93.82298709280884, 93.799938537185, 93.73463429625077, 93.95743700061463, 93.4618930547019, 93.20835894283958, 93.50799016594961, 93.34280885064535, 93.38890596189306, 93.69622003687769, 93.45036877688999, 93.84987707437, 93.61170866625692, 93.66164720344192, 93.75768285187462, 93.5041487400123, 93.6040258143823, 93.72311001843885, 93.53488014751076, 93.51951444376152, 93.82682851874615, 93.5579287031346, 93.71542716656423, 93.55024585125999, 93.5579287031346, 93.59250153657038, 93.62323294406883, 93.5540872771973, 93.88829133374308, 94.15719114935465, 93.73463429625077, 94.05731407498463, 94.15334972341734, 93.62323294406883, 94.02658266748617, 93.88060848186846, 93.67701290719116, 93.68469575906576, 94.18792255685311, 93.92670559311617, 93.56561155500921, 93.63475722188076, 94.15719114935465, 93.70390288875231, 93.85371850030731, 93.91518131530424, 93.95359557467732, 93.83451137062077, 93.81146281499693, 93.93054701905348, 93.87292562999386, 93.68085433312845, 93.7077443146896, 94.11877688998156, 93.98432698217579, 94.05731407498463, 94.02658266748617, 94.08420405654579, 93.91518131530424, 93.73079287031346, 93.8921327596804, 93.91902274124155, 93.9459127228027, 93.77304855562384, 94.04578979717272, 93.83835279655808, 94.04578979717272, 94.24170251997542, 94.0918869084204, 94.06115550092194, 93.93822987092808, 93.83066994468346, 94.01505838967425, 94.1379840196681, 94.32237246465888, 94.20712968653964, 94.1917639827904, 93.97664413030117, 94.23786109403811, 94.16103257529196, 94.22633681622618, 94.22249539028887, 94.01121696373694, 94.2340196681008, 94.13414259373079, 94.09956976029503, 94.19944683466503, 94.27627535341118, 94.36846957590657, 93.9958512599877, 94.07267977873387, 94.02274124154886, 94.25322679778733, 94.31084818684695, 94.11109403810694, 94.24170251997542, 94.20328826060233, 94.25706822372464, 94.19944683466503, 94.20328826060233, 94.2839582052858, 94.42224953902888, 94.2340196681008, 94.31468961278426, 94.45298094652735, 94.15334972341734, 94.11493546404425, 94.29932390903504, 94.20328826060233, 94.51060233558697, 94.50676090964966, 94.3300553165335, 94.30700676090964, 94.3262138905962, 94.53749231714812, 94.4299323909035, 94.29548248309773, 94.42609096496619, 94.38767670559312, 94.49907805777505, 94.42224953902888, 94.31084818684695, 94.40304240934235, 94.42609096496619, 94.39151813153042, 94.39151813153042, 94.55669944683467, 94.51060233558697, 94.61816226183159, 94.64121081745544, 94.47602950215119, 94.53749231714812, 94.4721880762139, 94.42224953902888, 94.62968653964352, 94.49523663183774, 94.55285802089736, 94.6258451137062, 94.66425937307929, 94.43377381684081, 94.64889366933005, 94.57974800245852, 94.4721880762139, 94.59127228027043, 94.54901659496005, 94.65657652120467, 94.7180393362016, 94.69883220651506, 94.58743085433314, 94.67194222495391, 94.54133374308543, 94.80255070682237, 94.50291948371235, 94.74108789182544, 94.62968653964352, 94.58358942839583, 94.87169637369392, 94.64889366933005, 94.71035648432698, 94.64889366933005, 94.67194222495391, 94.76029502151198, 94.80255070682237, 94.84480639213275, 94.76029502151198, 94.72572218807622, 94.77181929932391, 94.73340503995082, 94.79870928088506, 94.72572218807622, 94.7679778733866, 94.79870928088506, 94.77566072526122, 94.6681007990166, 94.69883220651506, 94.91779348494161, 94.89474492931777, 94.83328211432084, 94.7180393362016, 94.74108789182544, 94.95236631837739, 94.66041794714198, 94.80639213275968, 94.89474492931777, 94.76029502151198, 94.84480639213275, 94.79870928088506, 94.82559926244622, 94.79486785494775, 94.93315918869084, 94.77181929932391, 94.73724646588813, 94.82944068838353, 94.86017209588199, 94.68346650276582, 94.76029502151198]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list_01 = [2.3849518770770977, 2.2417600981911345, 2.2415069635644516, 2.2409557133186153, 2.2419579268147953, 2.240125378942102, 2.240931454066662, 2.2417839348800785, 2.242252948807507, 2.241471426273749, 2.241231945472035, 2.2413479971691843, 2.241036834432504, 2.2407813569717616, 2.2415969093963706]\n",
        "train_acc_list_01 = [18.56855479089465, 18.617257808364215, 18.746426680783483, 18.60243515087348, 18.61937533086289, 18.886183165696135, 18.60243515087348, 18.598200105876124, 18.604552673372154, 18.740074113287452, 18.731604023292746, 18.814187400741133, 18.848067760719957, 18.82901005823187, 18.752779248279513]\n",
        "test_loss_list_01 = [2.2387831538331273, 2.241503697984359, 2.241926829020182, 2.240557459055209, 2.2406100852816713, 2.252836311564726, 2.2468298743752873, 2.2446437150824305, 2.2425780202828203, 2.240177970306546, 2.243702617346072, 2.2441832843948815, 2.253918958645241, 2.245230858232461, 2.2435202879064224]\n",
        "test_acc_list_01 = [18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 14.27858020897357, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463]\n",
        "train_loss_list_001 = [2.2888378896687414, 2.232192633274771, 1.4893088792236193, 0.5001831288098643, 0.3830500937251218, 0.32867970416539405, 0.29770232389774426, 0.27895135900919354, 0.2627035691970732, 0.24814978033950336, 0.23438045485475198, 0.2243682525668364, 0.21301924496848731, 0.20818865677811266, 0.1958482328166322]\n",
        "train_acc_list_001 = [18.50926416093171, 18.968766543144522, 47.80307040762308, 84.23292747485442, 88.15881418740074, 89.9142403388036, 91.04923239809423, 91.63790365272631, 92.08893594494441, 92.58020116463737, 93.03758602435151, 93.36791953414505, 93.715193223928, 93.84012705134992, 94.35044997353097]\n",
        "test_loss_list_001 = [2.2451091665847627, 2.2302937355695986, 0.684039752711268, 0.4374444575286379, 0.39873581839834943, 0.36398082489476485, 0.3313831433507742, 0.3210300371854329, 0.2847569804346445, 0.29315854806233854, 0.27853756525791157, 0.26896954926789973, 0.2596830692069203, 0.26028463620619446, 0.2491153629460171]\n",
        "test_acc_list_001 = [18.88060848186847, 19.053472649047325, 78.20759065765212, 86.33988936693301, 87.90334972341734, 88.90596189305471, 90.08143822987093, 90.81515058389674, 91.70636140135218, 91.54118008604794, 91.8638598647818, 92.2480024585126, 92.43239090350338, 92.50537799631223, 92.82037492317149]\n",
        "train_loss_list_0001 = [1.8565612323885041, 0.5532636212785715, 0.4003713336094285, 0.3402645644860539, 0.309145948165639, 0.2848975209968523, 0.262982070708501, 0.24855145600026216, 0.2386487888167221, 0.2278864942390098, 0.21327269485164788, 0.20556094100683686, 0.19517452138549268, 0.18913837452608395, 0.18232752032436653]\n",
        "train_acc_list_0001 = [34.7993647432504, 82.35468501852831, 87.65272631021705, 89.51614610905241, 90.6723133933298, 91.51932239280042, 92.26892535733192, 92.59925886712546, 92.97617787188989, 93.34674430915828, 93.69401799894123, 93.97353096876654, 94.31868713605083, 94.53255690841715, 94.71254632080466]\n",
        "test_loss_list_0001 = [1.0861167063315709, 0.46979708385233787, 0.41340532643245714, 0.3364271931350231, 0.3262409484561752, 0.34040282589986043, 0.28803076817854945, 0.2942232322678262, 0.2779932311169949, 0.27514038454083833, 0.2478603608906269, 0.2512748020463714, 0.26197264781769586, 0.2462065773194327, 0.24985540344142446]\n",
        "test_acc_list_0001 = [64.00583896742471, 85.58312845728334, 87.41933005531654, 89.81637984019667, 89.970036877689, 89.99308543331284, 91.59111862323294, 91.35295021511985, 91.82928703134604, 92.03672403196066, 92.76275353411187, 92.83574062692071, 92.90488629379226, 92.98555623847572, 92.87031346035648]\n",
        "train_loss_list_00001 = [1.2440541174192092, 0.4583125376927497, 0.36052036624613815, 0.3151768069603256, 0.2875107263081119, 0.26835051384883196, 0.25187577066948097, 0.23390740957767336, 0.22118305101950317, 0.21190600004299545, 0.20555683115350845, 0.1952596850249018, 0.18612936185546683, 0.18178928815090883, 0.17480877608181986]\n",
        "train_acc_list_00001 = [57.18581259925887, 85.53943885653786, 88.79618845950239, 90.25304393859184, 91.214399152991, 92.01058761249338, 92.36421386977237, 93.09899417681312, 93.43991529910005, 93.80412916887242, 93.86765484383271, 94.25092641609317, 94.500794070937, 94.64478560084702, 94.82477501323451]\n",
        "test_loss_list_00001 = [0.5935118007017117, 0.43328142947718207, 0.3668157114994292, 0.34575528556517526, 0.3590214093964474, 0.3134572241893586, 0.2903973020467104, 0.28062032824199573, 0.26905518266208034, 0.26447016406146917, 0.27132851287138227, 0.26421160440818936, 0.25491809147391836, 0.24584030433028353, 0.2630316608895858]\n",
        "test_acc_list_00001 = [80.93500307314075, 86.63567916410571, 88.97510755992624, 89.3438844499078, 88.99431468961278, 90.58466502765826, 91.54502151198525, 91.56422864167179, 92.02135832821143, 92.25184388444991, 91.97910264290104, 92.37861094038107, 92.72433927473878, 92.96250768285188, 92.59757221880763]\n",
        "\n",
        "\n",
        "train_loss_list_const = [1.5944857293674293, 0.4869092239677745, 0.3710624326454592, 0.3296906750215101, 0.29884549410039496, 0.27146996971633697, 0.2604337949053382, 0.24417246496532022, 0.23419785655045575, 0.22457996957751147, 0.21173857996457315, 0.20202396321425917, 0.1943116241051414, 0.18745123495052501, 0.1774632519257424, 0.17214223601650902, 0.16552220173886797, 0.15473239550866733, 0.15404348591276948, 0.14213740165398372, 0.13690419029192066, 0.13265049481779578, 0.12643728150322348, 0.1223953123176647, 0.11572830584960256, 0.10872174589453028, 0.10607895330301306, 0.10143619665729645, 0.09846852844464908, 0.09155762590831373, 0.09176684167932689, 0.08320119946962853, 0.07973214024854547, 0.07931727142424037, 0.06983993944351063, 0.07091535234995448, 0.0671700671211713, 0.06470882359558974, 0.061619590856841586, 0.061459206172131346, 0.057121708876198427, 0.053374216585170206, 0.0498615506466044, 0.05073856983490308, 0.04987424655961312, 0.046790412364042994, 0.04711244383266544, 0.04263703334889801, 0.043034394194061555, 0.04126133985860339, 0.03625233271121373, 0.03783284285014904, 0.0379838087573284, 0.03416189216178284, 0.03383261713717981, 0.03370409071676403, 0.033968668481206325, 0.03165312097212274, 0.03038872497441464, 0.02841417055683044, 0.029356409022635036, 0.02889881345480024, 0.028939039615515447, 0.02568263415692672, 0.027656220918051838, 0.026301205131101617, 0.023402816848356156, 0.024891200900357974, 0.022950206206843942, 0.022271349775869285, 0.0250707899006071, 0.022344395426495713, 0.019562920480307198, 0.022191016989943715, 0.02281398802152065, 0.0203801996838415, 0.020550807893653537, 0.02117716848076041, 0.019771566791103416, 0.022764958889409142, 0.017198871523580392, 0.02078044665365311, 0.020109122048629333, 0.01588759385191313, 0.01774437841210012, 0.020397339600326397, 0.01534856222032823, 0.016435430918308794, 0.01711455919842304, 0.01770936636729633, 0.018040290616609177, 0.01564757464108749, 0.015251886521385311, 0.01534154344099917, 0.015427147975282334, 0.01419120683843651, 0.01811075882562171, 0.016642653122089984, 0.01305790530750528, 0.013949767823940267, 0.014656124457392462, 0.013457846312966957, 0.014343252129156173, 0.013850259383991146, 0.012460459422866397, 0.012413865320129272, 0.013889214684304953, 0.01221698466477806, 0.015144580592092777, 0.010421825547786688, 0.01315658220278792, 0.014559990600058128, 0.012665790851758906, 0.01143317199561781, 0.011717887349175301, 0.011158559258571676, 0.012634062059679534, 0.011741106513152792, 0.012515452659958159, 0.011751638104695717, 0.011457235534427129, 0.011438692509360596, 0.012512708038015468, 0.008602326916451698, 0.010682771495822786, 0.011196336384384224, 0.009268063090769713, 0.009998142540840234, 0.012729091897495166, 0.010267185127698763, 0.010566324319321028, 0.010713477439735606, 0.010119398856430022, 0.012654226244056406, 0.009628543746825273, 0.0102562899387879, 0.010169452542220153, 0.011113815507671713, 0.01246779066452764, 0.007518461719419533, 0.007389902458939484, 0.011104748547043264, 0.01135712010794771, 0.008941290669380526, 0.009924649224834431, 0.009838336234147059, 0.007612355080635734, 0.009282812758648675, 0.008423602285464519, 0.009065308554876006, 0.010365516194014629, 0.006115070432431249, 0.006490228478146479, 0.011122480412586441, 0.00872685812082587, 0.008791860558152852, 0.009787340952780856, 0.008418811355189983, 0.007423892029066022, 0.009898100130695444, 0.008828991230531168, 0.007722080036888794, 0.009378696144602659, 0.00829515335842403, 0.006554383687879633, 0.006126663065066873, 0.010910632581322311, 0.007716069375264186, 0.006358072765228986, 0.007697185452836498, 0.008877029283653514, 0.008559577807589543, 0.00707696825589556, 0.00850764569837624, 0.0062317561151193695, 0.0066259507287960455, 0.008618629247509826, 0.007878521594295732, 0.007498281048782551, 0.006598806819186967, 0.006230976371933824, 0.006141664580167378, 0.007923200858393221, 0.008045455975570557, 0.00792612456981016, 0.007601726206049413, 0.007419711852838272, 0.004584900550783639, 0.00685006011318293, 0.008050930358640324, 0.008370204849807444, 0.005507358266772813, 0.006857401374734981, 0.008172955556791895, 0.004244687181667273, 0.008633549227632837, 0.008705449396538356, 0.0067560189635043925, 0.006710546794999935, 0.00550614756518684, 0.0069180619830691474, 0.00835907400430254, 0.005712247417545953, 0.006098340349643973, 0.007154704712266156, 0.0061693518397019, 0.005750073719051822, 0.005536363746094307, 0.006909997050809464, 0.006392437509654407, 0.004530645426771153, 0.00935107147158372, 0.005991038718238898, 0.004413133404240297, 0.007152540924743997, 0.006123794079590131, 0.004265024197029019, 0.007488072243164834, 0.006849384789138319, 0.006199823599835201, 0.004939858833603685, 0.006542191565922548, 0.006134184821023406, 0.006127372573951168, 0.006120713498504846, 0.005705225819374859, 0.0054501006014126396, 0.00476737067947397, 0.005204442173390228, 0.006888031104115721, 0.005438639611901732, 0.005198591940753804, 0.006226397409455738, 0.00398913007948585, 0.006725981141281091, 0.0057350191533619786, 0.004547678196042624, 0.003760278201966155, 0.004862335098237806, 0.0053206211415940214, 0.004511028374038128, 0.006714381331292853, 0.006507630908786355, 0.004150821972775074, 0.005154698122662017, 0.005662354828146746, 0.004606740834664833, 0.004731273036621596, 0.004916682220897782, 0.0048261530498232945, 0.006001339212270287, 0.007263619651073496, 0.0038986454153652154, 0.005036929771996826, 0.005049693082181207, 0.00644224065052205, 0.004578751418381182, 0.004686958738437559, 0.003585059260631736, 0.003101582141557616, 0.007221638428928184, 0.005000114876925785, 0.0051700827908760325, 0.005254589704386773, 0.00406367993565528, 0.0036232792127412505, 0.006171727926178867, 0.005875237415135579, 0.00392443083374783, 0.006561909474143628, 0.0037487958143322068, 0.004443046452993911, 0.005447318903908573, 0.004017829913798502, 0.00472008602112813, 0.0055074588290326305, 0.005279580161203713, 0.0036886348879267314, 0.0035813810519778975, 0.004883374933941502, 0.004846986711028166, 0.004054715485154408, 0.004949977601104352, 0.0046826732979178415, 0.004384392334953691, 0.004187341836835158, 0.004243570542040143, 0.005868256121943751, 0.00557879406272601, 0.0037290168971954394, 0.0024883511193469366, 0.004939023949055762, 0.004748034648033541, 0.0038421762496379727, 0.003486280636854575, 0.0036575751641310756, 0.00670746078328728, 0.004645855993902198, 0.004636629221823842, 0.004005611049817734]\n",
        "train_acc_list_const = [44.03176283748015, 84.49761778718899, 88.61196400211752, 89.87400741132875, 91.01958708311275, 91.79460031762838, 92.2350449973531, 92.80889359449444, 93.03546850185283, 93.37427210164108, 93.80412916887242, 94.0052938062467, 94.38009528851244, 94.6278454208576, 94.88618316569614, 95.01323451561673, 95.1868713605082, 95.66543144520911, 95.57226045526734, 95.91953414505029, 96.01905770248808, 96.16093170989942, 96.35150873478031, 96.54632080465855, 96.74113287453679, 96.86183165696136, 96.96347273689783, 97.10746426680784, 97.21757543673901, 97.32133403917416, 97.3276866066702, 97.63896241397565, 97.72154579142403, 97.7342509264161, 97.88883006881949, 97.87400741132875, 97.96506087877184, 98.09211222869243, 98.24245632609846, 98.17257808364214, 98.28692429857067, 98.34833245103229, 98.44997353096876, 98.49444150344097, 98.49655902593965, 98.54102699841185, 98.55796717840127, 98.67443091582848, 98.63419798835362, 98.74219163578613, 98.87559555320276, 98.77607199576495, 98.78030704076231, 98.9793541556379, 98.94970884065643, 98.9433562731604, 98.90947591318158, 98.98782424563261, 98.96453149814717, 99.08099523557438, 99.05770248808894, 99.05134992059291, 99.0428798305982, 99.1784012705135, 99.13393329804128, 99.1148755955532, 99.20381154049761, 99.18051879301217, 99.27580730545262, 99.23980942297511, 99.1784012705135, 99.25674960296453, 99.3499205929063, 99.26310217046056, 99.2503970354685, 99.32874536791954, 99.28215987294865, 99.27157226045527, 99.29486500794071, 99.24404446797247, 99.42615140285865, 99.2779248279513, 99.31180518793012, 99.46214928533615, 99.40921122286925, 99.36262572789836, 99.49602964531498, 99.43673901535203, 99.42615140285865, 99.40921122286925, 99.41132874536792, 99.47697194282689, 99.49602964531498, 99.49814716781366, 99.47273689782953, 99.5044997353097, 99.39015352038115, 99.45156167284277, 99.57649550026468, 99.55320275277924, 99.5404976177872, 99.5659078877713, 99.53414505029116, 99.54685018528322, 99.58073054526204, 99.5849655902594, 99.49391212281631, 99.58920063525674, 99.51720487030175, 99.66543144520911, 99.5574377977766, 99.56167284277396, 99.56802541026998, 99.60402329274748, 99.62519851773425, 99.6019057702488, 99.5934356802541, 99.62519851773425, 99.60402329274748, 99.61461090524087, 99.61461090524087, 99.63790365272631, 99.6019057702488, 99.70566437268396, 99.66119640021175, 99.64002117522499, 99.69507676019057, 99.68025410269983, 99.58920063525674, 99.6569613552144, 99.62519851773425, 99.6569613552144, 99.66754896770779, 99.54261514028586, 99.69719428268925, 99.63366860772896, 99.67178401270513, 99.60825833774484, 99.6019057702488, 99.75436739015352, 99.75436739015352, 99.59555320275278, 99.62731604023293, 99.6929592376919, 99.65484383271573, 99.66966649020645, 99.7289571201694, 99.71836950767602, 99.69084171519323, 99.67601905770249, 99.66119640021175, 99.79671784012704, 99.78824775013234, 99.62519851773425, 99.71836950767602, 99.72683959767072, 99.68872419269455, 99.73107464266808, 99.73107464266808, 99.67601905770249, 99.70989941768131, 99.73319216516676, 99.68025410269983, 99.7204870301747, 99.78824775013234, 99.80518793012176, 99.64849126521969, 99.73107464266808, 99.79883536262572, 99.74377977766014, 99.69719428268925, 99.69719428268925, 99.77130757014294, 99.71625198517734, 99.7924827951297, 99.784012705135, 99.70354685018528, 99.75860243515088, 99.73319216516676, 99.79036527263102, 99.78189518263632, 99.79036527263102, 99.73530968766543, 99.69719428268925, 99.75013234515616, 99.72472207517205, 99.76071995764956, 99.84330333509793, 99.784012705135, 99.73530968766543, 99.69084171519323, 99.82212811011117, 99.76919004764426, 99.73319216516676, 99.85812599258867, 99.70989941768131, 99.70142932768661, 99.76071995764956, 99.78824775013234, 99.8284806776072, 99.7924827951297, 99.7204870301747, 99.8009528851244, 99.81577554261514, 99.75436739015352, 99.7924827951297, 99.81365802011646, 99.8009528851244, 99.77554261514028, 99.77977766013764, 99.85389094759132, 99.67601905770249, 99.81789306511382, 99.86871360508205, 99.74377977766014, 99.78189518263632, 99.84965590259397, 99.77130757014294, 99.78824775013234, 99.82001058761249, 99.81365802011646, 99.77130757014294, 99.78189518263632, 99.79883536262572, 99.80730545262044, 99.82636315510852, 99.79036527263102, 99.83059820010588, 99.81154049761778, 99.7564849126522, 99.8094229751191, 99.8284806776072, 99.81154049761778, 99.88353626257279, 99.75224986765484, 99.79671784012704, 99.85389094759132, 99.88565378507147, 99.83059820010588, 99.80730545262044, 99.84118581259926, 99.77130757014294, 99.81154049761778, 99.85177342509265, 99.83271572260455, 99.8094229751191, 99.8284806776072, 99.86236103758603, 99.8369507676019, 99.82424563260984, 99.8009528851244, 99.78824775013234, 99.84330333509793, 99.82636315510852, 99.8284806776072, 99.78189518263632, 99.85812599258867, 99.85177342509265, 99.89624139756485, 99.89835892006353, 99.77130757014294, 99.84118581259926, 99.81577554261514, 99.8369507676019, 99.85600847008999, 99.86659608258337, 99.81154049761778, 99.79671784012704, 99.87718369507677, 99.79883536262572, 99.85812599258867, 99.85389094759132, 99.83906829010058, 99.88353626257279, 99.84753838009529, 99.8369507676019, 99.83906829010058, 99.88565378507147, 99.87506617257809, 99.83271572260455, 99.84330333509793, 99.85812599258867, 99.86659608258337, 99.8369507676019, 99.8284806776072, 99.84753838009529, 99.85177342509265, 99.7924827951297, 99.79671784012704, 99.88141874007411, 99.92376919004765, 99.86024351508735, 99.84753838009529, 99.85600847008999, 99.87506617257809, 99.87718369507677, 99.79671784012704, 99.86024351508735, 99.85177342509265, 99.87083112758073]\n",
        "test_loss_list_const = [0.9403148495099124, 0.46971940738605517, 0.34306909633325594, 0.3384028169892582, 0.3164549315823059, 0.31764499748162195, 0.282772644879479, 0.27239831069520876, 0.2788072020618939, 0.2605946859454407, 0.2569786167758353, 0.25151387514436946, 0.23040011815507622, 0.23389688687508597, 0.23323347488893012, 0.22432381940969065, 0.23885389055837603, 0.22965874502837075, 0.23512107754747072, 0.23369459731175618, 0.22183155821745887, 0.2491116562639089, 0.23399925456546686, 0.25359821337841304, 0.23499618255186314, 0.22481991891183106, 0.2324665375966944, 0.23745987874766192, 0.24356824685545528, 0.24952035153503804, 0.23930090873082185, 0.25436064657554325, 0.25367995643732594, 0.26609678615761156, 0.2672480665622096, 0.2746706861893044, 0.25824960023529975, 0.26025373894063863, 0.2791316908072023, 0.26918739248432366, 0.2763673086997633, 0.2997446244436444, 0.2916372886438872, 0.3003798500981693, 0.2806282534501424, 0.2931029381979184, 0.29610180335265457, 0.3087221756942716, 0.32797835285172744, 0.30948590080929445, 0.31205729929292025, 0.3264701991711798, 0.31612680869761345, 0.3249107254029927, 0.3218845636081681, 0.3128845189679779, 0.3158182442261308, 0.3337518404621412, 0.34533743259003935, 0.32409434777447116, 0.3321887091642209, 0.3324712138604738, 0.33405348602864965, 0.3346349073199592, 0.3316200801226146, 0.35435040097446274, 0.3272651647738017, 0.3605334343612377, 0.3315225986252521, 0.36446072681642633, 0.37520757036320135, 0.3792235794586732, 0.3674935271379118, 0.3611337201596767, 0.35125305392213313, 0.3664919300844856, 0.3579704279956572, 0.358177753384499, 0.37499817195074525, 0.3516562903372973, 0.36848902222061275, 0.37098927200570997, 0.37174441270968495, 0.3886868230103716, 0.36849470644751015, 0.3531273581982389, 0.3660184144471571, 0.37421249364520986, 0.3748011891273599, 0.3667490633350669, 0.3643053767691348, 0.37450089440772344, 0.37444961906465535, 0.3669995936729452, 0.37039543743080955, 0.38952702733085437, 0.39265704210208474, 0.3686957350274658, 0.3655954713695774, 0.39540933398529887, 0.37815322328870205, 0.3858970995974161, 0.37387897242682383, 0.37425000457020074, 0.38926156392941874, 0.3786647673787586, 0.37843504400156874, 0.4000659274078869, 0.38383779517721894, 0.38883355182513374, 0.38116612786189746, 0.39886108679952575, 0.4039541971048011, 0.4085461224575399, 0.4040325807553588, 0.40590017215878355, 0.38623136059180196, 0.38693335685221586, 0.41144327609343273, 0.3980357802459313, 0.4007313655798926, 0.40366758222636934, 0.38523547610669745, 0.42097171660804866, 0.4246416387294291, 0.40921079142786126, 0.4052840040850581, 0.4231068941344525, 0.38739459854824576, 0.4194799537094785, 0.41747657055327414, 0.4164134435739149, 0.3939945743985328, 0.393845445202554, 0.4172966992665155, 0.4208380446276244, 0.408174136686641, 0.43686796033608855, 0.3842315211042981, 0.4022488394277353, 0.41533097460427704, 0.4123304054533661, 0.3947430606748836, 0.4344836208585869, 0.42245670334509045, 0.4120090134880122, 0.4084896664774301, 0.4208405370477076, 0.4342218170389898, 0.4342225538966173, 0.414541769489719, 0.42016579197290554, 0.43672865715023934, 0.3970657474470927, 0.4120631425404081, 0.4072342333657777, 0.4361969401840778, 0.40728934426043256, 0.4082725720443562, 0.43000999789721533, 0.4488575828864294, 0.4490402939628956, 0.42708765610358584, 0.4119907574152903, 0.4171651719850214, 0.46127884272559017, 0.424621216852364, 0.42663686249551236, 0.443727373143238, 0.4234333980655042, 0.4163527505554478, 0.41962003758446514, 0.43725314623146666, 0.4135830312939909, 0.4485339165719993, 0.438206387026345, 0.42548432996423513, 0.4139385414218493, 0.422614497990877, 0.44258191624619797, 0.5020544356643679, 0.42181872726217207, 0.45136614169414135, 0.42586187040889817, 0.4617275644605066, 0.42785005136758236, 0.4181436715811929, 0.43456211805745376, 0.456493051905258, 0.4419752404130265, 0.439949865836431, 0.458016514120733, 0.45355927485370023, 0.4266304138993077, 0.4774111132959233, 0.43147677636942733, 0.42984621565533326, 0.4257584124226888, 0.41347907219703, 0.44227188734301165, 0.44509480125270784, 0.42359101505694435, 0.45620652116542937, 0.4532003666402078, 0.4544208156419735, 0.4288948582868804, 0.45772276510365817, 0.4652134206776014, 0.4481767811538542, 0.45044724102926387, 0.45808432965228957, 0.4832801581086481, 0.4330355538119215, 0.450099578350965, 0.44405912482818843, 0.4245691280629413, 0.47918898603130206, 0.4441465862088508, 0.4232571859679678, 0.42079631317699073, 0.46119900758140814, 0.4447093260310152, 0.45832808096619215, 0.4615354106998911, 0.46208279893970955, 0.44396297247651234, 0.44644354116719437, 0.4640259245666219, 0.466397661642701, 0.46452038604811785, 0.4498248620908342, 0.4491576017337103, 0.465446551038208, 0.47035092626716574, 0.4713521593012938, 0.4597488452506927, 0.4512224776633814, 0.48416280633240355, 0.4734927043455708, 0.46729338517887337, 0.46055449181357305, 0.44595291037751617, 0.45059747884606977, 0.4550898942740305, 0.4553583972247354, 0.484807043427638, 0.47273430763972085, 0.45941312947109636, 0.4590342458538419, 0.4751177701332113, 0.47533442642466694, 0.45637929342760175, 0.4544383191900766, 0.4463540438526109, 0.45517611408414427, 0.4642113188975582, 0.44581558580930325, 0.4676268981386195, 0.46861990691874833, 0.47670730448090565, 0.4613685188541079, 0.46828112538502203, 0.47280027833310706, 0.44121663249097764, 0.4606796685479554, 0.5259090004415781, 0.48354477134040175, 0.469667333146265, 0.4692415113266393, 0.45400641179944884, 0.47124824484846756, 0.4692198993460111, 0.44249545135900525, 0.4556816854969571, 0.4647824307538423, 0.47515893393360514, 0.44086019356972445, 0.47874322776481804, 0.48824521492911027, 0.4832109012732319, 0.45331234720138397, 0.46669590011166007, 0.4659509803990231, 0.4592359331253843, 0.4794528412395248, 0.4734935767528619, 0.4946094546649678, 0.47707713397183255, 0.4516702479244593, 0.44164761501893984, 0.48553112554637823, 0.4603754159190929, 0.4560155915005096, 0.48063205852739366, 0.4870615708185177, 0.503982417563926, 0.4785489882890354, 0.44539653152848285, 0.4539821001874539, 0.47518135870204253]\n",
        "test_acc_list_const = [71.6080208973571, 85.50245851259987, 89.67040565457899, 89.58589428395821, 90.46173939766442, 90.5577750460971, 91.50276582667486, 92.0520897357099, 91.96757836508912, 92.47464658881377, 92.6820835894284, 92.88952059004302, 93.56177012907192, 93.4580516287646, 93.66548862937923, 93.80377996312231, 93.34665027658266, 93.71158574062692, 93.5041487400123, 93.57329440688383, 94.06499692685925, 93.33896742470804, 93.72311001843885, 92.94330055316533, 93.68853718500307, 94.18792255685311, 93.96127842655194, 93.91518131530424, 93.82682851874615, 94.03042409342348, 94.1917639827904, 93.77304855562384, 93.9420712968654, 93.68853718500307, 94.08420405654579, 93.77304855562384, 93.91518131530424, 94.14566687154272, 93.93822987092808, 94.14566687154272, 94.06499692685925, 93.73079287031346, 93.8921327596804, 94.07267977873387, 94.01889981561156, 94.22633681622618, 93.86140135218193, 93.90749846342962, 93.97664413030117, 94.07652120467118, 94.03042409342348, 93.97280270436386, 94.21865396435157, 94.07267977873387, 94.09956976029503, 94.21865396435157, 94.30700676090964, 93.88444990780577, 94.06883835279656, 94.30316533497235, 94.23017824216349, 94.19944683466503, 94.05731407498463, 94.36846957590657, 94.26475107559926, 94.04578979717272, 94.31468961278426, 94.13030116779349, 94.3262138905962, 94.06499692685925, 93.98432698217579, 94.34926244622004, 94.3262138905962, 94.10341118623234, 94.35694529809466, 93.99969268592501, 94.21481253841426, 94.41840811309157, 94.13414259373079, 94.23017824216349, 94.39920098340504, 94.10341118623234, 94.0381069452981, 94.36846957590657, 94.44529809465274, 94.29548248309773, 94.39920098340504, 94.2839582052858, 94.34542102028273, 94.51444376152428, 94.34926244622004, 94.1418254456054, 94.45682237246466, 94.5221266133989, 94.41072526121697, 94.45298094652735, 93.9958512599877, 94.24938537185002, 94.57974800245852, 94.1418254456054, 94.18792255685311, 94.34926244622004, 94.48371235402581, 94.39535955746773, 94.39151813153042, 94.39535955746773, 94.41072526121697, 94.1418254456054, 94.24554394591273, 94.39535955746773, 94.49907805777505, 94.3262138905962, 94.22633681622618, 94.34542102028273, 94.42224953902888, 94.27627535341118, 94.46450522433928, 94.1840811309158, 94.1917639827904, 94.50291948371235, 94.09956976029503, 94.27243392747388, 94.38383527965581, 94.40304240934235, 94.10725261216963, 94.36462814996926, 94.52980946527352, 94.36078672403197, 94.50676090964966, 94.1917639827904, 94.25706822372464, 94.31084818684695, 94.43761524277812, 94.36462814996926, 94.42224953902888, 94.21097111247695, 94.38383527965581, 94.18023970497849, 94.21865396435157, 94.5259680393362, 94.53365089121081, 94.21865396435157, 94.56822372464659, 94.21865396435157, 94.31468961278426, 94.41072526121697, 94.50291948371235, 94.35310387215735, 94.51060233558697, 94.26090964966195, 94.3338967424708, 94.4299323909035, 94.40688383527966, 94.66425937307929, 94.41072526121697, 94.39535955746773, 94.23786109403811, 94.44145666871543, 94.30316533497235, 94.3761524277812, 94.04578979717272, 94.36846957590657, 94.3300553165335, 94.44145666871543, 94.64505224339274, 93.99200983405039, 94.53365089121081, 94.52980946527352, 94.3799938537185, 94.38767670559312, 94.54133374308543, 94.36078672403197, 94.44913952059004, 94.28779963122311, 94.48755377996312, 94.51828518746159, 94.4721880762139, 94.45682237246466, 94.39920098340504, 94.51444376152428, 94.10341118623234, 94.61432083589429, 94.41840811309157, 94.21865396435157, 94.46450522433928, 94.44913952059004, 94.4798709280885, 94.61432083589429, 94.22633681622618, 94.63352796558083, 94.51060233558697, 94.44145666871543, 94.5221266133989, 94.50291948371235, 93.93438844499079, 94.4721880762139, 94.48371235402581, 94.63736939151813, 94.5759065765212, 94.61047940995698, 94.39920098340504, 94.56822372464659, 94.21481253841426, 94.59895513214505, 94.32237246465888, 94.59127228027043, 94.25322679778733, 94.4299323909035, 94.54517516902274, 94.3338967424708, 94.40688383527966, 94.08420405654579, 94.5259680393362, 94.59895513214505, 94.6681007990166, 94.61047940995698, 94.44529809465274, 94.48371235402581, 94.48755377996312, 94.67962507682851, 94.36846957590657, 94.09572833435772, 94.35694529809466, 94.43377381684081, 94.55669944683467, 94.61047940995698, 94.36462814996926, 94.6220036877689, 94.69499078057775, 94.39535955746773, 94.47602950215119, 94.43761524277812, 94.3300553165335, 94.68730792870313, 94.56822372464659, 94.45298094652735, 94.61816226183159, 94.40304240934235, 94.48371235402581, 94.62968653964352, 94.54517516902274, 94.51828518746159, 94.60663798401967, 94.55669944683467, 94.50676090964966, 94.52980946527352, 94.59511370620774, 94.46066379840197, 94.40304240934235, 94.49139520590043, 94.4299323909035, 94.34157959434542, 94.6681007990166, 94.58358942839583, 94.61047940995698, 94.34926244622004, 94.57974800245852, 94.50676090964966, 94.71035648432698, 94.70651505838967, 94.5221266133989, 94.3761524277812, 94.50291948371235, 94.59127228027043, 94.52980946527352, 94.41456668715428, 94.53749231714812, 94.29932390903504, 94.5720651505839, 94.68730792870313, 94.61816226183159, 94.55285802089736, 94.6681007990166, 94.70651505838967, 94.59127228027043, 94.40304240934235, 94.69114935464044, 94.68730792870313, 94.5221266133989, 94.45682237246466, 94.51828518746159, 94.57974800245852, 94.61432083589429, 94.6220036877689, 94.34542102028273, 94.53749231714812, 94.40688383527966, 94.49523663183774, 94.42609096496619, 94.6757836508912, 94.42224953902888, 94.43377381684081, 94.56438229870928, 94.65273509526736, 94.49139520590043, 94.4721880762139, 94.3338967424708, 94.56054087277197, 94.70651505838967, 94.45682237246466]\n",
        "train_loss_list_cosine = [1.4922631353059113, 0.4829037673670425, 0.3722163860390826, 0.33177220926375245, 0.30060175129876227, 0.2739472426776964, 0.2571369293057499, 0.24421742562517565, 0.23246381484315323, 0.22585582048670064, 0.2115350698794776, 0.2027042203842786, 0.19336028488953586, 0.18796989869901803, 0.17635167322467335, 0.17350608562712425, 0.16392932351688705, 0.15824737119157786, 0.15103116432623812, 0.14280527628211148, 0.13911624957976465, 0.12778460819445814, 0.12645664817696503, 0.12117257786202963, 0.11404008718162048, 0.11024573898614261, 0.10301367604352918, 0.09962807324661957, 0.09590673247266429, 0.09140933892338822, 0.08890839097617563, 0.08312069913650028, 0.07912878103322049, 0.07539688910895247, 0.0703017507051307, 0.06730901317161112, 0.06545131962347604, 0.06006853946239806, 0.05934957680948644, 0.055661115047034776, 0.05501487654656535, 0.04808283364715756, 0.048917624036854686, 0.04978224897863177, 0.04599738403836765, 0.043476243917943865, 0.04154425250810538, 0.044289368833225914, 0.039924735993999975, 0.038134503976484824, 0.03573395804436448, 0.03633099462935413, 0.03407486775469762, 0.03466019954098273, 0.03175460545031914, 0.030751291346965928, 0.02861118235254312, 0.026954070920645837, 0.026345375887917098, 0.027870583739497655, 0.02564559653584358, 0.02595152790995118, 0.02664908538916373, 0.017370514487667858, 0.02682043477352759, 0.02288949422978072, 0.023653404380517905, 0.021177587269342865, 0.01825827019425427, 0.020935813946993946, 0.02142309610224594, 0.021006980622225117, 0.017297085196421504, 0.020975425832167893, 0.016085938927383088, 0.019187606733535582, 0.016529617174421272, 0.017658664713450265, 0.017461861724943822, 0.01473946469002416, 0.013888732157426767, 0.018813401251478592, 0.018245284999345322, 0.013429369967483517, 0.013213716673361565, 0.014179550147890478, 0.013165565478723676, 0.013442610755386424, 0.011938090494525969, 0.013952531346849035, 0.012777711567658235, 0.01361946394184095, 0.01415981548655886, 0.0132249127186142, 0.008202303269795845, 0.013768190907590946, 0.01187117434464689, 0.011072834069390408, 0.011375907309212407, 0.008056683960140176, 0.010813868758558904, 0.010755819991023294, 0.011055263281323623, 0.009854590676922565, 0.009897416544746738, 0.007694702545891319, 0.010974907935630917, 0.009073475042950686, 0.00957946586983009, 0.008521491170432235, 0.009181163735296882, 0.008033073926435217, 0.0075308985053745, 0.007736907337884813, 0.008905222457779166, 0.0075535059801524005, 0.008956405532912782, 0.00652504011866664, 0.007371700082288889, 0.007105531808951359, 0.009608815764438242, 0.006718999051766881, 0.006360923469938631, 0.006009158956909616, 0.005984226889401126, 0.006414240613540077, 0.008046470651133564, 0.006813095370703897, 0.004941387222356937, 0.006961080895634556, 0.0061504495415253034, 0.0044804993527444515, 0.005129817030924186, 0.0051584241011990275, 0.005904302868943308, 0.007040096893751609, 0.006497695825108162, 0.006231063241578372, 0.0045172427992303645, 0.004859256595798965, 0.005357292924210756, 0.0036115553579709734, 0.004059441841032762, 0.003976077818785122, 0.004686579279045762, 0.0047765986038821135, 0.0052203105924466305, 0.004452093922426263, 0.00366650621972908, 0.004813907496713725, 0.003619882242262452, 0.003379426909955734, 0.003642032724296165, 0.003996376050219234, 0.0027063930590780893, 0.0038246032934270147, 0.0035842454059730284, 0.0031291756938454703, 0.003011805406823518, 0.004053137657470737, 0.003489878551598058, 0.002363212944410382, 0.003003648613003568, 0.002955618500260455, 0.002723948648240625, 0.0037992216377342554, 0.0022372855783796795, 0.002972214323449715, 0.0029076020603400095, 0.002062946104120749, 0.0018581666937362376, 0.002313281514427136, 0.0030319557007007047, 0.0023327526890917524, 0.001145549262753536, 0.0016048090303144983, 0.0012414230531554533, 0.0032581396387386516, 0.0029362018629135516, 0.001846583708184242, 0.0012200965248589121, 0.001480849612214921, 0.002236391388236972, 0.0015825234965880586, 0.0009787926420108245, 0.001177633173575748, 0.0017891079141621141, 0.001687879517755926, 0.0027874750058864543, 0.0015881228617325382, 0.0013229923813940598, 0.0013549324787301051, 0.0013446073458035835, 0.0015000026920124656, 0.0012523066383357765, 0.000858102498131121, 0.001035546338261591, 0.0019881639752974524, 0.0016324728715111814, 0.001434380197954095, 0.0009871888154247414, 0.0008098082351434123, 0.0006027996978315856, 0.0008483088922217938, 0.0007788997386943644, 0.0014305688407895846, 0.0007751793022755421, 0.0005314110757599325, 0.0008008613197451418, 0.0005263145778683319, 0.0005657160187561332, 0.0008550505100606119, 0.0004061993359216422, 0.0008238756363850807, 0.0004022425374623086, 0.0007475109217440491, 0.0008010522570415786, 0.0008719455779264424, 0.0008102537067081855, 0.0006836875085969626, 0.000508847326879101, 0.00048076220441707043, 0.0003452522178527452, 0.0005887742460472115, 0.0007575123468680768, 0.0007316459814930954, 0.0006069562958639393, 0.00040482217438505993, 0.000413289390024444, 0.0002512041357034269, 0.00035321101188540014, 0.0002289985919775472, 0.00031013075728369315, 0.00048718760862263036, 0.00024815363102110633, 0.0002127035124524638, 0.00040036594692248164, 0.00022304668872061623, 0.0002539508311412804, 0.00033001078329677927, 0.0005046333238317957, 0.0002472649033615957, 0.00045928622355630194, 0.00022344069632032276, 0.00026002821110843074, 0.00015903297622683538, 0.00040398000405654787, 0.0002816041986039678, 0.00024018690979448482, 0.00029882328487923265, 0.00034577957207554183, 0.00028206157859465874, 0.00017730721445880266, 0.00011439717115794378, 0.00023043857095496602, 0.00013369225500067483, 0.00022097409422235014, 0.00013705560435689807, 0.000107482878806249, 0.0002181373778419168, 9.028549420162597e-05, 8.780925534434591e-05, 0.00015708037552040194, 0.00013996531600239778, 0.00015848159901898015, 9.64823931015096e-05, 0.00021168788567095436, 0.000136050276398834, 0.00010472044780222292, 0.0002206477002030788, 0.00013934975601720095, 0.00010074895917807085, 9.32180454911494e-05, 0.0001472607405713623, 0.00011569483834266491, 7.036176643037457e-05, 0.0001351957850342979, 0.000159376835863944, 0.00010674569268919185, 0.0001185344251126515, 8.718404802724013e-05, 0.00011900450702660869, 0.00012125460894170754, 0.00011230571605721275, 0.00010454212349596384, 0.0001487989837777661, 6.830416546227417e-05, 6.393169394098347e-05, 0.00020005663507281302, 0.00010431355027100938, 0.00017086659031600928, 7.99108349451789e-05, 0.00011386882648142751, 9.478551902827757e-05, 0.00010300004370941282, 8.344318782375742e-05, 0.0001274786575664539, 8.525987476745569e-05, 7.452874058836112e-05, 0.00011550953129777401]\n",
        "train_acc_list_cosine = [47.73742721016411, 84.54632080465855, 88.4425622022234, 89.96717840127052, 90.86712546320804, 91.64849126521969, 92.33245103229221, 92.79195341450503, 93.21757543673901, 93.32980412916888, 93.78295394388566, 94.04129168872419, 94.3928004235045, 94.53679195341451, 94.81206987824245, 95.05134992059291, 95.20169401799895, 95.48755955532027, 95.65484383271573, 95.92165166754897, 95.9640021175225, 96.39385918475384, 96.32186341979883, 96.55479089465325, 96.74748544203283, 96.74748544203283, 97.07146638433034, 97.24722075172049, 97.19640021175225, 97.32980412916888, 97.39332980412917, 97.62413975648491, 97.67496029645315, 97.79777660137638, 97.90365272631021, 97.96717840127052, 98.07940709370037, 98.21492853361568, 98.19163578613023, 98.32715722604553, 98.32292218104817, 98.55584965590259, 98.57702488088935, 98.43938591847538, 98.58973001588141, 98.68713605082054, 98.66807834833246, 98.61302276336686, 98.80359978824775, 98.78454208575967, 98.83748014822658, 98.80783483324511, 98.86077289571202, 98.89253573319216, 98.9433562731604, 99.02805717310747, 99.05982001058761, 99.13816834303864, 99.14452091053468, 99.0979354155638, 99.13393329804128, 99.1148755955532, 99.10217046056114, 99.45579671784013, 99.15510852302806, 99.22710428798305, 99.22498676548439, 99.32451032292218, 99.39650608787719, 99.32662784542086, 99.27368978295394, 99.3774483853891, 99.41132874536792, 99.29698253043938, 99.45367919534145, 99.3668607728957, 99.47061937533087, 99.43885653785071, 99.43250397035469, 99.51932239280042, 99.5404976177872, 99.33509793541556, 99.3774483853891, 99.5299100052938, 99.54473266278454, 99.5129698253044, 99.57014293276866, 99.57014293276866, 99.6294335627316, 99.48967707781895, 99.55320275277924, 99.5659078877713, 99.53202752779248, 99.58073054526204, 99.73742721016411, 99.53838009528852, 99.61461090524087, 99.6294335627316, 99.63366860772896, 99.7289571201694, 99.63155108523029, 99.64213869772367, 99.61884595023822, 99.71201694017999, 99.69084171519323, 99.74589730015882, 99.63790365272631, 99.72260455267337, 99.69719428268925, 99.72260455267337, 99.66754896770779, 99.70354685018528, 99.73954473266278, 99.72683959767072, 99.67601905770249, 99.73319216516676, 99.71625198517734, 99.80518793012176, 99.7289571201694, 99.77130757014294, 99.6929592376919, 99.7924827951297, 99.77766013763896, 99.784012705135, 99.80730545262044, 99.78613022763366, 99.76071995764956, 99.79036527263102, 99.83906829010058, 99.76707252514558, 99.7924827951297, 99.86659608258337, 99.84118581259926, 99.8284806776072, 99.79883536262572, 99.77977766013764, 99.79671784012704, 99.79671784012704, 99.83059820010588, 99.84965590259397, 99.8369507676019, 99.86871360508205, 99.89412387506617, 99.87083112758073, 99.85177342509265, 99.85177342509265, 99.81365802011646, 99.85389094759132, 99.87294865007941, 99.84118581259926, 99.87294865007941, 99.89200635256749, 99.88353626257279, 99.87294865007941, 99.90682901005823, 99.87506617257809, 99.87506617257809, 99.89624139756485, 99.90047644256221, 99.85812599258867, 99.88988883006881, 99.91953414505029, 99.90894653255691, 99.89412387506617, 99.90047644256221, 99.88988883006881, 99.92165166754897, 99.88777130757015, 99.90047644256221, 99.92588671254632, 99.93223928004235, 99.91953414505029, 99.88565378507147, 99.91953414505029, 99.95764955002647, 99.94917946003176, 99.95341450502912, 99.90471148755955, 99.91529910005293, 99.94070937003706, 99.95764955002647, 99.95764955002647, 99.93435680254103, 99.94706193753309, 99.9724722075172, 99.96611964002118, 99.9364743250397, 99.94282689253573, 99.91953414505029, 99.94706193753309, 99.94917946003176, 99.96188459502382, 99.95341450502912, 99.94917946003176, 99.95129698253044, 99.96823716251986, 99.97458973001588, 99.9364743250397, 99.9555320275278, 99.95129698253044, 99.9640021175225, 99.97458973001588, 99.97458973001588, 99.9640021175225, 99.97882477501324, 99.94706193753309, 99.97458973001588, 99.97882477501324, 99.9724722075172, 99.98094229751192, 99.98305982001058, 99.96188459502382, 99.98305982001058, 99.97035468501853, 99.98517734250926, 99.97458973001588, 99.95976707252514, 99.9640021175225, 99.9640021175225, 99.97670725251456, 99.98729486500794, 99.97670725251456, 99.98305982001058, 99.97670725251456, 99.97882477501324, 99.97882477501324, 99.97670725251456, 99.98305982001058, 99.98729486500794, 99.98517734250926, 99.98941238750662, 99.98941238750662, 99.98941238750662, 99.98729486500794, 99.98941238750662, 99.9915299100053, 99.98094229751192, 99.9915299100053, 99.98517734250926, 99.98094229751192, 99.98729486500794, 99.98941238750662, 99.98094229751192, 99.98941238750662, 99.98305982001058, 99.99364743250398, 99.98517734250926, 99.98729486500794, 99.98941238750662, 99.9915299100053, 99.98094229751192, 99.98305982001058, 99.9915299100053, 99.99576495500264, 99.98517734250926, 99.9915299100053, 99.98941238750662, 99.99576495500264, 99.99576495500264, 99.98941238750662, 99.99364743250398, 100.0, 99.9915299100053, 99.9915299100053, 99.98941238750662, 99.99576495500264, 99.98729486500794, 99.99576495500264, 99.99364743250398, 99.9915299100053, 99.99364743250398, 99.99364743250398, 99.99576495500264, 99.98941238750662, 99.9915299100053, 99.99788247750132, 99.98941238750662, 99.9915299100053, 99.99788247750132, 99.99576495500264, 99.99576495500264, 99.99364743250398, 99.9915299100053, 99.99576495500264, 99.99364743250398, 99.99364743250398, 99.99788247750132, 100.0, 99.98941238750662, 99.99576495500264, 99.99364743250398, 99.99576495500264, 99.9915299100053, 99.99576495500264, 99.99364743250398, 99.99576495500264, 99.9915299100053, 99.99788247750132, 99.99576495500264, 99.99576495500264]\n",
        "test_loss_list_cosine = [0.8128060245630788, 0.43769084055926283, 0.3670157931160693, 0.3468363919094497, 0.320717461845454, 0.29874755628407, 0.2880513122414841, 0.27089574385215254, 0.2712526289636598, 0.25813622246770296, 0.27607545629143715, 0.2508813175281473, 0.23157318663217275, 0.23963026514313385, 0.23259784851004095, 0.21799716919514478, 0.2553474395992417, 0.23297278822271847, 0.23411084861293727, 0.24059130224015782, 0.22228851220479198, 0.23491562519441633, 0.23474602972832964, 0.24284341398115253, 0.2531394691699568, 0.2231898330809439, 0.23778781842659502, 0.2260036576612323, 0.2408501879476449, 0.2531653588601187, 0.2437402953521586, 0.25621635543511195, 0.25155149374668506, 0.25073284860335143, 0.24820336959708264, 0.2527083886933385, 0.265060295743466, 0.2638632501977697, 0.27953584988911945, 0.28330178079469237, 0.2898024614425559, 0.285278165183377, 0.28805743263322203, 0.29073836449898927, 0.3027702011898452, 0.3096775676069014, 0.30173745603921515, 0.3021223259468873, 0.30385272365574745, 0.3038429137702812, 0.31642839670473455, 0.31861640270068947, 0.3238279426963452, 0.3290770745598802, 0.3349987664123011, 0.3213929162525079, 0.33029763011590524, 0.34525904689422426, 0.3447993094028503, 0.330581444653445, 0.321795464146371, 0.3393226447423883, 0.33919910747813536, 0.36219359443102983, 0.34984895151437206, 0.34508059543155717, 0.3312660858683361, 0.3478064838449891, 0.37045160054649207, 0.36637233636871563, 0.3615635437069132, 0.34741060503338483, 0.36895488022698786, 0.3510999241478595, 0.3899413192069487, 0.3609655104285362, 0.3731252212287383, 0.38873775874940203, 0.3712333329387155, 0.37048297439354894, 0.39439879441816433, 0.373478519492874, 0.36712522848564033, 0.3914665420420979, 0.38469330903471394, 0.3913668092230664, 0.38145966287337096, 0.37784753624788103, 0.38290000464036766, 0.3881754014622785, 0.39466715339279057, 0.38420211229765533, 0.3928751589150588, 0.3871026979576723, 0.39617241206852827, 0.42028651262323063, 0.38088225707521334, 0.3810242618087168, 0.38620046643978534, 0.41002833922667536, 0.40598774530157883, 0.4145270490324965, 0.4107616166036357, 0.39369509952124576, 0.3983155651949346, 0.4124191829971239, 0.40144294117778245, 0.4075972944425017, 0.41423957538791, 0.4235737093589634, 0.4338786544680011, 0.41868019625818464, 0.4211389536400983, 0.42915419865783083, 0.43854167406428973, 0.44444839669135855, 0.4239410953870153, 0.4140438513154639, 0.4214930013826519, 0.4469692508014394, 0.42807763744620425, 0.4140391830473627, 0.40559402319109616, 0.4296429092794949, 0.447968863443855, 0.4537148197985017, 0.4160180985562357, 0.42932759126757875, 0.4655364652474721, 0.44017627959450084, 0.4476152099646153, 0.45807484502666723, 0.4661675389518267, 0.46650424552624864, 0.4483312015951264, 0.43905310305839806, 0.4307253006571794, 0.4221340075886224, 0.43024291792957514, 0.4534414633743319, 0.439005637879246, 0.455770535284982, 0.4492703357884916, 0.46906485925337266, 0.4463952654680493, 0.4687405661497192, 0.4440763243108842, 0.4453616064936653, 0.466629167502819, 0.4585120371433304, 0.45512742501017, 0.4547710154278606, 0.46565503765847166, 0.47993873200361054, 0.46249571734783695, 0.47604353346076667, 0.4764781709778689, 0.4798887720623729, 0.47362343249294686, 0.477798045156341, 0.475341152007162, 0.46233028874677773, 0.47649588846765895, 0.4905768512090778, 0.49597619045023605, 0.4826731762720966, 0.46256263993963526, 0.4635071344308409, 0.4836937281412675, 0.4737923650753538, 0.47685492698641896, 0.49306667890107514, 0.5079087016615542, 0.47638218596294596, 0.47670910018496215, 0.5044073596980203, 0.5164688170211864, 0.5278736670528922, 0.4981932266542286, 0.4995226736783105, 0.5136372742845732, 0.47614785384697217, 0.5164846121424845, 0.4969261799825985, 0.504426286427998, 0.5065735648545966, 0.5321676667674682, 0.5085760319283615, 0.5058678923307133, 0.505933805387102, 0.5226816843154237, 0.5136628734705714, 0.49541927424862103, 0.5014571096729853, 0.5036171923942782, 0.4995590589264883, 0.5177521720453275, 0.5068038706028578, 0.5099713094623796, 0.4981300873846254, 0.5104206896560523, 0.5262805088377539, 0.5344062875499767, 0.5306157725908812, 0.5357992586351055, 0.5134817360370767, 0.5160781835849562, 0.5250964877469575, 0.5374465012448091, 0.5358220952946473, 0.5378625886514783, 0.5421092164064711, 0.5452210189199403, 0.5411839835076392, 0.5549019162050065, 0.5455894065546054, 0.5472353847934773, 0.5522625445497825, 0.5455132025983367, 0.538114388518985, 0.5509232937326363, 0.536594347077577, 0.5530879087015694, 0.5591187921909652, 0.5578411847065368, 0.5505201403601163, 0.564152417302716, 0.5473365803345052, 0.5458330702511411, 0.5650019636080947, 0.5671965913600562, 0.5513512968934853, 0.5562908629117552, 0.5620296227371356, 0.5617975970529312, 0.5581122653555426, 0.5675084015614736, 0.5657756982859699, 0.5526409040846606, 0.5721038452220634, 0.5690990948042942, 0.5608643038853474, 0.5609114988775556, 0.560604001846692, 0.5697500870121397, 0.5728723204822517, 0.5595448732640886, 0.5570606883866632, 0.5764513230818671, 0.556780993596048, 0.5647389884220948, 0.5543315944429624, 0.5563175826808255, 0.5460904273889301, 0.5574481183030716, 0.552725079560689, 0.5607348622220075, 0.5605667959076955, 0.5626120614498515, 0.5649992083380507, 0.5637176143309083, 0.5681553194373526, 0.5555440176634446, 0.5632415316604059, 0.5714907293874478, 0.5667051952526284, 0.5770144956528812, 0.5655391789224072, 0.57731314366444, 0.5691993516018413, 0.5740196594384079, 0.5696190810367446, 0.5625503671834332, 0.5508576962696251, 0.5676857517648708, 0.575283505166333, 0.5721206341997958, 0.5749247564988977, 0.5699750101832929, 0.580029735533411, 0.5632244463092374, 0.5753009883919731, 0.5712149130710054, 0.5708004219147066, 0.5698616763567734, 0.5771289975888495, 0.577879674463332, 0.5692181185472245, 0.591953951887102, 0.5678756476480368, 0.5756764801849118, 0.578166978474816, 0.5680953554498652, 0.5808629620039616, 0.5650976837804431, 0.5722748214018294, 0.5711645722097042, 0.5751072838172024, 0.5794820262486681, 0.5856684064892047]\n",
        "test_acc_list_cosine = [75.21511985248924, 86.48970497848802, 88.72157344806392, 89.39766441303011, 90.33113091579594, 91.21850030731407, 91.4259373079287, 92.23647818070067, 92.30562384757222, 92.6859250153657, 91.92532267977873, 92.7819606637984, 93.44652735095268, 93.21988322065151, 93.53872157344806, 93.85371850030731, 92.85494775660726, 93.56561155500921, 93.33512599877075, 93.48494161032575, 93.96896127842655, 93.75768285187462, 93.72311001843885, 93.5579287031346, 93.30439459127228, 94.19560540872772, 93.9459127228027, 94.2340196681008, 93.98048555623848, 93.700061462815, 94.24554394591273, 93.68085433312845, 93.82298709280884, 94.05731407498463, 94.24938537185002, 94.29164105716042, 93.83835279655808, 94.26090964966195, 94.03042409342348, 93.97280270436386, 94.00353411186232, 94.06883835279656, 94.13414259373079, 94.00737553779963, 94.12261831591887, 93.93822987092808, 94.16871542716656, 93.9958512599877, 93.96127842655194, 94.22249539028887, 93.93438844499079, 93.91518131530424, 94.0918869084204, 94.01121696373694, 94.15719114935465, 94.29164105716042, 94.31853103872157, 94.01889981561156, 94.15719114935465, 93.88060848186846, 94.54133374308543, 94.34542102028273, 94.14950829748003, 94.09572833435772, 94.36078672403197, 94.21481253841426, 94.3262138905962, 94.27627535341118, 93.92286416717886, 94.05731407498463, 94.09572833435772, 94.41456668715428, 94.24554394591273, 94.31853103872157, 94.24170251997542, 94.06115550092194, 94.0419483712354, 93.95359557467732, 94.30316533497235, 94.42224953902888, 94.1917639827904, 94.02274124154886, 94.43761524277812, 94.12645974185618, 94.20712968653964, 94.40304240934235, 94.35694529809466, 94.29164105716042, 94.48371235402581, 94.13030116779349, 94.31853103872157, 94.3338967424708, 94.41840811309157, 94.66041794714198, 94.48755377996312, 94.09956976029503, 94.26090964966195, 94.34157959434542, 94.38767670559312, 94.4798709280885, 94.38767670559312, 94.36078672403197, 94.16871542716656, 94.64505224339274, 94.36846957590657, 94.48371235402581, 94.28011677934849, 94.46450522433928, 94.28779963122311, 94.36078672403197, 94.35694529809466, 94.55285802089736, 94.35310387215735, 94.48755377996312, 94.34926244622004, 94.47602950215119, 94.64889366933005, 94.46450522433928, 94.44145666871543, 94.54133374308543, 94.52980946527352, 94.54901659496005, 94.69883220651506, 94.58743085433314, 94.16871542716656, 94.41456668715428, 94.56438229870928, 94.5759065765212, 94.21097111247695, 94.54517516902274, 94.44529809465274, 94.46834665027659, 94.27627535341118, 94.34157959434542, 94.71035648432698, 94.49139520590043, 94.60663798401967, 94.75645359557468, 94.74108789182544, 94.66041794714198, 94.59895513214505, 94.66041794714198, 94.57974800245852, 94.57974800245852, 94.71035648432698, 94.54901659496005, 94.7679778733866, 94.7679778733866, 94.56438229870928, 94.67194222495391, 94.66425937307929, 94.74492931776275, 94.79486785494775, 94.44529809465274, 94.73724646588813, 94.43761524277812, 94.72956361401353, 94.72572218807622, 94.74877074370006, 94.66041794714198, 94.4798709280885, 94.80639213275968, 94.61432083589429, 94.50676090964966, 94.70651505838967, 94.70651505838967, 94.59895513214505, 94.78334357713584, 94.76413644744929, 94.79870928088506, 94.69883220651506, 94.59511370620774, 94.42609096496619, 94.87169637369392, 94.86785494775661, 94.74492931776275, 94.74108789182544, 94.29932390903504, 94.86785494775661, 94.84864781807006, 94.68346650276582, 94.94852489244008, 94.69499078057775, 94.82175783650891, 94.86017209588199, 94.89090350338046, 94.6258451137062, 94.82559926244622, 94.83328211432084, 94.90242778119237, 94.88322065150584, 94.89090350338046, 94.96004917025199, 94.87553779963122, 94.9139520590043, 94.91779348494161, 94.99462200368777, 94.94852489244008, 94.85248924400737, 94.83712354025815, 94.90626920712968, 94.76029502151198, 94.78334357713584, 94.99078057775046, 94.88706207744315, 95.02151198524892, 94.97925629993854, 94.97541487400123, 94.87553779963122, 94.97541487400123, 94.88706207744315, 94.76413644744929, 94.84096496619546, 94.9638905961893, 94.96004917025199, 95.01382913337432, 94.99846342962508, 94.91011063306699, 94.92163491087892, 94.93315918869084, 94.9139520590043, 95.12907191149354, 95.03687768899816, 94.8640135218193, 94.82559926244622, 94.98693915181315, 94.90626920712968, 94.95236631837739, 94.98309772587585, 94.92931776275353, 94.74877074370006, 95.11754763368162, 95.02151198524892, 95.15596189305471, 94.98693915181315, 94.89858635525508, 94.99462200368777, 94.98693915181315, 95.07145052243392, 94.9638905961893, 94.97925629993854, 94.9638905961893, 94.98309772587585, 94.99462200368777, 94.95236631837739, 95.059926244622, 95.01767055931161, 95.11754763368162, 95.02919483712354, 95.13291333743085, 95.09834050399509, 95.109864781807, 95.159803318992, 95.16748617086662, 94.98693915181315, 95.22126613398893, 95.06376767055932, 95.12138905961893, 95.1521204671174, 95.04071911493547, 95.16364474492931, 95.17132759680393, 95.09834050399509, 95.1521204671174, 95.08681622618316, 95.059926244622, 95.20974185617702, 95.059926244622, 95.02151198524892, 95.14443761524278, 95.109864781807, 95.12907191149354, 95.17901044867855, 95.20974185617702, 95.17132759680393, 95.17132759680393, 95.19437615242778, 95.02151198524892, 95.17132759680393, 95.12907191149354, 95.19437615242778, 95.19821757836509, 95.12907191149354, 95.09449907805778, 95.109864781807, 95.24431468961278, 95.12907191149354, 95.16748617086662, 95.01382913337432, 95.22894898586355, 95.06760909649662, 95.11370620774431, 95.21742470805162, 95.159803318992, 95.22894898586355, 95.14059618930547, 95.109864781807, 95.14827904118009, 95.13675476336816, 95.09449907805778]\n",
        "train_loss_list_step = [1.5044204033810271, 0.49208608810817644, 0.37830603914209177, 0.33356322168415475, 0.30051283241529775, 0.2731129997548695, 0.25931339448464275, 0.24751879757416603, 0.23341218238762076, 0.22551919120114025, 0.21245744068246225, 0.20461816869696303, 0.19351729513385754, 0.18888680500591673, 0.1790498158513369, 0.17313903582794718, 0.16563977062863708, 0.15938490460599017, 0.1541340289319434, 0.14407484958246147, 0.13983443950815252, 0.13245242257630277, 0.12638695052443805, 0.12125968426304458, 0.11788717211605249, 0.11147532753424269, 0.10493720971996869, 0.10114793143257862, 0.0986312177905505, 0.09325535373060603, 0.06156983597173802, 0.050082290781091464, 0.04566037622696864, 0.042090283896077454, 0.037587769078418896, 0.03525592588446025, 0.034040165755286976, 0.031216950466235478, 0.030938032096463855, 0.029107897011765093, 0.02691679777953582, 0.024817724369887797, 0.02327188092970408, 0.022008026162917072, 0.021083637177603455, 0.020199724932410693, 0.020445302296316296, 0.01811823503557229, 0.016885934063002497, 0.016305384605991728, 0.015734232263797315, 0.01582022506202922, 0.01498541162761943, 0.013855328222454038, 0.012501976389670114, 0.011526370763357194, 0.012135067156394237, 0.011748958516110058, 0.01063293406628148, 0.010577014861956521, 0.009060478683824204, 0.00822005160518879, 0.007903504668233972, 0.0073660259777244665, 0.007953700653634155, 0.006759841606805863, 0.0069558188274406844, 0.007026840372753726, 0.006894602971896024, 0.0063807274687209935, 0.006584282455781946, 0.006663436767551165, 0.006154599042466102, 0.006103539849245962, 0.006048463034234704, 0.00614482418276391, 0.006322457585395453, 0.006028014029933677, 0.006245754332408476, 0.00581059281254909, 0.005709030747089944, 0.005923201890116833, 0.00537758718227695, 0.005807922808844647, 0.005243748521360034, 0.004974289045318631, 0.005323306708443634, 0.005775554073696229, 0.005093053230784864, 0.005536427160324011, 0.0052108629591284105, 0.004749226128792285, 0.005127630246161776, 0.005065120122888249, 0.004685268356975264, 0.005243820296808683, 0.0049121043785080205, 0.005093204319377071, 0.005370734221114888, 0.005008859195898547, 0.005249464887712999, 0.004638611593474268, 0.004728934781005616, 0.005035177127017733, 0.004871168427265863, 0.005197190184529169, 0.004969428170602996, 0.004836069228517858, 0.005308753678475464, 0.004877718586694237, 0.0046394178739258, 0.004715173941094246, 0.005071522070231865, 0.005326596872854554, 0.004782932753934018, 0.00452435254143787, 0.005128751716483132, 0.004405792564614916, 0.005072576416472445, 0.004683375086275378, 0.004745950062742184, 0.005199869099014116, 0.004980171865863766, 0.004840858398257126, 0.0046297258561474545, 0.004515150684752434, 0.005211585615480793, 0.0046955638231178315, 0.004485690257187711, 0.004824536544108968, 0.004908301922912914, 0.005288558260384356, 0.004324457162535601, 0.004497020012851448, 0.004832310812420976, 0.004629586771150122, 0.00469785524222389, 0.0051049072852895816, 0.004760202388700317, 0.00467975922823293, 0.004756802660750659, 0.004577458516745399, 0.0046352100029977686, 0.0045855785586362005, 0.0045302773612948275, 0.00527981271794404, 0.004842230399004512, 0.005128007588957292, 0.0047336897144657575, 0.00452429031405685, 0.004871112167589785, 0.004707888855899686, 0.004987144301444176, 0.004576647475924656, 0.0050210763291275344, 0.004869554478370365, 0.005145938421950075, 0.0049324849194394005, 0.005007630027365255, 0.004694439974090723, 0.005161294266647788, 0.005086089323139447, 0.0045723088694465674, 0.0049315163145030455, 0.005043683056833177, 0.004782498019489051, 0.005208640262013842, 0.004906343821498318, 0.004504839811582297, 0.004765471186647242, 0.004636792729359163, 0.0048650642755604795, 0.00440384292277286, 0.005331020985203442, 0.004309070902481564, 0.005132160800841049, 0.0048202084057818, 0.0042395592619208135, 0.004579358937877841, 0.004854094481356265, 0.004861267090400903, 0.004641431140822869, 0.004752275634260134, 0.005280723348089562, 0.0049866622946490744, 0.004713922302353166, 0.0049851976106240516, 0.0049522482596579456, 0.0047414097434517405, 0.005198805679831853, 0.005367297160049249, 0.004801289511232152, 0.004936103088396973, 0.004513620775553264, 0.005461517685133368, 0.005038318795404084, 0.004621399451298942, 0.004861979541443869, 0.004728190580562456, 0.00424972596940481, 0.004389661508080736, 0.004675524760544651, 0.004678315578823907, 0.005468495250993464, 0.004904287783485815, 0.004628283689890138, 0.004957924133196958, 0.004207717516179686, 0.004497190112673026, 0.0054004669855449725, 0.004738672002041044, 0.004832978636525801, 0.004593934473861766, 0.00428500973297249, 0.004800252157330026, 0.004579809123577946, 0.0047766751335504105, 0.004927973183699149, 0.003962443585616099, 0.005313499298157754, 0.005448946752569696, 0.004626204171514537, 0.00486521731347835, 0.00464669697920995, 0.00473364783223967, 0.0051715146905747635, 0.004559630797677459, 0.004807513805560901, 0.004414798407140251, 0.0055058369074104515, 0.004735360263094689, 0.00523965430700253, 0.004729446927025576, 0.0046202187999946675, 0.004473174700221971, 0.005083680975845281, 0.00473606128646736, 0.004358955291625135, 0.004462744230992882, 0.00445035025452959, 0.004661599569843557, 0.004881452044679059, 0.004753526896741532, 0.004433994836329803, 0.004789544894031266, 0.004920090539939323, 0.005374149948520697, 0.004787621462456112, 0.004773729829575745, 0.0044936602159881545, 0.004565336937149469, 0.004793149001421886, 0.004954056282359589, 0.00447947610881707, 0.004911790105014495, 0.004962967739427566, 0.004856595992693377, 0.0049429370942105965, 0.004495315723300048, 0.005116432885005868, 0.004663471084030195, 0.005105866048162296, 0.0048063792890729005, 0.004996042520567828, 0.0048235019935209174, 0.0052012274302681025, 0.004979272351725164, 0.004979745390991236, 0.005170751338667516, 0.004858217863982788, 0.004663840651548111, 0.005053441917670759, 0.004584236903877065, 0.004780888221675258, 0.004612757356412744, 0.004703525326821731, 0.005128545914503725, 0.005097627184756964, 0.004888794860495936, 0.004546376856262884, 0.0049637655482588335, 0.004738951851973477, 0.005256142429007358, 0.004507868475276601, 0.004676521270572799, 0.0042853716939203765, 0.004519075942008067, 0.004505532296253924, 0.004830542344546013, 0.004257976696643534, 0.004718432969060035, 0.004490681710655531, 0.004793198622190861, 0.004690485046958793, 0.004381172440937899, 0.004597538895479108, 0.0050219146489527905, 0.004799934776746264, 0.004984567811975109, 0.004093347564194513]\n",
        "train_acc_list_step = [47.394388565378506, 84.38962413975648, 88.32609846479619, 89.65802011646373, 90.93912122816305, 91.76071995764956, 92.30492323980943, 92.69878242456326, 93.1371095817893, 93.283218634198, 93.84859714134463, 93.97988353626258, 94.4203282159873, 94.56643726839597, 94.83324510322922, 95.06405505558496, 95.14452091053468, 95.38168343038645, 95.61461090524087, 95.86871360508205, 95.94282689253573, 96.23928004235044, 96.33456855479089, 96.51667548967708, 96.58231868713605, 96.79618845950239, 96.97829539438857, 97.11805187930122, 97.2006352567496, 97.23451561672843, 98.32927474854421, 98.71254632080466, 98.81630492323981, 98.94970884065643, 99.04499735309687, 99.05558496559026, 99.12969825304394, 99.22075172048703, 99.16993118051879, 99.24616199047115, 99.32027527792482, 99.33721545791424, 99.38168343038645, 99.40285865537321, 99.45156167284277, 99.46426680783483, 99.43885653785071, 99.49814716781366, 99.51932239280042, 99.55320275277924, 99.58073054526204, 99.56379036527264, 99.56379036527264, 99.6209634727369, 99.65907887771307, 99.68237162519851, 99.61884595023822, 99.62731604023293, 99.69507676019057, 99.72260455267337, 99.73742721016411, 99.7564849126522, 99.76919004764426, 99.79883536262572, 99.7649550026469, 99.83906829010058, 99.80307040762308, 99.81154049761778, 99.81154049761778, 99.83271572260455, 99.8009528851244, 99.82424563260984, 99.8369507676019, 99.82424563260984, 99.83906829010058, 99.83483324510323, 99.8369507676019, 99.84965590259397, 99.82636315510852, 99.83059820010588, 99.83906829010058, 99.84118581259926, 99.86236103758603, 99.85600847008999, 99.86236103758603, 99.87083112758073, 99.8369507676019, 99.8284806776072, 99.84753838009529, 99.85600847008999, 99.85389094759132, 99.87083112758073, 99.85812599258867, 99.86024351508735, 99.88353626257279, 99.85600847008999, 99.87506617257809, 99.85600847008999, 99.86236103758603, 99.84753838009529, 99.85600847008999, 99.87930121757543, 99.86236103758603, 99.8644785600847, 99.85177342509265, 99.86024351508735, 99.85389094759132, 99.85812599258867, 99.84330333509793, 99.86024351508735, 99.87718369507677, 99.86871360508205, 99.85812599258867, 99.84753838009529, 99.86659608258337, 99.87083112758073, 99.85389094759132, 99.88988883006881, 99.86024351508735, 99.87718369507677, 99.86659608258337, 99.84118581259926, 99.87083112758073, 99.85812599258867, 99.88777130757015, 99.88988883006881, 99.84542085759661, 99.87294865007941, 99.87083112758073, 99.85177342509265, 99.86024351508735, 99.86236103758603, 99.87506617257809, 99.8644785600847, 99.86024351508735, 99.86659608258337, 99.86871360508205, 99.85600847008999, 99.87083112758073, 99.87294865007941, 99.87294865007941, 99.88353626257279, 99.86236103758603, 99.87930121757543, 99.87930121757543, 99.86024351508735, 99.86024351508735, 99.83483324510323, 99.8644785600847, 99.89200635256749, 99.87083112758073, 99.84965590259397, 99.84965590259397, 99.88141874007411, 99.84753838009529, 99.87506617257809, 99.85389094759132, 99.85812599258867, 99.86236103758603, 99.85600847008999, 99.85600847008999, 99.84118581259926, 99.87930121757543, 99.86024351508735, 99.85812599258867, 99.86871360508205, 99.85389094759132, 99.86236103758603, 99.87718369507677, 99.85812599258867, 99.87083112758073, 99.85600847008999, 99.87506617257809, 99.87506617257809, 99.88777130757015, 99.85812599258867, 99.85600847008999, 99.88777130757015, 99.86659608258337, 99.87718369507677, 99.86659608258337, 99.86659608258337, 99.88141874007411, 99.85177342509265, 99.85600847008999, 99.87718369507677, 99.86236103758603, 99.87930121757543, 99.87506617257809, 99.85177342509265, 99.84118581259926, 99.86236103758603, 99.85812599258867, 99.87506617257809, 99.82636315510852, 99.85600847008999, 99.87718369507677, 99.88353626257279, 99.87294865007941, 99.89624139756485, 99.88353626257279, 99.87718369507677, 99.87083112758073, 99.84542085759661, 99.8644785600847, 99.88141874007411, 99.85600847008999, 99.89412387506617, 99.88141874007411, 99.85177342509265, 99.85389094759132, 99.87930121757543, 99.88565378507147, 99.88353626257279, 99.86871360508205, 99.86871360508205, 99.8644785600847, 99.86871360508205, 99.89624139756485, 99.84330333509793, 99.84542085759661, 99.88353626257279, 99.86236103758603, 99.87930121757543, 99.87506617257809, 99.87083112758073, 99.87294865007941, 99.87294865007941, 99.86871360508205, 99.84330333509793, 99.86659608258337, 99.84542085759661, 99.87506617257809, 99.8644785600847, 99.89200635256749, 99.84542085759661, 99.86871360508205, 99.87506617257809, 99.88141874007411, 99.87083112758073, 99.87083112758073, 99.86659608258337, 99.86659608258337, 99.86871360508205, 99.88565378507147, 99.87083112758073, 99.84753838009529, 99.88777130757015, 99.87718369507677, 99.88353626257279, 99.87083112758073, 99.86236103758603, 99.87718369507677, 99.88777130757015, 99.84965590259397, 99.85600847008999, 99.86236103758603, 99.85812599258867, 99.87718369507677, 99.8644785600847, 99.86024351508735, 99.87506617257809, 99.8644785600847, 99.8644785600847, 99.87930121757543, 99.85177342509265, 99.86024351508735, 99.85177342509265, 99.84118581259926, 99.86871360508205, 99.87718369507677, 99.86024351508735, 99.88988883006881, 99.8644785600847, 99.88141874007411, 99.87506617257809, 99.85177342509265, 99.86871360508205, 99.87506617257809, 99.87506617257809, 99.87930121757543, 99.86871360508205, 99.85812599258867, 99.88988883006881, 99.88353626257279, 99.89412387506617, 99.87930121757543, 99.86659608258337, 99.86659608258337, 99.88988883006881, 99.87083112758073, 99.87506617257809, 99.87083112758073, 99.87718369507677, 99.87930121757543, 99.87083112758073, 99.84330333509793, 99.87718369507677, 99.87294865007941, 99.88565378507147]\n",
        "test_loss_list_step = [0.7521707053278007, 0.41922322909037274, 0.3967930445191907, 0.35680990534670215, 0.3250766685049908, 0.30643289908766747, 0.27803952721696273, 0.26461896110399097, 0.26403681638047977, 0.2668783114309989, 0.2609864034708224, 0.25131638298797254, 0.24851202544774495, 0.24535505795011334, 0.23515790532909187, 0.23713360856488055, 0.2406249167215006, 0.24633904404061682, 0.22870721204169825, 0.23081522924350759, 0.23210598513776182, 0.23380596216256713, 0.23264140811036615, 0.2492445185597913, 0.24672274596477842, 0.23322659500819795, 0.2332557491848574, 0.2325650899324055, 0.2574915187433362, 0.23942345477567584, 0.21860766188953729, 0.22490006466122234, 0.22599033247588166, 0.2330661828027052, 0.23813083913980745, 0.2410290090677639, 0.25265564182408007, 0.25388093285408675, 0.25897152009694013, 0.2653597001095905, 0.267472947769634, 0.2757880237756991, 0.28352711534164116, 0.2891207381023788, 0.2937827925471699, 0.3016136303891008, 0.29683313752506296, 0.3059604737939605, 0.31242154799766986, 0.313900034299449, 0.3219801964347853, 0.33177046875889393, 0.3355082166837711, 0.33843776536192377, 0.34364481102309974, 0.3455076480613035, 0.34563379169569586, 0.35321818167051555, 0.35429473890576, 0.35551488774317297, 0.36010582940470354, 0.35364405492631096, 0.3636273516743791, 0.36364757276012327, 0.35665451670887277, 0.3549169595261999, 0.3703063836905594, 0.36219835961146246, 0.3605355029998749, 0.3578162646861564, 0.36774474406140106, 0.3633771698702784, 0.3653042366746448, 0.3691165424956411, 0.36504066177625577, 0.3715900047225695, 0.377250660302154, 0.3705682765610297, 0.3703488810170515, 0.37351591648606985, 0.37677567382343113, 0.37160208523638694, 0.3709423184650494, 0.3721590631769276, 0.3763345616011351, 0.3800408639488559, 0.37680745827874135, 0.3771674718214747, 0.3848353975142042, 0.3798498407590623, 0.3773468048494382, 0.3819653425073507, 0.3785710880149375, 0.37396970703654614, 0.37354924148131236, 0.38423583746029466, 0.3775230319622685, 0.36990976165614875, 0.3731101816017911, 0.3812584620443921, 0.3834761079017292, 0.38286398570327196, 0.3858671422372116, 0.3799955192816389, 0.3770175949378195, 0.3851937071338077, 0.38235137271968755, 0.38205398502303106, 0.38025479872400564, 0.3843826920378442, 0.38355313671533675, 0.3811237645043316, 0.37951354816665545, 0.3756716687428564, 0.38028262046110983, 0.38163883488296585, 0.38113248269712807, 0.38293040138395396, 0.38826611508414444, 0.3821931903378344, 0.3927168071534777, 0.3814124613087259, 0.38783683029788674, 0.384523262598497, 0.3777104146395098, 0.3834854347001323, 0.37351655306331083, 0.37643494215958256, 0.38091917734081837, 0.3816218973667014, 0.38416977602915436, 0.3851156278796421, 0.3824646536297366, 0.37491797317988146, 0.3818769982979432, 0.37861419655382633, 0.3842346754938583, 0.37655236199498177, 0.37527537754024654, 0.37987153332077844, 0.3830455071018899, 0.3794875234803733, 0.38347619624041457, 0.39262797689868834, 0.37537328259763764, 0.3836479137067263, 0.3833252173236699, 0.37210422399563386, 0.387080483238998, 0.3719306717816211, 0.37743640282446994, 0.3764378684846794, 0.37271541687568616, 0.37936363588361177, 0.3804472682075392, 0.371879458409168, 0.38374935142586336, 0.3781576618488294, 0.3732976428844838, 0.3825277503210065, 0.37921793430167083, 0.38190348801550034, 0.3811836604795912, 0.3839620574760963, 0.3847266497711341, 0.3755092674263698, 0.38182204500680755, 0.3782161524446279, 0.383978412504874, 0.3785792852560168, 0.37529283202728075, 0.38586872027200814, 0.37986113718144743, 0.3781509461410928, 0.385330743640296, 0.3866525995362477, 0.3780062812672672, 0.38378071400574315, 0.38093069549102115, 0.38920295537065935, 0.38671026251041424, 0.3797818386251582, 0.3798417299438049, 0.3740828680422376, 0.38145504804218516, 0.38200287843196123, 0.3875016655645096, 0.38280217152308016, 0.38698648213974984, 0.3808485120260978, 0.38649215569317924, 0.3831135995238654, 0.3805198727321683, 0.38389426964682105, 0.38486584148132336, 0.3807648518761876, 0.3849117392856701, 0.38554816227406263, 0.38357163151251333, 0.3802410184734446, 0.3815109624947403, 0.382021243195506, 0.3853015545938237, 0.377494075719048, 0.38711871966427447, 0.3774435311857173, 0.3905071147407095, 0.3806642469603057, 0.38146250298721535, 0.3838621310104488, 0.39097126777849944, 0.38649382303450625, 0.3796885114930132, 0.38229777593183895, 0.3814927838633166, 0.37877432031410874, 0.37605211639082897, 0.37402715621625676, 0.3860787514177169, 0.3734693876285033, 0.3757060868665576, 0.3836308258823028, 0.3818031575019453, 0.3754675344144012, 0.3817046451035376, 0.3788763102984019, 0.38524902579100695, 0.3789480691410455, 0.383543092720941, 0.39350814419780294, 0.3787268260402568, 0.3791520138837251, 0.3838299135951435, 0.3793616016559741, 0.3767985271874304, 0.3790097143866268, 0.37912912847583785, 0.38894346707007466, 0.3742955804382469, 0.38342078903909116, 0.3848988146238102, 0.37459168269061577, 0.38242888645561157, 0.3820874952601598, 0.38645922241951614, 0.37736037103276626, 0.37371054134450343, 0.3787742013148233, 0.3864014833873394, 0.38398269388605566, 0.3823276355716528, 0.38692381239368345, 0.38753888685731036, 0.37855833295878827, 0.37346514290673477, 0.37298866356814314, 0.3816173101830132, 0.3767242920311058, 0.3841047031056209, 0.38750297252965327, 0.369205586767445, 0.3753658247490724, 0.38275528518373475, 0.3800859667141648, 0.3870932775020015, 0.37551862878414494, 0.3840912418421723, 0.3726249620291021, 0.37941730108486454, 0.38384662088298915, 0.37976392629720707, 0.37775822057772207, 0.38208118103006306, 0.3800909294399853, 0.37907081163104844, 0.3812881442672555, 0.3761177863743083, 0.38076628101350485, 0.37963548230518607, 0.3693312196718419, 0.37269937157557875, 0.3810739266987452, 0.37098004634254705, 0.38030854867332997, 0.3809964715083148, 0.37681868622152537, 0.3750186215574835, 0.3816620926098788, 0.3736031973351012, 0.38245252669588026, 0.3819549302798787, 0.38419706475756626, 0.3735217913966991, 0.3787470100191878, 0.37715579814040195, 0.3867923977976555, 0.3815506379960068, 0.3814866198135503, 0.3853403181933305, 0.3877864530602214]\n",
        "test_acc_list_step = [75.81054087277197, 86.9660417947142, 87.58451137062077, 89.28242163491088, 90.1160110633067, 91.05331899200984, 91.73325138291334, 92.37092808850646, 92.390135218193, 92.24031960663798, 92.37092808850646, 92.9778733866011, 93.1737861094038, 93.04701905347265, 93.41963736939152, 93.37738168408113, 93.3159188690842, 93.0278119237861, 93.7077443146896, 93.72695144437616, 93.68853718500307, 93.78073140749846, 93.73847572218807, 93.26213890596189, 93.30055316533497, 93.93438844499079, 94.06883835279656, 94.11877688998156, 93.6040258143823, 94.0419483712354, 94.87553779963122, 94.83712354025815, 94.81023355869699, 94.87169637369392, 94.91779348494161, 94.89474492931777, 94.86785494775661, 94.84096496619546, 94.71419791026429, 94.56054087277197, 94.75645359557468, 94.73724646588813, 94.64889366933005, 94.64889366933005, 94.69499078057775, 94.72956361401353, 94.6757836508912, 94.74492931776275, 94.6220036877689, 94.65273509526736, 94.54901659496005, 94.53749231714812, 94.63352796558083, 94.61816226183159, 94.61432083589429, 94.5221266133989, 94.64121081745544, 94.56054087277197, 94.74492931776275, 94.61047940995698, 94.59127228027043, 94.75645359557468, 94.75645359557468, 94.66425937307929, 94.73340503995082, 94.6757836508912, 94.53365089121081, 94.64505224339274, 94.66041794714198, 94.70267363245236, 94.6258451137062, 94.75645359557468, 94.80255070682237, 94.67962507682851, 94.58358942839583, 94.54517516902274, 94.6258451137062, 94.7180393362016, 94.66425937307929, 94.6681007990166, 94.61432083589429, 94.78718500307313, 94.76029502151198, 94.68730792870313, 94.64121081745544, 94.62968653964352, 94.69114935464044, 94.58743085433314, 94.58358942839583, 94.64889366933005, 94.69114935464044, 94.70651505838967, 94.70267363245236, 94.66041794714198, 94.69883220651506, 94.61432083589429, 94.71419791026429, 94.82559926244622, 94.69883220651506, 94.72572218807622, 94.49523663183774, 94.75261216963737, 94.61047940995698, 94.64505224339274, 94.64889366933005, 94.66041794714198, 94.70267363245236, 94.55669944683467, 94.65657652120467, 94.6681007990166, 94.77566072526122, 94.85633066994468, 94.66041794714198, 94.78334357713584, 94.79102642901044, 94.63352796558083, 94.78718500307313, 94.64505224339274, 94.56438229870928, 94.73340503995082, 94.61047940995698, 94.68730792870313, 94.70267363245236, 94.61432083589429, 94.67194222495391, 94.77181929932391, 94.73724646588813, 94.64505224339274, 94.69499078057775, 94.65273509526736, 94.69883220651506, 94.6757836508912, 94.71035648432698, 94.69883220651506, 94.67194222495391, 94.66425937307929, 94.6757836508912, 94.76029502151198, 94.75645359557468, 94.70267363245236, 94.64505224339274, 94.74108789182544, 94.54133374308543, 94.54901659496005, 94.74877074370006, 94.67962507682851, 94.67194222495391, 94.67194222495391, 94.69883220651506, 94.8140749846343, 94.64121081745544, 94.7180393362016, 94.74492931776275, 94.78334357713584, 94.65273509526736, 94.5759065765212, 94.61816226183159, 94.61047940995698, 94.76413644744929, 94.79870928088506, 94.67962507682851, 94.69114935464044, 94.72572218807622, 94.60663798401967, 94.5720651505839, 94.75261216963737, 94.80255070682237, 94.70267363245236, 94.66041794714198, 94.66425937307929, 94.69114935464044, 94.72188076213891, 94.68730792870313, 94.55285802089736, 94.75645359557468, 94.64505224339274, 94.7180393362016, 94.53749231714812, 94.72572218807622, 94.65657652120467, 94.6258451137062, 94.61432083589429, 94.72188076213891, 94.77566072526122, 94.62968653964352, 94.66041794714198, 94.54901659496005, 94.6757836508912, 94.66425937307929, 94.55669944683467, 94.7679778733866, 94.74877074370006, 94.68346650276582, 94.63352796558083, 94.5759065765212, 94.61047940995698, 94.66425937307929, 94.71419791026429, 94.59895513214505, 94.61816226183159, 94.72956361401353, 94.7180393362016, 94.67194222495391, 94.68346650276582, 94.59127228027043, 94.76029502151198, 94.69499078057775, 94.66041794714198, 94.6681007990166, 94.69499078057775, 94.64889366933005, 94.67962507682851, 94.78718500307313, 94.67962507682851, 94.77950215119853, 94.6757836508912, 94.74492931776275, 94.84096496619546, 94.73340503995082, 94.72188076213891, 94.69883220651506, 94.61047940995698, 94.5720651505839, 94.72572218807622, 94.70651505838967, 94.6681007990166, 94.6258451137062, 94.64121081745544, 94.65657652120467, 94.48371235402581, 94.6757836508912, 94.73724646588813, 94.59895513214505, 94.70651505838967, 94.76029502151198, 94.75645359557468, 94.76413644744929, 94.6220036877689, 94.72188076213891, 94.7180393362016, 94.66425937307929, 94.6681007990166, 94.62968653964352, 94.71419791026429, 94.68346650276582, 94.69883220651506, 94.69114935464044, 94.70267363245236, 94.60663798401967, 94.67194222495391, 94.55285802089736, 94.59895513214505, 94.66041794714198, 94.61432083589429, 94.59895513214505, 94.73724646588813, 94.63352796558083, 94.73724646588813, 94.77566072526122, 94.60279655808236, 94.76413644744929, 94.74108789182544, 94.65273509526736, 94.58743085433314, 94.59511370620774, 94.80255070682237, 94.61047940995698, 94.71035648432698, 94.65273509526736, 94.6681007990166, 94.75261216963737, 94.72188076213891, 94.61816226183159, 94.65657652120467, 94.79870928088506, 94.64121081745544, 94.66425937307929, 94.69883220651506, 94.64889366933005, 94.83712354025815, 94.69883220651506, 94.71419791026429, 94.77181929932391, 94.63352796558083, 94.74492931776275, 94.69883220651506, 94.61816226183159, 94.73340503995082, 94.73724646588813, 94.67194222495391, 94.72956361401353, 94.69114935464044, 94.77181929932391, 94.65657652120467, 94.68346650276582, 94.60663798401967, 94.64889366933005, 94.72188076213891, 94.70651505838967, 94.70267363245236]\n",
        "train_loss_list_linear = [1.0505396097010067, 0.41031305952285363, 0.34015994405924144, 0.3038505317194029, 0.27671615290771007, 0.2539030461090044, 0.24293845730422312, 0.2313264879753919, 0.22205986383403867, 0.21279181394635177, 0.20017173882260877, 0.19267895106014196, 0.18516845405505603, 0.18218227440546844, 0.17281363284604012, 0.16757936995264475, 0.1608044776490064, 0.1526946394343363, 0.1503949542921087, 0.14460325269861435, 0.1379334318267338, 0.13074936200347212, 0.12652073063834654, 0.12361395870888137, 0.11955785878043027, 0.11495751238915171, 0.10918548092407586, 0.10549298947964741, 0.10151599690843081, 0.09746526491886312, 0.09452159306524084, 0.09095732697353857, 0.08772212413056352, 0.08348307219462667, 0.08420775942554884, 0.07539896097104885, 0.07707164484154443, 0.07315899130855473, 0.07151561531902047, 0.06873627238071506, 0.06587666986752452, 0.060232834865876696, 0.059981180607343754, 0.06175297403930204, 0.05708458852451951, 0.05296678836066225, 0.057111382709063976, 0.053133565748266894, 0.05266129370491073, 0.050845057070684346, 0.04849550221115351, 0.0469664877891379, 0.04695639163596419, 0.04652371517443136, 0.04273645039004221, 0.042981587787635805, 0.040151388551610795, 0.03779632822411049, 0.04038962408859879, 0.04276739005321198, 0.035970243897294085, 0.0389290425395164, 0.034595519276827996, 0.034754975707721986, 0.03550548981976138, 0.034968214722219, 0.033513369056052934, 0.033747713374943145, 0.03146642715614011, 0.03088276023026253, 0.03096695340504784, 0.030255960882325888, 0.029922214430077934, 0.026458163596317426, 0.030674233368527256, 0.026803990912770092, 0.028383832357679988, 0.029045142340322395, 0.026921882543196714, 0.025758919344589033, 0.027054627095336514, 0.026907914471629563, 0.02701804478293749, 0.026367548554233428, 0.0261913293705708, 0.02126370853306549, 0.023870387598386333, 0.02343241305678558, 0.026290576351723865, 0.024439686884867598, 0.02106365902915024, 0.022621461096346544, 0.022915108798849105, 0.026723837544939667, 0.021479926218290063, 0.022599881692684752, 0.02025709556021947, 0.019150306689010323, 0.0245601841217654, 0.02191897199554167, 0.01751039288937257, 0.019330726730524307, 0.019164910448808477, 0.024538845911642237, 0.015059424295682626, 0.020382445553086728, 0.022040061389306198, 0.018767614002140155, 0.019355695793985044, 0.019107737408797596, 0.017818392037986942, 0.017417942697963446, 0.021768966118750215, 0.017869346996114355, 0.017951215525590914, 0.01640004834187341, 0.01777675135988997, 0.019044938084980482, 0.015788522874557873, 0.022552578193564623, 0.019296770346089745, 0.014811130384146363, 0.015027894850487148, 0.018469126043487265, 0.018098258266041212, 0.014053442152699729, 0.015989337644331865, 0.018125206405116463, 0.016060621967932922, 0.014774189744400855, 0.01385672290374157, 0.018077892058706895, 0.01727216446030504, 0.014268980369684513, 0.015724211079724697, 0.016640492273927766, 0.014277730624145401, 0.016971004403142326, 0.014619164786088283, 0.016830329990812937, 0.01374343925469295, 0.012524507506886185, 0.01549116191725552, 0.013598875393053052, 0.0151209402418879, 0.014543565561276227, 0.01439005304061679, 0.01173475706005888, 0.016134371224896974, 0.011917180406560032, 0.012485065904832735, 0.0135999512963786, 0.01591276152397482, 0.011587380154317354, 0.009772636422980987, 0.01344329923276031, 0.015756404174912624, 0.014378462400149205, 0.014014234783006126, 0.012014912663851542, 0.01611480384725613, 0.012844832076313767, 0.013578993814619104, 0.011672752483787808, 0.014098815763512592, 0.012935566670426325, 0.015032568787632202, 0.012224024389351091, 0.014372231721552152, 0.011108738588597336, 0.011174887407980897, 0.016397042357398143, 0.007919598232576864, 0.011932518933157513, 0.012465056306042874, 0.012968409797498517, 0.012325529049170838, 0.011099239793170491, 0.011867815633636468, 0.012848886264346762, 0.012182228031996032, 0.011562556203482023, 0.010714672763688681, 0.013554639555182596, 0.012572727286869529, 0.01000095399527329, 0.011995266647266443, 0.011573533052920085, 0.012627172824654724, 0.013339461832399402, 0.010677528724291735, 0.011543188395016422, 0.010650319310586942, 0.009674460366150607, 0.010641892690030602, 0.012230279986791725, 0.01145689182891605, 0.01050705676828164, 0.00970218700554869, 0.011088927940358716, 0.010147759896973753, 0.0144252521625949, 0.009280225094541092, 0.0071248399373703905, 0.013807516299508516, 0.01255990872180107, 0.011576532107942895, 0.008876261175843192, 0.009988300831966212, 0.00985363164915767, 0.01084158176923064, 0.011923880645647825, 0.010890684340816107, 0.011154608579179295, 0.010417346872992216, 0.00848533340091224, 0.009280508183635208, 0.012399288785308195, 0.0072954312946646145, 0.007857306191068237, 0.013135696996073112, 0.007985612870672613, 0.009203406998578557, 0.011825267453099425, 0.009498726522558978, 0.010254976841146922, 0.012558721663513307, 0.007996204799592626, 0.007471766532690587, 0.01177365344033834, 0.010632048703338191, 0.01163114502161057, 0.008471069070895856, 0.008329393120841747, 0.010546008776983772, 0.007213423388353836, 0.009344507342992069, 0.01120725078885267, 0.008723668579122892, 0.008438797396795254, 0.009862698597215867, 0.011641287900165775, 0.009182740705942006, 0.00702334287046593, 0.010164535000019891, 0.008744396335386742, 0.009129313499368456, 0.008084669785307412, 0.010208096987843342, 0.011367271971373962, 0.007117051272343865, 0.008831601460626934, 0.00922884266564445, 0.009587360389017516, 0.008383878850809764, 0.007664171105706518, 0.010088089144474219, 0.007183663350923383, 0.010528553138330638, 0.010388069153280813, 0.008873608166863086, 0.007134666277026189, 0.00842848956719368, 0.008079520347828268, 0.009280271344362845, 0.008955260842407897, 0.010437573963957794, 0.007464409461843186, 0.00868264229136562, 0.008687793158964795, 0.008216140737795655, 0.007891892086514802, 0.007512410591053153, 0.008665417990315836, 0.008976994133145053, 0.008162402152780846, 0.008810677293791461, 0.006835815719566462, 0.007850608918254282, 0.010575828147088042, 0.008934358710317107, 0.0076927311314263745, 0.007455576667292384, 0.005771527193367357, 0.008088972039366073, 0.011438270648105747, 0.005829402718917685, 0.009437662637821316, 0.006043440636937449, 0.007854135620251515, 0.009180552848274084, 0.010004485826416422, 0.007954190094466415, 0.0069836680795018575, 0.006469878575636705, 0.008808609063802835, 0.007791922163181544, 0.006387679697369947, 0.009282423594551585, 0.007415674569260415]\n",
        "train_acc_list_linear = [64.05293806246691, 87.14875595553202, 89.55002646903124, 90.75913181577555, 91.62308099523557, 92.33456855479089, 92.87030174695606, 93.13922710428798, 93.25780836421387, 93.82106934886183, 94.23822128110112, 94.36315510852303, 94.60667019587083, 94.74007411328745, 95.01111699311805, 95.09158284806776, 95.25674960296453, 95.67178401270513, 95.61461090524087, 95.74377977766014, 95.9915299100053, 96.22869242985706, 96.29645314981472, 96.45526733721546, 96.42985706723134, 96.65431445209106, 96.86818422445738, 96.87241926945474, 97.08628904182108, 97.16040232927475, 97.2641609317099, 97.3361566966649, 97.35309687665432, 97.51191106405506, 97.51191106405506, 97.76601376389624, 97.7342509264161, 97.88035997882477, 97.82953943885654, 97.96717840127052, 98.00317628374802, 98.16622551614611, 98.13658020116463, 98.11540497617787, 98.1937533086289, 98.37374272101641, 98.19798835362626, 98.44150344097406, 98.33562731604023, 98.41821069348862, 98.43303335097936, 98.4923239809423, 98.5643197458973, 98.4923239809423, 98.66807834833246, 98.64902064584436, 98.69348861831656, 98.76971942826893, 98.68925357331922, 98.64690312334568, 98.82053996823716, 98.74219163578613, 98.81842244573849, 98.86500794070938, 98.88194812069878, 98.78877713075701, 98.90100582318688, 98.90312334568554, 98.96664902064585, 98.97723663313923, 98.98358920063525, 99.0428798305982, 99.01111699311805, 99.12122816304924, 98.98570672313393, 99.10852302805718, 99.09158284806776, 99.05134992059291, 99.1233456855479, 99.1868713605082, 99.0873478030704, 99.10217046056114, 99.08523028057174, 99.13181577554262, 99.11064055055584, 99.33721545791424, 99.22710428798305, 99.17628374801482, 99.0979354155638, 99.20169401799895, 99.2694547379566, 99.2419269454738, 99.25463208046585, 99.08099523557438, 99.26733721545791, 99.2419269454738, 99.30968766543144, 99.3223928004235, 99.1868713605082, 99.25463208046585, 99.40921122286925, 99.36262572789836, 99.3499205929063, 99.16357861302276, 99.52355743779778, 99.31815775542616, 99.2779248279513, 99.37109581789306, 99.3499205929063, 99.37321334039174, 99.36050820539968, 99.42826892535733, 99.22075172048703, 99.41979883536263, 99.3943885653785, 99.46426680783483, 99.39015352038115, 99.33509793541556, 99.47697194282689, 99.28427739544733, 99.36050820539968, 99.5574377977766, 99.5044997353097, 99.41556379036527, 99.38803599788248, 99.5489677077819, 99.49179460031763, 99.37956590788777, 99.47697194282689, 99.5214399152991, 99.55108523028058, 99.42403388035999, 99.45156167284277, 99.51720487030175, 99.47273689782953, 99.42403388035999, 99.5299100052938, 99.41132874536792, 99.48755955532027, 99.46638433033351, 99.53202752779248, 99.57014293276866, 99.49179460031763, 99.53626257278984, 99.50026469031233, 99.50026469031233, 99.5299100052938, 99.61884595023822, 99.45579671784013, 99.59978824775013, 99.55108523028058, 99.55532027527792, 99.47273689782953, 99.62519851773425, 99.67813658020117, 99.57437797776602, 99.45579671784013, 99.5299100052938, 99.50873478030704, 99.61672842773955, 99.46214928533615, 99.59555320275278, 99.57014293276866, 99.6209634727369, 99.53414505029116, 99.58920063525674, 99.5044997353097, 99.57014293276866, 99.53626257278984, 99.58073054526204, 99.6209634727369, 99.46850185283219, 99.72260455267337, 99.58920063525674, 99.55955532027528, 99.56379036527264, 99.60825833774484, 99.64849126521969, 99.60402329274748, 99.5574377977766, 99.60825833774484, 99.61884595023822, 99.63790365272631, 99.56802541026998, 99.58284806776072, 99.66754896770779, 99.58708311275808, 99.62519851773425, 99.57226045526734, 99.59767072525146, 99.64213869772367, 99.60614081524616, 99.63366860772896, 99.66331392271043, 99.65907887771307, 99.58920063525674, 99.61884595023822, 99.67813658020117, 99.66119640021175, 99.62731604023293, 99.68237162519851, 99.57437797776602, 99.66543144520911, 99.80307040762308, 99.5574377977766, 99.5659078877713, 99.63366860772896, 99.69931180518793, 99.66966649020645, 99.67178401270513, 99.63578613022763, 99.59767072525146, 99.63790365272631, 99.66119640021175, 99.64425622022235, 99.72683959767072, 99.68660667019587, 99.61249338274219, 99.7734250926416, 99.7480148226575, 99.56379036527264, 99.74589730015882, 99.67813658020117, 99.62731604023293, 99.69931180518793, 99.65484383271573, 99.59767072525146, 99.7649550026469, 99.73319216516676, 99.61037586024352, 99.65060878771837, 99.62731604023293, 99.72260455267337, 99.73107464266808, 99.66543144520911, 99.76071995764956, 99.71413446267867, 99.62731604023293, 99.74166225516146, 99.71201694017999, 99.67178401270513, 99.58708311275808, 99.69507676019057, 99.73954473266278, 99.68025410269983, 99.71836950767602, 99.70566437268396, 99.73107464266808, 99.65272631021705, 99.61037586024352, 99.79460031762838, 99.71201694017999, 99.68872419269455, 99.70778189518263, 99.7564849126522, 99.7564849126522, 99.66543144520911, 99.76919004764426, 99.64213869772367, 99.66966649020645, 99.71836950767602, 99.76071995764956, 99.73319216516676, 99.75224986765484, 99.6929592376919, 99.67813658020117, 99.65907887771307, 99.72472207517205, 99.68872419269455, 99.71413446267867, 99.72472207517205, 99.76071995764956, 99.7924827951297, 99.69719428268925, 99.68448914769719, 99.73107464266808, 99.73742721016411, 99.76283748014822, 99.75224986765484, 99.66543144520911, 99.70354685018528, 99.7564849126522, 99.76283748014822, 99.79460031762838, 99.72260455267337, 99.64002117522499, 99.79036527263102, 99.69084171519323, 99.8009528851244, 99.75860243515088, 99.73107464266808, 99.6569613552144, 99.75013234515616, 99.77977766013764, 99.79883536262572, 99.70142932768661, 99.69507676019057, 99.7734250926416, 99.70989941768131, 99.7480148226575]\n",
        "test_loss_list_linear = [0.6001535044873462, 0.4386981564993952, 0.32313791202271686, 0.3436826935001448, 0.3047397654576629, 0.28500792238057826, 0.26378655251042515, 0.2683426228297107, 0.27285771396960695, 0.2532431707516605, 0.25363739693135606, 0.25782927618745494, 0.2381599084200228, 0.24098271157081222, 0.23853215436432876, 0.22876223501767598, 0.2459365823762674, 0.24074384005849853, 0.23645990466078123, 0.23522190126937395, 0.23930488064812094, 0.23977934446770185, 0.22856533881642072, 0.2503986795633739, 0.2523015177096514, 0.2286284007497278, 0.2522817155821066, 0.23435633048853455, 0.25106761294106644, 0.23668714861075082, 0.25767872133748787, 0.2464016547937896, 0.24990028208669493, 0.25964026233437015, 0.2509775684014255, 0.25643125852095144, 0.25149472640352505, 0.2636343431750349, 0.2690670864802657, 0.2728121512952973, 0.2877597091719508, 0.2628066184400928, 0.289077399882908, 0.2637001336643509, 0.2718517802947876, 0.30822780637034014, 0.2853905560412243, 0.3029168626914422, 0.2886001042948634, 0.30125397385335434, 0.2918206257928236, 0.31290997323744435, 0.3016124471894228, 0.3364011060409978, 0.30226883032888757, 0.30097986143264993, 0.3160975992350894, 0.324009220079318, 0.2896068644545534, 0.28848243194321793, 0.30809056048518885, 0.31027250127026845, 0.31317287329219134, 0.3204711799728958, 0.323861251740406, 0.30745547297684583, 0.313345615642474, 0.31225476020435783, 0.32664194748755176, 0.3649349614393477, 0.3289772248781268, 0.34484266127715363, 0.34519970382326376, 0.3562348909566508, 0.34551431653181125, 0.3507686852495752, 0.3557186407749267, 0.34874562685396154, 0.3626312471414898, 0.32565479080977977, 0.3328241787111277, 0.3502718099739914, 0.35965223179436195, 0.33991847547026827, 0.32748869253212914, 0.3568636128751963, 0.366436490642966, 0.36107295554350405, 0.34370773008056715, 0.343383331070928, 0.3387923375794701, 0.33830804925631075, 0.3589262347485797, 0.3675877243079537, 0.3818419510498643, 0.35536084654649686, 0.35281344309595286, 0.3716038850629154, 0.35846942082485733, 0.34315043032242387, 0.36869288876871853, 0.3778895241226636, 0.38406652862242624, 0.352233542257226, 0.3620971898712656, 0.37449508531055614, 0.37493528568131085, 0.36601132105159406, 0.36861560567665624, 0.3717369966793294, 0.369726698087784, 0.3781300349087984, 0.35664984070714195, 0.3536169459751132, 0.406203392372631, 0.3737115380445532, 0.36973171939562055, 0.37459768973948326, 0.40706704461983606, 0.3881958369896108, 0.37344259142364356, 0.3670044747710812, 0.3696818688847855, 0.37015472998952165, 0.38189552233134416, 0.3749138325744984, 0.3891030573742647, 0.3678013856029686, 0.36681280497863306, 0.3926687809620418, 0.4037954283184281, 0.4014408321065061, 0.3803745116673264, 0.38791271775741787, 0.37617705127808687, 0.3729255223537193, 0.36608954668775495, 0.3800407153572522, 0.37864029130843635, 0.3903196998415332, 0.39593674976597815, 0.4233312764953749, 0.41837985076092404, 0.4037898524353902, 0.39368393412772934, 0.40711697678574743, 0.3831657852119237, 0.39936033349630295, 0.4122653530234946, 0.4055917443908459, 0.391819414573119, 0.3998463283493823, 0.3856644867325896, 0.38932094629853964, 0.45811930652159977, 0.4117749379582557, 0.3846923142269838, 0.38948512334814844, 0.3886060610632686, 0.4141315968488069, 0.39312611405244646, 0.40893315564037536, 0.3917342342217179, 0.4033093715232669, 0.40686407040658534, 0.402124198153615, 0.3760559221218322, 0.41189432279298116, 0.3945868524777539, 0.4064665962668026, 0.4281328501380688, 0.3944055610610282, 0.4145982712644207, 0.39619766517231864, 0.4008463170269833, 0.4100334781368135, 0.3959438812681565, 0.4048078361277779, 0.3898382809506181, 0.3940078625638111, 0.41857934771699135, 0.40016016322553305, 0.42573420590191496, 0.4076968614815497, 0.40349324788971275, 0.41680151768320917, 0.4404344489323158, 0.4126104597257925, 0.41489401694464806, 0.39322311091510687, 0.39995222047482637, 0.4001541677862406, 0.40168759471955984, 0.4003755842317261, 0.4116448443930815, 0.3964004161512004, 0.40526396525092423, 0.40040231844885094, 0.436148803023731, 0.42100324081804824, 0.42921065237811384, 0.4016612854548821, 0.4237121128860642, 0.439825722326835, 0.40309608828586835, 0.4022408921143734, 0.40526854241376414, 0.4139130270638156, 0.42377193091327653, 0.4159748113506437, 0.43089511207140546, 0.41587188575124623, 0.42511460325662415, 0.42877695572507735, 0.4061464395730154, 0.4108063448129185, 0.4237177971266575, 0.4147245909142144, 0.4100258317867331, 0.44727529514599224, 0.40763759292552576, 0.4096112189969669, 0.4398777195818576, 0.4397024883450392, 0.42169872507014694, 0.4525160300362782, 0.4039532243748944, 0.42747145336048276, 0.41558541071500776, 0.4243580105614063, 0.4347197997529863, 0.40417124533697085, 0.4213481634563091, 0.42768024219492196, 0.41689553120922224, 0.42403014976640835, 0.43499788091353636, 0.40865986559576556, 0.39625785907949596, 0.41788156942793114, 0.42710175029203, 0.39860118992453186, 0.40182735008534554, 0.43774937425612237, 0.4086107819651564, 0.42035614554861594, 0.42495340735231546, 0.42730405055943477, 0.41829457241749646, 0.4210991954211803, 0.39986408456210415, 0.43789072635163573, 0.4252349973893633, 0.4179426681058591, 0.4189265777229094, 0.42167082347327334, 0.4152061831133038, 0.4278368429582128, 0.42154261065354826, 0.39703031598279875, 0.42471260858663157, 0.4301079974747172, 0.4101321147526523, 0.4378960633869557, 0.412081252308745, 0.4338206772089881, 0.4061669551530013, 0.4237026169088067, 0.4324107744557527, 0.4283796655671561, 0.4480068746077664, 0.42207027151815446, 0.4178310304186216, 0.42660820346289113, 0.423226847276822, 0.44835605301127274, 0.4472741484733335, 0.4282900923771747, 0.4323618656535651, 0.4345184702519784, 0.4196194001455225, 0.4023636024691822, 0.4263699218977754, 0.45100487594940136, 0.4451090616046214, 0.4251364004779972, 0.4428678105822673, 0.42934233425459, 0.4463761615374025, 0.42917647955519167, 0.43650849757935195, 0.40754444616865, 0.4072841090405835, 0.43068673907249583, 0.42376545924857695, 0.4177722467438263, 0.41679786920105794, 0.4319006550381435, 0.42252690807057947, 0.4652784176572573]\n",
        "test_acc_list_linear = [81.16548862937923, 86.56653349723418, 90.2581438229871, 89.47065150583897, 90.86893054701905, 91.77934849416103, 92.38629379225569, 92.26336816226183, 92.23263675476336, 92.60525507068223, 92.71665642286416, 92.73586355255071, 93.29287031346036, 93.27366318377382, 93.4081130915796, 93.58481868469576, 93.13921327596803, 93.3620159803319, 93.6078672403196, 93.46957590657652, 93.5540872771973, 93.73847572218807, 93.77688998156115, 93.21220036877689, 93.30055316533497, 94.06883835279656, 93.46957590657652, 94.04963122311001, 93.73847572218807, 93.8959741856177, 93.44652735095268, 93.91518131530424, 93.73847572218807, 93.6578057775046, 93.9881684081131, 93.98432698217579, 93.95743700061463, 93.98432698217579, 93.8921327596804, 93.92670559311617, 93.68469575906576, 94.3338967424708, 93.6040258143823, 94.10341118623234, 94.14950829748003, 93.79609711124769, 94.17255685310387, 93.64244007375538, 93.9958512599877, 93.56945298094652, 94.00353411186232, 93.67317148125385, 93.96127842655194, 93.50799016594961, 93.99969268592501, 94.3300553165335, 93.97664413030117, 93.78457283343577, 94.16871542716656, 94.10341118623234, 94.16103257529196, 94.16487400122925, 94.05731407498463, 94.11877688998156, 94.12645974185618, 94.16487400122925, 94.17639827904118, 94.17639827904118, 94.18792255685311, 94.02658266748617, 94.1379840196681, 93.90365703749232, 93.80762138905962, 93.93054701905348, 94.00353411186232, 93.98432698217579, 93.97280270436386, 94.06499692685925, 93.99969268592501, 94.25706822372464, 94.41840811309157, 94.01505838967425, 93.78457283343577, 94.14566687154272, 94.25706822372464, 94.04963122311001, 93.8959741856177, 93.97280270436386, 94.13030116779349, 94.41072526121697, 94.20712968653964, 94.31084818684695, 93.77304855562384, 93.85755992624462, 93.78073140749846, 94.21865396435157, 94.3262138905962, 93.96896127842655, 94.20328826060233, 94.43377381684081, 94.19944683466503, 94.17639827904118, 93.82298709280884, 94.3761524277812, 94.31084818684695, 94.13030116779349, 94.18023970497849, 94.27627535341118, 94.14950829748003, 94.30700676090964, 94.41456668715428, 94.35310387215735, 94.10341118623234, 94.3262138905962, 93.75, 94.23786109403811, 94.28779963122311, 93.94975414874001, 93.8921327596804, 93.83066994468346, 94.08420405654579, 94.37231100184388, 94.17255685310387, 94.44913952059004, 94.14950829748003, 94.3761524277812, 94.22249539028887, 94.21865396435157, 94.53365089121081, 94.14566687154272, 94.16487400122925, 93.91133988936693, 93.98432698217579, 94.2839582052858, 94.19944683466503, 94.21481253841426, 94.3338967424708, 93.98432698217579, 94.18792255685311, 94.03042409342348, 94.27627535341118, 93.83066994468346, 94.12261831591887, 94.02274124154886, 94.13030116779349, 94.0880454824831, 94.52980946527352, 94.0880454824831, 94.02658266748617, 94.3262138905962, 94.31084818684695, 94.42609096496619, 94.39535955746773, 94.42224953902888, 93.99969268592501, 94.16871542716656, 94.35310387215735, 94.23017824216349, 94.26475107559926, 94.39535955746773, 94.49139520590043, 94.26090964966195, 94.51444376152428, 94.40304240934235, 94.15719114935465, 94.16103257529196, 94.41840811309157, 94.17639827904118, 94.31853103872157, 94.31853103872157, 94.00737553779963, 94.3338967424708, 94.34926244622004, 94.3338967424708, 94.24554394591273, 94.51060233558697, 94.39151813153042, 94.27243392747388, 94.43377381684081, 94.3262138905962, 94.23017824216349, 94.24170251997542, 93.86908420405655, 94.13030116779349, 94.29164105716042, 94.35694529809466, 94.19560540872772, 94.3799938537185, 94.3338967424708, 94.25322679778733, 94.2839582052858, 94.2340196681008, 94.26859250153657, 94.69114935464044, 94.16487400122925, 94.46834665027659, 94.27243392747388, 94.36846957590657, 94.10725261216963, 94.43377381684081, 94.23786109403811, 94.46450522433928, 94.32237246465888, 94.28779963122311, 94.11493546404425, 94.16871542716656, 94.4299323909035, 94.30700676090964, 94.19560540872772, 94.46066379840197, 94.17639827904118, 94.53365089121081, 94.16871542716656, 94.29164105716042, 94.29548248309773, 94.49523663183774, 94.25706822372464, 94.10341118623234, 94.64889366933005, 94.30316533497235, 94.42609096496619, 94.44529809465274, 94.19944683466503, 94.1917639827904, 94.44529809465274, 94.16103257529196, 94.3338967424708, 94.54901659496005, 94.4721880762139, 94.31084818684695, 94.39920098340504, 94.49139520590043, 94.3761524277812, 94.45298094652735, 94.31468961278426, 94.45682237246466, 94.30316533497235, 94.55285802089736, 94.4299323909035, 94.40688383527966, 94.34542102028273, 94.4299323909035, 94.49139520590043, 94.27243392747388, 94.47602950215119, 94.26090964966195, 94.24554394591273, 94.54517516902274, 94.3300553165335, 94.29548248309773, 94.61432083589429, 94.11877688998156, 94.40304240934235, 94.27243392747388, 94.59511370620774, 94.2839582052858, 94.39151813153042, 94.39535955746773, 94.21865396435157, 94.49523663183774, 94.24554394591273, 94.51060233558697, 94.59127228027043, 94.43377381684081, 94.31853103872157, 94.46834665027659, 94.43761524277812, 94.43761524277812, 94.37231100184388, 94.34157959434542, 94.00353411186232, 94.38383527965581, 94.58358942839583, 94.5759065765212, 94.41840811309157, 94.07652120467118, 94.09956976029503, 94.3799938537185, 94.30700676090964, 94.29932390903504, 94.38383527965581, 94.5259680393362, 94.26859250153657, 94.35310387215735, 94.36462814996926, 94.37231100184388, 94.38767670559312, 94.35694529809466, 94.36846957590657, 94.34542102028273, 94.40304240934235, 94.28779963122311, 94.46834665027659, 94.56054087277197, 94.62968653964352, 94.35310387215735, 94.43761524277812, 94.39535955746773, 94.26475107559926, 93.79609711124769]\n",
        "train_loss_list_exp = [1.5108453227575556, 0.45569133027620756, 0.38955149030297753, 0.38201055714108434, 0.3791554563736851, 0.37846633355791975, 0.37789585104156637, 0.37833547551780533, 0.37885424543202406, 0.3798585463830126, 0.3801606303146538, 0.3785690202864851, 0.380058984244419, 0.38191306041831247, 0.3755665107309657, 0.37845069464790787, 0.3810173268240642, 0.37906573632060675, 0.38025176412044825, 0.37710853672124506, 0.37910957913088605, 0.3795646985129612, 0.37768419939004955, 0.37915716253645054, 0.3763544963304266, 0.37615088787343764, 0.3767496614356028, 0.3780435439209305, 0.37970296291477956, 0.3768931365190806, 0.3782231926433439, 0.37863624968179843, 0.3801447201146666, 0.38008787327504095, 0.3784985868184547, 0.3775513489152681, 0.3797982176387213, 0.378436125835106, 0.37775709227656284, 0.3784049479618951, 0.3812676381047179, 0.377607957215167, 0.37778531927564923, 0.3785970114110931, 0.3763440459320539, 0.37659880587563604, 0.38260392961786366, 0.3788121801404772, 0.3782783241937477, 0.37851579224837184, 0.37789375260270386, 0.37802613358995135, 0.3780238080800064, 0.38144587775879113, 0.3779912972595634, 0.3791698661600025, 0.3766854087996289, 0.37804330437163997, 0.3786587099718854, 0.37654282059772876, 0.3787634488609102, 0.3799068733524824, 0.37713607115958764, 0.37751451530430696, 0.3801226066299247, 0.37911422368956776, 0.3805197463610631, 0.37700172774190827, 0.3788724349847008, 0.3784166157326401, 0.38057625192775313, 0.37780569122251134, 0.38091700400924944, 0.3794297932528545, 0.3814112742338077, 0.37960589101644066, 0.37835461256626823, 0.37888884483798735, 0.3795582245439695, 0.37849634607148364, 0.37682519744082194, 0.37698636250444223, 0.3814956169464401, 0.3775139360570003, 0.379286357057773, 0.37696852730864755, 0.3763741816123973, 0.37921798265561824, 0.3787178158921601, 0.3787029756682352, 0.37783306570557074, 0.3772085670452454, 0.377733004771597, 0.3765660253401371, 0.3802021978912638, 0.37975252381346736, 0.3792028047528047, 0.3785102287039847, 0.3774594428739574, 0.3806928960773034, 0.3792232088441771, 0.37827566011649805, 0.38008303063994825, 0.3778988100325835, 0.3803016372974003, 0.3791544923614357, 0.38108806501882186, 0.37818956795100594, 0.37843327103106955, 0.38068289166382013, 0.38067130910025704, 0.37755737811084683, 0.37938234137325755, 0.3798758971497295, 0.37914535011540906, 0.37674440685811084, 0.37839110369281714, 0.3803737539505248, 0.3769939240966709, 0.37618244623104086, 0.3802966764625818, 0.376914295521855, 0.38093100847590583, 0.3785027291797364, 0.38015424458153524, 0.3799492670753138, 0.3770360491140102, 0.3781222916311688, 0.37643939103214397, 0.37902817707559283, 0.37899113222350916, 0.3788869072428241, 0.37866583002130516, 0.38024979207896925, 0.37998249028433306, 0.37665241209633626, 0.3791340853221371, 0.3768656629776244, 0.3781943121738227, 0.37956079872966136, 0.3790555945660687, 0.3767165967280949, 0.3785593254860178, 0.3785436793071468, 0.3786095209157241, 0.37977585394369556, 0.37895426606421223, 0.37726105127715803, 0.3811360982456181, 0.3777372095239195, 0.37985739951857384, 0.3775850888190231, 0.37942486108964696, 0.3774096647575296, 0.37866538746893247, 0.37956950698441605, 0.37765448986676325, 0.3765703885535884, 0.37493878506063444, 0.3787174464322041, 0.3790939331539278, 0.3798395115024029, 0.3802033216972661, 0.3778447469237051, 0.37922620373528176, 0.3788863822696655, 0.3776389633010073, 0.37965354569720705, 0.37847416466329153, 0.38210083584636856, 0.3769269231858292, 0.376283608316406, 0.3791458204105941, 0.3800827085891067, 0.3800524467940576, 0.379729083156198, 0.3786869500145357, 0.37784998191566, 0.37798632447150987, 0.37959555187199495, 0.3772686913326827, 0.37737370587299834, 0.3774894487647829, 0.3773925192149351, 0.37866038096144916, 0.3799699583673865, 0.37930802181161194, 0.37929151229419034, 0.37872534558217374, 0.38051237880699035, 0.3785012537224829, 0.37907243408969427, 0.3786347356188265, 0.3772817448306536, 0.37789186423386983, 0.377803510807071, 0.37891541296227516, 0.3768928705596019, 0.3764786839808229, 0.37735731827049723, 0.37620092403436417, 0.37594923223583354, 0.37946950848186567, 0.37934725721515616, 0.3799315985383057, 0.37742675118006985, 0.3800657158137014, 0.38043200723362486, 0.3790072623468673, 0.37812678907621844, 0.380248889528962, 0.3795400029679301, 0.37716066687895355, 0.37926767851279036, 0.3811982564002195, 0.3785968935344277, 0.37829239452434427, 0.3791591095003655, 0.37750050801087204, 0.3755794913788152, 0.37707439347657407, 0.3798365030023787, 0.37901620768757693, 0.3787338199815776, 0.3768027005237616, 0.378330920527621, 0.3773879486774688, 0.37904529988281127, 0.3793277707364824, 0.37877471587522243, 0.3773436505135482, 0.38000925620235404, 0.37747299065434836, 0.3783611831707037, 0.38008849536823386, 0.3802644786069064, 0.37820831300604957, 0.37898807813158525, 0.37832132050500006, 0.37651883493755567, 0.3780955279745707, 0.3794448867157546, 0.37681427719147226, 0.37846876878725483, 0.37892088152690306, 0.3757547806433546, 0.3790287462272618, 0.3771775669764051, 0.37960463952081314, 0.38023487892415786, 0.3773157599818739, 0.3790636116734688, 0.37847773605568946, 0.3790109474969104, 0.3778621018094422, 0.3795850692483468, 0.38052020777208695, 0.3777503734681664, 0.38023589062819957, 0.37878056504539037, 0.3778544519813403, 0.3769152445117956, 0.37817029428837423, 0.3794189135879682, 0.3814815942268708, 0.3767595124357761, 0.376037456559618, 0.3787438214067521, 0.37781988387185383, 0.3780568046621514, 0.3789186481295562, 0.3792363849031893, 0.3795603767723895, 0.38027571985715125, 0.37710057130350977, 0.37790428061633896, 0.3758348726887044, 0.37957349484205893, 0.3801340398184329, 0.37570041014250055, 0.37696901714898706, 0.3780564808748602, 0.37865450675409984, 0.37989892315896867, 0.37623435913062675, 0.3776249932402841, 0.3777484252120098, 0.37681428954853274, 0.3777670302203677, 0.37880285431537525, 0.3775242522641572, 0.37820892516513505, 0.37701137581976446, 0.378751233704691, 0.37953954490865793, 0.3773617081364319, 0.3809620241324107, 0.37960273697770386, 0.3808248356428896, 0.37698043551709914]\n",
        "train_acc_list_exp = [47.21651667548968, 85.66437268395977, 87.928004235045, 88.11858125992589, 88.25410269984118, 88.07623080995235, 88.23928004235044, 88.18422445738486, 88.14187400741133, 88.09740603493913, 88.06564319745897, 88.14399152991001, 88.0084700899947, 87.9915299100053, 88.28163049232398, 88.06776071995765, 88.09105346744309, 88.11434621492853, 87.96188459502382, 88.11222869242985, 88.12916887241927, 88.13975648491265, 88.2435150873478, 88.16728427739545, 88.09105346744309, 88.20116463737428, 88.20328215987296, 88.01482265749074, 88.0084700899947, 88.20116463737428, 88.15034409740603, 88.22445738485972, 88.04870301746956, 88.06776071995765, 88.28586553732133, 88.19269454737956, 88.11011116993119, 88.18422445738486, 88.20539968237162, 88.08046585494971, 88.071995764955, 88.23292747485442, 88.23292747485442, 88.15881418740074, 88.26680783483324, 88.32398094229751, 88.06140815246162, 88.18210693488618, 88.17363684489148, 88.15881418740074, 88.13975648491265, 88.15034409740603, 88.12281630492323, 88.01694017998942, 88.19481206987824, 88.18634197988354, 88.3070407623081, 88.16093170989942, 88.16516675489677, 88.19481206987824, 88.25622022233986, 88.12493382742191, 88.1355214399153, 88.16093170989942, 88.09105346744309, 88.24563260984648, 88.11858125992589, 88.10799364743251, 88.09317098994177, 88.09528851244045, 88.0359978824775, 88.17787188988883, 88.10375860243515, 88.1355214399153, 88.08470089994707, 88.09105346744309, 88.23928004235044, 88.12916887241927, 88.10375860243515, 88.26045526733722, 88.24775013234516, 88.16093170989942, 88.05717310746427, 88.11858125992589, 88.15034409740603, 88.24563260984648, 88.14822657490735, 88.05293806246691, 88.1630492323981, 88.12281630492323, 88.32821598729487, 88.21386977236634, 88.23928004235044, 88.26892535733192, 88.23928004235044, 87.91529910005293, 88.05293806246691, 88.16093170989942, 88.26892535733192, 88.10587612493383, 88.13128639491795, 88.28374801482266, 88.21810481736368, 88.20328215987296, 88.18422445738486, 88.09740603493913, 88.00635256749602, 88.08681842244575, 88.10375860243515, 88.09528851244045, 88.06140815246162, 88.16093170989942, 88.10799364743251, 88.12493382742191, 88.08681842244575, 88.29433562731604, 88.08258337744839, 88.0084700899947, 88.17363684489148, 88.27527792482795, 88.10587612493383, 88.12069878242457, 87.98941238750662, 88.22022233986236, 88.12916887241927, 88.12281630492323, 88.14399152991001, 88.16516675489677, 88.14399152991001, 88.09528851244045, 88.18845950238222, 88.19692959237692, 88.15246161990471, 88.06564319745897, 88.12069878242457, 88.25833774483854, 88.15246161990471, 88.25622022233986, 88.11646373742721, 88.10375860243515, 88.11434621492853, 88.18422445738486, 88.14610905240868, 88.27316040232928, 88.15881418740074, 88.16728427739545, 88.13763896241397, 88.08681842244575, 88.03811540497618, 88.15457914240339, 88.15034409740603, 88.28586553732133, 88.13975648491265, 88.29010058231869, 88.25622022233986, 88.21810481736368, 88.22445738485972, 88.27316040232928, 88.2710428798306, 88.11222869242985, 88.02541026998412, 88.02541026998412, 88.08258337744839, 88.14610905240868, 88.09740603493913, 88.18634197988354, 88.18634197988354, 88.17998941238751, 88.11646373742721, 88.06140815246162, 88.05293806246691, 88.2710428798306, 88.071995764955, 88.22233986236104, 88.09317098994177, 88.12493382742191, 88.18634197988354, 88.11434621492853, 88.21810481736368, 88.00211752249868, 88.11858125992589, 88.27527792482795, 88.30280571731075, 88.24563260984648, 88.15246161990471, 88.12493382742191, 88.11434621492853, 88.02117522498676, 88.24563260984648, 88.06987824245633, 88.23716251985178, 88.0635256749603, 88.14399152991001, 88.22445738485972, 88.19269454737956, 88.26045526733722, 88.1630492323981, 88.1990471148756, 88.14399152991001, 88.27527792482795, 88.24563260984648, 88.32609846479619, 88.02117522498676, 88.15669666490207, 88.071995764955, 88.16728427739545, 88.15034409740603, 88.12281630492323, 88.15881418740074, 88.17787188988883, 88.06987824245633, 88.08470089994707, 88.11646373742721, 88.13763896241397, 88.1355214399153, 88.18845950238222, 88.18845950238222, 88.14399152991001, 88.25410269984118, 88.22445738485972, 88.17998941238751, 88.07623080995235, 88.14187400741133, 88.09952355743779, 88.31127580730545, 88.19481206987824, 88.2265749073584, 88.10164107993647, 88.12705134992059, 88.1905770248809, 88.11434621492853, 88.13128639491795, 88.22233986236104, 88.17998941238751, 88.13128639491795, 88.12705134992059, 88.24563260984648, 88.16940179989412, 88.06987824245633, 88.24986765484384, 88.12493382742191, 88.15457914240339, 88.22869242985706, 88.06987824245633, 88.08470089994707, 88.23928004235044, 88.07411328745368, 88.215987294865, 88.15881418740074, 88.14822657490735, 88.12069878242457, 88.10799364743251, 88.27527792482795, 88.00423504499736, 88.17363684489148, 88.18422445738486, 88.1355214399153, 88.18634197988354, 88.08893594494441, 88.11011116993119, 88.09105346744309, 88.2265749073584, 88.20539968237162, 88.13763896241397, 88.11646373742721, 88.29221810481737, 88.20116463737428, 88.18845950238222, 88.14399152991001, 88.0359978824775, 88.15246161990471, 88.14399152991001, 88.07834833245103, 88.30068819481207, 88.21386977236634, 88.1715193223928, 88.34515616728427, 87.99788247750132, 87.91106405505559, 88.15034409740603, 88.15669666490207, 88.1630492323981, 88.13975648491265, 88.08470089994707, 88.3705664372684, 88.22869242985706, 88.1355214399153, 88.17787188988883, 88.14610905240868, 88.18210693488618, 88.27527792482795, 88.17363684489148, 88.15034409740603, 88.26469031233457, 88.14187400741133, 88.26045526733722, 88.08681842244575, 88.09952355743779, 88.16728427739545, 88.12916887241927]\n",
        "test_loss_list_exp = [0.7599668520338395, 0.41391250622623105, 0.392150557216476, 0.3908533724207504, 0.3910430484980929, 0.3915113131059151, 0.3893689420439449, 0.39278720669886646, 0.3913472319642703, 0.3898209324654411, 0.3919214537622882, 0.3930370972729197, 0.3875842901567618, 0.3887199363579937, 0.3931986349178295, 0.3915082841527228, 0.39034265929869577, 0.38962281656031517, 0.39410267273585003, 0.3923792628680958, 0.3913208600498882, 0.3940731910806076, 0.3901035269978, 0.3910912872091228, 0.38958175787154364, 0.3905664257266942, 0.3923723243323027, 0.3930160653795682, 0.3910279740013328, 0.39261947053612445, 0.39096589437594603, 0.391885179076709, 0.3932027472730945, 0.39465218753206965, 0.38952960351518556, 0.3913427036182553, 0.39291733357251857, 0.38893387598149914, 0.3907097995865579, 0.39178018352272465, 0.38997077613192443, 0.38897235888768644, 0.39309656094102297, 0.3918713939686616, 0.38930126476813764, 0.3919200319431576, 0.3909892667742336, 0.3936515018782195, 0.3921337866900014, 0.3897011807444049, 0.389769053050116, 0.3902200594106141, 0.38991651070468564, 0.3911624582228707, 0.3900841569491461, 0.38808364508783116, 0.39078373330480914, 0.3911517692693308, 0.3905084923494096, 0.392090796986047, 0.3915708720245782, 0.3868667685664168, 0.3898552243469977, 0.3904008638186782, 0.38936665559224054, 0.3908520120323873, 0.3909977381574173, 0.3894340494538055, 0.3913114227938886, 0.38867319550584345, 0.3922208594340904, 0.39250611046365663, 0.3933124074748918, 0.3923281282916957, 0.3923263843445217, 0.3901771641537255, 0.39134884019400556, 0.38913627334085166, 0.39259269424513277, 0.39072176375809836, 0.3921530346806143, 0.39100976697370116, 0.39079241250075547, 0.3899397780643959, 0.3929170014373228, 0.3889102427398457, 0.38894158966985404, 0.39256049473496046, 0.3897372295020842, 0.39233241295989824, 0.39279353041567056, 0.394793822411813, 0.3923672336865874, 0.38789207575952306, 0.391120845853698, 0.3914312515042576, 0.3890334714715387, 0.38947177671042144, 0.3914659348334752, 0.39052898429480254, 0.38922284959870224, 0.3926088816541083, 0.3930806327684253, 0.392626012584158, 0.3940140974580073, 0.3908073946687521, 0.3892480945762466, 0.3914746798428835, 0.3904998555925547, 0.39119182118013796, 0.39440940546931, 0.39126060596283746, 0.39380357363352586, 0.3891267492344566, 0.3960636570027061, 0.3884448780879086, 0.39201007885675804, 0.3887008518418845, 0.3946489935704306, 0.39012787235425966, 0.3926868131201641, 0.391174876587648, 0.39185114438627283, 0.3916316426121721, 0.39595334516728625, 0.3925237079315326, 0.3923376457510041, 0.3896145831574412, 0.3903454670719072, 0.38917045835770814, 0.3901947463552157, 0.38894958538459795, 0.39138510881685745, 0.3923698270729944, 0.391075956543871, 0.3886808121905607, 0.3895190705855687, 0.3937717227666986, 0.3892171628334943, 0.3928420579462659, 0.3929640487128613, 0.39179296280239145, 0.39022867213569434, 0.3915944700585861, 0.3908092556338684, 0.38998721832153843, 0.3919305083360158, 0.38930609106433156, 0.3915616778620318, 0.39198073642510994, 0.3898476078083702, 0.3942314390750492, 0.39122209778311207, 0.39321639089315547, 0.39556199356037025, 0.39069073004465477, 0.3901906652631713, 0.3918502590089452, 0.3921655998656563, 0.3897442833027419, 0.39050330440787706, 0.39026205139417275, 0.3908781868716081, 0.39245978443353785, 0.3903481831007144, 0.39445868105280635, 0.3892170712351799, 0.390042986282531, 0.3925706706941128, 0.39200084298556925, 0.3911351076528138, 0.39240403268851487, 0.3938527383348521, 0.3900003031480546, 0.3922225654709573, 0.3932921053001694, 0.39253465655971975, 0.3926194997540876, 0.3874349548097919, 0.39200115612908903, 0.3948955665908608, 0.3898700127998988, 0.38938687602971117, 0.3864065995257275, 0.39098570473930416, 0.3909385864641152, 0.3916171494067884, 0.39381099733359676, 0.3898175518740626, 0.3911994169740116, 0.39155133265782804, 0.39230194549058, 0.39114902234252763, 0.39017920955723406, 0.3944379236622184, 0.3916288679283039, 0.39200210498244153, 0.39363550146420795, 0.3911365672361617, 0.39186333141782703, 0.39117470471297994, 0.38984077394593, 0.39264219977399883, 0.3900822729018389, 0.38898184559508864, 0.3910406724202867, 0.3907987319809549, 0.39212526841198697, 0.39124271427007284, 0.3914998725202738, 0.38839635442869336, 0.3937654222781752, 0.3902626017875531, 0.3955241435883092, 0.39323169840317146, 0.39330630157800284, 0.38655056892072454, 0.39017754106544983, 0.3952399448436849, 0.39261257144458156, 0.3932858498073092, 0.39499423879326556, 0.3910665978111473, 0.3922448316041161, 0.39132243789294185, 0.3905644398547855, 0.39066209877822916, 0.39234474681171716, 0.3916584701806891, 0.39128884439374884, 0.38904376978091165, 0.39183107588221044, 0.3900741520611679, 0.3930635173969409, 0.38995324082526506, 0.39311547854951784, 0.3887742017121876, 0.3909773443113355, 0.38996322007448064, 0.38965144855718986, 0.39268765284442436, 0.390111534545819, 0.3925885803559247, 0.39064669915858435, 0.3895316495030534, 0.3907163414154567, 0.3907886814399093, 0.39067917981860684, 0.3914143468673323, 0.39072976911477014, 0.39038341521632436, 0.39487345085716713, 0.39242782728636966, 0.39340144921751585, 0.391638440317383, 0.38982952050134245, 0.39072682591629965, 0.39341319571523103, 0.39195014886996327, 0.3917578032203749, 0.38969211536003096, 0.3894471714601797, 0.3898716885961738, 0.39170538400318106, 0.39051353675769823, 0.39179608466870647, 0.3921622397998969, 0.39439454747765673, 0.39445579307628614, 0.39132673534400325, 0.3867330071972866, 0.3894542139388767, 0.39122700530524346, 0.3883669243431559, 0.38984382444737004, 0.3917064682817927, 0.3907441727670969, 0.39336200226463525, 0.3893578349083078, 0.3913763885696729, 0.3897980788320887, 0.3937839002293699, 0.39068301957027585, 0.3921215317997278, 0.39208485865417647, 0.3916279115513259, 0.3911278141918136, 0.39544760121726524, 0.38938475297946556, 0.39043475139667005, 0.39209672136634005, 0.39414026142627584, 0.3913420901871195, 0.39028101735839654, 0.3931575333516972, 0.39237413212072614, 0.38880399651094977, 0.3887612786801422, 0.3894933131979961, 0.3917112931901333]\n",
        "test_acc_list_exp = [75.58773816840811, 87.06976029502151, 87.53073140749846, 87.8918254456054, 87.76889981561156, 87.74969268592501, 87.98786109403811, 87.76121696373694, 87.87645974185618, 87.71896127842655, 87.74969268592501, 87.79578979717272, 87.96481253841426, 87.81115550092194, 87.81499692685925, 87.80731407498463, 87.9417639827904, 87.93023970497849, 87.76505838967425, 87.78042409342348, 87.66133988936693, 87.7458512599877, 87.7458512599877, 87.85725261216963, 87.66518131530424, 87.71127842655194, 87.65365703749232, 87.66133988936693, 87.74200983405039, 87.7458512599877, 87.85725261216963, 87.89566687154272, 87.64981561155501, 87.61140135218193, 88.02243392747388, 87.85725261216963, 87.50384142593731, 87.91871542716656, 87.82652120467118, 87.80731407498463, 87.82652120467118, 87.93792255685311, 87.6920712968654, 87.72280270436386, 87.82267977873387, 87.81499692685925, 87.87645974185618, 87.66133988936693, 87.8418869084204, 87.76889981561156, 87.88030116779349, 87.78042409342348, 87.7919483712354, 87.79963122311001, 87.81883835279656, 87.8918254456054, 87.92639827904118, 87.94944683466503, 87.78426551936079, 87.8918254456054, 87.7881069452981, 88.00322679778733, 87.87645974185618, 87.7881069452981, 87.68054701905348, 87.81499692685925, 87.71896127842655, 87.82267977873387, 87.7919483712354, 87.81883835279656, 87.83036263060848, 87.78042409342348, 87.78042409342348, 87.71511985248924, 87.72664413030117, 87.82267977873387, 87.80731407498463, 87.88030116779349, 87.83036263060848, 87.84956976029503, 87.91487400122925, 87.73048555623848, 87.81115550092194, 87.73048555623848, 87.7381684081131, 87.72664413030117, 87.8918254456054, 87.66518131530424, 87.8418869084204, 87.7919483712354, 87.69975414874001, 87.79578979717272, 87.77658266748617, 87.90334972341734, 87.75353411186232, 87.83036263060848, 87.86877688998156, 87.86877688998156, 87.7919483712354, 87.84572833435772, 87.91871542716656, 87.84956976029503, 87.83036263060848, 87.80347264904732, 87.70743700061463, 87.75353411186232, 87.9840196681008, 87.70359557467732, 87.84956976029503, 87.87645974185618, 87.7381684081131, 87.81115550092194, 87.6459741856177, 87.7458512599877, 87.51920712968654, 88.05700676090964, 87.79578979717272, 87.91871542716656, 87.56914566687155, 87.82267977873387, 87.73432698217579, 87.75737553779963, 87.81115550092194, 87.85341118623234, 87.74969268592501, 87.68054701905348, 87.63444990780577, 87.83420405654579, 87.91103257529196, 87.76505838967425, 87.80731407498463, 88.04548248309773, 87.81883835279656, 87.82652120467118, 87.6920712968654, 87.76121696373694, 87.74200983405039, 87.80347264904732, 87.96481253841426, 87.8380454824831, 87.6459741856177, 87.85341118623234, 87.92255685310387, 87.68438844499079, 87.7458512599877, 87.78426551936079, 87.81883835279656, 87.98017824216349, 87.88030116779349, 87.6421327596804, 87.9340811309158, 87.65365703749232, 87.68438844499079, 87.60371850030731, 87.66133988936693, 87.6459741856177, 87.78042409342348, 87.77658266748617, 87.58451137062077, 87.97633681622618, 87.83420405654579, 87.73048555623848, 87.84572833435772, 87.75737553779963, 87.76121696373694, 87.63829133374308, 87.91103257529196, 87.75353411186232, 87.57298709280884, 87.70743700061463, 87.73048555623848, 87.77658266748617, 87.76505838967425, 88.02243392747388, 87.84572833435772, 87.73048555623848, 87.88414259373079, 87.69975414874001, 87.88030116779349, 87.88030116779349, 87.68054701905348, 87.92639827904118, 87.95712968653964, 87.91103257529196, 87.68054701905348, 87.76121696373694, 87.78426551936079, 87.67286416717886, 87.67670559311617, 87.88414259373079, 87.79578979717272, 87.66133988936693, 87.83036263060848, 87.86493546404425, 87.78426551936079, 87.67286416717886, 87.81115550092194, 87.6421327596804, 87.70359557467732, 87.71127842655194, 87.84956976029503, 87.82267977873387, 87.7919483712354, 87.71127842655194, 87.83036263060848, 87.8879840196681, 87.76121696373694, 87.73048555623848, 87.76121696373694, 87.8380454824831, 87.75353411186232, 87.72280270436386, 87.84956976029503, 87.59987707437, 87.6959127228027, 87.62292562999386, 88.03779963122311, 87.77658266748617, 87.7381684081131, 87.80731407498463, 87.56530424093424, 87.63060848186846, 87.69975414874001, 87.71896127842655, 87.89566687154272, 87.83420405654579, 87.76121696373694, 87.63060848186846, 87.79578979717272, 87.65365703749232, 87.82267977873387, 87.85341118623234, 87.85341118623234, 87.80347264904732, 87.87645974185618, 87.84572833435772, 87.96097111247695, 87.84572833435772, 87.71511985248924, 87.85725261216963, 87.78426551936079, 87.78426551936079, 87.61908420405655, 87.86493546404425, 87.96097111247695, 87.86109403810694, 87.75737553779963, 87.84956976029503, 87.7881069452981, 87.84572833435772, 87.70359557467732, 87.71127842655194, 87.65749846342962, 87.62292562999386, 87.71896127842655, 87.96865396435157, 87.97633681622618, 87.76121696373694, 87.75353411186232, 87.81115550092194, 87.8879840196681, 87.82652120467118, 87.87645974185618, 87.68438844499079, 87.8918254456054, 87.63060848186846, 87.76505838967425, 87.5960356484327, 87.66133988936693, 87.9840196681008, 87.88030116779349, 87.86877688998156, 87.77274124154886, 87.9417639827904, 87.90719114935465, 87.7881069452981, 87.69975414874001, 87.77658266748617, 87.84572833435772, 87.67286416717886, 87.80731407498463, 87.74200983405039, 87.71896127842655, 87.6459741856177, 87.68438844499079, 87.86877688998156, 87.82267977873387, 87.78426551936079, 87.72664413030117, 87.98017824216349, 87.66518131530424, 87.6959127228027, 87.75353411186232, 87.91487400122925, 87.89566687154272, 87.72280270436386, 87.91487400122925, 87.88030116779349, 87.91103257529196, 87.75353411186232]\n",
        "\n",
        "train_loss_list_5e4 = [1.4802386524877573, 0.5145602391825782, 0.4270873037860969, 0.3911595640143728, 0.3666531684275888, 0.3449077741482717, 0.3271806535278232, 0.31609291052269095, 0.30408265799041684, 0.29365021432560634, 0.28160075124524797, 0.2770334360439603, 0.26884961707724464, 0.2641961875202533, 0.25375436896554177, 0.2502837141112583, 0.2470766868048567, 0.23852058467745457, 0.23988390942091542, 0.23230066057546997, 0.2278682370051782, 0.22565480602304464, 0.22504844034953816, 0.2194394195467476, 0.2184035734028674, 0.21561946675464067, 0.21413457837854297, 0.21124843120332656, 0.21243113762923696, 0.20603898459374098, 0.20471800083350036, 0.20373490892936222, 0.20350655851769578, 0.20246598469774899, 0.1970768437334677, 0.1986252810896897, 0.19743034299109685, 0.19816612322034874, 0.19047524803254987, 0.19524715533909112, 0.19178811626301873, 0.1882726750904467, 0.19070912812783467, 0.18814858636720394, 0.18752294930258417, 0.1876332449012494, 0.18488309697972405, 0.1866223762634647, 0.1871858422712582, 0.18362822461378606, 0.18218793275798886, 0.18321568313935585, 0.17826895162382422, 0.18373284461658176, 0.17807404935198426, 0.1832454036466959, 0.17776847138072094, 0.17643137354918612, 0.17395331942776676, 0.17643668946777255, 0.17554401157065458, 0.17678272510124093, 0.17290620969199552, 0.16925092947575943, 0.16994382533923721, 0.17394802385189023, 0.17083977395037647, 0.16816641190430012, 0.16901177336489606, 0.17049305643734894, 0.1686860634806518, 0.16724471368279237, 0.16594552272945884, 0.16281739412365245, 0.16591493490588696, 0.16363012161721704, 0.1613770334055107, 0.1623856468898494, 0.16301845652139607, 0.162652157650892, 0.1592392718626393, 0.15867647268180926, 0.16240244100329676, 0.15764614089636944, 0.15857719487654484, 0.1579075544810069, 0.15415261916028775, 0.15517796557850955, 0.15625821325554434, 0.15458142586799495, 0.15438464393985626, 0.1572210744003132, 0.15048929238262862, 0.14945686451060985, 0.1505291250440971, 0.15305787657556658, 0.14894246368550348, 0.147898023902643, 0.15114493573180382, 0.14610998743897693, 0.15086569631002783, 0.1484981526729214, 0.1449999701039901, 0.14578966223975506, 0.14083756948631954, 0.14295084347448697, 0.14506198940319098, 0.14481029806905968, 0.14044388525470647, 0.13931026286508655, 0.13716323383980328, 0.14206144404484006, 0.14102018442822667, 0.13867486305635796, 0.13811369409385896, 0.1344010781062651, 0.13408899530364568, 0.1343176202560828, 0.13219609678417524, 0.13159260965459715, 0.13269671327295665, 0.13183512560377114, 0.12735726753487012, 0.12682472022002952, 0.12942703321656884, 0.1311747801154448, 0.12721967680990534, 0.1226911582961315, 0.12565258846474373, 0.12294719365695467, 0.12425805079601807, 0.12707497591410227, 0.12184429030895717, 0.11833101225799823, 0.11996741157762081, 0.12062076490806532, 0.11433951715184099, 0.11977466401267989, 0.11421744397217988, 0.11513779362345614, 0.11420854406688756, 0.11369123289659418, 0.11147084164869818, 0.10956912912014458, 0.10847692516680332, 0.10791541966369886, 0.11187728162006877, 0.1076874076956656, 0.10829882281340235, 0.10742072247190851, 0.10462035163530328, 0.10231590462308428, 0.10307718535789306, 0.10392249831399782, 0.10356237866277214, 0.10049116989057562, 0.10070552909620571, 0.10252807007188881, 0.0956033191373597, 0.09626625927523545, 0.09980903398548442, 0.09713503368814705, 0.09384269924546645, 0.0914715800489877, 0.08955621895276838, 0.09167020563995289, 0.090305259370505, 0.09060482793616328, 0.08901907593203673, 0.08711527488610851, 0.08550666729467833, 0.08462664417589341, 0.08277387312313082, 0.0831775262416782, 0.08437745708992407, 0.07903462006309168, 0.08039322875561268, 0.08088428793136827, 0.07809663470834494, 0.07452205053169714, 0.07565905422547727, 0.07784699146360724, 0.07322079027046356, 0.07456694824272782, 0.07434099188786213, 0.07195496950349026, 0.06945457162340563, 0.06995407763495032, 0.06968635793139295, 0.06799854681420779, 0.0665388950500082, 0.06632828803324117, 0.06345813088421899, 0.06316864148266917, 0.06253555392074149, 0.06164193874588463, 0.0611763400048381, 0.06035886153573711, 0.05943589109351965, 0.05881024452638093, 0.05468251183250449, 0.0560565908366264, 0.05653399054899933, 0.05562049048248588, 0.051311384115448895, 0.052225344276767435, 0.05155897112739926, 0.049526256553293645, 0.0512430043146519, 0.04822691060415896, 0.047050642753196606, 0.04643308828764047, 0.04417288107519873, 0.04520953228056108, 0.04404221094140233, 0.04265529490744841, 0.042559341516164015, 0.04379070952893071, 0.03938076019963517, 0.03966092142758532, 0.04086068194664934, 0.03843482981150753, 0.03796145588356426, 0.03604420552757091, 0.03480926173332786, 0.035225326502645204, 0.032808877683565184, 0.03266389738152505, 0.03565281066162698, 0.03340759336365372, 0.031172389340609674, 0.030425866454685747, 0.03243158847853279, 0.028973121385324777, 0.026384176789886422, 0.026978798047197744, 0.026420692340937774, 0.0263953241601657, 0.024609150329029293, 0.025792538209220138, 0.02489618113110483, 0.02391088762519967, 0.023590878200172107, 0.023072245570535704, 0.021719697244505735, 0.01978167379306513, 0.0224422716837235, 0.01906526322807198, 0.02043990451882929, 0.0194835271951326, 0.01808247663810102, 0.018989686450673657, 0.019306088054456765, 0.017153792940761622, 0.016933396825223636, 0.015568477072599054, 0.015502154842576865, 0.016233396913596307, 0.015708776872332502, 0.01585502114947027, 0.014598894385724433, 0.013940555258366332, 0.013421826192990531, 0.013425068274836957, 0.011932213828257893, 0.013165008134731069, 0.012398852705501201, 0.013773291333588622, 0.011650380652604831, 0.011782062857671765, 0.011110670982890316, 0.011588883696175698, 0.011087752127802874, 0.010806514879342845, 0.009987936503137777, 0.01071135072052721, 0.010115013357193833, 0.010417424021213035, 0.010015223428940142, 0.009483683681138224, 0.008984847573833514, 0.009285728387196414, 0.009138723183864965, 0.009764598564845498, 0.00961481691192997, 0.009357751404228027, 0.009121658366620237, 0.00870354351181244, 0.008720945385802124, 0.008100192143321845, 0.008175653403910094, 0.008284241586373387, 0.008199483759103085, 0.008166708315275913, 0.008518998447406865, 0.008448232254922026, 0.009054402698987343, 0.008894852471574484, 0.008632234484777561, 0.008205045137029434]\n",
        "train_acc_list_5e4 = [48.277395447326626, 83.64002117522499, 86.79724722075171, 88.12493382742191, 88.89147697194282, 89.55214399152992, 89.9947061937533, 90.69772366331392, 90.96029645314981, 91.36050820539968, 91.55955532027528, 91.68660667019587, 92.08046585494971, 92.15457914240339, 92.65643197458974, 92.62466913710958, 92.80254102699841, 93.1011116993118, 93.08840656431974, 93.11593435680254, 93.28110111169931, 93.37850714663843, 93.46956061408153, 93.68131286394917, 93.59661196400212, 93.65166754896771, 93.93118051879301, 93.87612493382743, 93.82953943885654, 94.09846479618847, 94.14716781365802, 94.02858655373214, 94.13022763366861, 94.16410799364743, 94.31445209105347, 94.212811011117, 94.20434092112228, 94.25939650608788, 94.53255690841715, 94.46056114346214, 94.500794070937, 94.56855479089465, 94.40974060349392, 94.58125992588671, 94.53255690841715, 94.62361037586024, 94.77183695076761, 94.66807834833246, 94.64690312334568, 94.67866596082584, 94.7443091582848, 94.71042879830598, 94.96029645314981, 94.68290100582318, 94.88406564319746, 94.70619375330863, 94.8268925357332, 94.91371095817892, 94.92429857067232, 94.99417681312865, 94.94759131815776, 94.89253573319216, 95.142403388036, 95.1148755955532, 95.13816834303864, 94.97511911064055, 95.08946532556908, 95.23133933298041, 95.18475383800953, 95.00264690312335, 95.13393329804128, 95.06617257808364, 95.18475383800953, 95.32874536791954, 95.26521969295923, 95.21651667548967, 95.358390682901, 95.29274748544204, 95.39862361037586, 95.3223928004235, 95.47908946532557, 95.358390682901, 95.3499205929063, 95.42826892535733, 95.46638433033351, 95.43462149285337, 95.57014293276866, 95.4219163578613, 95.45579671784013, 95.63366860772896, 95.55532027527792, 95.5214399152991, 95.74166225516146, 95.71836950767602, 95.66543144520911, 95.5849655902594, 95.67178401270513, 95.75860243515088, 95.6569613552144, 95.80518793012176, 95.6929592376919, 95.71836950767602, 95.84542085759661, 95.8644785600847, 95.95764955002647, 95.92165166754897, 95.84965590259397, 95.78613022763366, 96.01694017998942, 95.98941238750662, 96.02117522498676, 95.97670725251456, 95.93859184753838, 96.09317098994177, 96.00211752249868, 96.11858125992589, 96.14822657490735, 96.15669666490207, 96.22022233986236, 96.18845950238222, 96.2075172048703, 96.16516675489677, 96.35997882477501, 96.38538909475913, 96.26257278983589, 96.2350449973531, 96.36421386977237, 96.56961355214399, 96.3790365272631, 96.54843832715723, 96.38750661725781, 96.3430386447856, 96.56749602964531, 96.65431445209106, 96.64584436209634, 96.6056114346215, 96.66490206458444, 96.52302805717311, 96.76654314452091, 96.68395976707252, 96.72631021704606, 96.67337215457914, 96.80254102699841, 96.92535733192165, 96.93170989941768, 96.84277395447327, 96.79195341450503, 96.9020645844362, 96.929592376919, 96.86394917946004, 97.01429327686607, 97.07146638433034, 97.05452620434092, 96.98041291688725, 97.05029115934357, 97.19428268925357, 97.12228692429856, 97.07993647432504, 97.32556908417152, 97.30227633668608, 97.18581259925887, 97.25357331921651, 97.28533615669666, 97.3446267866596, 97.43991529910005, 97.44415034409741, 97.41450502911593, 97.43144520910535, 97.46744309158285, 97.45262043409211, 97.56484912652196, 97.57755426151402, 97.60508205399682, 97.66225516146109, 97.53732133403918, 97.7067231339333, 97.66013763896241, 97.65378507146639, 97.76601376389624, 97.87612493382743, 97.8507146638433, 97.80836421386977, 97.89306511381683, 97.83800952885125, 97.86553732133405, 97.91847538380095, 98.0052938062467, 97.92906299629433, 97.97141344626786, 98.07517204870302, 98.04552673372154, 98.08152461619905, 98.14505029115935, 98.18951826363156, 98.1852832186342, 98.24033880359978, 98.18316569613552, 98.27633668607729, 98.26786659608258, 98.24669137109582, 98.44150344097406, 98.41821069348862, 98.30598200105877, 98.37374272101641, 98.54314452091053, 98.52196929592377, 98.51561672842774, 98.61725780836422, 98.53890947591319, 98.66807834833246, 98.59820010587613, 98.64690312334568, 98.74854420328217, 98.68925357331922, 98.70407623080995, 98.74642668078349, 98.76548438327157, 98.67019587083112, 98.89677077818952, 98.80359978824775, 98.77818951826363, 98.854420328216, 98.86924298570672, 98.94123875066173, 98.97511911064055, 98.97511911064055, 99.07252514557968, 99.06617257808364, 98.96029645314981, 99.04711487559555, 99.09158284806776, 99.08523028057174, 99.02593965060879, 99.16569613552144, 99.26098464796189, 99.214399152991, 99.2419269454738, 99.2503970354685, 99.31604023292748, 99.22498676548439, 99.30333509793542, 99.29062996294336, 99.28427739544733, 99.32874536791954, 99.36897829539438, 99.44944415034409, 99.34568554790894, 99.46638433033351, 99.40709370037057, 99.43673901535203, 99.47273689782953, 99.44944415034409, 99.46003176283747, 99.50661725780836, 99.51085230280572, 99.56167284277396, 99.58708311275808, 99.5299100052938, 99.56379036527264, 99.56802541026998, 99.59978824775013, 99.61461090524087, 99.65272631021705, 99.62519851773425, 99.7204870301747, 99.63578613022763, 99.67813658020117, 99.64002117522499, 99.69931180518793, 99.69719428268925, 99.72472207517205, 99.68448914769719, 99.70566437268396, 99.7204870301747, 99.76283748014822, 99.71625198517734, 99.75860243515088, 99.72683959767072, 99.74377977766014, 99.76707252514558, 99.78613022763366, 99.78189518263632, 99.7564849126522, 99.75224986765484, 99.75224986765484, 99.75436739015352, 99.7649550026469, 99.79460031762838, 99.7924827951297, 99.7924827951297, 99.81154049761778, 99.82424563260984, 99.79460031762838, 99.82001058761249, 99.80307040762308, 99.7649550026469, 99.73954473266278, 99.77766013763896, 99.78613022763366, 99.81365802011646]\n",
        "test_loss_list_5e4 = [0.8332259766027039, 0.6243491267748907, 0.4589659714815663, 0.4804437569543427, 0.427488262965983, 0.4090569673800001, 0.3808782717906961, 0.35566168862814995, 0.33532728992548644, 0.3060293437949583, 0.3283534451734786, 0.2954925663184886, 0.28179204358043625, 0.30891463029034, 0.3015652174750964, 0.28739879868340257, 0.30414057471880723, 0.2780561326254232, 0.2854650225125107, 0.2774708117632305, 0.2591885347649747, 0.2489084253343297, 0.26453685285706147, 0.2969295278775926, 0.2676425111644408, 0.24623633818883522, 0.2657030882569505, 0.2502379405367024, 0.2519026789814234, 0.24595809209288336, 0.2657459572907172, 0.24606278589835354, 0.26561722005991373, 0.26659162207415293, 0.27839325434144807, 0.24987868000479305, 0.24812665541528486, 0.24487176610558642, 0.2487423630321727, 0.2408910162162547, 0.23539556405853992, 0.24755567443721435, 0.24770869559371003, 0.24718565263730638, 0.239624624816226, 0.24662797083603402, 0.2383566615445649, 0.2449228444374075, 0.23434106121752776, 0.2598630274627723, 0.2621555033998162, 0.23332552688525005, 0.23622890707908892, 0.24026584826117636, 0.2526375870494282, 0.23738020169092158, 0.2334751014177706, 0.2508628443796553, 0.2439220617068749, 0.23279762205978236, 0.24812749104903026, 0.23510865852528928, 0.24088096268036785, 0.2441782963772615, 0.2372785997120481, 0.2591664406160514, 0.24067971285651713, 0.2356143380497016, 0.22471256245512003, 0.23980964548593642, 0.22926432496922858, 0.22651415298163308, 0.23500783566166372, 0.24451460265645794, 0.24563370374780075, 0.238621749385607, 0.24734771609598516, 0.24317575092701352, 0.2357527948002897, 0.23691468079592667, 0.2317277501318969, 0.2389378704167172, 0.2415773031100923, 0.22682394105575832, 0.24305828332024462, 0.23989992117618814, 0.24055047181672326, 0.2310189416851191, 0.2320305799663651, 0.24022304533305122, 0.23773051294333794, 0.23404084142370551, 0.22795141963105575, 0.24373077162924936, 0.2519711878004612, 0.24464675623412227, 0.24204467719092088, 0.23641339631057254, 0.22852404189168238, 0.23696348417148577, 0.23560348832431963, 0.2293583228636314, 0.22483952014761813, 0.2419880603750547, 0.24878904088308998, 0.23500451945937148, 0.23449834377742282, 0.23925709554596858, 0.22304683715543328, 0.23272172509528258, 0.23005880462918796, 0.2274422672230239, 0.24031657513742352, 0.23025121483221359, 0.23231310611042907, 0.23103023649138563, 0.23504440533910312, 0.23217533609154178, 0.24353400623316274, 0.23417713490369566, 0.23423893881194732, 0.228971739591775, 0.23540909750861863, 0.22729575031382196, 0.24729683045663087, 0.23187032206824013, 0.23728142879611136, 0.2368104361983783, 0.23822332604550847, 0.24225895598019456, 0.24053165952072425, 0.23070352232339336, 0.23100480022748895, 0.23646043049281135, 0.2497626494835405, 0.2328693575280554, 0.2476191526169286, 0.24926081999186792, 0.22955395642887144, 0.23540200713072337, 0.24256359033432662, 0.23535256659356402, 0.22585722259884955, 0.23929224061030968, 0.2362090051356776, 0.2323842137759807, 0.23957762155024445, 0.23328206293723164, 0.24211033893858686, 0.24111823025433457, 0.2357599771234627, 0.23172222019410602, 0.23386546501926347, 0.2514914059828894, 0.2433166304037121, 0.2378106768261276, 0.22987670018611586, 0.230959424767278, 0.23733513216104576, 0.24432600466717108, 0.23943918337132417, 0.24938077077853912, 0.24655740848724164, 0.2478333150456641, 0.25572608141045944, 0.24093728490612087, 0.24338949404145574, 0.24488756612089335, 0.2545141223452839, 0.24692704464655882, 0.2547990579669382, 0.2425614733468084, 0.24526120959689804, 0.2513910967254025, 0.24536202011593417, 0.24960096059914896, 0.24901917171390617, 0.24787598632860416, 0.25261163421194344, 0.24638482389569866, 0.2529066298522201, 0.24463685903259935, 0.2410413659926431, 0.2477684386403245, 0.2515937814708142, 0.25298880261606443, 0.2537755832423036, 0.24700629240011468, 0.25862112336372045, 0.2538327400974345, 0.25513705633142414, 0.25248319325128604, 0.25129741044970705, 0.25710856879823935, 0.2602397995252235, 0.2535753774401896, 0.25966272229219184, 0.26258349593947916, 0.2575982420467863, 0.25366368794850275, 0.26840075843182265, 0.2604089367623423, 0.26335741690926107, 0.26115994502370266, 0.27586783806556, 0.2699058622340946, 0.2690262606038767, 0.26124058914023873, 0.2695645693068703, 0.269948998539179, 0.2699922961274199, 0.2781037180410588, 0.26918452950742316, 0.2656496547746892, 0.27793413524826366, 0.2727473574854872, 0.26761945906807394, 0.28111935063612226, 0.2739635939691581, 0.2791407521741063, 0.27640176702327296, 0.27548765678269166, 0.2720885994460653, 0.2716641928087555, 0.29375703590830754, 0.2741913403402649, 0.2834579655352761, 0.2800045997799173, 0.2746915383556602, 0.28199352176093, 0.2871078792502921, 0.2875168366430729, 0.2797919814200962, 0.2876746460211043, 0.2826278824541791, 0.2888666771662732, 0.28122600651912244, 0.2926978058565189, 0.2832188007111351, 0.2851658707082856, 0.2897379945543613, 0.2852640647404626, 0.29130700086334754, 0.29031389254127066, 0.301364694575907, 0.29234155917576715, 0.30245087251943703, 0.29240411739138994, 0.2921989212068273, 0.29711635103997064, 0.30123716481395213, 0.29383554357085745, 0.2976747955944316, 0.2940556710385078, 0.29956718912238584, 0.2989022258906534, 0.3001505616760137, 0.29694770219023614, 0.296321282619793, 0.30678381798241067, 0.2958103663415885, 0.29974740378412545, 0.29243658106847137, 0.29601444117724895, 0.2992933266522253, 0.3056794012914978, 0.3159245145963688, 0.3084262056714472, 0.3033023014774217, 0.2999487395635715, 0.305078714501624, 0.29950604752144394, 0.3001469790424202, 0.3019553049303153, 0.3047376123220459, 0.30005093909087865, 0.2961901391984201, 0.3029952965421127, 0.2991081877867235, 0.3034919787508746, 0.30540474100659293, 0.29712304374312654, 0.304328731279455, 0.30321371049492385, 0.3067686218019648, 0.30573491976760764, 0.30279137281810536, 0.304144932066693, 0.3118132336941712, 0.3035398056803673, 0.3062666428193231, 0.3056575841106036, 0.30131836995190264, 0.3063342159285265, 0.31325177424678613, 0.2976547159327596, 0.3059330361475255, 0.2988617525655119, 0.29846939126796584, 0.3085705796804498]\n",
        "test_acc_list_5e4 = [73.40580823601721, 79.92086662569146, 86.0479409956976, 85.25276582667486, 87.02750460971113, 87.60755992624462, 88.40273509526736, 89.31699446834665, 89.8740012292563, 90.96496619545175, 90.20052243392747, 91.4259373079287, 91.59496004917025, 91.09557467732022, 91.30301167793485, 91.72172710510141, 91.09557467732022, 91.82928703134604, 91.69483712354025, 91.82160417947142, 92.55147510755992, 92.82805777504609, 92.3939766441303, 91.19161032575292, 92.22111247695145, 92.82037492317149, 92.24031960663798, 92.8242163491088, 92.70513214505225, 92.94330055316533, 92.41318377381684, 92.88952059004302, 92.42086662569146, 92.1980639213276, 92.02519975414874, 92.71665642286416, 92.86647203441917, 92.96250768285188, 92.83958205285802, 93.14305470190534, 93.32744314689613, 92.98939766441303, 93.01244622003688, 93.0278119237861, 93.2160417947142, 92.97019053472648, 93.29287031346036, 93.04317762753534, 93.25829748002458, 92.65903503380454, 92.59373079287032, 93.4119545175169, 93.1238475722188, 93.28134603564843, 92.85494775660726, 93.31207744314689, 93.42347879532882, 93.00476336816226, 93.08927473878303, 93.51951444376152, 93.00860479409957, 93.36585740626921, 93.20835894283958, 92.98555623847572, 93.39274738783037, 92.60141364474492, 93.18146896127843, 93.35049170251997, 93.68469575906576, 93.27750460971113, 93.51951444376152, 93.6578057775046, 93.51951444376152, 93.1238475722188, 92.98555623847572, 93.27366318377382, 93.15457897971727, 93.38890596189306, 93.45421020282728, 93.32360172095882, 93.44268592501537, 93.38122311001844, 93.4618930547019, 93.59634296250768, 93.42347879532882, 93.34665027658266, 93.31976029502151, 93.7077443146896, 93.42347879532882, 93.34665027658266, 93.49646588813768, 93.48110018438844, 93.6578057775046, 93.37354025814382, 93.1238475722188, 93.1737861094038, 93.24677320221267, 93.45421020282728, 93.71542716656423, 93.39658881376766, 93.5041487400123, 93.58097725875845, 93.9881684081131, 93.26982175783651, 93.0201290719115, 93.61555009219423, 93.52335586969883, 93.39274738783037, 93.91518131530424, 93.71926859250154, 93.86140135218193, 93.77688998156115, 93.37354025814382, 93.69237861094038, 93.6578057775046, 93.80762138905962, 93.49262446220037, 93.66548862937923, 93.40427166564228, 93.6578057775046, 93.68469575906576, 93.66548862937923, 93.61939151813154, 94.08420405654579, 93.59634296250768, 93.65012292563, 93.6539643515673, 93.61939151813154, 93.61170866625692, 93.48878303626306, 93.58097725875845, 93.73847572218807, 93.73079287031346, 93.66164720344192, 93.4119545175169, 93.75, 93.48110018438844, 93.58097725875845, 93.86524277811924, 93.73079287031346, 93.47725875845114, 93.90749846342962, 94.19944683466503, 93.66933005531654, 93.66933005531654, 93.79609711124769, 93.700061462815, 93.88060848186846, 93.80762138905962, 93.76920712968654, 93.75, 93.86524277811924, 93.88829133374308, 93.6078672403196, 93.75768285187462, 93.76152427781193, 93.87292562999386, 94.0419483712354, 93.93054701905348, 93.85371850030731, 93.91902274124155, 93.6539643515673, 93.9881684081131, 93.66548862937923, 93.53872157344806, 93.89981561155501, 94.01505838967425, 94.06115550092194, 93.7077443146896, 93.799938537185, 93.59250153657038, 93.91902274124155, 94.02658266748617, 93.82298709280884, 94.01505838967425, 93.90749846342962, 93.96511985248924, 93.91133988936693, 93.76536570374923, 94.0918869084204, 93.78841425937308, 93.96896127842655, 94.09956976029503, 94.14566687154272, 93.8921327596804, 93.91518131530424, 93.8921327596804, 94.26090964966195, 93.80762138905962, 93.88060848186846, 94.01889981561156, 94.07652120467118, 94.14566687154272, 94.10341118623234, 93.8460356484327, 94.02658266748617, 93.91902274124155, 93.86908420405655, 94.12645974185618, 94.1379840196681, 93.93822987092808, 94.07652120467118, 94.08420405654579, 94.0381069452981, 93.85755992624462, 94.02658266748617, 94.00353411186232, 94.18792255685311, 94.02274124154886, 94.0918869084204, 94.21865396435157, 93.99200983405039, 94.06499692685925, 94.28011677934849, 93.99969268592501, 93.96896127842655, 94.28011677934849, 93.97664413030117, 94.22633681622618, 94.24170251997542, 94.05731407498463, 94.13414259373079, 94.1917639827904, 94.49139520590043, 93.99200983405039, 94.28779963122311, 94.03042409342348, 94.38767670559312, 94.19944683466503, 94.1840811309158, 94.11109403810694, 94.07267977873387, 94.23017824216349, 94.38383527965581, 94.32237246465888, 94.21481253841426, 94.22249539028887, 94.24938537185002, 94.31853103872157, 94.34926244622004, 94.21865396435157, 94.29932390903504, 94.25322679778733, 94.26090964966195, 94.29548248309773, 94.41840811309157, 94.17639827904118, 94.36078672403197, 94.37231100184388, 94.13414259373079, 94.19944683466503, 94.34157959434542, 94.34157959434542, 94.36846957590657, 94.35310387215735, 94.24170251997542, 94.37231100184388, 94.36462814996926, 94.39920098340504, 94.25706822372464, 94.43761524277812, 94.50676090964966, 94.53749231714812, 94.40304240934235, 94.40688383527966, 94.24554394591273, 94.41072526121697, 94.35310387215735, 94.45298094652735, 94.41072526121697, 94.36846957590657, 94.35310387215735, 94.34926244622004, 94.48755377996312, 94.30316533497235, 94.5259680393362, 94.54133374308543, 94.36846957590657, 94.4299323909035, 94.4299323909035, 94.34157959434542, 94.45682237246466, 94.36462814996926, 94.5259680393362, 94.38383527965581, 94.34542102028273, 94.43761524277812, 94.35694529809466, 94.34157959434542, 94.35310387215735, 94.39920098340504, 94.3761524277812, 94.44913952059004, 94.39920098340504, 94.25706822372464, 94.40688383527966, 94.40304240934235, 94.53749231714812, 94.49139520590043, 94.39920098340504]\n",
        "train_loss_list_1e2 = [1.5527255651427478, 0.7627066397246952, 0.6569193178399145, 0.5921073180388629, 0.546498658453546, 0.5105415238597528, 0.47226018775607836, 0.45309981219167633, 0.43924911138488026, 0.43100208492298436, 0.4212728382610693, 0.41300465610776815, 0.4045538008213043, 0.4054832284043475, 0.3981784124603763, 0.3888350862070797, 0.39557689973493904, 0.3849596505565695, 0.38653959386393955, 0.3762374546146651, 0.3782367709529432, 0.378456503718203, 0.3755849917245105, 0.3749620059560631, 0.36809034039819144, 0.36870617429576913, 0.3730806762448494, 0.36712608452088785, 0.36660385567967485, 0.36959863246133334, 0.3632444916217308, 0.3644418810118182, 0.36485538633696757, 0.36698362255484107, 0.3613756539295036, 0.35718589920177046, 0.36429271227137505, 0.36132887983063694, 0.3543453055022532, 0.3603528007097684, 0.3564891892720044, 0.35404422895372073, 0.35419524224793036, 0.35227534304143293, 0.3567796440790016, 0.3542361634614345, 0.35447870001075715, 0.34879108730370434, 0.3530310347958955, 0.3487641865887293, 0.3490374646778029, 0.34873911532444685, 0.34579125224413265, 0.3513515398592807, 0.3442573002602673, 0.3463528130435685, 0.3472748042606726, 0.3470753237483947, 0.34206526155071204, 0.3467505073482751, 0.34544345078268024, 0.3452807960067661, 0.34138486039670823, 0.3389452373997629, 0.3426176652029601, 0.34132564726076153, 0.3424493167296981, 0.3418385875095843, 0.3418787742856395, 0.34008422926027926, 0.339485853748916, 0.33518301576456727, 0.3393149601653985, 0.34077820496830513, 0.33797502247137107, 0.3379082687017394, 0.33854370699503883, 0.34068398940853956, 0.3345345121815922, 0.3382403756464077, 0.3322211721644492, 0.3343696817957612, 0.33672725802999204, 0.33137381682551004, 0.33107420422520417, 0.33307342098011233, 0.3294808776559545, 0.33175269314428657, 0.33532339277952344, 0.3320228717514493, 0.3307226697200036, 0.32816360587996196, 0.32942063278622097, 0.32721746008247543, 0.3277757565584286, 0.3272739693643601, 0.32677768360631576, 0.32788037005963366, 0.3252505469855254, 0.323792631427447, 0.3279054834025339, 0.324119375973213, 0.3267514456093796, 0.31973684213671905, 0.3212464982416572, 0.3202035142800349, 0.32931598875580764, 0.32346599341084964, 0.32207406047721543, 0.3172987587245176, 0.3217875505204446, 0.31829690812079886, 0.31830294115271995, 0.31893069468701113, 0.31877699518591407, 0.312669252032833, 0.31463882803593873, 0.31603938769195783, 0.3125667206155575, 0.3170066020756879, 0.31546638635438956, 0.31003526991944974, 0.31431218396195876, 0.3105714437034395, 0.30887893088626345, 0.3134051806881499, 0.31058979543243964, 0.30319389363695287, 0.3121243864701692, 0.3095417679325352, 0.30828995017339866, 0.3101163512565256, 0.3107531458381715, 0.3027265773717627, 0.3056968258325323, 0.3024863699668145, 0.30128581620005734, 0.3016197922107004, 0.299526641968143, 0.2986661980387964, 0.2984643379645296, 0.29741050064725283, 0.2990728767745217, 0.2990959255191369, 0.29619891364076917, 0.29736113919798274, 0.2955238256593384, 0.29230425500772833, 0.2962869802706933, 0.29052087436442775, 0.29694146858805887, 0.29178089206296254, 0.29490946855163835, 0.29239766099830955, 0.2924875763212116, 0.28727431777046947, 0.2880558754168552, 0.2857201242753807, 0.2816028292909224, 0.2807034290014567, 0.28417113209319955, 0.28439420837212387, 0.2830575307008374, 0.2813569333777841, 0.27838333346817873, 0.27398664932344663, 0.2771222206958264, 0.27646606219654807, 0.2802742558927717, 0.2772935264034646, 0.2740311811004228, 0.2723778088483707, 0.2723173482752428, 0.2726296799858088, 0.27150820464218856, 0.26980353556674347, 0.26999763235813234, 0.271178680888521, 0.2675590464052792, 0.2703916107816748, 0.266955815921954, 0.2677417722982443, 0.2664734733096629, 0.2616598152007837, 0.2618987378348826, 0.26313713015256535, 0.2639927453182254, 0.259921546025974, 0.258447800689758, 0.25932324954326236, 0.25735331044045245, 0.2553924481841284, 0.2541026953193877, 0.25215241785456494, 0.2530908481013484, 0.25340088620418455, 0.25041529901709336, 0.2508946477478436, 0.2468922998766266, 0.2466819979311005, 0.24553922820220472, 0.24616448865915702, 0.24352020812713035, 0.245112945454392, 0.24095632723434185, 0.23944184220418696, 0.2382999281007746, 0.23802665427206007, 0.23865492385496614, 0.23545282233132903, 0.2377435686142464, 0.23764096109605418, 0.2318824281900879, 0.23145699398062095, 0.23044203516590564, 0.22966974194295361, 0.2281639202823484, 0.22935683029857754, 0.2232396638974911, 0.22491911413062232, 0.22704380019731962, 0.2246162830410288, 0.22387785173770858, 0.22307075927574138, 0.21958557272022008, 0.21611802863477045, 0.2176091348737236, 0.21643275698914438, 0.21815684972820568, 0.21690630444343173, 0.2144953469596904, 0.21045533636317343, 0.21018598590116838, 0.20738654356540703, 0.21073267158615558, 0.20730509647347417, 0.20378858273026096, 0.2044903948034859, 0.20360999324215137, 0.1994028746233723, 0.20446652236588925, 0.20125386017932478, 0.19629940685379474, 0.19656375153520242, 0.19888904985535114, 0.19253174583036403, 0.19341871907636726, 0.1911584874236487, 0.19066347582593843, 0.19120384700253082, 0.18559699137076777, 0.1865291543365494, 0.18703436189227635, 0.18597213580679442, 0.1833313486076952, 0.18305706753721082, 0.1841687772897524, 0.18172660851623954, 0.17979243877255496, 0.1783425477017878, 0.17759921925702715, 0.17537852715912874, 0.1756007778571873, 0.17712873846373262, 0.1747176744104401, 0.17266693390239546, 0.16993015551268248, 0.17234542241022194, 0.17027409417115577, 0.17333801079612116, 0.16846228924991316, 0.16776102249255673, 0.17062995226442976, 0.169317345773463, 0.16778956143675136, 0.16644412503252184, 0.16641084437932424, 0.16423629589762467, 0.16514317738775638, 0.16319842011665264, 0.16312887173317636, 0.16379846247836827, 0.16102075314295647, 0.16114530194459892, 0.16031344735687017, 0.1607335551180006, 0.1630021522381926, 0.1587733793246552, 0.16005276916876718, 0.15837314123222176, 0.1588358394296835, 0.1603796430699386, 0.16015164662142434, 0.15751453530457285, 0.15853346335047952, 0.15670412618333732, 0.16078843703357185, 0.15981154410335108, 0.1568133581258094, 0.1594764508286789]\n",
        "train_acc_list_1e2 = [46.18951826363155, 75.3859184753838, 79.5214399152991, 82.13446267866595, 83.8369507676019, 84.98676548438327, 86.23186871360508, 86.93276866066702, 87.21651667548967, 87.58284806776072, 87.91953414505029, 88.02541026998412, 88.43620963472736, 88.48914769719428, 88.59925886712546, 88.89994706193754, 88.69666490206458, 89.00582318687135, 88.91053467443092, 89.20910534674431, 89.17734250926416, 89.13075701429328, 89.19428268925357, 89.38909475913182, 89.55637903652726, 89.499205929063, 89.42721016410799, 89.4907358390683, 89.41450502911593, 89.40603493912123, 89.69401799894123, 89.6156696664902, 89.5436739015352, 89.5796717840127, 89.7427210164108, 89.89306511381683, 89.7511911064055, 89.74483853890948, 89.93965060878772, 89.74060349391212, 89.93118051879301, 89.99682371625198, 89.97564849126522, 89.99258867125464, 89.88459502382213, 90.10693488618317, 90.01588141874008, 90.13234515616729, 89.9777660137639, 90.18316569613552, 90.14716781365802, 90.19163578613023, 90.26151402858656, 90.0582318687136, 90.34409740603493, 90.23186871360508, 90.30174695606141, 90.29539438856538, 90.37162519851773, 90.16410799364743, 90.25092641609317, 90.20434092112228, 90.40338803599788, 90.49867654843833, 90.26151402858656, 90.33774483853891, 90.25092641609317, 90.47750132345156, 90.31021704605611, 90.4372683959767, 90.39491794600318, 90.67866596082584, 90.47114875595553, 90.35044997353097, 90.40550555849656, 90.57278983589201, 90.46691371095818, 90.46056114346214, 90.64902064584436, 90.43091582848068, 90.65749073583906, 90.53890947591319, 90.6003176283748, 90.6807834833245, 90.61302276336686, 90.65325569084172, 90.62996294335628, 90.7993647432504, 90.58337744838539, 90.66807834833246, 90.716781365802, 90.78242456326099, 90.7993647432504, 90.81842244573849, 90.94123875066173, 90.78454208575967, 90.83748014822658, 90.72313393329804, 90.87347803070408, 90.8628904182107, 90.74642668078349, 91.11064055055584, 90.8798305982001, 91.0619375330863, 90.98570672313393, 91.02805717310747, 90.86924298570672, 90.98782424563261, 91.01535203811541, 91.1784012705135, 90.95606140815246, 91.12122816304924, 91.19745897300159, 91.08523028057174, 91.12758073054526, 91.27157226045527, 91.08523028057174, 91.14452091053468, 91.28851244044468, 91.07676019057702, 91.21863419798835, 91.36897829539438, 91.16146109052409, 91.29486500794071, 91.27368978295394, 91.19534145050291, 91.23133933298041, 91.56167284277396, 91.24827951296983, 91.33933298041292, 91.43250397035469, 91.41556379036527, 91.32027527792482, 91.68025410269983, 91.61672842773955, 91.68448914769719, 91.64002117522499, 91.61249338274219, 91.63790365272631, 91.7564849126522, 91.73954473266278, 91.7289571201694, 91.77977766013764, 91.66966649020645, 91.83059820010588, 91.80518793012176, 91.76919004764426, 91.96823716251986, 91.82636315510852, 92.06776071995765, 91.928004235045, 91.95129698253044, 91.87294865007941, 91.93435680254103, 91.9724722075172, 92.0635256749603, 92.14822657490735, 92.10164107993647, 92.33245103229221, 92.26257278983589, 92.11858125992589, 92.11858125992589, 92.2795129698253, 92.27527792482795, 92.4955002646903, 92.51032292218105, 92.36633139227104, 92.41926945473796, 92.2435150873478, 92.47220751720486, 92.55690841715193, 92.66913710958178, 92.55267337215457, 92.52726310217047, 92.55902593965061, 92.69242985706722, 92.6945473795659, 92.57384859714135, 92.76019057702489, 92.68395976707252, 92.78348332451033, 92.67972472207518, 92.69878242456326, 92.93170989941768, 92.92535733192165, 92.81948120698783, 92.6945473795659, 92.91688724192694, 92.95288512440445, 93.0291159343568, 93.07146638433034, 93.11805187930122, 93.10322922181048, 93.13499205929062, 93.12652196929592, 93.19216516675489, 93.25145579671783, 93.10958178930652, 93.24510322922181, 93.37427210164108, 93.38485971413446, 93.37003705664372, 93.54155637903652, 93.41450502911593, 93.60084700899947, 93.55214399152992, 93.7067231339333, 93.63472736897829, 93.67284277395447, 93.7702488088936, 93.63260984647962, 93.65590259396507, 93.86341979883537, 93.8147167813658, 93.86130227633669, 93.92694547379566, 94.01164637374272, 94.00952885124404, 94.08364213869773, 94.02646903123346, 93.92482795129698, 94.01799894123874, 94.11752249867655, 94.21069348861832, 94.17046056114346, 94.36315510852303, 94.39068290100582, 94.32292218104817, 94.20434092112228, 94.24669137109582, 94.3928004235045, 94.56643726839597, 94.54102699841185, 94.59820010587613, 94.54102699841185, 94.72101641079936, 94.72948650079407, 94.6998411858126, 94.67654843832716, 94.84171519322393, 94.716781365802, 94.9348861831657, 94.98147167813659, 94.98147167813659, 94.87347803070408, 95.05558496559026, 95.16569613552144, 95.06405505558496, 95.21228163049233, 95.14875595553202, 95.23557437797777, 95.23980942297511, 95.30333509793542, 95.25886712546321, 95.39650608787719, 95.40921122286925, 95.44944415034409, 95.40497617787189, 95.54261514028586, 95.56802541026998, 95.59131815775542, 95.58920063525674, 95.61037586024352, 95.6019057702488, 95.64637374272101, 95.76707252514558, 95.84330333509793, 95.68872419269455, 95.80518793012176, 95.64002117522499, 95.86659608258337, 95.90047644256221, 95.7924827951297, 95.80730545262044, 95.92165166754897, 95.91318157755425, 95.91741662255161, 95.99576495500264, 95.9364743250397, 96.02117522498676, 96.10587612493383, 95.99576495500264, 96.09105346744309, 96.06140815246162, 96.1905770248809, 96.06564319745897, 95.94070937003706, 96.08893594494441, 96.11858125992589, 96.20963472736898, 96.23716251985178, 96.15669666490207, 96.11858125992589, 96.215987294865, 96.13340391741663, 96.22445738485972, 96.09105346744309, 96.1630492323981, 96.2519851773425, 96.12916887241927]\n",
        "test_loss_list_1e2 = [1.3247391093595355, 1.4372946111594929, 1.221768849620632, 1.04543336437029, 0.5333422891357366, 0.5233590896515286, 0.49802757405182896, 0.527402958010926, 0.4824988966186841, 0.4684229656761768, 0.49865198369119684, 0.47028423013056025, 0.4917339387477613, 0.4712933559774184, 0.4579176530241966, 0.4159662657800843, 0.4274352864891875, 0.4261161432254548, 0.4882675524727971, 0.4069283462914766, 0.5291290985895138, 0.43474191529493705, 0.4278999815384547, 0.39875933359943183, 0.4106285313473028, 0.43170712793282434, 0.41362819207065243, 0.3644701678524999, 0.3623922984389698, 0.40609714147799153, 0.4058787767939708, 0.3751636910087922, 0.4418983731199713, 0.4179941722575356, 0.41956588603994427, 0.46951715706610214, 0.3965024639140157, 0.5922169619623352, 0.40990259790537403, 0.4151836939009966, 0.42209712372106667, 0.41093988658166397, 0.35882656948239194, 0.4029843812333603, 0.43423887977705283, 0.38498282746649254, 0.39403144240963695, 0.4007577213148276, 0.3736706005007613, 0.37586646884971975, 0.3973163742499024, 0.6469484598321074, 0.40791156398607237, 0.3950306963832939, 0.3600991340536697, 0.3767053528013183, 0.3982468478965993, 0.4106949169700052, 0.43118151138518374, 0.38861456491491375, 0.3672837637657044, 0.39455970508210797, 0.39955709195312333, 0.4675620217241493, 0.4306795852733593, 0.36596085978489296, 0.35671870439660314, 0.36200251970805375, 0.3765243383160993, 0.34345025160149034, 0.3757677159326918, 0.5893007164784506, 0.40471649535146415, 0.3984757918937534, 0.4169327455262343, 0.5200155050146813, 0.3654365451896892, 0.3924478087939468, 0.35877613043960405, 0.5022338476835513, 0.38222038358742116, 0.3505580706485346, 0.3415709840608578, 0.3706067300894681, 0.377026793492191, 0.35964643889490294, 0.39072127464939566, 0.4306023905382437, 0.3463652852703543, 0.4006179951715703, 0.3697551385883023, 0.3627208395331514, 0.3898671181032471, 0.4780514804171581, 0.3634938892780566, 0.5188167922052682, 0.3604150237698181, 0.35250089288342235, 0.3252626089196579, 0.3479796185651246, 0.3554346494961019, 0.3606822587111417, 0.3899602895127792, 0.40885376608839225, 0.3573276655492829, 0.350634348816147, 0.3364679439395082, 0.3656962395590894, 0.5394042597389689, 0.3279435099193863, 0.3306477459184095, 0.3268139467519872, 0.36625869447986287, 0.39838596696362777, 0.381319522857666, 0.351467617002188, 0.33824310422528026, 0.34666523709893227, 0.45219722740790425, 0.31658349412621234, 0.3582795552325015, 0.3590943407924736, 0.34482148333507423, 0.33396274413840443, 0.34916307185502615, 0.4292598191429587, 0.32467805952125905, 0.3357189829443015, 0.4115390076356776, 0.31629938288938764, 0.39742738264156324, 0.3100176707786672, 0.30909344707341757, 0.412040871074971, 0.31328197568655014, 0.31859667730682034, 0.3150135146490499, 0.31053604923334777, 0.3349029737360337, 0.5725868261035751, 0.33193637963895706, 0.3257182556770596, 0.3234951227319007, 0.33131444804808674, 0.32013773888933894, 0.3087677399755693, 0.31707401840271904, 0.3055169947226258, 0.30974703742300763, 0.32284443306864474, 0.3504650902514364, 0.30639906819252405, 0.32764800407868977, 0.34437243019541103, 0.3272252177783087, 0.3307260185041848, 0.30390654708824905, 0.30922777417535874, 0.29515569072728065, 0.30915866412368476, 0.30551093005958724, 0.3073367017741297, 0.30962466521590365, 0.31125049318606945, 0.3018519765898293, 0.3057772036568791, 0.30710070960077585, 0.3110234323231613, 0.3019946907080856, 0.30974323379204555, 0.3310978485673082, 0.3041924396712406, 0.28668475308108565, 0.29636092403647946, 0.3006317568029843, 0.2844046983940929, 0.35042893543255094, 0.2765379489636889, 0.2871919143579754, 0.30185085483918, 0.2806559734265594, 0.29328389626507667, 0.2850948043313681, 0.34038400401671726, 0.29658816148545225, 0.2900318577885628, 0.3164708414936767, 0.2704349430460556, 0.28909379587161776, 0.2766348912070195, 0.2954838537410194, 0.2803557366132736, 0.2698567771151954, 0.28018723702167764, 0.2962662287275581, 0.27532919876131356, 0.2694316993595338, 0.2662475112022138, 0.28343176304855766, 0.29671202501391664, 0.27179979993139997, 0.28223006507637455, 0.2900701365707552, 0.2721700471844159, 0.26092747346881556, 0.26539998760848654, 0.26519521442698496, 0.27638812151317504, 0.2700666151575598, 0.2631940585652403, 0.2667978351533997, 0.2727283788662331, 0.26423550035585375, 0.2590555296297751, 0.27035205230555115, 0.26533449850246016, 0.2622304866273029, 0.2551094347282368, 0.26004181678096455, 0.27162871688750445, 0.24523562127176454, 0.2569774056912637, 0.251955021099717, 0.25539296215363577, 0.25087627918258604, 0.25988455777805225, 0.26432519333035337, 0.25944741738631444, 0.2533169457068046, 0.2480630805606351, 0.2482597777145166, 0.24768747327228388, 0.2565581485921261, 0.27113300364683657, 0.24470692774390473, 0.24457374613220786, 0.24245896723632718, 0.24782768728248045, 0.24563301789263883, 0.25142964742639484, 0.24496393921036347, 0.24507040318612958, 0.2429210834716465, 0.24630526723522767, 0.24146442790972253, 0.24734521755839095, 0.24448578213067615, 0.24797514888147512, 0.24284489299444592, 0.24469150555338345, 0.24450236584479904, 0.24315517479736432, 0.2357017386741206, 0.24220527233738526, 0.23520723346839933, 0.23920954018831253, 0.23878256140240267, 0.23733027867388493, 0.23719553802819812, 0.24080834782444963, 0.23478803265036322, 0.23879089633769848, 0.23768455305081956, 0.24077472737168565, 0.23638445438415395, 0.23551501680676842, 0.24104665650748738, 0.2356582859202343, 0.23666301727587102, 0.23688774943059565, 0.23403703056129754, 0.235051355598604, 0.23781126664549695, 0.23505273757173734, 0.2296296937719864, 0.23563272381822267, 0.2333506334609553, 0.23327350784458367, 0.2340209804037038, 0.23465112187698775, 0.23493007486503498, 0.2349644734652019, 0.2349888317506103, 0.2317731349783785, 0.2339920843129649, 0.23434527083208748, 0.23276801688560084, 0.23693077834140436, 0.2345955311959865, 0.23683660813406401, 0.23327333925693644, 0.23412100777176081, 0.23264528898631825, 0.23628339854379496, 0.23472745530307293, 0.23727154311742268, 0.23173269472431904, 0.23639510246906795, 0.23554416570593328, 0.23380914703011513]\n",
        "test_acc_list_1e2 = [56.81084818684696, 54.15258143822987, 59.11954517516902, 67.14428395820529, 85.01843884449907, 85.47172710510141, 85.13368162261831, 84.16564228641671, 86.33988936693301, 86.0978795328826, 85.54087277197296, 86.13245236631838, 85.2681315304241, 86.41287645974185, 86.75476336816226, 87.79963122311001, 87.76121696373694, 88.2221880762139, 85.62538414259373, 88.20298094652735, 84.53826060233558, 87.7458512599877, 87.51920712968654, 89.18638598647819, 88.2721266133989, 87.17732022126613, 87.8418869084204, 89.44376152427782, 89.52827289489859, 88.41425937307929, 88.28749231714812, 89.37461585740627, 87.20421020282728, 87.83036263060848, 87.66133988936693, 86.71250768285188, 88.33358942839583, 82.68669330055316, 88.54870928088506, 87.51152427781193, 87.83420405654579, 88.10310387215735, 89.62814996926859, 88.24523663183774, 87.93792255685311, 89.12492317148126, 88.72157344806392, 88.62553779963122, 89.69345421020283, 89.18638598647819, 88.24907805777505, 80.7659803318992, 88.48724646588813, 88.57559926244622, 89.98924400737553, 89.1479717271051, 88.84449907805778, 88.09157959434542, 87.25799016594961, 88.89059618930547, 89.37461585740627, 88.57944068838353, 88.48724646588813, 87.0582360172096, 87.6421327596804, 89.19406883835279, 90.0660725261217, 89.82022126613398, 88.93669330055316, 90.22357098955132, 89.32467732022127, 83.88137676705593, 88.01090964966195, 89.02120467117393, 88.24523663183774, 84.43070067609096, 89.69345421020283, 88.7062077443147, 90.0660725261217, 85.70221266133989, 88.79840196681008, 89.91241548862938, 90.34265519360787, 89.30931161647203, 89.48217578365089, 90.0660725261217, 88.67163491087892, 87.88414259373079, 90.2158881376767, 88.36432083589429, 89.48217578365089, 89.68961278426552, 88.49492931776275, 86.10556238475722, 89.4860172095882, 85.2220344191764, 89.60510141364475, 89.93930547019053, 90.77289489858636, 90.18131530424094, 89.85479409956976, 89.64351567301783, 88.859864781807, 87.96481253841426, 89.85095267363245, 89.98156115550093, 90.58082360172097, 89.44760295021511, 83.9582052858021, 90.56545789797173, 90.71527350952674, 90.63460356484327, 89.3899815611555, 88.23755377996312, 88.84449907805778, 89.64735709895513, 90.29655808236018, 90.08143822987093, 86.79317762753534, 91.07636754763368, 89.51290719114935, 89.83942839582053, 90.20052243392747, 90.61155500921942, 90.20436385986478, 87.51920712968654, 90.85740626920713, 90.80746773202213, 88.31822372464659, 91.16087891825445, 88.64090350338046, 91.20697602950216, 91.18776889981561, 88.29901659496005, 91.08789182544561, 91.02258758451137, 90.99569760295022, 91.32221880762138, 90.33497234173325, 83.41272280270437, 90.75368776889981, 90.65381069452981, 90.89966195451751, 90.79978488014751, 90.9419176398279, 91.30301167793485, 91.03027043638599, 91.64874001229256, 91.40288875230486, 90.91886908420406, 90.13905961893055, 91.13014751075599, 90.70374923171481, 90.13905961893055, 90.51551936078673, 90.67685925015365, 91.379840196681, 91.22234173325138, 91.6180086047941, 91.24923171481254, 91.329901659496, 91.28380454824831, 91.45282728948986, 90.97264904732637, 91.51044867854948, 91.3759987707437, 91.38752304855562, 91.16856177012907, 91.57191149354641, 91.18776889981561, 90.64228641671788, 91.29917025199754, 91.88306699446835, 91.58343577135832, 91.50660725261217, 91.91763982790411, 89.92393976644131, 92.40550092194222, 91.87922556853104, 91.72940995697603, 92.17885679164105, 92.0098340503995, 91.98294406883835, 90.3119237861094, 91.68715427166565, 91.76782421634911, 91.08020897357099, 92.5361094038107, 91.83312845728334, 92.08282114320836, 91.83696988322065, 92.11739397664412, 92.65903503380454, 92.23263675476336, 91.71788567916411, 92.31714812538414, 92.50921942224954, 92.65519360786725, 92.14044253226798, 91.75245851259987, 92.42086662569146, 92.15964966195452, 91.71788567916411, 92.55531653349723, 92.84726490473264, 92.57452366318377, 92.5860479409957, 92.32483097725876, 92.52842655193608, 92.68976644130301, 92.58220651505839, 92.440073755378, 92.74738783036263, 92.8242163491088, 92.59757221880763, 92.5361094038107, 92.7819606637984, 92.86647203441917, 92.79732636754764, 92.46696373693915, 93.28134603564843, 92.98555623847572, 93.04701905347265, 93.26213890596189, 93.1737861094038, 92.78580208973571, 92.80885064535956, 92.91256914566686, 93.11616472034419, 93.13153042409343, 93.15457897971727, 93.3581745543946, 92.99323909035034, 92.47848801475108, 93.25061462814998, 93.39658881376766, 93.43116164720344, 93.28134603564843, 93.37738168408113, 93.15457897971727, 93.24677320221267, 93.23524892440074, 93.30055316533497, 93.2160417947142, 93.44652735095268, 93.26982175783651, 93.39658881376766, 93.31976029502151, 93.51567301782421, 93.25061462814998, 93.42347879532882, 93.29671173939767, 93.6539643515673, 93.4081130915796, 93.72311001843885, 93.65012292563, 93.53103872157345, 93.37738168408113, 93.57329440688383, 93.62707437000614, 93.6578057775046, 93.63475722188076, 93.63091579594345, 93.61170866625692, 93.63475722188076, 93.69237861094038, 93.55024585125999, 93.75768285187462, 93.71158574062692, 93.66933005531654, 93.85755992624462, 93.71158574062692, 93.58097725875845, 93.5579287031346, 93.7922556853104, 93.61939151813154, 93.799938537185, 93.8921327596804, 93.75, 93.88829133374308, 93.88444990780577, 93.77688998156115, 93.77304855562384, 93.87676705593117, 93.83066994468346, 93.81146281499693, 93.97280270436386, 93.72311001843885, 93.75, 93.70390288875231, 93.76920712968654, 93.75768285187462, 93.82682851874615, 93.78073140749846, 93.88829133374308, 93.74231714812538, 93.87676705593117, 93.81530424093424, 93.80762138905962, 93.8460356484327]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gl0LsIRRubRI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list_5e4)), train_acc_list_5e4, 'b')\n",
        "plt.plot(range(len(train_acc_list_1e2)), train_acc_list_1e2, 'r')\n",
        "#plt.plot(range(len(train_acc_list_step)), train_acc_list_step, 'g')\n",
        "#plt.plot(range(len(train_acc_list_linear)), train_acc_list_linear, 'y')\n",
        "#plt.plot(range(len(train_acc_list_exp)), train_acc_list_exp, 'purple')\n",
        "\n",
        "plt.plot(range(len(test_acc_list_5e4)), test_acc_list_5e4, color='b', linestyle='--')\n",
        "plt.plot(range(len(test_acc_list_1e2)), test_acc_list_1e2,color='r', linestyle='--')\n",
        "#plt.plot(range(len(test_acc_list_step)), test_acc_list_step, color='g', linestyle='--')\n",
        "#plt.plot(range(len(test_acc_list_linear)), test_acc_list_linear, color='y', linestyle='--')\n",
        "#plt.plot(range(len(test_acc_list_exp)), test_acc_list_exp, color='purple', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "#plt.ylim([85, 101])\n",
        "plt.title(\"Combined accuracy\")\n",
        "plt.legend(['train with weight decay coefficient = 5e-4', 'train with weight decay coefficient = 1e-2',\n",
        "            'test with weight decay coefficient = 5e-4', 'test with weight decay coefficient = 1e-2'])\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "CePsAT_WuePm",
        "outputId": "be8987da-5548-4826-cc2c-e0001765f365"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list_5e4)), train_loss_list_5e4, 'b')\n",
        "plt.plot(range(len(train_loss_list_1e2)), train_loss_list_1e2, 'r')\n",
        "#plt.plot(range(len(train_loss_list_step)), train_loss_list_step, 'g')\n",
        "#plt.plot(range(len(train_loss_list_linear)), train_loss_list_linear, 'y')\n",
        "#plt.plot(range(len(train_loss_list_exp)), train_loss_list_exp, 'purple')\n",
        "\n",
        "plt.plot(range(len(test_loss_list_5e4)), test_loss_list_5e4, color='b', linestyle='--')\n",
        "plt.plot(range(len(test_loss_list_1e2)), test_loss_list_1e2,color='r', linestyle='--')\n",
        "#plt.plot(range(len(test_loss_list_step)), test_loss_list_step, color='g', linestyle='--')\n",
        "#plt.plot(range(len(test_loss_list_linear)), test_loss_list_linear, color='y', linestyle='--')\n",
        "#plt.plot(range(len(test_loss_list_exp)), test_loss_list_exp, color='purple', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "#plt.ylim([85, 101])\n",
        "plt.title(\"Combined loss\")\n",
        "plt.legend(['train with weight decay coefficient = 5e-4', 'train with weight decay coefficient = 1e-2',\n",
        "            'test with weight decay coefficient = 5e-4', 'test with weight decay coefficient = 1e-2'])\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "b0GRGij1uf-2",
        "outputId": "e59ead0d-8cff-4c0b-9069-9ce75227f096"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}