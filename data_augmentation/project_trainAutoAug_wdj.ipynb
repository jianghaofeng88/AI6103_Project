{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "75eb9200e74b4078bd8e8e09a491b8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd0903d776cf44cab69229fbffb6ec2b",
              "IPY_MODEL_5914d62d1ef9442598e9cc900843613b",
              "IPY_MODEL_e7b4a8d1bac34d28b2a6ad0132130b9e"
            ],
            "layout": "IPY_MODEL_1f207da1f7ed4abd97dcdeca163cf629"
          }
        },
        "bd0903d776cf44cab69229fbffb6ec2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e431e4aa5854581897368c85b90e446",
            "placeholder": "​",
            "style": "IPY_MODEL_e07d2e319e3b46d6bf81625e40cd4968",
            "value": "100%"
          }
        },
        "5914d62d1ef9442598e9cc900843613b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce3145d3030f422c8712b33a37f18258",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35ad9df8dc23478eb7fd6f4872edd6bd",
            "value": 182040794
          }
        },
        "e7b4a8d1bac34d28b2a6ad0132130b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60b2713c3a304ff898c64b6acef8764a",
            "placeholder": "​",
            "style": "IPY_MODEL_9c367d63be624821bf33043d47157c98",
            "value": " 182040794/182040794 [00:02&lt;00:00, 107525993.25it/s]"
          }
        },
        "1f207da1f7ed4abd97dcdeca163cf629": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e431e4aa5854581897368c85b90e446": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e07d2e319e3b46d6bf81625e40cd4968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce3145d3030f422c8712b33a37f18258": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35ad9df8dc23478eb7fd6f4872edd6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60b2713c3a304ff898c64b6acef8764a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c367d63be624821bf33043d47157c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "621798e5c7a94d6c88056e55e39d8a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13f12ebb293746d6a4c26028400b20b6",
              "IPY_MODEL_3422efda4b4c4f0899ce3fd29aeb9366",
              "IPY_MODEL_ccc04cd6ec22424b911e8b5fb03e4945"
            ],
            "layout": "IPY_MODEL_b76bbeb58add499698b313f7e6e52e05"
          }
        },
        "13f12ebb293746d6a4c26028400b20b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a9fdf30c0334c95b9f7a0c6a8eaeca9",
            "placeholder": "​",
            "style": "IPY_MODEL_a8f7ee6088b64b60a3feea8a0f418d31",
            "value": "100%"
          }
        },
        "3422efda4b4c4f0899ce3fd29aeb9366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16b8d466ac734a0eb4bbdc556731e4be",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_848797e922614d87a0b042129fe37602",
            "value": 64275384
          }
        },
        "ccc04cd6ec22424b911e8b5fb03e4945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e2d20821a144c9cb7d3b5747f7afde3",
            "placeholder": "​",
            "style": "IPY_MODEL_3107253601a3425fa818051daf26278d",
            "value": " 64275384/64275384 [00:00&lt;00:00, 93889388.69it/s]"
          }
        },
        "b76bbeb58add499698b313f7e6e52e05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a9fdf30c0334c95b9f7a0c6a8eaeca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f7ee6088b64b60a3feea8a0f418d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16b8d466ac734a0eb4bbdc556731e4be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "848797e922614d87a0b042129fe37602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e2d20821a144c9cb7d3b5747f7afde3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3107253601a3425fa818051daf26278d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHuJUs3ir2g2"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.SVHN),\n",
        "    transforms.ToTensor(),\n",
        "    \n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "75eb9200e74b4078bd8e8e09a491b8c0",
            "bd0903d776cf44cab69229fbffb6ec2b",
            "5914d62d1ef9442598e9cc900843613b",
            "e7b4a8d1bac34d28b2a6ad0132130b9e",
            "1f207da1f7ed4abd97dcdeca163cf629",
            "3e431e4aa5854581897368c85b90e446",
            "e07d2e319e3b46d6bf81625e40cd4968",
            "ce3145d3030f422c8712b33a37f18258",
            "35ad9df8dc23478eb7fd6f4872edd6bd",
            "60b2713c3a304ff898c64b6acef8764a",
            "9c367d63be624821bf33043d47157c98",
            "621798e5c7a94d6c88056e55e39d8a7c",
            "13f12ebb293746d6a4c26028400b20b6",
            "3422efda4b4c4f0899ce3fd29aeb9366",
            "ccc04cd6ec22424b911e8b5fb03e4945",
            "b76bbeb58add499698b313f7e6e52e05",
            "7a9fdf30c0334c95b9f7a0c6a8eaeca9",
            "a8f7ee6088b64b60a3feea8a0f418d31",
            "16b8d466ac734a0eb4bbdc556731e4be",
            "848797e922614d87a0b042129fe37602",
            "7e2d20821a144c9cb7d3b5747f7afde3",
            "3107253601a3425fa818051daf26278d"
          ]
        },
        "id": "Ow5ZNogRuEpq",
        "outputId": "72982738-e633-41ed-c553-38bba6207505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75eb9200e74b4078bd8e8e09a491b8c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "621798e5c7a94d6c88056e55e39d8a7c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))\n",
        "print(trainset[4][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExjAMaRquGPp",
        "outputId": "86b8e9dc-c4f5-4bc8-f9c4-5af47486501d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "metadata": {
        "id": "PoGC4Zo7uIX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "metadata": {
        "id": "HLTkfp-wuKXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x4c_HGo6uM22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "metadata": {
        "id": "KAh57cWQuOmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "I8CXMbsguP81",
        "outputId": "fad37cd9-6d0d-4165-d3e4-e0fe0d587f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSHmDHpkuRe5",
        "outputId": "e81c02c3-274e-4f0b-f4fc-8ce58be1d4d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFogi3EguTDh",
        "outputId": "be0d4a46-94d4-4594-c52b-2fae841c48e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3374, accuracy : 17.53\n",
            "iteration : 100, loss : 2.2384, accuracy : 20.60\n",
            "iteration : 150, loss : 2.0340, accuracy : 28.31\n",
            "iteration : 200, loss : 1.7671, accuracy : 38.18\n",
            "iteration : 250, loss : 1.5559, accuracy : 45.89\n",
            "iteration : 300, loss : 1.3921, accuracy : 51.91\n",
            "iteration : 350, loss : 1.2714, accuracy : 56.38\n",
            "Epoch :   1, training loss : 1.2340, training accuracy : 57.70, test loss : 0.8547, test accuracy : 72.76\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.4817, accuracy : 84.75\n",
            "iteration : 100, loss : 0.4713, accuracy : 85.08\n",
            "iteration : 150, loss : 0.4692, accuracy : 85.32\n",
            "iteration : 200, loss : 0.4663, accuracy : 85.49\n",
            "iteration : 250, loss : 0.4629, accuracy : 85.53\n",
            "iteration : 300, loss : 0.4582, accuracy : 85.74\n",
            "iteration : 350, loss : 0.4550, accuracy : 85.81\n",
            "Epoch :   2, training loss : 0.4539, training accuracy : 85.87, test loss : 0.6585, test accuracy : 79.31\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.4070, accuracy : 87.56\n",
            "iteration : 100, loss : 0.4147, accuracy : 87.33\n",
            "iteration : 150, loss : 0.4144, accuracy : 87.36\n",
            "iteration : 200, loss : 0.4085, accuracy : 87.39\n",
            "iteration : 250, loss : 0.4029, accuracy : 87.55\n",
            "iteration : 300, loss : 0.4028, accuracy : 87.55\n",
            "iteration : 350, loss : 0.3995, accuracy : 87.69\n",
            "Epoch :   3, training loss : 0.3989, training accuracy : 87.69, test loss : 0.4442, test accuracy : 86.37\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3800, accuracy : 88.17\n",
            "iteration : 100, loss : 0.3738, accuracy : 88.56\n",
            "iteration : 150, loss : 0.3733, accuracy : 88.67\n",
            "iteration : 200, loss : 0.3730, accuracy : 88.66\n",
            "iteration : 250, loss : 0.3736, accuracy : 88.54\n",
            "iteration : 300, loss : 0.3708, accuracy : 88.62\n",
            "iteration : 350, loss : 0.3705, accuracy : 88.70\n",
            "Epoch :   4, training loss : 0.3695, training accuracy : 88.76, test loss : 0.3682, test accuracy : 88.90\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.3472, accuracy : 89.78\n",
            "iteration : 100, loss : 0.3548, accuracy : 89.38\n",
            "iteration : 150, loss : 0.3568, accuracy : 89.19\n",
            "iteration : 200, loss : 0.3628, accuracy : 89.04\n",
            "iteration : 250, loss : 0.3617, accuracy : 89.06\n",
            "iteration : 300, loss : 0.3623, accuracy : 89.09\n",
            "iteration : 350, loss : 0.3583, accuracy : 89.16\n",
            "Epoch :   5, training loss : 0.3572, training accuracy : 89.19, test loss : 0.4941, test accuracy : 84.18\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.3349, accuracy : 89.73\n",
            "iteration : 100, loss : 0.3429, accuracy : 89.59\n",
            "iteration : 150, loss : 0.3416, accuracy : 89.70\n",
            "iteration : 200, loss : 0.3398, accuracy : 89.77\n",
            "iteration : 250, loss : 0.3401, accuracy : 89.72\n",
            "iteration : 300, loss : 0.3414, accuracy : 89.77\n",
            "iteration : 350, loss : 0.3367, accuracy : 89.96\n",
            "Epoch :   6, training loss : 0.3365, training accuracy : 89.96, test loss : 0.3691, test accuracy : 88.76\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.3046, accuracy : 90.84\n",
            "iteration : 100, loss : 0.3284, accuracy : 90.24\n",
            "iteration : 150, loss : 0.3245, accuracy : 90.36\n",
            "iteration : 200, loss : 0.3270, accuracy : 90.26\n",
            "iteration : 250, loss : 0.3255, accuracy : 90.33\n",
            "iteration : 300, loss : 0.3220, accuracy : 90.45\n",
            "iteration : 350, loss : 0.3249, accuracy : 90.38\n",
            "Epoch :   7, training loss : 0.3232, training accuracy : 90.43, test loss : 0.3590, test accuracy : 89.15\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.3226, accuracy : 90.36\n",
            "iteration : 100, loss : 0.3229, accuracy : 90.38\n",
            "iteration : 150, loss : 0.3229, accuracy : 90.23\n",
            "iteration : 200, loss : 0.3147, accuracy : 90.52\n",
            "iteration : 250, loss : 0.3171, accuracy : 90.48\n",
            "iteration : 300, loss : 0.3165, accuracy : 90.53\n",
            "iteration : 350, loss : 0.3133, accuracy : 90.63\n",
            "Epoch :   8, training loss : 0.3139, training accuracy : 90.62, test loss : 0.3806, test accuracy : 88.55\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.3209, accuracy : 90.50\n",
            "iteration : 100, loss : 0.3091, accuracy : 90.89\n",
            "iteration : 150, loss : 0.3074, accuracy : 90.98\n",
            "iteration : 200, loss : 0.3075, accuracy : 91.01\n",
            "iteration : 250, loss : 0.3043, accuracy : 91.13\n",
            "iteration : 300, loss : 0.3045, accuracy : 91.07\n",
            "iteration : 350, loss : 0.3033, accuracy : 91.05\n",
            "Epoch :   9, training loss : 0.3027, training accuracy : 91.05, test loss : 0.3423, test accuracy : 89.67\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2723, accuracy : 91.56\n",
            "iteration : 100, loss : 0.2883, accuracy : 91.36\n",
            "iteration : 150, loss : 0.2858, accuracy : 91.42\n",
            "iteration : 200, loss : 0.2833, accuracy : 91.45\n",
            "iteration : 250, loss : 0.2834, accuracy : 91.50\n",
            "iteration : 300, loss : 0.2840, accuracy : 91.48\n",
            "iteration : 350, loss : 0.2866, accuracy : 91.40\n",
            "Epoch :  10, training loss : 0.2875, training accuracy : 91.41, test loss : 0.3357, test accuracy : 90.24\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.2923, accuracy : 91.34\n",
            "iteration : 100, loss : 0.2832, accuracy : 91.44\n",
            "iteration : 150, loss : 0.2767, accuracy : 91.59\n",
            "iteration : 200, loss : 0.2810, accuracy : 91.45\n",
            "iteration : 250, loss : 0.2795, accuracy : 91.56\n",
            "iteration : 300, loss : 0.2789, accuracy : 91.59\n",
            "iteration : 350, loss : 0.2777, accuracy : 91.68\n",
            "Epoch :  11, training loss : 0.2782, training accuracy : 91.68, test loss : 0.3110, test accuracy : 90.95\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.2682, accuracy : 91.98\n",
            "iteration : 100, loss : 0.2760, accuracy : 91.80\n",
            "iteration : 150, loss : 0.2747, accuracy : 91.78\n",
            "iteration : 200, loss : 0.2732, accuracy : 91.86\n",
            "iteration : 250, loss : 0.2726, accuracy : 91.85\n",
            "iteration : 300, loss : 0.2701, accuracy : 91.95\n",
            "iteration : 350, loss : 0.2681, accuracy : 91.96\n",
            "Epoch :  12, training loss : 0.2686, training accuracy : 91.92, test loss : 0.2993, test accuracy : 90.96\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.2524, accuracy : 92.88\n",
            "iteration : 100, loss : 0.2491, accuracy : 92.68\n",
            "iteration : 150, loss : 0.2512, accuracy : 92.52\n",
            "iteration : 200, loss : 0.2539, accuracy : 92.50\n",
            "iteration : 250, loss : 0.2546, accuracy : 92.48\n",
            "iteration : 300, loss : 0.2548, accuracy : 92.49\n",
            "iteration : 350, loss : 0.2571, accuracy : 92.44\n",
            "Epoch :  13, training loss : 0.2591, training accuracy : 92.38, test loss : 0.3139, test accuracy : 90.68\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.2435, accuracy : 92.48\n",
            "iteration : 100, loss : 0.2469, accuracy : 92.43\n",
            "iteration : 150, loss : 0.2464, accuracy : 92.51\n",
            "iteration : 200, loss : 0.2483, accuracy : 92.57\n",
            "iteration : 250, loss : 0.2503, accuracy : 92.58\n",
            "iteration : 300, loss : 0.2552, accuracy : 92.46\n",
            "iteration : 350, loss : 0.2533, accuracy : 92.51\n",
            "Epoch :  14, training loss : 0.2540, training accuracy : 92.51, test loss : 0.2749, test accuracy : 91.91\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.2371, accuracy : 92.89\n",
            "iteration : 100, loss : 0.2430, accuracy : 92.66\n",
            "iteration : 150, loss : 0.2513, accuracy : 92.52\n",
            "iteration : 200, loss : 0.2472, accuracy : 92.72\n",
            "iteration : 250, loss : 0.2473, accuracy : 92.75\n",
            "iteration : 300, loss : 0.2466, accuracy : 92.83\n",
            "iteration : 350, loss : 0.2473, accuracy : 92.80\n",
            "Epoch :  15, training loss : 0.2486, training accuracy : 92.76, test loss : 0.2926, test accuracy : 91.58\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.2623, accuracy : 92.05\n",
            "iteration : 100, loss : 0.2422, accuracy : 92.73\n",
            "iteration : 150, loss : 0.2414, accuracy : 92.79\n",
            "iteration : 200, loss : 0.2495, accuracy : 92.61\n",
            "iteration : 250, loss : 0.2500, accuracy : 92.67\n",
            "iteration : 300, loss : 0.2491, accuracy : 92.71\n",
            "iteration : 350, loss : 0.2454, accuracy : 92.79\n",
            "Epoch :  16, training loss : 0.2440, training accuracy : 92.84, test loss : 0.2924, test accuracy : 91.43\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.2179, accuracy : 93.55\n",
            "iteration : 100, loss : 0.2217, accuracy : 93.35\n",
            "iteration : 150, loss : 0.2326, accuracy : 93.03\n",
            "iteration : 200, loss : 0.2335, accuracy : 93.02\n",
            "iteration : 250, loss : 0.2358, accuracy : 92.97\n",
            "iteration : 300, loss : 0.2398, accuracy : 92.93\n",
            "iteration : 350, loss : 0.2386, accuracy : 92.96\n",
            "Epoch :  17, training loss : 0.2392, training accuracy : 92.96, test loss : 0.2915, test accuracy : 91.60\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.2290, accuracy : 93.55\n",
            "iteration : 100, loss : 0.2287, accuracy : 93.46\n",
            "iteration : 150, loss : 0.2379, accuracy : 93.10\n",
            "iteration : 200, loss : 0.2351, accuracy : 93.15\n",
            "iteration : 250, loss : 0.2316, accuracy : 93.19\n",
            "iteration : 300, loss : 0.2301, accuracy : 93.21\n",
            "iteration : 350, loss : 0.2329, accuracy : 93.15\n",
            "Epoch :  18, training loss : 0.2337, training accuracy : 93.15, test loss : 0.3100, test accuracy : 91.09\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.2033, accuracy : 93.69\n",
            "iteration : 100, loss : 0.2083, accuracy : 93.86\n",
            "iteration : 150, loss : 0.2172, accuracy : 93.68\n",
            "iteration : 200, loss : 0.2215, accuracy : 93.48\n",
            "iteration : 250, loss : 0.2266, accuracy : 93.39\n",
            "iteration : 300, loss : 0.2273, accuracy : 93.39\n",
            "iteration : 350, loss : 0.2280, accuracy : 93.35\n",
            "Epoch :  19, training loss : 0.2279, training accuracy : 93.35, test loss : 0.2777, test accuracy : 91.74\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.2189, accuracy : 93.72\n",
            "iteration : 100, loss : 0.2223, accuracy : 93.66\n",
            "iteration : 150, loss : 0.2191, accuracy : 93.70\n",
            "iteration : 200, loss : 0.2250, accuracy : 93.45\n",
            "iteration : 250, loss : 0.2206, accuracy : 93.57\n",
            "iteration : 300, loss : 0.2209, accuracy : 93.52\n",
            "iteration : 350, loss : 0.2232, accuracy : 93.46\n",
            "Epoch :  20, training loss : 0.2250, training accuracy : 93.40, test loss : 0.2716, test accuracy : 92.13\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.2001, accuracy : 94.22\n",
            "iteration : 100, loss : 0.2028, accuracy : 94.25\n",
            "iteration : 150, loss : 0.2074, accuracy : 93.97\n",
            "iteration : 200, loss : 0.2171, accuracy : 93.68\n",
            "iteration : 250, loss : 0.2210, accuracy : 93.61\n",
            "iteration : 300, loss : 0.2188, accuracy : 93.67\n",
            "iteration : 350, loss : 0.2201, accuracy : 93.62\n",
            "Epoch :  21, training loss : 0.2189, training accuracy : 93.64, test loss : 0.2659, test accuracy : 92.30\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.1954, accuracy : 94.30\n",
            "iteration : 100, loss : 0.2081, accuracy : 94.07\n",
            "iteration : 150, loss : 0.2089, accuracy : 93.96\n",
            "iteration : 200, loss : 0.2099, accuracy : 93.91\n",
            "iteration : 250, loss : 0.2101, accuracy : 93.92\n",
            "iteration : 300, loss : 0.2142, accuracy : 93.83\n",
            "iteration : 350, loss : 0.2144, accuracy : 93.81\n",
            "Epoch :  22, training loss : 0.2159, training accuracy : 93.79, test loss : 0.2805, test accuracy : 91.75\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.2190, accuracy : 93.42\n",
            "iteration : 100, loss : 0.2153, accuracy : 93.44\n",
            "iteration : 150, loss : 0.2124, accuracy : 93.71\n",
            "iteration : 200, loss : 0.2123, accuracy : 93.79\n",
            "iteration : 250, loss : 0.2123, accuracy : 93.77\n",
            "iteration : 300, loss : 0.2128, accuracy : 93.73\n",
            "iteration : 350, loss : 0.2124, accuracy : 93.72\n",
            "Epoch :  23, training loss : 0.2125, training accuracy : 93.68, test loss : 0.2707, test accuracy : 92.26\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.2025, accuracy : 94.44\n",
            "iteration : 100, loss : 0.2070, accuracy : 93.99\n",
            "iteration : 150, loss : 0.2049, accuracy : 94.06\n",
            "iteration : 200, loss : 0.2058, accuracy : 94.04\n",
            "iteration : 250, loss : 0.2082, accuracy : 94.04\n",
            "iteration : 300, loss : 0.2085, accuracy : 93.99\n",
            "iteration : 350, loss : 0.2092, accuracy : 93.95\n",
            "Epoch :  24, training loss : 0.2103, training accuracy : 93.92, test loss : 0.2468, test accuracy : 92.84\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.1819, accuracy : 94.59\n",
            "iteration : 100, loss : 0.1898, accuracy : 94.49\n",
            "iteration : 150, loss : 0.1915, accuracy : 94.45\n",
            "iteration : 200, loss : 0.1966, accuracy : 94.28\n",
            "iteration : 250, loss : 0.2002, accuracy : 94.12\n",
            "iteration : 300, loss : 0.2005, accuracy : 94.12\n",
            "iteration : 350, loss : 0.2029, accuracy : 94.05\n",
            "Epoch :  25, training loss : 0.2057, training accuracy : 93.98, test loss : 0.2648, test accuracy : 92.40\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.1933, accuracy : 94.28\n",
            "iteration : 100, loss : 0.2008, accuracy : 94.12\n",
            "iteration : 150, loss : 0.2021, accuracy : 94.13\n",
            "iteration : 200, loss : 0.2021, accuracy : 94.15\n",
            "iteration : 250, loss : 0.2050, accuracy : 94.09\n",
            "iteration : 300, loss : 0.2032, accuracy : 94.09\n",
            "iteration : 350, loss : 0.2030, accuracy : 94.04\n",
            "Epoch :  26, training loss : 0.2033, training accuracy : 94.03, test loss : 0.2729, test accuracy : 92.08\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.1912, accuracy : 94.34\n",
            "iteration : 100, loss : 0.1997, accuracy : 94.06\n",
            "iteration : 150, loss : 0.2001, accuracy : 94.16\n",
            "iteration : 200, loss : 0.2015, accuracy : 94.15\n",
            "iteration : 250, loss : 0.2016, accuracy : 94.11\n",
            "iteration : 300, loss : 0.2023, accuracy : 94.10\n",
            "iteration : 350, loss : 0.2007, accuracy : 94.15\n",
            "Epoch :  27, training loss : 0.2007, training accuracy : 94.15, test loss : 0.2413, test accuracy : 93.09\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.1791, accuracy : 94.95\n",
            "iteration : 100, loss : 0.1865, accuracy : 94.73\n",
            "iteration : 150, loss : 0.1912, accuracy : 94.50\n",
            "iteration : 200, loss : 0.1973, accuracy : 94.37\n",
            "iteration : 250, loss : 0.1956, accuracy : 94.41\n",
            "iteration : 300, loss : 0.1966, accuracy : 94.42\n",
            "iteration : 350, loss : 0.1978, accuracy : 94.36\n",
            "Epoch :  28, training loss : 0.1987, training accuracy : 94.35, test loss : 0.2661, test accuracy : 92.47\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.1884, accuracy : 94.30\n",
            "iteration : 100, loss : 0.1875, accuracy : 94.40\n",
            "iteration : 150, loss : 0.1898, accuracy : 94.32\n",
            "iteration : 200, loss : 0.1910, accuracy : 94.30\n",
            "iteration : 250, loss : 0.1966, accuracy : 94.22\n",
            "iteration : 300, loss : 0.1959, accuracy : 94.31\n",
            "iteration : 350, loss : 0.1967, accuracy : 94.27\n",
            "Epoch :  29, training loss : 0.1969, training accuracy : 94.26, test loss : 0.2648, test accuracy : 92.49\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.1917, accuracy : 94.59\n",
            "iteration : 100, loss : 0.1959, accuracy : 94.44\n",
            "iteration : 150, loss : 0.1941, accuracy : 94.56\n",
            "iteration : 200, loss : 0.1916, accuracy : 94.57\n",
            "iteration : 250, loss : 0.1917, accuracy : 94.51\n",
            "iteration : 300, loss : 0.1934, accuracy : 94.48\n",
            "iteration : 350, loss : 0.1959, accuracy : 94.40\n",
            "Epoch :  30, training loss : 0.1957, training accuracy : 94.39, test loss : 0.2645, test accuracy : 92.47\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.1900, accuracy : 94.31\n",
            "iteration : 100, loss : 0.1864, accuracy : 94.45\n",
            "iteration : 150, loss : 0.1864, accuracy : 94.55\n",
            "iteration : 200, loss : 0.1877, accuracy : 94.53\n",
            "iteration : 250, loss : 0.1935, accuracy : 94.35\n",
            "iteration : 300, loss : 0.1946, accuracy : 94.28\n",
            "iteration : 350, loss : 0.1951, accuracy : 94.27\n",
            "Epoch :  31, training loss : 0.1952, training accuracy : 94.24, test loss : 0.2507, test accuracy : 92.93\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.1752, accuracy : 94.98\n",
            "iteration : 100, loss : 0.1815, accuracy : 94.88\n",
            "iteration : 150, loss : 0.1911, accuracy : 94.52\n",
            "iteration : 200, loss : 0.1918, accuracy : 94.49\n",
            "iteration : 250, loss : 0.1903, accuracy : 94.54\n",
            "iteration : 300, loss : 0.1930, accuracy : 94.46\n",
            "iteration : 350, loss : 0.1915, accuracy : 94.52\n",
            "Epoch :  32, training loss : 0.1929, training accuracy : 94.47, test loss : 0.2605, test accuracy : 92.42\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.1816, accuracy : 94.84\n",
            "iteration : 100, loss : 0.1835, accuracy : 94.80\n",
            "iteration : 150, loss : 0.1881, accuracy : 94.60\n",
            "iteration : 200, loss : 0.1872, accuracy : 94.67\n",
            "iteration : 250, loss : 0.1875, accuracy : 94.63\n",
            "iteration : 300, loss : 0.1890, accuracy : 94.61\n",
            "iteration : 350, loss : 0.1878, accuracy : 94.60\n",
            "Epoch :  33, training loss : 0.1878, training accuracy : 94.60, test loss : 0.2517, test accuracy : 92.87\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.1833, accuracy : 94.69\n",
            "iteration : 100, loss : 0.1746, accuracy : 94.89\n",
            "iteration : 150, loss : 0.1817, accuracy : 94.73\n",
            "iteration : 200, loss : 0.1842, accuracy : 94.61\n",
            "iteration : 250, loss : 0.1856, accuracy : 94.63\n",
            "iteration : 300, loss : 0.1864, accuracy : 94.60\n",
            "iteration : 350, loss : 0.1883, accuracy : 94.56\n",
            "Epoch :  34, training loss : 0.1892, training accuracy : 94.55, test loss : 0.2407, test accuracy : 93.24\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.1579, accuracy : 95.17\n",
            "iteration : 100, loss : 0.1695, accuracy : 94.96\n",
            "iteration : 150, loss : 0.1745, accuracy : 94.94\n",
            "iteration : 200, loss : 0.1747, accuracy : 94.89\n",
            "iteration : 250, loss : 0.1779, accuracy : 94.84\n",
            "iteration : 300, loss : 0.1800, accuracy : 94.79\n",
            "iteration : 350, loss : 0.1827, accuracy : 94.70\n",
            "Epoch :  35, training loss : 0.1829, training accuracy : 94.69, test loss : 0.2411, test accuracy : 93.35\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.1857, accuracy : 94.56\n",
            "iteration : 100, loss : 0.1821, accuracy : 94.69\n",
            "iteration : 150, loss : 0.1807, accuracy : 94.70\n",
            "iteration : 200, loss : 0.1793, accuracy : 94.78\n",
            "iteration : 250, loss : 0.1799, accuracy : 94.72\n",
            "iteration : 300, loss : 0.1821, accuracy : 94.64\n",
            "iteration : 350, loss : 0.1838, accuracy : 94.61\n",
            "Epoch :  36, training loss : 0.1839, training accuracy : 94.60, test loss : 0.2387, test accuracy : 93.17\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.1750, accuracy : 94.89\n",
            "iteration : 100, loss : 0.1751, accuracy : 94.84\n",
            "iteration : 150, loss : 0.1758, accuracy : 94.83\n",
            "iteration : 200, loss : 0.1811, accuracy : 94.72\n",
            "iteration : 250, loss : 0.1820, accuracy : 94.67\n",
            "iteration : 300, loss : 0.1833, accuracy : 94.63\n",
            "iteration : 350, loss : 0.1840, accuracy : 94.62\n",
            "Epoch :  37, training loss : 0.1843, training accuracy : 94.61, test loss : 0.2425, test accuracy : 93.21\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.1720, accuracy : 95.08\n",
            "iteration : 100, loss : 0.1779, accuracy : 94.88\n",
            "iteration : 150, loss : 0.1798, accuracy : 94.81\n",
            "iteration : 200, loss : 0.1793, accuracy : 94.82\n",
            "iteration : 250, loss : 0.1803, accuracy : 94.75\n",
            "iteration : 300, loss : 0.1838, accuracy : 94.64\n",
            "iteration : 350, loss : 0.1833, accuracy : 94.66\n",
            "Epoch :  38, training loss : 0.1830, training accuracy : 94.67, test loss : 0.2394, test accuracy : 93.28\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.1752, accuracy : 94.91\n",
            "iteration : 100, loss : 0.1715, accuracy : 94.96\n",
            "iteration : 150, loss : 0.1711, accuracy : 95.02\n",
            "iteration : 200, loss : 0.1711, accuracy : 94.95\n",
            "iteration : 250, loss : 0.1725, accuracy : 94.90\n",
            "iteration : 300, loss : 0.1748, accuracy : 94.82\n",
            "iteration : 350, loss : 0.1784, accuracy : 94.69\n",
            "Epoch :  39, training loss : 0.1799, training accuracy : 94.66, test loss : 0.2456, test accuracy : 93.12\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.1795, accuracy : 94.80\n",
            "iteration : 100, loss : 0.1812, accuracy : 94.66\n",
            "iteration : 150, loss : 0.1804, accuracy : 94.65\n",
            "iteration : 200, loss : 0.1812, accuracy : 94.69\n",
            "iteration : 250, loss : 0.1798, accuracy : 94.74\n",
            "iteration : 300, loss : 0.1827, accuracy : 94.69\n",
            "iteration : 350, loss : 0.1815, accuracy : 94.73\n",
            "Epoch :  40, training loss : 0.1813, training accuracy : 94.72, test loss : 0.2464, test accuracy : 93.09\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.1593, accuracy : 95.25\n",
            "iteration : 100, loss : 0.1699, accuracy : 94.97\n",
            "iteration : 150, loss : 0.1789, accuracy : 94.67\n",
            "iteration : 200, loss : 0.1790, accuracy : 94.75\n",
            "iteration : 250, loss : 0.1780, accuracy : 94.78\n",
            "iteration : 300, loss : 0.1788, accuracy : 94.78\n",
            "iteration : 350, loss : 0.1787, accuracy : 94.78\n",
            "Epoch :  41, training loss : 0.1785, training accuracy : 94.79, test loss : 0.2441, test accuracy : 93.13\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.1710, accuracy : 94.83\n",
            "iteration : 100, loss : 0.1675, accuracy : 95.01\n",
            "iteration : 150, loss : 0.1695, accuracy : 95.05\n",
            "iteration : 200, loss : 0.1710, accuracy : 95.04\n",
            "iteration : 250, loss : 0.1731, accuracy : 94.98\n",
            "iteration : 300, loss : 0.1724, accuracy : 94.95\n",
            "iteration : 350, loss : 0.1736, accuracy : 94.92\n",
            "Epoch :  42, training loss : 0.1739, training accuracy : 94.90, test loss : 0.2694, test accuracy : 92.53\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.1696, accuracy : 95.03\n",
            "iteration : 100, loss : 0.1727, accuracy : 94.87\n",
            "iteration : 150, loss : 0.1737, accuracy : 94.92\n",
            "iteration : 200, loss : 0.1764, accuracy : 94.82\n",
            "iteration : 250, loss : 0.1764, accuracy : 94.83\n",
            "iteration : 300, loss : 0.1772, accuracy : 94.77\n",
            "iteration : 350, loss : 0.1757, accuracy : 94.80\n",
            "Epoch :  43, training loss : 0.1762, training accuracy : 94.81, test loss : 0.2634, test accuracy : 92.77\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.1503, accuracy : 95.45\n",
            "iteration : 100, loss : 0.1555, accuracy : 95.31\n",
            "iteration : 150, loss : 0.1614, accuracy : 95.26\n",
            "iteration : 200, loss : 0.1682, accuracy : 95.02\n",
            "iteration : 250, loss : 0.1695, accuracy : 95.00\n",
            "iteration : 300, loss : 0.1738, accuracy : 94.89\n",
            "iteration : 350, loss : 0.1751, accuracy : 94.92\n",
            "Epoch :  44, training loss : 0.1758, training accuracy : 94.89, test loss : 0.2529, test accuracy : 92.75\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.1727, accuracy : 95.28\n",
            "iteration : 100, loss : 0.1736, accuracy : 95.02\n",
            "iteration : 150, loss : 0.1755, accuracy : 94.92\n",
            "iteration : 200, loss : 0.1753, accuracy : 94.91\n",
            "iteration : 250, loss : 0.1730, accuracy : 94.90\n",
            "iteration : 300, loss : 0.1714, accuracy : 94.95\n",
            "iteration : 350, loss : 0.1714, accuracy : 94.96\n",
            "Epoch :  45, training loss : 0.1712, training accuracy : 94.97, test loss : 0.2382, test accuracy : 93.26\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.1793, accuracy : 94.83\n",
            "iteration : 100, loss : 0.1758, accuracy : 95.01\n",
            "iteration : 150, loss : 0.1772, accuracy : 94.88\n",
            "iteration : 200, loss : 0.1725, accuracy : 94.98\n",
            "iteration : 250, loss : 0.1727, accuracy : 94.96\n",
            "iteration : 300, loss : 0.1735, accuracy : 94.96\n",
            "iteration : 350, loss : 0.1753, accuracy : 94.90\n",
            "Epoch :  46, training loss : 0.1757, training accuracy : 94.89, test loss : 0.2391, test accuracy : 93.36\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.1634, accuracy : 95.34\n",
            "iteration : 100, loss : 0.1607, accuracy : 95.47\n",
            "iteration : 150, loss : 0.1692, accuracy : 95.30\n",
            "iteration : 200, loss : 0.1684, accuracy : 95.23\n",
            "iteration : 250, loss : 0.1690, accuracy : 95.17\n",
            "iteration : 300, loss : 0.1695, accuracy : 95.15\n",
            "iteration : 350, loss : 0.1706, accuracy : 95.11\n",
            "Epoch :  47, training loss : 0.1718, training accuracy : 95.06, test loss : 0.2521, test accuracy : 92.96\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.1790, accuracy : 94.77\n",
            "iteration : 100, loss : 0.1671, accuracy : 94.98\n",
            "iteration : 150, loss : 0.1671, accuracy : 94.99\n",
            "iteration : 200, loss : 0.1684, accuracy : 94.96\n",
            "iteration : 250, loss : 0.1716, accuracy : 94.96\n",
            "iteration : 300, loss : 0.1730, accuracy : 94.95\n",
            "iteration : 350, loss : 0.1748, accuracy : 94.96\n",
            "Epoch :  48, training loss : 0.1739, training accuracy : 94.97, test loss : 0.2428, test accuracy : 93.02\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.1480, accuracy : 95.95\n",
            "iteration : 100, loss : 0.1555, accuracy : 95.69\n",
            "iteration : 150, loss : 0.1574, accuracy : 95.58\n",
            "iteration : 200, loss : 0.1628, accuracy : 95.40\n",
            "iteration : 250, loss : 0.1652, accuracy : 95.34\n",
            "iteration : 300, loss : 0.1654, accuracy : 95.33\n",
            "iteration : 350, loss : 0.1666, accuracy : 95.27\n",
            "Epoch :  49, training loss : 0.1669, training accuracy : 95.24, test loss : 0.2521, test accuracy : 93.00\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.1634, accuracy : 95.48\n",
            "iteration : 100, loss : 0.1631, accuracy : 95.26\n",
            "iteration : 150, loss : 0.1637, accuracy : 95.19\n",
            "iteration : 200, loss : 0.1652, accuracy : 95.14\n",
            "iteration : 250, loss : 0.1670, accuracy : 95.10\n",
            "iteration : 300, loss : 0.1687, accuracy : 95.05\n",
            "iteration : 350, loss : 0.1716, accuracy : 94.97\n",
            "Epoch :  50, training loss : 0.1723, training accuracy : 94.94, test loss : 0.2451, test accuracy : 92.86\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.1603, accuracy : 95.58\n",
            "iteration : 100, loss : 0.1528, accuracy : 95.63\n",
            "iteration : 150, loss : 0.1599, accuracy : 95.43\n",
            "iteration : 200, loss : 0.1595, accuracy : 95.39\n",
            "iteration : 250, loss : 0.1602, accuracy : 95.36\n",
            "iteration : 300, loss : 0.1668, accuracy : 95.19\n",
            "iteration : 350, loss : 0.1671, accuracy : 95.20\n",
            "Epoch :  51, training loss : 0.1674, training accuracy : 95.19, test loss : 0.2500, test accuracy : 92.78\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.1665, accuracy : 95.03\n",
            "iteration : 100, loss : 0.1583, accuracy : 95.27\n",
            "iteration : 150, loss : 0.1571, accuracy : 95.27\n",
            "iteration : 200, loss : 0.1587, accuracy : 95.33\n",
            "iteration : 250, loss : 0.1601, accuracy : 95.33\n",
            "iteration : 300, loss : 0.1610, accuracy : 95.28\n",
            "iteration : 350, loss : 0.1668, accuracy : 95.14\n",
            "Epoch :  52, training loss : 0.1674, training accuracy : 95.11, test loss : 0.2431, test accuracy : 93.15\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.1533, accuracy : 95.84\n",
            "iteration : 100, loss : 0.1570, accuracy : 95.73\n",
            "iteration : 150, loss : 0.1557, accuracy : 95.62\n",
            "iteration : 200, loss : 0.1572, accuracy : 95.56\n",
            "iteration : 250, loss : 0.1631, accuracy : 95.34\n",
            "iteration : 300, loss : 0.1657, accuracy : 95.26\n",
            "iteration : 350, loss : 0.1652, accuracy : 95.26\n",
            "Epoch :  53, training loss : 0.1645, training accuracy : 95.26, test loss : 0.2341, test accuracy : 93.43\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.1661, accuracy : 95.27\n",
            "iteration : 100, loss : 0.1662, accuracy : 95.31\n",
            "iteration : 150, loss : 0.1634, accuracy : 95.37\n",
            "iteration : 200, loss : 0.1612, accuracy : 95.42\n",
            "iteration : 250, loss : 0.1605, accuracy : 95.39\n",
            "iteration : 300, loss : 0.1621, accuracy : 95.35\n",
            "iteration : 350, loss : 0.1645, accuracy : 95.25\n",
            "Epoch :  54, training loss : 0.1657, training accuracy : 95.19, test loss : 0.2221, test accuracy : 93.84\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.1483, accuracy : 95.77\n",
            "iteration : 100, loss : 0.1571, accuracy : 95.57\n",
            "iteration : 150, loss : 0.1659, accuracy : 95.38\n",
            "iteration : 200, loss : 0.1660, accuracy : 95.27\n",
            "iteration : 250, loss : 0.1635, accuracy : 95.36\n",
            "iteration : 300, loss : 0.1652, accuracy : 95.28\n",
            "iteration : 350, loss : 0.1637, accuracy : 95.29\n",
            "Epoch :  55, training loss : 0.1635, training accuracy : 95.27, test loss : 0.2472, test accuracy : 93.05\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.1502, accuracy : 95.55\n",
            "iteration : 100, loss : 0.1481, accuracy : 95.62\n",
            "iteration : 150, loss : 0.1541, accuracy : 95.38\n",
            "iteration : 200, loss : 0.1585, accuracy : 95.25\n",
            "iteration : 250, loss : 0.1594, accuracy : 95.26\n",
            "iteration : 300, loss : 0.1627, accuracy : 95.20\n",
            "iteration : 350, loss : 0.1622, accuracy : 95.21\n",
            "Epoch :  56, training loss : 0.1624, training accuracy : 95.20, test loss : 0.2361, test accuracy : 93.24\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.1551, accuracy : 95.52\n",
            "iteration : 100, loss : 0.1548, accuracy : 95.59\n",
            "iteration : 150, loss : 0.1617, accuracy : 95.25\n",
            "iteration : 200, loss : 0.1602, accuracy : 95.30\n",
            "iteration : 250, loss : 0.1621, accuracy : 95.22\n",
            "iteration : 300, loss : 0.1616, accuracy : 95.24\n",
            "iteration : 350, loss : 0.1619, accuracy : 95.29\n",
            "Epoch :  57, training loss : 0.1614, training accuracy : 95.30, test loss : 0.2380, test accuracy : 93.46\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.1517, accuracy : 95.34\n",
            "iteration : 100, loss : 0.1493, accuracy : 95.45\n",
            "iteration : 150, loss : 0.1481, accuracy : 95.62\n",
            "iteration : 200, loss : 0.1539, accuracy : 95.41\n",
            "iteration : 250, loss : 0.1582, accuracy : 95.33\n",
            "iteration : 300, loss : 0.1580, accuracy : 95.31\n",
            "iteration : 350, loss : 0.1609, accuracy : 95.25\n",
            "Epoch :  58, training loss : 0.1613, training accuracy : 95.24, test loss : 0.2389, test accuracy : 93.39\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.1509, accuracy : 95.91\n",
            "iteration : 100, loss : 0.1416, accuracy : 96.02\n",
            "iteration : 150, loss : 0.1491, accuracy : 95.79\n",
            "iteration : 200, loss : 0.1526, accuracy : 95.61\n",
            "iteration : 250, loss : 0.1570, accuracy : 95.52\n",
            "iteration : 300, loss : 0.1584, accuracy : 95.48\n",
            "iteration : 350, loss : 0.1607, accuracy : 95.40\n",
            "Epoch :  59, training loss : 0.1612, training accuracy : 95.36, test loss : 0.2358, test accuracy : 93.30\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.1532, accuracy : 95.67\n",
            "iteration : 100, loss : 0.1523, accuracy : 95.60\n",
            "iteration : 150, loss : 0.1501, accuracy : 95.64\n",
            "iteration : 200, loss : 0.1545, accuracy : 95.55\n",
            "iteration : 250, loss : 0.1550, accuracy : 95.54\n",
            "iteration : 300, loss : 0.1554, accuracy : 95.50\n",
            "iteration : 350, loss : 0.1581, accuracy : 95.44\n",
            "Epoch :  60, training loss : 0.1591, training accuracy : 95.40, test loss : 0.2408, test accuracy : 93.14\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.1574, accuracy : 95.45\n",
            "iteration : 100, loss : 0.1603, accuracy : 95.35\n",
            "iteration : 150, loss : 0.1583, accuracy : 95.45\n",
            "iteration : 200, loss : 0.1585, accuracy : 95.45\n",
            "iteration : 250, loss : 0.1584, accuracy : 95.44\n",
            "iteration : 300, loss : 0.1604, accuracy : 95.30\n",
            "iteration : 350, loss : 0.1592, accuracy : 95.34\n",
            "Epoch :  61, training loss : 0.1600, training accuracy : 95.30, test loss : 0.2557, test accuracy : 92.91\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.1480, accuracy : 95.59\n",
            "iteration : 100, loss : 0.1454, accuracy : 95.57\n",
            "iteration : 150, loss : 0.1460, accuracy : 95.59\n",
            "iteration : 200, loss : 0.1525, accuracy : 95.54\n",
            "iteration : 250, loss : 0.1578, accuracy : 95.41\n",
            "iteration : 300, loss : 0.1583, accuracy : 95.38\n",
            "iteration : 350, loss : 0.1617, accuracy : 95.30\n",
            "Epoch :  62, training loss : 0.1610, training accuracy : 95.28, test loss : 0.2338, test accuracy : 93.53\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.1577, accuracy : 95.28\n",
            "iteration : 100, loss : 0.1532, accuracy : 95.49\n",
            "iteration : 150, loss : 0.1516, accuracy : 95.51\n",
            "iteration : 200, loss : 0.1535, accuracy : 95.49\n",
            "iteration : 250, loss : 0.1551, accuracy : 95.47\n",
            "iteration : 300, loss : 0.1560, accuracy : 95.42\n",
            "iteration : 350, loss : 0.1568, accuracy : 95.40\n",
            "Epoch :  63, training loss : 0.1576, training accuracy : 95.37, test loss : 0.2347, test accuracy : 93.46\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.1472, accuracy : 95.83\n",
            "iteration : 100, loss : 0.1441, accuracy : 95.79\n",
            "iteration : 150, loss : 0.1508, accuracy : 95.65\n",
            "iteration : 200, loss : 0.1534, accuracy : 95.51\n",
            "iteration : 250, loss : 0.1566, accuracy : 95.41\n",
            "iteration : 300, loss : 0.1555, accuracy : 95.38\n",
            "iteration : 350, loss : 0.1549, accuracy : 95.44\n",
            "Epoch :  64, training loss : 0.1540, training accuracy : 95.47, test loss : 0.2314, test accuracy : 93.54\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.1453, accuracy : 95.77\n",
            "iteration : 100, loss : 0.1557, accuracy : 95.45\n",
            "iteration : 150, loss : 0.1480, accuracy : 95.71\n",
            "iteration : 200, loss : 0.1538, accuracy : 95.56\n",
            "iteration : 250, loss : 0.1557, accuracy : 95.48\n",
            "iteration : 300, loss : 0.1570, accuracy : 95.41\n",
            "iteration : 350, loss : 0.1571, accuracy : 95.40\n",
            "Epoch :  65, training loss : 0.1577, training accuracy : 95.39, test loss : 0.2443, test accuracy : 93.24\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.1535, accuracy : 95.86\n",
            "iteration : 100, loss : 0.1528, accuracy : 95.79\n",
            "iteration : 150, loss : 0.1497, accuracy : 95.68\n",
            "iteration : 200, loss : 0.1493, accuracy : 95.68\n",
            "iteration : 250, loss : 0.1542, accuracy : 95.53\n",
            "iteration : 300, loss : 0.1544, accuracy : 95.61\n",
            "iteration : 350, loss : 0.1549, accuracy : 95.58\n",
            "Epoch :  66, training loss : 0.1553, training accuracy : 95.56, test loss : 0.2439, test accuracy : 93.17\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.1423, accuracy : 95.83\n",
            "iteration : 100, loss : 0.1464, accuracy : 95.86\n",
            "iteration : 150, loss : 0.1455, accuracy : 95.90\n",
            "iteration : 200, loss : 0.1458, accuracy : 95.81\n",
            "iteration : 250, loss : 0.1520, accuracy : 95.60\n",
            "iteration : 300, loss : 0.1540, accuracy : 95.57\n",
            "iteration : 350, loss : 0.1563, accuracy : 95.49\n",
            "Epoch :  67, training loss : 0.1562, training accuracy : 95.47, test loss : 0.2297, test accuracy : 93.67\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.1527, accuracy : 95.31\n",
            "iteration : 100, loss : 0.1536, accuracy : 95.37\n",
            "iteration : 150, loss : 0.1554, accuracy : 95.36\n",
            "iteration : 200, loss : 0.1524, accuracy : 95.50\n",
            "iteration : 250, loss : 0.1541, accuracy : 95.45\n",
            "iteration : 300, loss : 0.1585, accuracy : 95.32\n",
            "iteration : 350, loss : 0.1589, accuracy : 95.30\n",
            "Epoch :  68, training loss : 0.1592, training accuracy : 95.29, test loss : 0.2495, test accuracy : 93.15\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.1693, accuracy : 95.33\n",
            "iteration : 100, loss : 0.1531, accuracy : 95.63\n",
            "iteration : 150, loss : 0.1560, accuracy : 95.42\n",
            "iteration : 200, loss : 0.1549, accuracy : 95.46\n",
            "iteration : 250, loss : 0.1567, accuracy : 95.45\n",
            "iteration : 300, loss : 0.1580, accuracy : 95.43\n",
            "iteration : 350, loss : 0.1574, accuracy : 95.45\n",
            "Epoch :  69, training loss : 0.1570, training accuracy : 95.47, test loss : 0.2391, test accuracy : 93.37\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.1432, accuracy : 95.84\n",
            "iteration : 100, loss : 0.1473, accuracy : 95.66\n",
            "iteration : 150, loss : 0.1435, accuracy : 95.76\n",
            "iteration : 200, loss : 0.1447, accuracy : 95.72\n",
            "iteration : 250, loss : 0.1472, accuracy : 95.68\n",
            "iteration : 300, loss : 0.1480, accuracy : 95.67\n",
            "iteration : 350, loss : 0.1501, accuracy : 95.64\n",
            "Epoch :  70, training loss : 0.1516, training accuracy : 95.61, test loss : 0.2546, test accuracy : 92.82\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.1406, accuracy : 96.00\n",
            "iteration : 100, loss : 0.1449, accuracy : 95.88\n",
            "iteration : 150, loss : 0.1464, accuracy : 95.78\n",
            "iteration : 200, loss : 0.1489, accuracy : 95.73\n",
            "iteration : 250, loss : 0.1526, accuracy : 95.62\n",
            "iteration : 300, loss : 0.1534, accuracy : 95.61\n",
            "iteration : 350, loss : 0.1545, accuracy : 95.57\n",
            "Epoch :  71, training loss : 0.1543, training accuracy : 95.56, test loss : 0.2380, test accuracy : 93.39\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.1338, accuracy : 95.97\n",
            "iteration : 100, loss : 0.1400, accuracy : 95.86\n",
            "iteration : 150, loss : 0.1430, accuracy : 95.75\n",
            "iteration : 200, loss : 0.1457, accuracy : 95.70\n",
            "iteration : 250, loss : 0.1443, accuracy : 95.70\n",
            "iteration : 300, loss : 0.1469, accuracy : 95.71\n",
            "iteration : 350, loss : 0.1490, accuracy : 95.63\n",
            "Epoch :  72, training loss : 0.1490, training accuracy : 95.59, test loss : 0.2321, test accuracy : 93.57\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.1310, accuracy : 96.20\n",
            "iteration : 100, loss : 0.1465, accuracy : 95.75\n",
            "iteration : 150, loss : 0.1498, accuracy : 95.66\n",
            "iteration : 200, loss : 0.1509, accuracy : 95.54\n",
            "iteration : 250, loss : 0.1528, accuracy : 95.50\n",
            "iteration : 300, loss : 0.1537, accuracy : 95.48\n",
            "iteration : 350, loss : 0.1533, accuracy : 95.48\n",
            "Epoch :  73, training loss : 0.1534, training accuracy : 95.47, test loss : 0.2523, test accuracy : 92.79\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.1474, accuracy : 95.92\n",
            "iteration : 100, loss : 0.1506, accuracy : 95.70\n",
            "iteration : 150, loss : 0.1509, accuracy : 95.58\n",
            "iteration : 200, loss : 0.1493, accuracy : 95.64\n",
            "iteration : 250, loss : 0.1471, accuracy : 95.69\n",
            "iteration : 300, loss : 0.1498, accuracy : 95.58\n",
            "iteration : 350, loss : 0.1501, accuracy : 95.61\n",
            "Epoch :  74, training loss : 0.1494, training accuracy : 95.62, test loss : 0.2409, test accuracy : 93.22\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.1385, accuracy : 96.02\n",
            "iteration : 100, loss : 0.1488, accuracy : 95.73\n",
            "iteration : 150, loss : 0.1513, accuracy : 95.62\n",
            "iteration : 200, loss : 0.1485, accuracy : 95.75\n",
            "iteration : 250, loss : 0.1481, accuracy : 95.76\n",
            "iteration : 300, loss : 0.1498, accuracy : 95.64\n",
            "iteration : 350, loss : 0.1494, accuracy : 95.69\n",
            "Epoch :  75, training loss : 0.1491, training accuracy : 95.68, test loss : 0.2353, test accuracy : 93.46\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.1341, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1404, accuracy : 95.77\n",
            "iteration : 150, loss : 0.1462, accuracy : 95.69\n",
            "iteration : 200, loss : 0.1464, accuracy : 95.77\n",
            "iteration : 250, loss : 0.1451, accuracy : 95.83\n",
            "iteration : 300, loss : 0.1463, accuracy : 95.76\n",
            "iteration : 350, loss : 0.1493, accuracy : 95.71\n",
            "Epoch :  76, training loss : 0.1496, training accuracy : 95.73, test loss : 0.2258, test accuracy : 93.80\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.1366, accuracy : 96.33\n",
            "iteration : 100, loss : 0.1422, accuracy : 96.03\n",
            "iteration : 150, loss : 0.1459, accuracy : 95.83\n",
            "iteration : 200, loss : 0.1475, accuracy : 95.71\n",
            "iteration : 250, loss : 0.1469, accuracy : 95.78\n",
            "iteration : 300, loss : 0.1468, accuracy : 95.73\n",
            "iteration : 350, loss : 0.1475, accuracy : 95.72\n",
            "Epoch :  77, training loss : 0.1488, training accuracy : 95.68, test loss : 0.2264, test accuracy : 93.83\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.1402, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1448, accuracy : 95.88\n",
            "iteration : 150, loss : 0.1468, accuracy : 95.71\n",
            "iteration : 200, loss : 0.1525, accuracy : 95.59\n",
            "iteration : 250, loss : 0.1496, accuracy : 95.71\n",
            "iteration : 300, loss : 0.1498, accuracy : 95.71\n",
            "iteration : 350, loss : 0.1506, accuracy : 95.70\n",
            "Epoch :  78, training loss : 0.1509, training accuracy : 95.70, test loss : 0.2365, test accuracy : 93.49\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.1470, accuracy : 95.83\n",
            "iteration : 100, loss : 0.1411, accuracy : 95.92\n",
            "iteration : 150, loss : 0.1437, accuracy : 95.84\n",
            "iteration : 200, loss : 0.1443, accuracy : 95.73\n",
            "iteration : 250, loss : 0.1427, accuracy : 95.77\n",
            "iteration : 300, loss : 0.1461, accuracy : 95.67\n",
            "iteration : 350, loss : 0.1490, accuracy : 95.60\n",
            "Epoch :  79, training loss : 0.1486, training accuracy : 95.62, test loss : 0.2343, test accuracy : 93.32\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.1180, accuracy : 96.69\n",
            "iteration : 100, loss : 0.1227, accuracy : 96.46\n",
            "iteration : 150, loss : 0.1274, accuracy : 96.29\n",
            "iteration : 200, loss : 0.1325, accuracy : 96.18\n",
            "iteration : 250, loss : 0.1357, accuracy : 96.08\n",
            "iteration : 300, loss : 0.1366, accuracy : 96.03\n",
            "iteration : 350, loss : 0.1402, accuracy : 95.93\n",
            "Epoch :  80, training loss : 0.1423, training accuracy : 95.88, test loss : 0.2312, test accuracy : 93.70\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.1417, accuracy : 95.89\n",
            "iteration : 100, loss : 0.1350, accuracy : 96.14\n",
            "iteration : 150, loss : 0.1388, accuracy : 95.97\n",
            "iteration : 200, loss : 0.1434, accuracy : 95.90\n",
            "iteration : 250, loss : 0.1446, accuracy : 95.86\n",
            "iteration : 300, loss : 0.1468, accuracy : 95.78\n",
            "iteration : 350, loss : 0.1462, accuracy : 95.78\n",
            "Epoch :  81, training loss : 0.1457, training accuracy : 95.79, test loss : 0.2435, test accuracy : 93.47\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.1254, accuracy : 96.31\n",
            "iteration : 100, loss : 0.1246, accuracy : 96.26\n",
            "iteration : 150, loss : 0.1251, accuracy : 96.29\n",
            "iteration : 200, loss : 0.1291, accuracy : 96.22\n",
            "iteration : 250, loss : 0.1335, accuracy : 96.16\n",
            "iteration : 300, loss : 0.1363, accuracy : 96.04\n",
            "iteration : 350, loss : 0.1406, accuracy : 95.93\n",
            "Epoch :  82, training loss : 0.1402, training accuracy : 95.95, test loss : 0.2479, test accuracy : 93.11\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.1296, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1273, accuracy : 96.37\n",
            "iteration : 150, loss : 0.1295, accuracy : 96.23\n",
            "iteration : 200, loss : 0.1340, accuracy : 96.15\n",
            "iteration : 250, loss : 0.1384, accuracy : 96.01\n",
            "iteration : 300, loss : 0.1409, accuracy : 95.95\n",
            "iteration : 350, loss : 0.1434, accuracy : 95.83\n",
            "Epoch :  83, training loss : 0.1441, training accuracy : 95.81, test loss : 0.2303, test accuracy : 93.70\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.1227, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1315, accuracy : 96.12\n",
            "iteration : 150, loss : 0.1414, accuracy : 95.85\n",
            "iteration : 200, loss : 0.1399, accuracy : 95.86\n",
            "iteration : 250, loss : 0.1414, accuracy : 95.88\n",
            "iteration : 300, loss : 0.1420, accuracy : 95.86\n",
            "iteration : 350, loss : 0.1433, accuracy : 95.80\n",
            "Epoch :  84, training loss : 0.1435, training accuracy : 95.78, test loss : 0.2307, test accuracy : 93.64\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.1453, accuracy : 95.89\n",
            "iteration : 100, loss : 0.1395, accuracy : 95.97\n",
            "iteration : 150, loss : 0.1350, accuracy : 96.01\n",
            "iteration : 200, loss : 0.1388, accuracy : 95.91\n",
            "iteration : 250, loss : 0.1402, accuracy : 95.86\n",
            "iteration : 300, loss : 0.1427, accuracy : 95.80\n",
            "iteration : 350, loss : 0.1433, accuracy : 95.80\n",
            "Epoch :  85, training loss : 0.1429, training accuracy : 95.81, test loss : 0.2417, test accuracy : 93.39\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.1396, accuracy : 95.95\n",
            "iteration : 100, loss : 0.1404, accuracy : 96.01\n",
            "iteration : 150, loss : 0.1368, accuracy : 96.03\n",
            "iteration : 200, loss : 0.1382, accuracy : 95.97\n",
            "iteration : 250, loss : 0.1389, accuracy : 95.99\n",
            "iteration : 300, loss : 0.1370, accuracy : 96.05\n",
            "iteration : 350, loss : 0.1380, accuracy : 95.98\n",
            "Epoch :  86, training loss : 0.1382, training accuracy : 95.98, test loss : 0.2479, test accuracy : 93.26\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.1265, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1324, accuracy : 96.03\n",
            "iteration : 150, loss : 0.1360, accuracy : 95.93\n",
            "iteration : 200, loss : 0.1393, accuracy : 95.85\n",
            "iteration : 250, loss : 0.1391, accuracy : 95.87\n",
            "iteration : 300, loss : 0.1392, accuracy : 95.90\n",
            "iteration : 350, loss : 0.1413, accuracy : 95.83\n",
            "Epoch :  87, training loss : 0.1414, training accuracy : 95.83, test loss : 0.2356, test accuracy : 93.42\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.1337, accuracy : 95.94\n",
            "iteration : 100, loss : 0.1342, accuracy : 96.01\n",
            "iteration : 150, loss : 0.1357, accuracy : 95.94\n",
            "iteration : 200, loss : 0.1394, accuracy : 95.87\n",
            "iteration : 250, loss : 0.1408, accuracy : 95.84\n",
            "iteration : 300, loss : 0.1400, accuracy : 95.84\n",
            "iteration : 350, loss : 0.1399, accuracy : 95.81\n",
            "Epoch :  88, training loss : 0.1405, training accuracy : 95.81, test loss : 0.2424, test accuracy : 93.31\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.1223, accuracy : 96.36\n",
            "iteration : 100, loss : 0.1243, accuracy : 96.35\n",
            "iteration : 150, loss : 0.1284, accuracy : 96.22\n",
            "iteration : 200, loss : 0.1323, accuracy : 96.05\n",
            "iteration : 250, loss : 0.1355, accuracy : 95.97\n",
            "iteration : 300, loss : 0.1375, accuracy : 95.91\n",
            "iteration : 350, loss : 0.1416, accuracy : 95.82\n",
            "Epoch :  89, training loss : 0.1418, training accuracy : 95.80, test loss : 0.2316, test accuracy : 93.75\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.1248, accuracy : 96.53\n",
            "iteration : 100, loss : 0.1274, accuracy : 96.37\n",
            "iteration : 150, loss : 0.1320, accuracy : 96.25\n",
            "iteration : 200, loss : 0.1325, accuracy : 96.20\n",
            "iteration : 250, loss : 0.1348, accuracy : 96.11\n",
            "iteration : 300, loss : 0.1367, accuracy : 96.09\n",
            "iteration : 350, loss : 0.1369, accuracy : 96.07\n",
            "Epoch :  90, training loss : 0.1366, training accuracy : 96.07, test loss : 0.2281, test accuracy : 93.81\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.1262, accuracy : 96.19\n",
            "iteration : 100, loss : 0.1280, accuracy : 96.23\n",
            "iteration : 150, loss : 0.1256, accuracy : 96.37\n",
            "iteration : 200, loss : 0.1270, accuracy : 96.32\n",
            "iteration : 250, loss : 0.1297, accuracy : 96.21\n",
            "iteration : 300, loss : 0.1321, accuracy : 96.15\n",
            "iteration : 350, loss : 0.1369, accuracy : 96.01\n",
            "Epoch :  91, training loss : 0.1371, training accuracy : 95.98, test loss : 0.2360, test accuracy : 93.58\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.1171, accuracy : 96.50\n",
            "iteration : 100, loss : 0.1231, accuracy : 96.35\n",
            "iteration : 150, loss : 0.1289, accuracy : 96.14\n",
            "iteration : 200, loss : 0.1329, accuracy : 96.00\n",
            "iteration : 250, loss : 0.1323, accuracy : 96.05\n",
            "iteration : 300, loss : 0.1358, accuracy : 95.97\n",
            "iteration : 350, loss : 0.1360, accuracy : 96.00\n",
            "Epoch :  92, training loss : 0.1365, training accuracy : 95.99, test loss : 0.2292, test accuracy : 93.61\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.1204, accuracy : 96.36\n",
            "iteration : 100, loss : 0.1297, accuracy : 96.22\n",
            "iteration : 150, loss : 0.1335, accuracy : 96.11\n",
            "iteration : 200, loss : 0.1401, accuracy : 95.93\n",
            "iteration : 250, loss : 0.1405, accuracy : 95.92\n",
            "iteration : 300, loss : 0.1393, accuracy : 95.94\n",
            "iteration : 350, loss : 0.1398, accuracy : 95.94\n",
            "Epoch :  93, training loss : 0.1403, training accuracy : 95.95, test loss : 0.2359, test accuracy : 93.46\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.1297, accuracy : 96.14\n",
            "iteration : 100, loss : 0.1286, accuracy : 96.23\n",
            "iteration : 150, loss : 0.1303, accuracy : 96.18\n",
            "iteration : 200, loss : 0.1298, accuracy : 96.17\n",
            "iteration : 250, loss : 0.1306, accuracy : 96.15\n",
            "iteration : 300, loss : 0.1323, accuracy : 96.12\n",
            "iteration : 350, loss : 0.1332, accuracy : 96.07\n",
            "Epoch :  94, training loss : 0.1343, training accuracy : 96.04, test loss : 0.2405, test accuracy : 93.40\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.1234, accuracy : 96.66\n",
            "iteration : 100, loss : 0.1202, accuracy : 96.63\n",
            "iteration : 150, loss : 0.1186, accuracy : 96.67\n",
            "iteration : 200, loss : 0.1210, accuracy : 96.59\n",
            "iteration : 250, loss : 0.1227, accuracy : 96.45\n",
            "iteration : 300, loss : 0.1286, accuracy : 96.25\n",
            "iteration : 350, loss : 0.1323, accuracy : 96.09\n",
            "Epoch :  95, training loss : 0.1325, training accuracy : 96.08, test loss : 0.2432, test accuracy : 93.22\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.1157, accuracy : 96.75\n",
            "iteration : 100, loss : 0.1172, accuracy : 96.62\n",
            "iteration : 150, loss : 0.1245, accuracy : 96.28\n",
            "iteration : 200, loss : 0.1307, accuracy : 96.14\n",
            "iteration : 250, loss : 0.1299, accuracy : 96.10\n",
            "iteration : 300, loss : 0.1313, accuracy : 96.09\n",
            "iteration : 350, loss : 0.1317, accuracy : 96.07\n",
            "Epoch :  96, training loss : 0.1351, training accuracy : 95.98, test loss : 0.2453, test accuracy : 93.11\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.1336, accuracy : 96.20\n",
            "iteration : 100, loss : 0.1336, accuracy : 96.11\n",
            "iteration : 150, loss : 0.1324, accuracy : 96.23\n",
            "iteration : 200, loss : 0.1328, accuracy : 96.16\n",
            "iteration : 250, loss : 0.1356, accuracy : 96.08\n",
            "iteration : 300, loss : 0.1335, accuracy : 96.13\n",
            "iteration : 350, loss : 0.1334, accuracy : 96.11\n",
            "Epoch :  97, training loss : 0.1344, training accuracy : 96.10, test loss : 0.2396, test accuracy : 93.49\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.1403, accuracy : 95.78\n",
            "iteration : 100, loss : 0.1373, accuracy : 95.91\n",
            "iteration : 150, loss : 0.1332, accuracy : 96.03\n",
            "iteration : 200, loss : 0.1332, accuracy : 96.05\n",
            "iteration : 250, loss : 0.1319, accuracy : 96.14\n",
            "iteration : 300, loss : 0.1317, accuracy : 96.15\n",
            "iteration : 350, loss : 0.1331, accuracy : 96.09\n",
            "Epoch :  98, training loss : 0.1339, training accuracy : 96.04, test loss : 0.2583, test accuracy : 92.97\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.1310, accuracy : 96.28\n",
            "iteration : 100, loss : 0.1292, accuracy : 96.16\n",
            "iteration : 150, loss : 0.1317, accuracy : 96.04\n",
            "iteration : 200, loss : 0.1324, accuracy : 96.05\n",
            "iteration : 250, loss : 0.1356, accuracy : 95.95\n",
            "iteration : 300, loss : 0.1356, accuracy : 95.93\n",
            "iteration : 350, loss : 0.1354, accuracy : 95.94\n",
            "Epoch :  99, training loss : 0.1353, training accuracy : 95.92, test loss : 0.2396, test accuracy : 93.53\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.1262, accuracy : 96.19\n",
            "iteration : 100, loss : 0.1321, accuracy : 96.11\n",
            "iteration : 150, loss : 0.1285, accuracy : 96.20\n",
            "iteration : 200, loss : 0.1328, accuracy : 96.04\n",
            "iteration : 250, loss : 0.1316, accuracy : 96.12\n",
            "iteration : 300, loss : 0.1321, accuracy : 96.12\n",
            "iteration : 350, loss : 0.1323, accuracy : 96.08\n",
            "Epoch : 100, training loss : 0.1319, training accuracy : 96.11, test loss : 0.2340, test accuracy : 93.48\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.1241, accuracy : 96.25\n",
            "iteration : 100, loss : 0.1200, accuracy : 96.41\n",
            "iteration : 150, loss : 0.1217, accuracy : 96.43\n",
            "iteration : 200, loss : 0.1234, accuracy : 96.39\n",
            "iteration : 250, loss : 0.1242, accuracy : 96.35\n",
            "iteration : 300, loss : 0.1257, accuracy : 96.30\n",
            "iteration : 350, loss : 0.1274, accuracy : 96.25\n",
            "Epoch : 101, training loss : 0.1286, training accuracy : 96.22, test loss : 0.2342, test accuracy : 93.63\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.1211, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1190, accuracy : 96.48\n",
            "iteration : 150, loss : 0.1217, accuracy : 96.40\n",
            "iteration : 200, loss : 0.1235, accuracy : 96.38\n",
            "iteration : 250, loss : 0.1287, accuracy : 96.26\n",
            "iteration : 300, loss : 0.1308, accuracy : 96.17\n",
            "iteration : 350, loss : 0.1307, accuracy : 96.15\n",
            "Epoch : 102, training loss : 0.1311, training accuracy : 96.13, test loss : 0.2400, test accuracy : 93.47\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.1224, accuracy : 96.48\n",
            "iteration : 100, loss : 0.1186, accuracy : 96.60\n",
            "iteration : 150, loss : 0.1199, accuracy : 96.51\n",
            "iteration : 200, loss : 0.1263, accuracy : 96.34\n",
            "iteration : 250, loss : 0.1285, accuracy : 96.27\n",
            "iteration : 300, loss : 0.1294, accuracy : 96.24\n",
            "iteration : 350, loss : 0.1290, accuracy : 96.27\n",
            "Epoch : 103, training loss : 0.1290, training accuracy : 96.26, test loss : 0.2439, test accuracy : 93.35\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.1397, accuracy : 95.80\n",
            "iteration : 100, loss : 0.1349, accuracy : 95.97\n",
            "iteration : 150, loss : 0.1271, accuracy : 96.25\n",
            "iteration : 200, loss : 0.1234, accuracy : 96.41\n",
            "iteration : 250, loss : 0.1244, accuracy : 96.39\n",
            "iteration : 300, loss : 0.1250, accuracy : 96.34\n",
            "iteration : 350, loss : 0.1274, accuracy : 96.27\n",
            "Epoch : 104, training loss : 0.1281, training accuracy : 96.25, test loss : 0.2674, test accuracy : 92.67\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.1204, accuracy : 96.52\n",
            "iteration : 100, loss : 0.1146, accuracy : 96.56\n",
            "iteration : 150, loss : 0.1207, accuracy : 96.45\n",
            "iteration : 200, loss : 0.1230, accuracy : 96.38\n",
            "iteration : 250, loss : 0.1213, accuracy : 96.42\n",
            "iteration : 300, loss : 0.1230, accuracy : 96.38\n",
            "iteration : 350, loss : 0.1244, accuracy : 96.35\n",
            "Epoch : 105, training loss : 0.1247, training accuracy : 96.36, test loss : 0.2269, test accuracy : 93.82\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.1202, accuracy : 96.58\n",
            "iteration : 100, loss : 0.1143, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1164, accuracy : 96.52\n",
            "iteration : 200, loss : 0.1183, accuracy : 96.48\n",
            "iteration : 250, loss : 0.1209, accuracy : 96.37\n",
            "iteration : 300, loss : 0.1227, accuracy : 96.36\n",
            "iteration : 350, loss : 0.1251, accuracy : 96.27\n",
            "Epoch : 106, training loss : 0.1258, training accuracy : 96.23, test loss : 0.2338, test accuracy : 93.80\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.1195, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1216, accuracy : 96.20\n",
            "iteration : 150, loss : 0.1206, accuracy : 96.28\n",
            "iteration : 200, loss : 0.1225, accuracy : 96.25\n",
            "iteration : 250, loss : 0.1239, accuracy : 96.22\n",
            "iteration : 300, loss : 0.1282, accuracy : 96.11\n",
            "iteration : 350, loss : 0.1312, accuracy : 96.00\n",
            "Epoch : 107, training loss : 0.1312, training accuracy : 96.01, test loss : 0.2334, test accuracy : 93.73\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.1207, accuracy : 96.50\n",
            "iteration : 100, loss : 0.1134, accuracy : 96.70\n",
            "iteration : 150, loss : 0.1204, accuracy : 96.50\n",
            "iteration : 200, loss : 0.1212, accuracy : 96.49\n",
            "iteration : 250, loss : 0.1227, accuracy : 96.44\n",
            "iteration : 300, loss : 0.1226, accuracy : 96.41\n",
            "iteration : 350, loss : 0.1234, accuracy : 96.38\n",
            "Epoch : 108, training loss : 0.1237, training accuracy : 96.38, test loss : 0.2259, test accuracy : 93.96\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.1069, accuracy : 96.66\n",
            "iteration : 100, loss : 0.1175, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1146, accuracy : 96.58\n",
            "iteration : 200, loss : 0.1173, accuracy : 96.52\n",
            "iteration : 250, loss : 0.1173, accuracy : 96.54\n",
            "iteration : 300, loss : 0.1197, accuracy : 96.45\n",
            "iteration : 350, loss : 0.1220, accuracy : 96.37\n",
            "Epoch : 109, training loss : 0.1237, training accuracy : 96.32, test loss : 0.2409, test accuracy : 93.46\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.1090, accuracy : 96.81\n",
            "iteration : 100, loss : 0.1138, accuracy : 96.66\n",
            "iteration : 150, loss : 0.1198, accuracy : 96.58\n",
            "iteration : 200, loss : 0.1256, accuracy : 96.37\n",
            "iteration : 250, loss : 0.1260, accuracy : 96.33\n",
            "iteration : 300, loss : 0.1257, accuracy : 96.33\n",
            "iteration : 350, loss : 0.1267, accuracy : 96.32\n",
            "Epoch : 110, training loss : 0.1271, training accuracy : 96.30, test loss : 0.2504, test accuracy : 93.21\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.1164, accuracy : 96.44\n",
            "iteration : 100, loss : 0.1156, accuracy : 96.52\n",
            "iteration : 150, loss : 0.1197, accuracy : 96.44\n",
            "iteration : 200, loss : 0.1204, accuracy : 96.43\n",
            "iteration : 250, loss : 0.1214, accuracy : 96.38\n",
            "iteration : 300, loss : 0.1215, accuracy : 96.38\n",
            "iteration : 350, loss : 0.1231, accuracy : 96.35\n",
            "Epoch : 111, training loss : 0.1229, training accuracy : 96.37, test loss : 0.2399, test accuracy : 93.51\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.1253, accuracy : 95.95\n",
            "iteration : 100, loss : 0.1219, accuracy : 96.30\n",
            "iteration : 150, loss : 0.1165, accuracy : 96.51\n",
            "iteration : 200, loss : 0.1174, accuracy : 96.49\n",
            "iteration : 250, loss : 0.1196, accuracy : 96.41\n",
            "iteration : 300, loss : 0.1190, accuracy : 96.41\n",
            "iteration : 350, loss : 0.1193, accuracy : 96.43\n",
            "Epoch : 112, training loss : 0.1203, training accuracy : 96.42, test loss : 0.2468, test accuracy : 93.34\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.1188, accuracy : 96.19\n",
            "iteration : 100, loss : 0.1162, accuracy : 96.28\n",
            "iteration : 150, loss : 0.1232, accuracy : 96.22\n",
            "iteration : 200, loss : 0.1247, accuracy : 96.22\n",
            "iteration : 250, loss : 0.1232, accuracy : 96.25\n",
            "iteration : 300, loss : 0.1240, accuracy : 96.23\n",
            "iteration : 350, loss : 0.1251, accuracy : 96.24\n",
            "Epoch : 113, training loss : 0.1257, training accuracy : 96.24, test loss : 0.2418, test accuracy : 93.39\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.1124, accuracy : 96.97\n",
            "iteration : 100, loss : 0.1118, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1144, accuracy : 96.59\n",
            "iteration : 200, loss : 0.1157, accuracy : 96.53\n",
            "iteration : 250, loss : 0.1171, accuracy : 96.49\n",
            "iteration : 300, loss : 0.1171, accuracy : 96.52\n",
            "iteration : 350, loss : 0.1193, accuracy : 96.42\n",
            "Epoch : 114, training loss : 0.1191, training accuracy : 96.43, test loss : 0.2318, test accuracy : 93.70\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0985, accuracy : 97.20\n",
            "iteration : 100, loss : 0.1030, accuracy : 97.08\n",
            "iteration : 150, loss : 0.1100, accuracy : 96.91\n",
            "iteration : 200, loss : 0.1099, accuracy : 96.87\n",
            "iteration : 250, loss : 0.1140, accuracy : 96.78\n",
            "iteration : 300, loss : 0.1133, accuracy : 96.80\n",
            "iteration : 350, loss : 0.1150, accuracy : 96.77\n",
            "Epoch : 115, training loss : 0.1162, training accuracy : 96.74, test loss : 0.2468, test accuracy : 93.45\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0982, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1090, accuracy : 96.65\n",
            "iteration : 150, loss : 0.1118, accuracy : 96.58\n",
            "iteration : 200, loss : 0.1179, accuracy : 96.44\n",
            "iteration : 250, loss : 0.1178, accuracy : 96.44\n",
            "iteration : 300, loss : 0.1190, accuracy : 96.41\n",
            "iteration : 350, loss : 0.1210, accuracy : 96.37\n",
            "Epoch : 116, training loss : 0.1204, training accuracy : 96.38, test loss : 0.2287, test accuracy : 93.85\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.1220, accuracy : 96.33\n",
            "iteration : 100, loss : 0.1176, accuracy : 96.63\n",
            "iteration : 150, loss : 0.1195, accuracy : 96.55\n",
            "iteration : 200, loss : 0.1212, accuracy : 96.45\n",
            "iteration : 250, loss : 0.1211, accuracy : 96.47\n",
            "iteration : 300, loss : 0.1195, accuracy : 96.51\n",
            "iteration : 350, loss : 0.1194, accuracy : 96.52\n",
            "Epoch : 117, training loss : 0.1193, training accuracy : 96.52, test loss : 0.2391, test accuracy : 93.61\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.1108, accuracy : 96.56\n",
            "iteration : 100, loss : 0.1156, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1159, accuracy : 96.45\n",
            "iteration : 200, loss : 0.1180, accuracy : 96.41\n",
            "iteration : 250, loss : 0.1184, accuracy : 96.39\n",
            "iteration : 300, loss : 0.1186, accuracy : 96.40\n",
            "iteration : 350, loss : 0.1191, accuracy : 96.40\n",
            "Epoch : 118, training loss : 0.1204, training accuracy : 96.37, test loss : 0.2333, test accuracy : 93.66\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.1200, accuracy : 96.25\n",
            "iteration : 100, loss : 0.1130, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1178, accuracy : 96.42\n",
            "iteration : 200, loss : 0.1176, accuracy : 96.48\n",
            "iteration : 250, loss : 0.1192, accuracy : 96.50\n",
            "iteration : 300, loss : 0.1184, accuracy : 96.50\n",
            "iteration : 350, loss : 0.1190, accuracy : 96.50\n",
            "Epoch : 119, training loss : 0.1188, training accuracy : 96.50, test loss : 0.2317, test accuracy : 93.76\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.1075, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1141, accuracy : 96.62\n",
            "iteration : 150, loss : 0.1152, accuracy : 96.60\n",
            "iteration : 200, loss : 0.1141, accuracy : 96.66\n",
            "iteration : 250, loss : 0.1138, accuracy : 96.64\n",
            "iteration : 300, loss : 0.1142, accuracy : 96.58\n",
            "iteration : 350, loss : 0.1156, accuracy : 96.53\n",
            "Epoch : 120, training loss : 0.1160, training accuracy : 96.51, test loss : 0.2456, test accuracy : 93.50\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.1172, accuracy : 96.64\n",
            "iteration : 100, loss : 0.1117, accuracy : 96.84\n",
            "iteration : 150, loss : 0.1091, accuracy : 96.85\n",
            "iteration : 200, loss : 0.1096, accuracy : 96.75\n",
            "iteration : 250, loss : 0.1097, accuracy : 96.69\n",
            "iteration : 300, loss : 0.1115, accuracy : 96.66\n",
            "iteration : 350, loss : 0.1140, accuracy : 96.60\n",
            "Epoch : 121, training loss : 0.1141, training accuracy : 96.60, test loss : 0.2354, test accuracy : 93.60\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.1062, accuracy : 96.94\n",
            "iteration : 100, loss : 0.1132, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1145, accuracy : 96.66\n",
            "iteration : 200, loss : 0.1112, accuracy : 96.75\n",
            "iteration : 250, loss : 0.1117, accuracy : 96.72\n",
            "iteration : 300, loss : 0.1119, accuracy : 96.74\n",
            "iteration : 350, loss : 0.1138, accuracy : 96.67\n",
            "Epoch : 122, training loss : 0.1143, training accuracy : 96.67, test loss : 0.2366, test accuracy : 93.72\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.1140, accuracy : 96.64\n",
            "iteration : 100, loss : 0.1133, accuracy : 96.72\n",
            "iteration : 150, loss : 0.1142, accuracy : 96.68\n",
            "iteration : 200, loss : 0.1141, accuracy : 96.68\n",
            "iteration : 250, loss : 0.1178, accuracy : 96.53\n",
            "iteration : 300, loss : 0.1171, accuracy : 96.55\n",
            "iteration : 350, loss : 0.1180, accuracy : 96.50\n",
            "Epoch : 123, training loss : 0.1185, training accuracy : 96.49, test loss : 0.2380, test accuracy : 93.53\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.1158, accuracy : 96.45\n",
            "iteration : 100, loss : 0.1114, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1135, accuracy : 96.64\n",
            "iteration : 200, loss : 0.1104, accuracy : 96.71\n",
            "iteration : 250, loss : 0.1125, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1142, accuracy : 96.62\n",
            "iteration : 350, loss : 0.1139, accuracy : 96.60\n",
            "Epoch : 124, training loss : 0.1151, training accuracy : 96.57, test loss : 0.2439, test accuracy : 93.52\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.1174, accuracy : 96.67\n",
            "iteration : 100, loss : 0.1155, accuracy : 96.66\n",
            "iteration : 150, loss : 0.1158, accuracy : 96.65\n",
            "iteration : 200, loss : 0.1122, accuracy : 96.74\n",
            "iteration : 250, loss : 0.1121, accuracy : 96.69\n",
            "iteration : 300, loss : 0.1128, accuracy : 96.65\n",
            "iteration : 350, loss : 0.1141, accuracy : 96.61\n",
            "Epoch : 125, training loss : 0.1153, training accuracy : 96.57, test loss : 0.2382, test accuracy : 93.83\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.1096, accuracy : 96.84\n",
            "iteration : 100, loss : 0.1021, accuracy : 97.06\n",
            "iteration : 150, loss : 0.1018, accuracy : 97.04\n",
            "iteration : 200, loss : 0.1052, accuracy : 96.95\n",
            "iteration : 250, loss : 0.1051, accuracy : 96.92\n",
            "iteration : 300, loss : 0.1079, accuracy : 96.85\n",
            "iteration : 350, loss : 0.1082, accuracy : 96.84\n",
            "Epoch : 126, training loss : 0.1089, training accuracy : 96.79, test loss : 0.2447, test accuracy : 93.56\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0994, accuracy : 96.97\n",
            "iteration : 100, loss : 0.1041, accuracy : 96.85\n",
            "iteration : 150, loss : 0.1060, accuracy : 96.81\n",
            "iteration : 200, loss : 0.1095, accuracy : 96.73\n",
            "iteration : 250, loss : 0.1100, accuracy : 96.75\n",
            "iteration : 300, loss : 0.1117, accuracy : 96.70\n",
            "iteration : 350, loss : 0.1129, accuracy : 96.68\n",
            "Epoch : 127, training loss : 0.1122, training accuracy : 96.72, test loss : 0.2389, test accuracy : 93.72\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0966, accuracy : 97.03\n",
            "iteration : 100, loss : 0.1039, accuracy : 96.76\n",
            "iteration : 150, loss : 0.1101, accuracy : 96.58\n",
            "iteration : 200, loss : 0.1102, accuracy : 96.55\n",
            "iteration : 250, loss : 0.1120, accuracy : 96.56\n",
            "iteration : 300, loss : 0.1118, accuracy : 96.56\n",
            "iteration : 350, loss : 0.1121, accuracy : 96.56\n",
            "Epoch : 128, training loss : 0.1130, training accuracy : 96.53, test loss : 0.2434, test accuracy : 93.55\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.1006, accuracy : 97.09\n",
            "iteration : 100, loss : 0.1052, accuracy : 96.99\n",
            "iteration : 150, loss : 0.1041, accuracy : 96.91\n",
            "iteration : 200, loss : 0.1017, accuracy : 97.03\n",
            "iteration : 250, loss : 0.1034, accuracy : 96.97\n",
            "iteration : 300, loss : 0.1048, accuracy : 96.96\n",
            "iteration : 350, loss : 0.1065, accuracy : 96.92\n",
            "Epoch : 129, training loss : 0.1082, training accuracy : 96.87, test loss : 0.2389, test accuracy : 93.56\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.1065, accuracy : 96.77\n",
            "iteration : 100, loss : 0.1041, accuracy : 97.00\n",
            "iteration : 150, loss : 0.1042, accuracy : 96.97\n",
            "iteration : 200, loss : 0.1035, accuracy : 96.99\n",
            "iteration : 250, loss : 0.1055, accuracy : 96.97\n",
            "iteration : 300, loss : 0.1053, accuracy : 96.94\n",
            "iteration : 350, loss : 0.1067, accuracy : 96.90\n",
            "Epoch : 130, training loss : 0.1079, training accuracy : 96.88, test loss : 0.2468, test accuracy : 93.59\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.1058, accuracy : 96.78\n",
            "iteration : 100, loss : 0.1012, accuracy : 96.96\n",
            "iteration : 150, loss : 0.1017, accuracy : 96.95\n",
            "iteration : 200, loss : 0.1065, accuracy : 96.83\n",
            "iteration : 250, loss : 0.1082, accuracy : 96.79\n",
            "iteration : 300, loss : 0.1096, accuracy : 96.76\n",
            "iteration : 350, loss : 0.1091, accuracy : 96.76\n",
            "Epoch : 131, training loss : 0.1098, training accuracy : 96.75, test loss : 0.2382, test accuracy : 93.62\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.1066, accuracy : 96.84\n",
            "iteration : 100, loss : 0.1080, accuracy : 96.83\n",
            "iteration : 150, loss : 0.1053, accuracy : 96.82\n",
            "iteration : 200, loss : 0.1025, accuracy : 96.89\n",
            "iteration : 250, loss : 0.1050, accuracy : 96.81\n",
            "iteration : 300, loss : 0.1052, accuracy : 96.81\n",
            "iteration : 350, loss : 0.1073, accuracy : 96.75\n",
            "Epoch : 132, training loss : 0.1072, training accuracy : 96.77, test loss : 0.2438, test accuracy : 93.55\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.1094, accuracy : 96.75\n",
            "iteration : 100, loss : 0.1093, accuracy : 96.71\n",
            "iteration : 150, loss : 0.1052, accuracy : 96.90\n",
            "iteration : 200, loss : 0.1079, accuracy : 96.82\n",
            "iteration : 250, loss : 0.1094, accuracy : 96.78\n",
            "iteration : 300, loss : 0.1109, accuracy : 96.73\n",
            "iteration : 350, loss : 0.1102, accuracy : 96.75\n",
            "Epoch : 133, training loss : 0.1109, training accuracy : 96.74, test loss : 0.2340, test accuracy : 93.89\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0912, accuracy : 97.33\n",
            "iteration : 100, loss : 0.1042, accuracy : 96.89\n",
            "iteration : 150, loss : 0.1015, accuracy : 97.03\n",
            "iteration : 200, loss : 0.1002, accuracy : 97.05\n",
            "iteration : 250, loss : 0.1005, accuracy : 97.00\n",
            "iteration : 300, loss : 0.1029, accuracy : 96.92\n",
            "iteration : 350, loss : 0.1033, accuracy : 96.93\n",
            "Epoch : 134, training loss : 0.1037, training accuracy : 96.94, test loss : 0.2259, test accuracy : 94.16\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.1037, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1062, accuracy : 96.98\n",
            "iteration : 150, loss : 0.1091, accuracy : 96.89\n",
            "iteration : 200, loss : 0.1094, accuracy : 96.80\n",
            "iteration : 250, loss : 0.1073, accuracy : 96.88\n",
            "iteration : 300, loss : 0.1072, accuracy : 96.90\n",
            "iteration : 350, loss : 0.1055, accuracy : 96.94\n",
            "Epoch : 135, training loss : 0.1054, training accuracy : 96.94, test loss : 0.2346, test accuracy : 93.73\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0914, accuracy : 97.12\n",
            "iteration : 100, loss : 0.0979, accuracy : 96.91\n",
            "iteration : 150, loss : 0.0985, accuracy : 97.01\n",
            "iteration : 200, loss : 0.1031, accuracy : 96.93\n",
            "iteration : 250, loss : 0.1037, accuracy : 96.94\n",
            "iteration : 300, loss : 0.1061, accuracy : 96.85\n",
            "iteration : 350, loss : 0.1072, accuracy : 96.83\n",
            "Epoch : 136, training loss : 0.1073, training accuracy : 96.84, test loss : 0.2277, test accuracy : 94.06\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0992, accuracy : 96.92\n",
            "iteration : 100, loss : 0.0974, accuracy : 97.12\n",
            "iteration : 150, loss : 0.1015, accuracy : 96.93\n",
            "iteration : 200, loss : 0.1041, accuracy : 96.88\n",
            "iteration : 250, loss : 0.1060, accuracy : 96.85\n",
            "iteration : 300, loss : 0.1043, accuracy : 96.91\n",
            "iteration : 350, loss : 0.1048, accuracy : 96.88\n",
            "Epoch : 137, training loss : 0.1047, training accuracy : 96.88, test loss : 0.2241, test accuracy : 94.15\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.1045, accuracy : 97.05\n",
            "iteration : 100, loss : 0.0999, accuracy : 97.14\n",
            "iteration : 150, loss : 0.0981, accuracy : 97.13\n",
            "iteration : 200, loss : 0.0982, accuracy : 97.14\n",
            "iteration : 250, loss : 0.1001, accuracy : 97.10\n",
            "iteration : 300, loss : 0.1057, accuracy : 96.91\n",
            "iteration : 350, loss : 0.1052, accuracy : 96.92\n",
            "Epoch : 138, training loss : 0.1052, training accuracy : 96.91, test loss : 0.2388, test accuracy : 93.62\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0939, accuracy : 97.19\n",
            "iteration : 100, loss : 0.0957, accuracy : 97.09\n",
            "iteration : 150, loss : 0.0987, accuracy : 97.06\n",
            "iteration : 200, loss : 0.1017, accuracy : 96.96\n",
            "iteration : 250, loss : 0.1038, accuracy : 96.90\n",
            "iteration : 300, loss : 0.1035, accuracy : 96.89\n",
            "iteration : 350, loss : 0.1035, accuracy : 96.91\n",
            "Epoch : 139, training loss : 0.1034, training accuracy : 96.92, test loss : 0.2331, test accuracy : 94.03\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.1022, accuracy : 96.97\n",
            "iteration : 100, loss : 0.1009, accuracy : 97.10\n",
            "iteration : 150, loss : 0.0979, accuracy : 97.19\n",
            "iteration : 200, loss : 0.0995, accuracy : 97.16\n",
            "iteration : 250, loss : 0.1023, accuracy : 97.07\n",
            "iteration : 300, loss : 0.1023, accuracy : 97.08\n",
            "iteration : 350, loss : 0.1006, accuracy : 97.12\n",
            "Epoch : 140, training loss : 0.1005, training accuracy : 97.14, test loss : 0.2375, test accuracy : 93.88\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0751, accuracy : 97.69\n",
            "iteration : 100, loss : 0.0865, accuracy : 97.51\n",
            "iteration : 150, loss : 0.0882, accuracy : 97.40\n",
            "iteration : 200, loss : 0.0924, accuracy : 97.17\n",
            "iteration : 250, loss : 0.0982, accuracy : 97.03\n",
            "iteration : 300, loss : 0.0995, accuracy : 96.98\n",
            "iteration : 350, loss : 0.1029, accuracy : 96.87\n",
            "Epoch : 141, training loss : 0.1023, training accuracy : 96.89, test loss : 0.2394, test accuracy : 93.68\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0985, accuracy : 97.20\n",
            "iteration : 100, loss : 0.0960, accuracy : 97.17\n",
            "iteration : 150, loss : 0.0945, accuracy : 97.24\n",
            "iteration : 200, loss : 0.0949, accuracy : 97.15\n",
            "iteration : 250, loss : 0.0959, accuracy : 97.10\n",
            "iteration : 300, loss : 0.0998, accuracy : 97.01\n",
            "iteration : 350, loss : 0.1006, accuracy : 96.98\n",
            "Epoch : 142, training loss : 0.1010, training accuracy : 96.99, test loss : 0.2320, test accuracy : 93.68\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0840, accuracy : 97.36\n",
            "iteration : 100, loss : 0.0887, accuracy : 97.31\n",
            "iteration : 150, loss : 0.0916, accuracy : 97.21\n",
            "iteration : 200, loss : 0.0948, accuracy : 97.20\n",
            "iteration : 250, loss : 0.0971, accuracy : 97.14\n",
            "iteration : 300, loss : 0.0967, accuracy : 97.11\n",
            "iteration : 350, loss : 0.0978, accuracy : 97.09\n",
            "Epoch : 143, training loss : 0.0982, training accuracy : 97.06, test loss : 0.2249, test accuracy : 94.19\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0862, accuracy : 97.55\n",
            "iteration : 100, loss : 0.0842, accuracy : 97.51\n",
            "iteration : 150, loss : 0.0867, accuracy : 97.46\n",
            "iteration : 200, loss : 0.0881, accuracy : 97.39\n",
            "iteration : 250, loss : 0.0917, accuracy : 97.28\n",
            "iteration : 300, loss : 0.0929, accuracy : 97.23\n",
            "iteration : 350, loss : 0.0949, accuracy : 97.17\n",
            "Epoch : 144, training loss : 0.0964, training accuracy : 97.14, test loss : 0.2320, test accuracy : 93.93\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0910, accuracy : 97.05\n",
            "iteration : 100, loss : 0.0966, accuracy : 97.04\n",
            "iteration : 150, loss : 0.0975, accuracy : 97.04\n",
            "iteration : 200, loss : 0.0987, accuracy : 97.02\n",
            "iteration : 250, loss : 0.0995, accuracy : 96.97\n",
            "iteration : 300, loss : 0.0989, accuracy : 96.98\n",
            "iteration : 350, loss : 0.0990, accuracy : 96.99\n",
            "Epoch : 145, training loss : 0.0988, training accuracy : 97.00, test loss : 0.2462, test accuracy : 93.57\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0897, accuracy : 97.23\n",
            "iteration : 100, loss : 0.0898, accuracy : 97.20\n",
            "iteration : 150, loss : 0.0933, accuracy : 97.15\n",
            "iteration : 200, loss : 0.0953, accuracy : 97.12\n",
            "iteration : 250, loss : 0.0952, accuracy : 97.16\n",
            "iteration : 300, loss : 0.0975, accuracy : 97.08\n",
            "iteration : 350, loss : 0.0990, accuracy : 97.05\n",
            "Epoch : 146, training loss : 0.0991, training accuracy : 97.04, test loss : 0.2447, test accuracy : 93.63\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0874, accuracy : 97.33\n",
            "iteration : 100, loss : 0.0851, accuracy : 97.36\n",
            "iteration : 150, loss : 0.0878, accuracy : 97.36\n",
            "iteration : 200, loss : 0.0901, accuracy : 97.28\n",
            "iteration : 250, loss : 0.0934, accuracy : 97.18\n",
            "iteration : 300, loss : 0.0965, accuracy : 97.12\n",
            "iteration : 350, loss : 0.0966, accuracy : 97.08\n",
            "Epoch : 147, training loss : 0.0964, training accuracy : 97.09, test loss : 0.2182, test accuracy : 94.16\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0936, accuracy : 97.23\n",
            "iteration : 100, loss : 0.0920, accuracy : 97.20\n",
            "iteration : 150, loss : 0.0895, accuracy : 97.33\n",
            "iteration : 200, loss : 0.0900, accuracy : 97.36\n",
            "iteration : 250, loss : 0.0917, accuracy : 97.30\n",
            "iteration : 300, loss : 0.0932, accuracy : 97.30\n",
            "iteration : 350, loss : 0.0938, accuracy : 97.24\n",
            "Epoch : 148, training loss : 0.0939, training accuracy : 97.23, test loss : 0.2358, test accuracy : 93.70\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0832, accuracy : 97.75\n",
            "iteration : 100, loss : 0.0881, accuracy : 97.48\n",
            "iteration : 150, loss : 0.0878, accuracy : 97.46\n",
            "iteration : 200, loss : 0.0908, accuracy : 97.37\n",
            "iteration : 250, loss : 0.0938, accuracy : 97.29\n",
            "iteration : 300, loss : 0.0967, accuracy : 97.22\n",
            "iteration : 350, loss : 0.0971, accuracy : 97.19\n",
            "Epoch : 149, training loss : 0.0966, training accuracy : 97.21, test loss : 0.2346, test accuracy : 93.85\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0869, accuracy : 97.50\n",
            "iteration : 100, loss : 0.0922, accuracy : 97.34\n",
            "iteration : 150, loss : 0.0889, accuracy : 97.45\n",
            "iteration : 200, loss : 0.0886, accuracy : 97.40\n",
            "iteration : 250, loss : 0.0895, accuracy : 97.35\n",
            "iteration : 300, loss : 0.0907, accuracy : 97.31\n",
            "iteration : 350, loss : 0.0921, accuracy : 97.28\n",
            "Epoch : 150, training loss : 0.0929, training accuracy : 97.25, test loss : 0.2343, test accuracy : 93.92\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0926, accuracy : 97.17\n",
            "iteration : 100, loss : 0.0936, accuracy : 97.16\n",
            "iteration : 150, loss : 0.0959, accuracy : 97.12\n",
            "iteration : 200, loss : 0.0956, accuracy : 97.17\n",
            "iteration : 250, loss : 0.0960, accuracy : 97.14\n",
            "iteration : 300, loss : 0.0972, accuracy : 97.09\n",
            "iteration : 350, loss : 0.0973, accuracy : 97.11\n",
            "Epoch : 151, training loss : 0.0973, training accuracy : 97.10, test loss : 0.2312, test accuracy : 93.95\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0975, accuracy : 97.06\n",
            "iteration : 100, loss : 0.0912, accuracy : 97.28\n",
            "iteration : 150, loss : 0.0947, accuracy : 97.15\n",
            "iteration : 200, loss : 0.0923, accuracy : 97.20\n",
            "iteration : 250, loss : 0.0931, accuracy : 97.21\n",
            "iteration : 300, loss : 0.0925, accuracy : 97.24\n",
            "iteration : 350, loss : 0.0939, accuracy : 97.23\n",
            "Epoch : 152, training loss : 0.0939, training accuracy : 97.23, test loss : 0.2332, test accuracy : 93.83\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0832, accuracy : 97.64\n",
            "iteration : 100, loss : 0.0902, accuracy : 97.44\n",
            "iteration : 150, loss : 0.0865, accuracy : 97.52\n",
            "iteration : 200, loss : 0.0880, accuracy : 97.45\n",
            "iteration : 250, loss : 0.0914, accuracy : 97.33\n",
            "iteration : 300, loss : 0.0923, accuracy : 97.29\n",
            "iteration : 350, loss : 0.0916, accuracy : 97.30\n",
            "Epoch : 153, training loss : 0.0911, training accuracy : 97.31, test loss : 0.2409, test accuracy : 93.81\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0869, accuracy : 97.48\n",
            "iteration : 100, loss : 0.0858, accuracy : 97.51\n",
            "iteration : 150, loss : 0.0886, accuracy : 97.40\n",
            "iteration : 200, loss : 0.0890, accuracy : 97.38\n",
            "iteration : 250, loss : 0.0885, accuracy : 97.37\n",
            "iteration : 300, loss : 0.0901, accuracy : 97.30\n",
            "iteration : 350, loss : 0.0922, accuracy : 97.29\n",
            "Epoch : 154, training loss : 0.0927, training accuracy : 97.26, test loss : 0.2354, test accuracy : 93.93\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0891, accuracy : 97.27\n",
            "iteration : 100, loss : 0.0882, accuracy : 97.27\n",
            "iteration : 150, loss : 0.0870, accuracy : 97.35\n",
            "iteration : 200, loss : 0.0877, accuracy : 97.38\n",
            "iteration : 250, loss : 0.0892, accuracy : 97.33\n",
            "iteration : 300, loss : 0.0881, accuracy : 97.35\n",
            "iteration : 350, loss : 0.0898, accuracy : 97.32\n",
            "Epoch : 155, training loss : 0.0910, training accuracy : 97.28, test loss : 0.2391, test accuracy : 93.87\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0895, accuracy : 97.41\n",
            "iteration : 100, loss : 0.0882, accuracy : 97.47\n",
            "iteration : 150, loss : 0.0866, accuracy : 97.55\n",
            "iteration : 200, loss : 0.0861, accuracy : 97.56\n",
            "iteration : 250, loss : 0.0874, accuracy : 97.46\n",
            "iteration : 300, loss : 0.0885, accuracy : 97.43\n",
            "iteration : 350, loss : 0.0890, accuracy : 97.41\n",
            "Epoch : 156, training loss : 0.0896, training accuracy : 97.40, test loss : 0.2414, test accuracy : 93.68\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0949, accuracy : 97.17\n",
            "iteration : 100, loss : 0.0887, accuracy : 97.43\n",
            "iteration : 150, loss : 0.0900, accuracy : 97.41\n",
            "iteration : 200, loss : 0.0919, accuracy : 97.36\n",
            "iteration : 250, loss : 0.0938, accuracy : 97.23\n",
            "iteration : 300, loss : 0.0917, accuracy : 97.26\n",
            "iteration : 350, loss : 0.0920, accuracy : 97.24\n",
            "Epoch : 157, training loss : 0.0918, training accuracy : 97.25, test loss : 0.2401, test accuracy : 93.71\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0957, accuracy : 97.05\n",
            "iteration : 100, loss : 0.0853, accuracy : 97.41\n",
            "iteration : 150, loss : 0.0883, accuracy : 97.26\n",
            "iteration : 200, loss : 0.0882, accuracy : 97.30\n",
            "iteration : 250, loss : 0.0908, accuracy : 97.31\n",
            "iteration : 300, loss : 0.0899, accuracy : 97.32\n",
            "iteration : 350, loss : 0.0903, accuracy : 97.33\n",
            "Epoch : 158, training loss : 0.0894, training accuracy : 97.35, test loss : 0.2303, test accuracy : 94.12\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0805, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0874, accuracy : 97.44\n",
            "iteration : 150, loss : 0.0846, accuracy : 97.50\n",
            "iteration : 200, loss : 0.0887, accuracy : 97.41\n",
            "iteration : 250, loss : 0.0882, accuracy : 97.42\n",
            "iteration : 300, loss : 0.0888, accuracy : 97.38\n",
            "iteration : 350, loss : 0.0901, accuracy : 97.34\n",
            "Epoch : 159, training loss : 0.0899, training accuracy : 97.34, test loss : 0.2359, test accuracy : 93.98\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0766, accuracy : 97.80\n",
            "iteration : 100, loss : 0.0793, accuracy : 97.70\n",
            "iteration : 150, loss : 0.0834, accuracy : 97.56\n",
            "iteration : 200, loss : 0.0820, accuracy : 97.62\n",
            "iteration : 250, loss : 0.0831, accuracy : 97.57\n",
            "iteration : 300, loss : 0.0848, accuracy : 97.56\n",
            "iteration : 350, loss : 0.0849, accuracy : 97.55\n",
            "Epoch : 160, training loss : 0.0842, training accuracy : 97.57, test loss : 0.2345, test accuracy : 94.06\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0737, accuracy : 97.83\n",
            "iteration : 100, loss : 0.0778, accuracy : 97.69\n",
            "iteration : 150, loss : 0.0789, accuracy : 97.58\n",
            "iteration : 200, loss : 0.0810, accuracy : 97.54\n",
            "iteration : 250, loss : 0.0838, accuracy : 97.41\n",
            "iteration : 300, loss : 0.0856, accuracy : 97.40\n",
            "iteration : 350, loss : 0.0841, accuracy : 97.46\n",
            "Epoch : 161, training loss : 0.0840, training accuracy : 97.44, test loss : 0.2357, test accuracy : 94.03\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0760, accuracy : 97.73\n",
            "iteration : 100, loss : 0.0839, accuracy : 97.40\n",
            "iteration : 150, loss : 0.0859, accuracy : 97.38\n",
            "iteration : 200, loss : 0.0853, accuracy : 97.38\n",
            "iteration : 250, loss : 0.0845, accuracy : 97.41\n",
            "iteration : 300, loss : 0.0869, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0882, accuracy : 97.34\n",
            "Epoch : 162, training loss : 0.0887, training accuracy : 97.34, test loss : 0.2268, test accuracy : 94.08\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0876, accuracy : 97.39\n",
            "iteration : 100, loss : 0.0789, accuracy : 97.59\n",
            "iteration : 150, loss : 0.0803, accuracy : 97.60\n",
            "iteration : 200, loss : 0.0782, accuracy : 97.68\n",
            "iteration : 250, loss : 0.0803, accuracy : 97.61\n",
            "iteration : 300, loss : 0.0827, accuracy : 97.56\n",
            "iteration : 350, loss : 0.0831, accuracy : 97.51\n",
            "Epoch : 163, training loss : 0.0837, training accuracy : 97.49, test loss : 0.2379, test accuracy : 93.92\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0691, accuracy : 98.00\n",
            "iteration : 100, loss : 0.0761, accuracy : 97.73\n",
            "iteration : 150, loss : 0.0771, accuracy : 97.75\n",
            "iteration : 200, loss : 0.0765, accuracy : 97.80\n",
            "iteration : 250, loss : 0.0794, accuracy : 97.68\n",
            "iteration : 300, loss : 0.0802, accuracy : 97.64\n",
            "iteration : 350, loss : 0.0820, accuracy : 97.60\n",
            "Epoch : 164, training loss : 0.0826, training accuracy : 97.58, test loss : 0.2423, test accuracy : 93.73\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0747, accuracy : 97.78\n",
            "iteration : 100, loss : 0.0757, accuracy : 97.68\n",
            "iteration : 150, loss : 0.0797, accuracy : 97.61\n",
            "iteration : 200, loss : 0.0810, accuracy : 97.55\n",
            "iteration : 250, loss : 0.0831, accuracy : 97.53\n",
            "iteration : 300, loss : 0.0839, accuracy : 97.46\n",
            "iteration : 350, loss : 0.0838, accuracy : 97.48\n",
            "Epoch : 165, training loss : 0.0840, training accuracy : 97.46, test loss : 0.2341, test accuracy : 93.89\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0723, accuracy : 97.88\n",
            "iteration : 100, loss : 0.0806, accuracy : 97.56\n",
            "iteration : 150, loss : 0.0830, accuracy : 97.47\n",
            "iteration : 200, loss : 0.0837, accuracy : 97.44\n",
            "iteration : 250, loss : 0.0820, accuracy : 97.50\n",
            "iteration : 300, loss : 0.0816, accuracy : 97.52\n",
            "iteration : 350, loss : 0.0816, accuracy : 97.53\n",
            "Epoch : 166, training loss : 0.0823, training accuracy : 97.51, test loss : 0.2457, test accuracy : 93.92\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0790, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0799, accuracy : 97.61\n",
            "iteration : 150, loss : 0.0764, accuracy : 97.73\n",
            "iteration : 200, loss : 0.0768, accuracy : 97.72\n",
            "iteration : 250, loss : 0.0758, accuracy : 97.77\n",
            "iteration : 300, loss : 0.0778, accuracy : 97.71\n",
            "iteration : 350, loss : 0.0797, accuracy : 97.65\n",
            "Epoch : 167, training loss : 0.0796, training accuracy : 97.66, test loss : 0.2344, test accuracy : 93.95\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0704, accuracy : 97.95\n",
            "iteration : 100, loss : 0.0686, accuracy : 97.96\n",
            "iteration : 150, loss : 0.0721, accuracy : 97.92\n",
            "iteration : 200, loss : 0.0730, accuracy : 97.88\n",
            "iteration : 250, loss : 0.0758, accuracy : 97.72\n",
            "iteration : 300, loss : 0.0769, accuracy : 97.69\n",
            "iteration : 350, loss : 0.0784, accuracy : 97.62\n",
            "Epoch : 168, training loss : 0.0784, training accuracy : 97.62, test loss : 0.2461, test accuracy : 93.77\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0746, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0804, accuracy : 97.47\n",
            "iteration : 150, loss : 0.0803, accuracy : 97.52\n",
            "iteration : 200, loss : 0.0837, accuracy : 97.42\n",
            "iteration : 250, loss : 0.0838, accuracy : 97.42\n",
            "iteration : 300, loss : 0.0839, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0841, accuracy : 97.40\n",
            "Epoch : 169, training loss : 0.0844, training accuracy : 97.40, test loss : 0.2308, test accuracy : 94.05\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0630, accuracy : 98.14\n",
            "iteration : 100, loss : 0.0727, accuracy : 97.91\n",
            "iteration : 150, loss : 0.0759, accuracy : 97.81\n",
            "iteration : 200, loss : 0.0763, accuracy : 97.84\n",
            "iteration : 250, loss : 0.0760, accuracy : 97.81\n",
            "iteration : 300, loss : 0.0759, accuracy : 97.79\n",
            "iteration : 350, loss : 0.0758, accuracy : 97.77\n",
            "Epoch : 170, training loss : 0.0762, training accuracy : 97.78, test loss : 0.2422, test accuracy : 93.84\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0766, accuracy : 97.55\n",
            "iteration : 100, loss : 0.0744, accuracy : 97.72\n",
            "iteration : 150, loss : 0.0712, accuracy : 97.88\n",
            "iteration : 200, loss : 0.0728, accuracy : 97.83\n",
            "iteration : 250, loss : 0.0740, accuracy : 97.78\n",
            "iteration : 300, loss : 0.0752, accuracy : 97.76\n",
            "iteration : 350, loss : 0.0772, accuracy : 97.68\n",
            "Epoch : 171, training loss : 0.0776, training accuracy : 97.66, test loss : 0.2452, test accuracy : 94.05\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0667, accuracy : 97.75\n",
            "iteration : 100, loss : 0.0778, accuracy : 97.54\n",
            "iteration : 150, loss : 0.0797, accuracy : 97.56\n",
            "iteration : 200, loss : 0.0794, accuracy : 97.57\n",
            "iteration : 250, loss : 0.0784, accuracy : 97.58\n",
            "iteration : 300, loss : 0.0795, accuracy : 97.57\n",
            "iteration : 350, loss : 0.0790, accuracy : 97.59\n",
            "Epoch : 172, training loss : 0.0784, training accuracy : 97.62, test loss : 0.2325, test accuracy : 94.24\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0658, accuracy : 98.20\n",
            "iteration : 100, loss : 0.0669, accuracy : 98.10\n",
            "iteration : 150, loss : 0.0707, accuracy : 97.93\n",
            "iteration : 200, loss : 0.0727, accuracy : 97.82\n",
            "iteration : 250, loss : 0.0766, accuracy : 97.72\n",
            "iteration : 300, loss : 0.0766, accuracy : 97.76\n",
            "iteration : 350, loss : 0.0772, accuracy : 97.75\n",
            "Epoch : 173, training loss : 0.0768, training accuracy : 97.76, test loss : 0.2376, test accuracy : 94.09\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0748, accuracy : 97.75\n",
            "iteration : 100, loss : 0.0754, accuracy : 97.68\n",
            "iteration : 150, loss : 0.0732, accuracy : 97.79\n",
            "iteration : 200, loss : 0.0742, accuracy : 97.76\n",
            "iteration : 250, loss : 0.0737, accuracy : 97.75\n",
            "iteration : 300, loss : 0.0756, accuracy : 97.72\n",
            "iteration : 350, loss : 0.0760, accuracy : 97.69\n",
            "Epoch : 174, training loss : 0.0760, training accuracy : 97.69, test loss : 0.2352, test accuracy : 94.06\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0642, accuracy : 98.06\n",
            "iteration : 100, loss : 0.0741, accuracy : 97.74\n",
            "iteration : 150, loss : 0.0725, accuracy : 97.81\n",
            "iteration : 200, loss : 0.0721, accuracy : 97.80\n",
            "iteration : 250, loss : 0.0705, accuracy : 97.86\n",
            "iteration : 300, loss : 0.0716, accuracy : 97.82\n",
            "iteration : 350, loss : 0.0731, accuracy : 97.75\n",
            "Epoch : 175, training loss : 0.0725, training accuracy : 97.77, test loss : 0.2402, test accuracy : 93.94\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0679, accuracy : 98.14\n",
            "iteration : 100, loss : 0.0722, accuracy : 97.91\n",
            "iteration : 150, loss : 0.0728, accuracy : 97.88\n",
            "iteration : 200, loss : 0.0744, accuracy : 97.87\n",
            "iteration : 250, loss : 0.0736, accuracy : 97.88\n",
            "iteration : 300, loss : 0.0735, accuracy : 97.85\n",
            "iteration : 350, loss : 0.0740, accuracy : 97.83\n",
            "Epoch : 176, training loss : 0.0739, training accuracy : 97.83, test loss : 0.2464, test accuracy : 93.83\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0758, accuracy : 97.86\n",
            "iteration : 100, loss : 0.0718, accuracy : 97.96\n",
            "iteration : 150, loss : 0.0715, accuracy : 97.95\n",
            "iteration : 200, loss : 0.0753, accuracy : 97.84\n",
            "iteration : 250, loss : 0.0739, accuracy : 97.85\n",
            "iteration : 300, loss : 0.0736, accuracy : 97.84\n",
            "iteration : 350, loss : 0.0730, accuracy : 97.84\n",
            "Epoch : 177, training loss : 0.0731, training accuracy : 97.83, test loss : 0.2417, test accuracy : 94.02\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0635, accuracy : 98.09\n",
            "iteration : 100, loss : 0.0707, accuracy : 97.92\n",
            "iteration : 150, loss : 0.0737, accuracy : 97.86\n",
            "iteration : 200, loss : 0.0719, accuracy : 97.85\n",
            "iteration : 250, loss : 0.0719, accuracy : 97.84\n",
            "iteration : 300, loss : 0.0724, accuracy : 97.83\n",
            "iteration : 350, loss : 0.0724, accuracy : 97.83\n",
            "Epoch : 178, training loss : 0.0724, training accuracy : 97.83, test loss : 0.2378, test accuracy : 94.14\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0704, accuracy : 97.81\n",
            "iteration : 100, loss : 0.0731, accuracy : 97.75\n",
            "iteration : 150, loss : 0.0742, accuracy : 97.74\n",
            "iteration : 200, loss : 0.0731, accuracy : 97.74\n",
            "iteration : 250, loss : 0.0749, accuracy : 97.74\n",
            "iteration : 300, loss : 0.0750, accuracy : 97.72\n",
            "iteration : 350, loss : 0.0734, accuracy : 97.76\n",
            "Epoch : 179, training loss : 0.0739, training accuracy : 97.74, test loss : 0.2321, test accuracy : 94.32\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0707, accuracy : 98.08\n",
            "iteration : 100, loss : 0.0671, accuracy : 98.02\n",
            "iteration : 150, loss : 0.0689, accuracy : 97.99\n",
            "iteration : 200, loss : 0.0699, accuracy : 97.90\n",
            "iteration : 250, loss : 0.0703, accuracy : 97.91\n",
            "iteration : 300, loss : 0.0722, accuracy : 97.84\n",
            "iteration : 350, loss : 0.0729, accuracy : 97.78\n",
            "Epoch : 180, training loss : 0.0734, training accuracy : 97.76, test loss : 0.2319, test accuracy : 94.21\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0736, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0703, accuracy : 97.90\n",
            "iteration : 150, loss : 0.0692, accuracy : 97.97\n",
            "iteration : 200, loss : 0.0718, accuracy : 97.92\n",
            "iteration : 250, loss : 0.0719, accuracy : 97.88\n",
            "iteration : 300, loss : 0.0700, accuracy : 97.93\n",
            "iteration : 350, loss : 0.0704, accuracy : 97.90\n",
            "Epoch : 181, training loss : 0.0711, training accuracy : 97.89, test loss : 0.2367, test accuracy : 94.19\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0646, accuracy : 97.98\n",
            "iteration : 100, loss : 0.0638, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0688, accuracy : 97.94\n",
            "iteration : 200, loss : 0.0685, accuracy : 97.98\n",
            "iteration : 250, loss : 0.0684, accuracy : 97.98\n",
            "iteration : 300, loss : 0.0694, accuracy : 97.96\n",
            "iteration : 350, loss : 0.0691, accuracy : 97.96\n",
            "Epoch : 182, training loss : 0.0690, training accuracy : 97.97, test loss : 0.2339, test accuracy : 93.98\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0617, accuracy : 98.06\n",
            "iteration : 100, loss : 0.0656, accuracy : 97.98\n",
            "iteration : 150, loss : 0.0628, accuracy : 98.09\n",
            "iteration : 200, loss : 0.0634, accuracy : 98.09\n",
            "iteration : 250, loss : 0.0641, accuracy : 98.07\n",
            "iteration : 300, loss : 0.0651, accuracy : 98.06\n",
            "iteration : 350, loss : 0.0680, accuracy : 97.98\n",
            "Epoch : 183, training loss : 0.0682, training accuracy : 97.96, test loss : 0.2358, test accuracy : 94.24\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0656, accuracy : 98.16\n",
            "iteration : 100, loss : 0.0661, accuracy : 98.12\n",
            "iteration : 150, loss : 0.0663, accuracy : 98.06\n",
            "iteration : 200, loss : 0.0680, accuracy : 98.05\n",
            "iteration : 250, loss : 0.0691, accuracy : 98.03\n",
            "iteration : 300, loss : 0.0696, accuracy : 98.03\n",
            "iteration : 350, loss : 0.0709, accuracy : 97.98\n",
            "Epoch : 184, training loss : 0.0701, training accuracy : 98.00, test loss : 0.2355, test accuracy : 94.16\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0727, accuracy : 97.94\n",
            "iteration : 100, loss : 0.0702, accuracy : 97.95\n",
            "iteration : 150, loss : 0.0700, accuracy : 97.95\n",
            "iteration : 200, loss : 0.0677, accuracy : 98.01\n",
            "iteration : 250, loss : 0.0667, accuracy : 98.06\n",
            "iteration : 300, loss : 0.0685, accuracy : 97.98\n",
            "iteration : 350, loss : 0.0683, accuracy : 97.98\n",
            "Epoch : 185, training loss : 0.0682, training accuracy : 97.99, test loss : 0.2413, test accuracy : 94.23\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0609, accuracy : 98.19\n",
            "iteration : 100, loss : 0.0693, accuracy : 97.94\n",
            "iteration : 150, loss : 0.0717, accuracy : 97.85\n",
            "iteration : 200, loss : 0.0706, accuracy : 97.89\n",
            "iteration : 250, loss : 0.0691, accuracy : 97.94\n",
            "iteration : 300, loss : 0.0680, accuracy : 97.98\n",
            "iteration : 350, loss : 0.0671, accuracy : 98.00\n",
            "Epoch : 186, training loss : 0.0673, training accuracy : 97.98, test loss : 0.2363, test accuracy : 94.22\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0545, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0625, accuracy : 98.13\n",
            "iteration : 150, loss : 0.0641, accuracy : 98.08\n",
            "iteration : 200, loss : 0.0640, accuracy : 98.10\n",
            "iteration : 250, loss : 0.0638, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0656, accuracy : 98.05\n",
            "iteration : 350, loss : 0.0655, accuracy : 98.06\n",
            "Epoch : 187, training loss : 0.0658, training accuracy : 98.06, test loss : 0.2450, test accuracy : 94.01\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0570, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0589, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0608, accuracy : 98.24\n",
            "iteration : 200, loss : 0.0609, accuracy : 98.22\n",
            "iteration : 250, loss : 0.0629, accuracy : 98.17\n",
            "iteration : 300, loss : 0.0653, accuracy : 98.11\n",
            "iteration : 350, loss : 0.0655, accuracy : 98.07\n",
            "Epoch : 188, training loss : 0.0656, training accuracy : 98.05, test loss : 0.2349, test accuracy : 94.23\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0653, accuracy : 97.91\n",
            "iteration : 100, loss : 0.0622, accuracy : 98.02\n",
            "iteration : 150, loss : 0.0594, accuracy : 98.20\n",
            "iteration : 200, loss : 0.0605, accuracy : 98.18\n",
            "iteration : 250, loss : 0.0616, accuracy : 98.13\n",
            "iteration : 300, loss : 0.0642, accuracy : 98.03\n",
            "iteration : 350, loss : 0.0644, accuracy : 98.04\n",
            "Epoch : 189, training loss : 0.0640, training accuracy : 98.05, test loss : 0.2451, test accuracy : 94.13\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0469, accuracy : 98.59\n",
            "iteration : 100, loss : 0.0511, accuracy : 98.47\n",
            "iteration : 150, loss : 0.0547, accuracy : 98.32\n",
            "iteration : 200, loss : 0.0574, accuracy : 98.23\n",
            "iteration : 250, loss : 0.0578, accuracy : 98.22\n",
            "iteration : 300, loss : 0.0615, accuracy : 98.10\n",
            "iteration : 350, loss : 0.0627, accuracy : 98.08\n",
            "Epoch : 190, training loss : 0.0633, training accuracy : 98.07, test loss : 0.2408, test accuracy : 94.10\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0699, accuracy : 97.84\n",
            "iteration : 100, loss : 0.0669, accuracy : 97.93\n",
            "iteration : 150, loss : 0.0654, accuracy : 98.01\n",
            "iteration : 200, loss : 0.0652, accuracy : 98.04\n",
            "iteration : 250, loss : 0.0646, accuracy : 98.06\n",
            "iteration : 300, loss : 0.0642, accuracy : 98.09\n",
            "iteration : 350, loss : 0.0652, accuracy : 98.05\n",
            "Epoch : 191, training loss : 0.0660, training accuracy : 98.02, test loss : 0.2395, test accuracy : 94.20\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0624, accuracy : 98.02\n",
            "iteration : 100, loss : 0.0641, accuracy : 97.99\n",
            "iteration : 150, loss : 0.0626, accuracy : 98.09\n",
            "iteration : 200, loss : 0.0633, accuracy : 98.10\n",
            "iteration : 250, loss : 0.0642, accuracy : 97.99\n",
            "iteration : 300, loss : 0.0643, accuracy : 98.02\n",
            "iteration : 350, loss : 0.0632, accuracy : 98.05\n",
            "Epoch : 192, training loss : 0.0630, training accuracy : 98.06, test loss : 0.2314, test accuracy : 94.28\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0516, accuracy : 98.61\n",
            "iteration : 100, loss : 0.0558, accuracy : 98.40\n",
            "iteration : 150, loss : 0.0579, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0599, accuracy : 98.24\n",
            "iteration : 250, loss : 0.0595, accuracy : 98.23\n",
            "iteration : 300, loss : 0.0597, accuracy : 98.23\n",
            "iteration : 350, loss : 0.0600, accuracy : 98.24\n",
            "Epoch : 193, training loss : 0.0601, training accuracy : 98.23, test loss : 0.2340, test accuracy : 94.37\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0580, accuracy : 98.42\n",
            "iteration : 100, loss : 0.0558, accuracy : 98.47\n",
            "iteration : 150, loss : 0.0554, accuracy : 98.45\n",
            "iteration : 200, loss : 0.0578, accuracy : 98.35\n",
            "iteration : 250, loss : 0.0572, accuracy : 98.34\n",
            "iteration : 300, loss : 0.0575, accuracy : 98.32\n",
            "iteration : 350, loss : 0.0587, accuracy : 98.29\n",
            "Epoch : 194, training loss : 0.0590, training accuracy : 98.26, test loss : 0.2489, test accuracy : 94.00\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0521, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0610, accuracy : 98.17\n",
            "iteration : 150, loss : 0.0583, accuracy : 98.23\n",
            "iteration : 200, loss : 0.0605, accuracy : 98.17\n",
            "iteration : 250, loss : 0.0601, accuracy : 98.12\n",
            "iteration : 300, loss : 0.0601, accuracy : 98.16\n",
            "iteration : 350, loss : 0.0602, accuracy : 98.16\n",
            "Epoch : 195, training loss : 0.0601, training accuracy : 98.16, test loss : 0.2467, test accuracy : 94.07\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0550, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0586, accuracy : 98.27\n",
            "iteration : 150, loss : 0.0583, accuracy : 98.26\n",
            "iteration : 200, loss : 0.0612, accuracy : 98.18\n",
            "iteration : 250, loss : 0.0618, accuracy : 98.17\n",
            "iteration : 300, loss : 0.0615, accuracy : 98.19\n",
            "iteration : 350, loss : 0.0601, accuracy : 98.24\n",
            "Epoch : 196, training loss : 0.0608, training accuracy : 98.23, test loss : 0.2451, test accuracy : 94.02\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0643, accuracy : 98.25\n",
            "iteration : 100, loss : 0.0629, accuracy : 98.27\n",
            "iteration : 150, loss : 0.0611, accuracy : 98.26\n",
            "iteration : 200, loss : 0.0593, accuracy : 98.27\n",
            "iteration : 250, loss : 0.0599, accuracy : 98.25\n",
            "iteration : 300, loss : 0.0600, accuracy : 98.25\n",
            "iteration : 350, loss : 0.0597, accuracy : 98.26\n",
            "Epoch : 197, training loss : 0.0599, training accuracy : 98.25, test loss : 0.2420, test accuracy : 94.25\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0586, accuracy : 98.14\n",
            "iteration : 100, loss : 0.0578, accuracy : 98.24\n",
            "iteration : 150, loss : 0.0609, accuracy : 98.14\n",
            "iteration : 200, loss : 0.0577, accuracy : 98.22\n",
            "iteration : 250, loss : 0.0580, accuracy : 98.25\n",
            "iteration : 300, loss : 0.0574, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0569, accuracy : 98.28\n",
            "Epoch : 198, training loss : 0.0564, training accuracy : 98.31, test loss : 0.2356, test accuracy : 94.31\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0517, accuracy : 98.30\n",
            "iteration : 100, loss : 0.0510, accuracy : 98.43\n",
            "iteration : 150, loss : 0.0536, accuracy : 98.41\n",
            "iteration : 200, loss : 0.0533, accuracy : 98.39\n",
            "iteration : 250, loss : 0.0560, accuracy : 98.33\n",
            "iteration : 300, loss : 0.0579, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0570, accuracy : 98.30\n",
            "Epoch : 199, training loss : 0.0577, training accuracy : 98.28, test loss : 0.2422, test accuracy : 94.11\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0551, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0541, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0554, accuracy : 98.36\n",
            "iteration : 200, loss : 0.0583, accuracy : 98.27\n",
            "iteration : 250, loss : 0.0567, accuracy : 98.32\n",
            "iteration : 300, loss : 0.0562, accuracy : 98.31\n",
            "iteration : 350, loss : 0.0562, accuracy : 98.32\n",
            "Epoch : 200, training loss : 0.0566, training accuracy : 98.30, test loss : 0.2413, test accuracy : 94.24\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0582, accuracy : 98.16\n",
            "iteration : 100, loss : 0.0578, accuracy : 98.14\n",
            "iteration : 150, loss : 0.0557, accuracy : 98.23\n",
            "iteration : 200, loss : 0.0554, accuracy : 98.26\n",
            "iteration : 250, loss : 0.0556, accuracy : 98.26\n",
            "iteration : 300, loss : 0.0552, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0538, accuracy : 98.35\n",
            "Epoch : 201, training loss : 0.0536, training accuracy : 98.36, test loss : 0.2454, test accuracy : 94.20\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0546, accuracy : 98.34\n",
            "iteration : 100, loss : 0.0571, accuracy : 98.30\n",
            "iteration : 150, loss : 0.0540, accuracy : 98.39\n",
            "iteration : 200, loss : 0.0532, accuracy : 98.39\n",
            "iteration : 250, loss : 0.0534, accuracy : 98.41\n",
            "iteration : 300, loss : 0.0541, accuracy : 98.39\n",
            "iteration : 350, loss : 0.0548, accuracy : 98.37\n",
            "Epoch : 202, training loss : 0.0550, training accuracy : 98.35, test loss : 0.2416, test accuracy : 94.26\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0408, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0473, accuracy : 98.71\n",
            "iteration : 150, loss : 0.0501, accuracy : 98.59\n",
            "iteration : 200, loss : 0.0528, accuracy : 98.45\n",
            "iteration : 250, loss : 0.0530, accuracy : 98.42\n",
            "iteration : 300, loss : 0.0543, accuracy : 98.39\n",
            "iteration : 350, loss : 0.0538, accuracy : 98.39\n",
            "Epoch : 203, training loss : 0.0531, training accuracy : 98.41, test loss : 0.2447, test accuracy : 94.20\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0484, accuracy : 98.52\n",
            "iteration : 100, loss : 0.0524, accuracy : 98.45\n",
            "iteration : 150, loss : 0.0530, accuracy : 98.40\n",
            "iteration : 200, loss : 0.0536, accuracy : 98.36\n",
            "iteration : 250, loss : 0.0539, accuracy : 98.35\n",
            "iteration : 300, loss : 0.0541, accuracy : 98.38\n",
            "iteration : 350, loss : 0.0549, accuracy : 98.34\n",
            "Epoch : 204, training loss : 0.0554, training accuracy : 98.33, test loss : 0.2393, test accuracy : 94.20\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0581, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0536, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0575, accuracy : 98.30\n",
            "iteration : 200, loss : 0.0551, accuracy : 98.36\n",
            "iteration : 250, loss : 0.0549, accuracy : 98.37\n",
            "iteration : 300, loss : 0.0544, accuracy : 98.36\n",
            "iteration : 350, loss : 0.0550, accuracy : 98.36\n",
            "Epoch : 205, training loss : 0.0549, training accuracy : 98.37, test loss : 0.2390, test accuracy : 94.28\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0517, accuracy : 98.36\n",
            "iteration : 100, loss : 0.0512, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0484, accuracy : 98.57\n",
            "iteration : 200, loss : 0.0504, accuracy : 98.50\n",
            "iteration : 250, loss : 0.0500, accuracy : 98.51\n",
            "iteration : 300, loss : 0.0509, accuracy : 98.47\n",
            "iteration : 350, loss : 0.0508, accuracy : 98.46\n",
            "Epoch : 206, training loss : 0.0509, training accuracy : 98.46, test loss : 0.2456, test accuracy : 94.42\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0526, accuracy : 98.47\n",
            "iteration : 100, loss : 0.0500, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0481, accuracy : 98.52\n",
            "iteration : 200, loss : 0.0499, accuracy : 98.47\n",
            "iteration : 250, loss : 0.0505, accuracy : 98.45\n",
            "iteration : 300, loss : 0.0513, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0524, accuracy : 98.38\n",
            "Epoch : 207, training loss : 0.0525, training accuracy : 98.37, test loss : 0.2389, test accuracy : 94.23\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0442, accuracy : 98.80\n",
            "iteration : 100, loss : 0.0485, accuracy : 98.58\n",
            "iteration : 150, loss : 0.0482, accuracy : 98.55\n",
            "iteration : 200, loss : 0.0484, accuracy : 98.53\n",
            "iteration : 250, loss : 0.0490, accuracy : 98.50\n",
            "iteration : 300, loss : 0.0495, accuracy : 98.46\n",
            "iteration : 350, loss : 0.0508, accuracy : 98.42\n",
            "Epoch : 208, training loss : 0.0505, training accuracy : 98.43, test loss : 0.2360, test accuracy : 94.31\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0371, accuracy : 98.83\n",
            "iteration : 100, loss : 0.0413, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0447, accuracy : 98.71\n",
            "iteration : 200, loss : 0.0477, accuracy : 98.65\n",
            "iteration : 250, loss : 0.0483, accuracy : 98.62\n",
            "iteration : 300, loss : 0.0498, accuracy : 98.56\n",
            "iteration : 350, loss : 0.0514, accuracy : 98.49\n",
            "Epoch : 209, training loss : 0.0519, training accuracy : 98.47, test loss : 0.2299, test accuracy : 94.45\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0533, accuracy : 98.42\n",
            "iteration : 100, loss : 0.0539, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0537, accuracy : 98.39\n",
            "iteration : 200, loss : 0.0533, accuracy : 98.44\n",
            "iteration : 250, loss : 0.0526, accuracy : 98.44\n",
            "iteration : 300, loss : 0.0507, accuracy : 98.49\n",
            "iteration : 350, loss : 0.0523, accuracy : 98.46\n",
            "Epoch : 210, training loss : 0.0527, training accuracy : 98.45, test loss : 0.2470, test accuracy : 94.15\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0438, accuracy : 98.70\n",
            "iteration : 100, loss : 0.0438, accuracy : 98.66\n",
            "iteration : 150, loss : 0.0439, accuracy : 98.66\n",
            "iteration : 200, loss : 0.0457, accuracy : 98.60\n",
            "iteration : 250, loss : 0.0455, accuracy : 98.60\n",
            "iteration : 300, loss : 0.0467, accuracy : 98.58\n",
            "iteration : 350, loss : 0.0467, accuracy : 98.56\n",
            "Epoch : 211, training loss : 0.0467, training accuracy : 98.55, test loss : 0.2438, test accuracy : 94.11\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0414, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0443, accuracy : 98.65\n",
            "iteration : 150, loss : 0.0456, accuracy : 98.65\n",
            "iteration : 200, loss : 0.0475, accuracy : 98.57\n",
            "iteration : 250, loss : 0.0500, accuracy : 98.51\n",
            "iteration : 300, loss : 0.0503, accuracy : 98.50\n",
            "iteration : 350, loss : 0.0514, accuracy : 98.45\n",
            "Epoch : 212, training loss : 0.0513, training accuracy : 98.45, test loss : 0.2420, test accuracy : 94.30\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0470, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0472, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0477, accuracy : 98.60\n",
            "iteration : 200, loss : 0.0485, accuracy : 98.58\n",
            "iteration : 250, loss : 0.0489, accuracy : 98.55\n",
            "iteration : 300, loss : 0.0482, accuracy : 98.56\n",
            "iteration : 350, loss : 0.0489, accuracy : 98.54\n",
            "Epoch : 213, training loss : 0.0497, training accuracy : 98.50, test loss : 0.2438, test accuracy : 94.20\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0464, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0428, accuracy : 98.82\n",
            "iteration : 150, loss : 0.0432, accuracy : 98.79\n",
            "iteration : 200, loss : 0.0446, accuracy : 98.73\n",
            "iteration : 250, loss : 0.0458, accuracy : 98.67\n",
            "iteration : 300, loss : 0.0462, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0462, accuracy : 98.67\n",
            "Epoch : 214, training loss : 0.0466, training accuracy : 98.66, test loss : 0.2362, test accuracy : 94.51\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0441, accuracy : 98.70\n",
            "iteration : 100, loss : 0.0429, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0425, accuracy : 98.74\n",
            "iteration : 200, loss : 0.0431, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0436, accuracy : 98.68\n",
            "iteration : 300, loss : 0.0445, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0447, accuracy : 98.69\n",
            "Epoch : 215, training loss : 0.0450, training accuracy : 98.66, test loss : 0.2412, test accuracy : 94.51\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0465, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0490, accuracy : 98.60\n",
            "iteration : 150, loss : 0.0485, accuracy : 98.60\n",
            "iteration : 200, loss : 0.0478, accuracy : 98.64\n",
            "iteration : 250, loss : 0.0482, accuracy : 98.63\n",
            "iteration : 300, loss : 0.0484, accuracy : 98.62\n",
            "iteration : 350, loss : 0.0486, accuracy : 98.58\n",
            "Epoch : 216, training loss : 0.0487, training accuracy : 98.59, test loss : 0.2395, test accuracy : 94.33\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0414, accuracy : 98.72\n",
            "iteration : 100, loss : 0.0407, accuracy : 98.80\n",
            "iteration : 150, loss : 0.0389, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0376, accuracy : 98.87\n",
            "iteration : 250, loss : 0.0397, accuracy : 98.80\n",
            "iteration : 300, loss : 0.0407, accuracy : 98.77\n",
            "iteration : 350, loss : 0.0416, accuracy : 98.73\n",
            "Epoch : 217, training loss : 0.0420, training accuracy : 98.72, test loss : 0.2458, test accuracy : 94.31\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0374, accuracy : 98.88\n",
            "iteration : 100, loss : 0.0417, accuracy : 98.78\n",
            "iteration : 150, loss : 0.0452, accuracy : 98.67\n",
            "iteration : 200, loss : 0.0450, accuracy : 98.67\n",
            "iteration : 250, loss : 0.0457, accuracy : 98.67\n",
            "iteration : 300, loss : 0.0466, accuracy : 98.64\n",
            "iteration : 350, loss : 0.0460, accuracy : 98.66\n",
            "Epoch : 218, training loss : 0.0455, training accuracy : 98.68, test loss : 0.2466, test accuracy : 94.33\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0422, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0417, accuracy : 98.75\n",
            "iteration : 150, loss : 0.0425, accuracy : 98.70\n",
            "iteration : 200, loss : 0.0420, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0425, accuracy : 98.70\n",
            "iteration : 300, loss : 0.0432, accuracy : 98.70\n",
            "iteration : 350, loss : 0.0440, accuracy : 98.68\n",
            "Epoch : 219, training loss : 0.0445, training accuracy : 98.66, test loss : 0.2372, test accuracy : 94.54\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0386, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0409, accuracy : 98.67\n",
            "iteration : 150, loss : 0.0436, accuracy : 98.56\n",
            "iteration : 200, loss : 0.0436, accuracy : 98.62\n",
            "iteration : 250, loss : 0.0437, accuracy : 98.65\n",
            "iteration : 300, loss : 0.0432, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0429, accuracy : 98.69\n",
            "Epoch : 220, training loss : 0.0430, training accuracy : 98.69, test loss : 0.2379, test accuracy : 94.43\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0398, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0394, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0409, accuracy : 98.76\n",
            "iteration : 200, loss : 0.0404, accuracy : 98.80\n",
            "iteration : 250, loss : 0.0414, accuracy : 98.75\n",
            "iteration : 300, loss : 0.0415, accuracy : 98.75\n",
            "iteration : 350, loss : 0.0419, accuracy : 98.74\n",
            "Epoch : 221, training loss : 0.0425, training accuracy : 98.74, test loss : 0.2483, test accuracy : 94.30\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0405, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0450, accuracy : 98.63\n",
            "iteration : 150, loss : 0.0454, accuracy : 98.59\n",
            "iteration : 200, loss : 0.0428, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0417, accuracy : 98.72\n",
            "iteration : 300, loss : 0.0427, accuracy : 98.69\n",
            "iteration : 350, loss : 0.0433, accuracy : 98.67\n",
            "Epoch : 222, training loss : 0.0432, training accuracy : 98.69, test loss : 0.2406, test accuracy : 94.43\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0380, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0354, accuracy : 98.89\n",
            "iteration : 150, loss : 0.0373, accuracy : 98.81\n",
            "iteration : 200, loss : 0.0392, accuracy : 98.78\n",
            "iteration : 250, loss : 0.0397, accuracy : 98.77\n",
            "iteration : 300, loss : 0.0402, accuracy : 98.74\n",
            "iteration : 350, loss : 0.0397, accuracy : 98.75\n",
            "Epoch : 223, training loss : 0.0392, training accuracy : 98.77, test loss : 0.2505, test accuracy : 94.39\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0419, accuracy : 98.66\n",
            "iteration : 100, loss : 0.0420, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0409, accuracy : 98.80\n",
            "iteration : 200, loss : 0.0407, accuracy : 98.81\n",
            "iteration : 250, loss : 0.0404, accuracy : 98.84\n",
            "iteration : 300, loss : 0.0408, accuracy : 98.82\n",
            "iteration : 350, loss : 0.0410, accuracy : 98.81\n",
            "Epoch : 224, training loss : 0.0413, training accuracy : 98.81, test loss : 0.2461, test accuracy : 94.50\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0395, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0425, accuracy : 98.79\n",
            "iteration : 150, loss : 0.0434, accuracy : 98.70\n",
            "iteration : 200, loss : 0.0428, accuracy : 98.70\n",
            "iteration : 250, loss : 0.0423, accuracy : 98.72\n",
            "iteration : 300, loss : 0.0420, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0419, accuracy : 98.71\n",
            "Epoch : 225, training loss : 0.0420, training accuracy : 98.70, test loss : 0.2412, test accuracy : 94.42\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0424, accuracy : 98.72\n",
            "iteration : 100, loss : 0.0380, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0379, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0375, accuracy : 98.91\n",
            "iteration : 250, loss : 0.0385, accuracy : 98.87\n",
            "iteration : 300, loss : 0.0392, accuracy : 98.85\n",
            "iteration : 350, loss : 0.0383, accuracy : 98.86\n",
            "Epoch : 226, training loss : 0.0382, training accuracy : 98.86, test loss : 0.2454, test accuracy : 94.31\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0386, accuracy : 98.80\n",
            "iteration : 100, loss : 0.0392, accuracy : 98.79\n",
            "iteration : 150, loss : 0.0377, accuracy : 98.89\n",
            "iteration : 200, loss : 0.0371, accuracy : 98.90\n",
            "iteration : 250, loss : 0.0374, accuracy : 98.91\n",
            "iteration : 300, loss : 0.0377, accuracy : 98.89\n",
            "iteration : 350, loss : 0.0391, accuracy : 98.83\n",
            "Epoch : 227, training loss : 0.0389, training accuracy : 98.84, test loss : 0.2459, test accuracy : 94.40\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0368, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0376, accuracy : 98.89\n",
            "iteration : 150, loss : 0.0368, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0388, accuracy : 98.79\n",
            "iteration : 250, loss : 0.0376, accuracy : 98.82\n",
            "iteration : 300, loss : 0.0380, accuracy : 98.82\n",
            "iteration : 350, loss : 0.0372, accuracy : 98.85\n",
            "Epoch : 228, training loss : 0.0380, training accuracy : 98.84, test loss : 0.2437, test accuracy : 94.43\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0316, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0327, accuracy : 99.03\n",
            "iteration : 150, loss : 0.0373, accuracy : 98.88\n",
            "iteration : 200, loss : 0.0375, accuracy : 98.86\n",
            "iteration : 250, loss : 0.0370, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0378, accuracy : 98.85\n",
            "iteration : 350, loss : 0.0381, accuracy : 98.85\n",
            "Epoch : 229, training loss : 0.0378, training accuracy : 98.86, test loss : 0.2433, test accuracy : 94.39\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0397, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0394, accuracy : 98.72\n",
            "iteration : 150, loss : 0.0362, accuracy : 98.84\n",
            "iteration : 200, loss : 0.0367, accuracy : 98.85\n",
            "iteration : 250, loss : 0.0376, accuracy : 98.81\n",
            "iteration : 300, loss : 0.0375, accuracy : 98.81\n",
            "iteration : 350, loss : 0.0376, accuracy : 98.80\n",
            "Epoch : 230, training loss : 0.0375, training accuracy : 98.81, test loss : 0.2412, test accuracy : 94.39\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0371, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0370, accuracy : 98.97\n",
            "iteration : 150, loss : 0.0372, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0366, accuracy : 98.95\n",
            "iteration : 250, loss : 0.0358, accuracy : 98.97\n",
            "iteration : 300, loss : 0.0359, accuracy : 98.98\n",
            "iteration : 350, loss : 0.0358, accuracy : 98.98\n",
            "Epoch : 231, training loss : 0.0358, training accuracy : 98.98, test loss : 0.2422, test accuracy : 94.56\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0369, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0358, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0330, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0346, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0350, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0341, accuracy : 99.02\n",
            "iteration : 350, loss : 0.0346, accuracy : 99.01\n",
            "Epoch : 232, training loss : 0.0348, training accuracy : 99.00, test loss : 0.2416, test accuracy : 94.51\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0389, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0373, accuracy : 98.87\n",
            "iteration : 150, loss : 0.0381, accuracy : 98.81\n",
            "iteration : 200, loss : 0.0382, accuracy : 98.85\n",
            "iteration : 250, loss : 0.0380, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0385, accuracy : 98.87\n",
            "iteration : 350, loss : 0.0375, accuracy : 98.90\n",
            "Epoch : 233, training loss : 0.0372, training accuracy : 98.90, test loss : 0.2413, test accuracy : 94.62\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0360, accuracy : 98.78\n",
            "iteration : 100, loss : 0.0376, accuracy : 98.78\n",
            "iteration : 150, loss : 0.0358, accuracy : 98.86\n",
            "iteration : 200, loss : 0.0353, accuracy : 98.92\n",
            "iteration : 250, loss : 0.0355, accuracy : 98.93\n",
            "iteration : 300, loss : 0.0349, accuracy : 98.94\n",
            "iteration : 350, loss : 0.0354, accuracy : 98.94\n",
            "Epoch : 234, training loss : 0.0350, training accuracy : 98.96, test loss : 0.2379, test accuracy : 94.64\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0300, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0318, accuracy : 99.07\n",
            "iteration : 150, loss : 0.0322, accuracy : 99.07\n",
            "iteration : 200, loss : 0.0325, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0322, accuracy : 99.05\n",
            "iteration : 300, loss : 0.0325, accuracy : 99.04\n",
            "iteration : 350, loss : 0.0321, accuracy : 99.05\n",
            "Epoch : 235, training loss : 0.0320, training accuracy : 99.05, test loss : 0.2414, test accuracy : 94.48\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0354, accuracy : 98.88\n",
            "iteration : 100, loss : 0.0344, accuracy : 98.98\n",
            "iteration : 150, loss : 0.0335, accuracy : 98.99\n",
            "iteration : 200, loss : 0.0340, accuracy : 98.98\n",
            "iteration : 250, loss : 0.0356, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0365, accuracy : 98.91\n",
            "iteration : 350, loss : 0.0359, accuracy : 98.92\n",
            "Epoch : 236, training loss : 0.0363, training accuracy : 98.91, test loss : 0.2404, test accuracy : 94.54\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0306, accuracy : 99.08\n",
            "iteration : 100, loss : 0.0312, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0335, accuracy : 99.03\n",
            "iteration : 200, loss : 0.0343, accuracy : 98.98\n",
            "iteration : 250, loss : 0.0346, accuracy : 98.98\n",
            "iteration : 300, loss : 0.0339, accuracy : 99.02\n",
            "iteration : 350, loss : 0.0336, accuracy : 99.03\n",
            "Epoch : 237, training loss : 0.0336, training accuracy : 99.04, test loss : 0.2413, test accuracy : 94.47\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0345, accuracy : 99.06\n",
            "iteration : 100, loss : 0.0339, accuracy : 99.04\n",
            "iteration : 150, loss : 0.0339, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0317, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0319, accuracy : 99.07\n",
            "iteration : 300, loss : 0.0326, accuracy : 99.04\n",
            "iteration : 350, loss : 0.0338, accuracy : 99.02\n",
            "Epoch : 238, training loss : 0.0338, training accuracy : 99.01, test loss : 0.2424, test accuracy : 94.42\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0298, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0310, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0322, accuracy : 99.03\n",
            "iteration : 200, loss : 0.0327, accuracy : 98.98\n",
            "iteration : 250, loss : 0.0331, accuracy : 98.98\n",
            "iteration : 300, loss : 0.0336, accuracy : 98.96\n",
            "iteration : 350, loss : 0.0327, accuracy : 99.00\n",
            "Epoch : 239, training loss : 0.0327, training accuracy : 99.00, test loss : 0.2417, test accuracy : 94.63\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0317, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0313, accuracy : 99.07\n",
            "iteration : 150, loss : 0.0320, accuracy : 99.08\n",
            "iteration : 200, loss : 0.0315, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0314, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0309, accuracy : 99.13\n",
            "iteration : 350, loss : 0.0307, accuracy : 99.13\n",
            "Epoch : 240, training loss : 0.0303, training accuracy : 99.15, test loss : 0.2440, test accuracy : 94.50\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0295, accuracy : 99.20\n",
            "iteration : 100, loss : 0.0300, accuracy : 99.16\n",
            "iteration : 150, loss : 0.0296, accuracy : 99.15\n",
            "iteration : 200, loss : 0.0284, accuracy : 99.21\n",
            "iteration : 250, loss : 0.0283, accuracy : 99.19\n",
            "iteration : 300, loss : 0.0287, accuracy : 99.17\n",
            "iteration : 350, loss : 0.0296, accuracy : 99.14\n",
            "Epoch : 241, training loss : 0.0295, training accuracy : 99.14, test loss : 0.2455, test accuracy : 94.55\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0278, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0277, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0283, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0296, accuracy : 99.15\n",
            "iteration : 250, loss : 0.0300, accuracy : 99.14\n",
            "iteration : 300, loss : 0.0310, accuracy : 99.09\n",
            "iteration : 350, loss : 0.0315, accuracy : 99.07\n",
            "Epoch : 242, training loss : 0.0315, training accuracy : 99.07, test loss : 0.2450, test accuracy : 94.63\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0252, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0288, accuracy : 99.15\n",
            "iteration : 150, loss : 0.0318, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0297, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0292, accuracy : 99.10\n",
            "iteration : 300, loss : 0.0288, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0290, accuracy : 99.11\n",
            "Epoch : 243, training loss : 0.0291, training accuracy : 99.11, test loss : 0.2389, test accuracy : 94.66\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0222, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0257, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0269, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0275, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0285, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0286, accuracy : 99.13\n",
            "iteration : 350, loss : 0.0295, accuracy : 99.10\n",
            "Epoch : 244, training loss : 0.0292, training accuracy : 99.11, test loss : 0.2488, test accuracy : 94.43\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0270, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0283, accuracy : 99.19\n",
            "iteration : 150, loss : 0.0278, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0294, accuracy : 99.13\n",
            "iteration : 250, loss : 0.0291, accuracy : 99.13\n",
            "iteration : 300, loss : 0.0293, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0287, accuracy : 99.17\n",
            "Epoch : 245, training loss : 0.0288, training accuracy : 99.16, test loss : 0.2421, test accuracy : 94.65\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0269, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0293, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0290, accuracy : 99.14\n",
            "iteration : 200, loss : 0.0291, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0285, accuracy : 99.18\n",
            "iteration : 300, loss : 0.0295, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0293, accuracy : 99.15\n",
            "Epoch : 246, training loss : 0.0295, training accuracy : 99.15, test loss : 0.2488, test accuracy : 94.58\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0314, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0287, accuracy : 99.17\n",
            "iteration : 150, loss : 0.0277, accuracy : 99.19\n",
            "iteration : 200, loss : 0.0268, accuracy : 99.20\n",
            "iteration : 250, loss : 0.0267, accuracy : 99.20\n",
            "iteration : 300, loss : 0.0268, accuracy : 99.20\n",
            "iteration : 350, loss : 0.0272, accuracy : 99.20\n",
            "Epoch : 247, training loss : 0.0270, training accuracy : 99.20, test loss : 0.2477, test accuracy : 94.47\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0222, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0219, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0234, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0243, accuracy : 99.24\n",
            "iteration : 250, loss : 0.0259, accuracy : 99.21\n",
            "iteration : 300, loss : 0.0263, accuracy : 99.20\n",
            "iteration : 350, loss : 0.0269, accuracy : 99.19\n",
            "Epoch : 248, training loss : 0.0273, training accuracy : 99.17, test loss : 0.2403, test accuracy : 94.59\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0273, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0292, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0283, accuracy : 99.22\n",
            "iteration : 200, loss : 0.0280, accuracy : 99.25\n",
            "iteration : 250, loss : 0.0270, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0263, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0259, accuracy : 99.28\n",
            "Epoch : 249, training loss : 0.0258, training accuracy : 99.28, test loss : 0.2464, test accuracy : 94.55\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0270, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0244, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0252, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0259, accuracy : 99.28\n",
            "iteration : 250, loss : 0.0262, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0268, accuracy : 99.22\n",
            "iteration : 350, loss : 0.0272, accuracy : 99.21\n",
            "Epoch : 250, training loss : 0.0268, training accuracy : 99.21, test loss : 0.2420, test accuracy : 94.66\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0218, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0255, accuracy : 99.24\n",
            "iteration : 150, loss : 0.0256, accuracy : 99.25\n",
            "iteration : 200, loss : 0.0257, accuracy : 99.25\n",
            "iteration : 250, loss : 0.0263, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0259, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0262, accuracy : 99.25\n",
            "Epoch : 251, training loss : 0.0264, training accuracy : 99.23, test loss : 0.2470, test accuracy : 94.72\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0257, accuracy : 99.19\n",
            "iteration : 100, loss : 0.0238, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0249, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0246, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0256, accuracy : 99.21\n",
            "iteration : 300, loss : 0.0251, accuracy : 99.23\n",
            "iteration : 350, loss : 0.0251, accuracy : 99.23\n",
            "Epoch : 252, training loss : 0.0261, training accuracy : 99.21, test loss : 0.2464, test accuracy : 94.70\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0237, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0269, accuracy : 99.26\n",
            "iteration : 150, loss : 0.0265, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0259, accuracy : 99.25\n",
            "iteration : 250, loss : 0.0253, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0256, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0254, accuracy : 99.27\n",
            "Epoch : 253, training loss : 0.0248, training accuracy : 99.29, test loss : 0.2441, test accuracy : 94.59\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0261, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0255, accuracy : 99.27\n",
            "iteration : 150, loss : 0.0244, accuracy : 99.30\n",
            "iteration : 200, loss : 0.0239, accuracy : 99.29\n",
            "iteration : 250, loss : 0.0239, accuracy : 99.31\n",
            "iteration : 300, loss : 0.0240, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0243, accuracy : 99.30\n",
            "Epoch : 254, training loss : 0.0245, training accuracy : 99.29, test loss : 0.2428, test accuracy : 94.67\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0240, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0222, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0239, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0247, accuracy : 99.28\n",
            "iteration : 250, loss : 0.0238, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0233, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0235, accuracy : 99.33\n",
            "Epoch : 255, training loss : 0.0239, training accuracy : 99.32, test loss : 0.2525, test accuracy : 94.54\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0235, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0234, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0255, accuracy : 99.22\n",
            "iteration : 200, loss : 0.0246, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0249, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0241, accuracy : 99.30\n",
            "iteration : 350, loss : 0.0246, accuracy : 99.29\n",
            "Epoch : 256, training loss : 0.0244, training accuracy : 99.30, test loss : 0.2452, test accuracy : 94.80\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0271, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0239, accuracy : 99.32\n",
            "iteration : 150, loss : 0.0237, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0230, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0245, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0248, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0247, accuracy : 99.30\n",
            "Epoch : 257, training loss : 0.0246, training accuracy : 99.30, test loss : 0.2505, test accuracy : 94.50\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0254, accuracy : 99.28\n",
            "iteration : 100, loss : 0.0229, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0217, accuracy : 99.34\n",
            "iteration : 200, loss : 0.0219, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0217, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0219, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0220, accuracy : 99.35\n",
            "Epoch : 258, training loss : 0.0218, training accuracy : 99.35, test loss : 0.2452, test accuracy : 94.74\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0214, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0208, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0201, accuracy : 99.37\n",
            "iteration : 200, loss : 0.0210, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0219, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0210, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0219, accuracy : 99.37\n",
            "Epoch : 259, training loss : 0.0221, training accuracy : 99.36, test loss : 0.2511, test accuracy : 94.63\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0194, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0202, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0219, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0221, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0228, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0231, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0233, accuracy : 99.34\n",
            "Epoch : 260, training loss : 0.0240, training accuracy : 99.32, test loss : 0.2491, test accuracy : 94.58\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0240, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0207, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0211, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0215, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0222, accuracy : 99.37\n",
            "iteration : 300, loss : 0.0221, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0211, accuracy : 99.40\n",
            "Epoch : 261, training loss : 0.0213, training accuracy : 99.39, test loss : 0.2447, test accuracy : 94.87\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0165, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0203, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0208, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0207, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0207, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0206, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0205, accuracy : 99.44\n",
            "Epoch : 262, training loss : 0.0207, training accuracy : 99.41, test loss : 0.2462, test accuracy : 94.65\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0244, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0229, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0221, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0213, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0219, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0210, accuracy : 99.40\n",
            "iteration : 350, loss : 0.0206, accuracy : 99.40\n",
            "Epoch : 263, training loss : 0.0208, training accuracy : 99.40, test loss : 0.2518, test accuracy : 94.71\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0234, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.26\n",
            "iteration : 150, loss : 0.0234, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0234, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0236, accuracy : 99.31\n",
            "iteration : 300, loss : 0.0240, accuracy : 99.31\n",
            "iteration : 350, loss : 0.0238, accuracy : 99.33\n",
            "Epoch : 264, training loss : 0.0238, training accuracy : 99.33, test loss : 0.2413, test accuracy : 94.65\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0179, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0170, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0173, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0179, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0186, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0192, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0194, accuracy : 99.46\n",
            "Epoch : 265, training loss : 0.0196, training accuracy : 99.45, test loss : 0.2461, test accuracy : 94.67\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0209, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0222, accuracy : 99.36\n",
            "iteration : 150, loss : 0.0227, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0217, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0224, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0223, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0217, accuracy : 99.38\n",
            "Epoch : 266, training loss : 0.0219, training accuracy : 99.38, test loss : 0.2393, test accuracy : 94.76\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0225, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0215, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0195, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0203, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0203, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0205, accuracy : 99.40\n",
            "iteration : 350, loss : 0.0195, accuracy : 99.44\n",
            "Epoch : 267, training loss : 0.0194, training accuracy : 99.45, test loss : 0.2400, test accuracy : 94.80\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0197, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0229, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0233, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0235, accuracy : 99.33\n",
            "iteration : 250, loss : 0.0229, accuracy : 99.35\n",
            "iteration : 300, loss : 0.0224, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0224, accuracy : 99.37\n",
            "Epoch : 268, training loss : 0.0220, training accuracy : 99.38, test loss : 0.2383, test accuracy : 94.84\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0196, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0203, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0213, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0208, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0208, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0206, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0210, accuracy : 99.40\n",
            "Epoch : 269, training loss : 0.0206, training accuracy : 99.42, test loss : 0.2426, test accuracy : 94.76\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0167, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0207, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0218, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0215, accuracy : 99.38\n",
            "iteration : 250, loss : 0.0213, accuracy : 99.40\n",
            "iteration : 300, loss : 0.0218, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0210, accuracy : 99.40\n",
            "Epoch : 270, training loss : 0.0212, training accuracy : 99.39, test loss : 0.2428, test accuracy : 94.73\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0219, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0192, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0198, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0203, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0204, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0194, accuracy : 99.44\n",
            "iteration : 350, loss : 0.0196, accuracy : 99.42\n",
            "Epoch : 271, training loss : 0.0194, training accuracy : 99.42, test loss : 0.2423, test accuracy : 94.77\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0206, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0195, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0194, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0192, accuracy : 99.47\n",
            "iteration : 250, loss : 0.0195, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0206, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0202, accuracy : 99.43\n",
            "Epoch : 272, training loss : 0.0202, training accuracy : 99.43, test loss : 0.2425, test accuracy : 94.73\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0175, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0181, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0186, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0188, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0192, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0194, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0193, accuracy : 99.46\n",
            "Epoch : 273, training loss : 0.0189, training accuracy : 99.48, test loss : 0.2432, test accuracy : 94.80\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0198, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0211, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0207, accuracy : 99.37\n",
            "iteration : 200, loss : 0.0203, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0192, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0194, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0192, accuracy : 99.43\n",
            "Epoch : 274, training loss : 0.0193, training accuracy : 99.43, test loss : 0.2498, test accuracy : 94.73\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0193, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0205, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0201, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0194, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0190, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0189, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0189, accuracy : 99.48\n",
            "Epoch : 275, training loss : 0.0191, training accuracy : 99.47, test loss : 0.2421, test accuracy : 94.77\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0191, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0187, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0190, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0185, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0184, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0182, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0187, accuracy : 99.51\n",
            "Epoch : 276, training loss : 0.0188, training accuracy : 99.51, test loss : 0.2455, test accuracy : 94.80\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0162, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0192, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0190, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0188, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0181, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0178, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0177, accuracy : 99.51\n",
            "Epoch : 277, training loss : 0.0176, training accuracy : 99.51, test loss : 0.2454, test accuracy : 94.78\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0206, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0188, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0187, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0177, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0186, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0181, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.51\n",
            "Epoch : 278, training loss : 0.0178, training accuracy : 99.50, test loss : 0.2537, test accuracy : 94.67\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0168, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0170, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0175, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0173, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0170, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.52\n",
            "Epoch : 279, training loss : 0.0178, training accuracy : 99.51, test loss : 0.2499, test accuracy : 94.70\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0160, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0167, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0179, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0181, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0178, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0180, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0178, accuracy : 99.49\n",
            "Epoch : 280, training loss : 0.0177, training accuracy : 99.48, test loss : 0.2442, test accuracy : 94.92\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0156, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0172, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0172, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0162, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0158, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0157, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0160, accuracy : 99.57\n",
            "Epoch : 281, training loss : 0.0160, training accuracy : 99.56, test loss : 0.2476, test accuracy : 94.89\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0162, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0158, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0176, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0173, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0169, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0170, accuracy : 99.54\n",
            "Epoch : 282, training loss : 0.0170, training accuracy : 99.54, test loss : 0.2491, test accuracy : 94.83\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0146, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0151, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0150, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0156, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0159, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0165, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0166, accuracy : 99.57\n",
            "Epoch : 283, training loss : 0.0164, training accuracy : 99.57, test loss : 0.2496, test accuracy : 94.72\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0174, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0175, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0171, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0165, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0169, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0167, accuracy : 99.58\n",
            "Epoch : 284, training loss : 0.0172, training accuracy : 99.56, test loss : 0.2487, test accuracy : 94.74\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0162, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0161, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0167, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0165, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0165, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0163, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0160, accuracy : 99.53\n",
            "Epoch : 285, training loss : 0.0164, training accuracy : 99.53, test loss : 0.2396, test accuracy : 94.95\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0146, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0189, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0182, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0191, accuracy : 99.47\n",
            "iteration : 250, loss : 0.0184, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0181, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0183, accuracy : 99.49\n",
            "Epoch : 286, training loss : 0.0181, training accuracy : 99.50, test loss : 0.2488, test accuracy : 94.66\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0169, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0179, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0190, accuracy : 99.50\n",
            "iteration : 200, loss : 0.0187, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0182, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0184, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0179, accuracy : 99.51\n",
            "Epoch : 287, training loss : 0.0178, training accuracy : 99.51, test loss : 0.2503, test accuracy : 94.81\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0221, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0188, accuracy : 99.50\n",
            "iteration : 150, loss : 0.0177, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0171, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0179, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0174, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0171, accuracy : 99.56\n",
            "Epoch : 288, training loss : 0.0170, training accuracy : 99.56, test loss : 0.2454, test accuracy : 94.89\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0173, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0168, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0181, accuracy : 99.50\n",
            "iteration : 200, loss : 0.0182, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0177, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0174, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0173, accuracy : 99.52\n",
            "Epoch : 289, training loss : 0.0172, training accuracy : 99.53, test loss : 0.2475, test accuracy : 94.76\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0166, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0163, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0165, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0166, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0166, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0165, accuracy : 99.55\n",
            "Epoch : 290, training loss : 0.0166, training accuracy : 99.55, test loss : 0.2444, test accuracy : 94.84\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0172, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0159, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0156, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0148, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0155, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0156, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0160, accuracy : 99.58\n",
            "Epoch : 291, training loss : 0.0158, training accuracy : 99.58, test loss : 0.2459, test accuracy : 94.80\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0192, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0176, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0173, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0173, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0181, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0180, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0177, accuracy : 99.53\n",
            "Epoch : 292, training loss : 0.0184, training accuracy : 99.52, test loss : 0.2439, test accuracy : 94.83\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0195, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0191, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0183, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0181, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0188, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0186, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0184, accuracy : 99.52\n",
            "Epoch : 293, training loss : 0.0181, training accuracy : 99.53, test loss : 0.2482, test accuracy : 94.79\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0158, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0174, accuracy : 99.50\n",
            "iteration : 200, loss : 0.0177, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0171, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0173, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0175, accuracy : 99.53\n",
            "Epoch : 294, training loss : 0.0175, training accuracy : 99.53, test loss : 0.2409, test accuracy : 94.93\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0152, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0165, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0172, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0178, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0176, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0171, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0171, accuracy : 99.52\n",
            "Epoch : 295, training loss : 0.0170, training accuracy : 99.52, test loss : 0.2496, test accuracy : 94.77\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0163, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0157, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0171, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0167, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0168, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0167, accuracy : 99.56\n",
            "Epoch : 296, training loss : 0.0165, training accuracy : 99.57, test loss : 0.2468, test accuracy : 94.74\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0150, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0158, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0166, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0171, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0168, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0168, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0165, accuracy : 99.57\n",
            "Epoch : 297, training loss : 0.0167, training accuracy : 99.56, test loss : 0.2454, test accuracy : 94.83\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0185, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0171, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0180, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0180, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0177, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0181, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0184, accuracy : 99.52\n",
            "Epoch : 298, training loss : 0.0187, training accuracy : 99.52, test loss : 0.2442, test accuracy : 94.86\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0160, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0154, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0151, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0160, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0164, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0161, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0161, accuracy : 99.54\n",
            "Epoch : 299, training loss : 0.0164, training accuracy : 99.53, test loss : 0.2486, test accuracy : 94.68\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0178, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0189, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0182, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0170, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0162, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0157, accuracy : 99.58\n",
            "Epoch : 300, training loss : 0.0159, training accuracy : 99.57, test loss : 0.2471, test accuracy : 94.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V59bF--4uUZz",
        "outputId": "cd61e257-0d5a-4001-a712-e246ed9c8fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.16236956734392866, 96.49661954517516)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : AutoAugment\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "J0T9g8oYuVvQ",
        "outputId": "22a5c481-27a1-4c45-96d8-cdf11a8cc4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : AutoAugment\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "zw1016HZuXjZ",
        "outputId": "95b7dc1a-ad78-49f4-8549-db39e96ba523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_auto = {train_loss_list}\") \n",
        "print(f\"train_acc_list_auto = {train_acc_list}\")\n",
        "print(f\"test_loss_list_auto = {test_loss_list}\")\n",
        "print(f\"test_acc_list_auto = {test_acc_list}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwZSFAgeuZ1W",
        "outputId": "506ba40e-dd9a-48b4-9008-533930c3c777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_auto = [1.233994613171916, 0.4539226504360757, 0.39892785723616436, 0.36951946796278967, 0.3572437209165516, 0.3364686794720368, 0.3232357050304813, 0.3139158589891625, 0.3026524600459308, 0.2874533208566629, 0.2781618708961701, 0.2686093915607225, 0.2590999076481111, 0.2540124244685095, 0.24861641480589947, 0.24399951269956138, 0.23923811831367695, 0.23367213850543106, 0.22791126531960196, 0.22499188158893327, 0.2189200812683196, 0.21588786823680084, 0.2124546934878277, 0.21025256699501338, 0.20572112148329819, 0.20327270222792457, 0.20072129237538755, 0.19866017976064024, 0.1968877356711442, 0.19574071886901287, 0.1951820781923891, 0.19288266742940194, 0.18784783969363222, 0.18916413984526464, 0.1828709624929803, 0.18385730885716312, 0.1843048237102626, 0.18300333889680825, 0.17988054518938712, 0.181258858744368, 0.178542331658647, 0.17391858212548866, 0.1761636258706168, 0.17584359522171944, 0.1711982707808496, 0.17569120711871603, 0.17175773992569143, 0.173915388350322, 0.16686817361571923, 0.17225952138543776, 0.16737438036699281, 0.1673657556839267, 0.16454098701759728, 0.16573736306814965, 0.1634644942918444, 0.1624382801535653, 0.16135445509741947, 0.16131306556747535, 0.16117559352177915, 0.15914395086083633, 0.1600172953392432, 0.16101780488865, 0.15760242929909288, 0.15396898807730616, 0.15771295737000662, 0.15534843960751685, 0.1561997470756372, 0.1591509728664709, 0.15701023427653443, 0.15164943112105858, 0.15431425330761647, 0.14900664126041135, 0.153352343944232, 0.1493505515198559, 0.14911859873732738, 0.14959679384460942, 0.14878295808222883, 0.15090822231599954, 0.14857992124024447, 0.1422798229043238, 0.14566040609466027, 0.14016860803251022, 0.14411792125764902, 0.1434947282459516, 0.14285532939361362, 0.13819237505678886, 0.1414163154436321, 0.14048225328144504, 0.14184128016556505, 0.13658087189766127, 0.1371091021209713, 0.13647958469705854, 0.14030104450033448, 0.1342820566864356, 0.132460017448801, 0.13510010811452297, 0.13437728684264308, 0.13391815061330148, 0.13526999323974617, 0.1319109741446933, 0.12862556951680804, 0.1311150672050511, 0.12895138073601536, 0.1280926277502604, 0.1246583734444648, 0.1258313457928295, 0.13119039074252775, 0.12371570407476044, 0.12365666381146527, 0.127088230446506, 0.12291975324938129, 0.12026552173503369, 0.12566383413687793, 0.11908095294665191, 0.11616113810701584, 0.12035970375709094, 0.11933369983522711, 0.12039770268226865, 0.11876794354621634, 0.11601000496244365, 0.11410852625643012, 0.1143225629872019, 0.11847472557833363, 0.11505588743492436, 0.11526977998232292, 0.10887061650069749, 0.11221126501314685, 0.1130289217270406, 0.10819881202193699, 0.10785190677800315, 0.10976265016993694, 0.10722535189372415, 0.11091778569710933, 0.1037355175993953, 0.10539038995720022, 0.10725624183934879, 0.10473110441492017, 0.10524187781431568, 0.10344582266580606, 0.10046376290965855, 0.10230541636102244, 0.10104834323188638, 0.09822337343016775, 0.09643481571358556, 0.09879150071501894, 0.09910423679120818, 0.09642951638213179, 0.09388373310609562, 0.09664719413996227, 0.09291944237360301, 0.09731551465454706, 0.09393294235100834, 0.0910643270961072, 0.09272648085686087, 0.09104305285162881, 0.08955065546138338, 0.0918019543374134, 0.0893830211510059, 0.08985447149400708, 0.08418837443579665, 0.08402341892968025, 0.08872269753487856, 0.08367900030440108, 0.08263780010818708, 0.08400570419503421, 0.08226117769352143, 0.07964462621849243, 0.07838223139732713, 0.08443687432180576, 0.07623482352440678, 0.0776083698258975, 0.07838072785779551, 0.0768121676771904, 0.07604568784546642, 0.0725143459350436, 0.07393465473551333, 0.07306322548538446, 0.07240707697952545, 0.07385282045316083, 0.07343979802507615, 0.07113057270969156, 0.06896691102797019, 0.06815409484296674, 0.07009156708804977, 0.0681532467273758, 0.06728222967456107, 0.06581138744748381, 0.06558232954362543, 0.06402831394629184, 0.06333761556619187, 0.06604849166542211, 0.0629559245281083, 0.06010610925356749, 0.058980082528953384, 0.06005086182922044, 0.060824787476400376, 0.05992534698989131, 0.05643098939306085, 0.057716956475704184, 0.05659668708124216, 0.053585430908806846, 0.055033171598655665, 0.05309754107929867, 0.055382789058363455, 0.05485604374056182, 0.05092235739269675, 0.05250531846346954, 0.05053107007873575, 0.0519121534665567, 0.05268361157520616, 0.0467233537332859, 0.05134408576742097, 0.04966532168808951, 0.04662036005569102, 0.0450139503640837, 0.04866801698750193, 0.04202018051501616, 0.04554983728052922, 0.04452912419968787, 0.04304213310670118, 0.04245415255982621, 0.043207022256715105, 0.03915923756804711, 0.04125189706607441, 0.04201531324972245, 0.038168357637885016, 0.03893532040768725, 0.03796733750174261, 0.03782938735188568, 0.037471614144540574, 0.03584066027936937, 0.03475043938310973, 0.0372414168192523, 0.0349701477995844, 0.032000547232671685, 0.036324127803135126, 0.03360092840889383, 0.03376660571619111, 0.03266529707985155, 0.03031955579255372, 0.029453861048233096, 0.03147298792379638, 0.02912828978151083, 0.02919105650051964, 0.028830317497697627, 0.029499432632982287, 0.026986113937475735, 0.02733181188448375, 0.025803588209598046, 0.02681095107315931, 0.026404689203034126, 0.026050992537829974, 0.024809241716038224, 0.024545829161385774, 0.023867841801785316, 0.02437650318747149, 0.024598442087124094, 0.02184943623618705, 0.02212401709032212, 0.024020484405087666, 0.02130729546814622, 0.020725422284217566, 0.02076827016183324, 0.02384621110337163, 0.01958747187485479, 0.021904331655620137, 0.019382887802573884, 0.02201010962111527, 0.02058606615812495, 0.021179698650891764, 0.019447557968882527, 0.020201372229405816, 0.01894510081315509, 0.019270095492140225, 0.019144803218715156, 0.018824441063149674, 0.01760245234608337, 0.017825258844961136, 0.01776745569889704, 0.017674809900367364, 0.015994397650768117, 0.01700054095782905, 0.016447655360661145, 0.017156456882351137, 0.01641642231447496, 0.018059637108855556, 0.017820714128034588, 0.017028371335592797, 0.017178229659197596, 0.016597093391827834, 0.015798507185733616, 0.018355573935441793, 0.018139768882747993, 0.01748620302146326, 0.017001588801414197, 0.016481924058458084, 0.016723642519140127, 0.018721227355233235, 0.01641849956732637, 0.015901307702231093]\n",
            "train_acc_list_auto = [57.69825304393859, 85.86977236633139, 87.68872419269455, 88.7580730545262, 89.19428268925357, 89.96294335627316, 90.42668078348332, 90.6193753308629, 91.05134992059291, 91.40921122286925, 91.68448914769719, 91.92376919004765, 92.38327157226045, 92.51032292218105, 92.7580730545262, 92.83642138697724, 92.9571201694018, 93.145579671784, 93.35097935415564, 93.40391741662255, 93.63684489147697, 93.787188988883, 93.67707781895183, 93.92059290629963, 93.98200105876126, 94.0307040762308, 94.1492853361567, 94.34621492853361, 94.2572789835892, 94.38644785600847, 94.23822128110112, 94.46903123345686, 94.59820010587613, 94.54526204340921, 94.68925357331922, 94.60455267337215, 94.61302276336686, 94.66807834833246, 94.65749073583906, 94.71889888830069, 94.78877713075701, 94.89677077818952, 94.80783483324511, 94.89253573319216, 94.97300158814187, 94.88618316569614, 95.06405505558496, 94.9708840656432, 95.24404446797247, 94.94123875066173, 95.18898888300689, 95.11275807305452, 95.25886712546321, 95.18898888300689, 95.26521969295923, 95.19745897300159, 95.30121757543674, 95.23980942297511, 95.36262572789836, 95.39650608787719, 95.29910005293806, 95.28427739544733, 95.36897829539438, 95.47273689782953, 95.39015352038115, 95.56379036527264, 95.47061937533087, 95.29062996294336, 95.47485442032821, 95.60614081524616, 95.55532027527792, 95.58920063525674, 95.47061937533087, 95.62308099523557, 95.68237162519851, 95.7289571201694, 95.68025410269983, 95.69719428268925, 95.61672842773955, 95.87718369507677, 95.78824775013234, 95.94706193753309, 95.81365802011646, 95.784012705135, 95.80518793012176, 95.98094229751192, 95.8284806776072, 95.80518793012176, 95.79671784012704, 96.06564319745897, 95.98305982001058, 95.98729486500794, 95.95341450502912, 96.04235044997353, 96.07834833245103, 95.97882477501324, 96.10164107993647, 96.04235044997353, 95.92165166754897, 96.11222869242985, 96.22445738485972, 96.13128639491795, 96.25622022233986, 96.24563260984648, 96.35574377977765, 96.23080995235574, 96.01482265749074, 96.3790365272631, 96.32186341979883, 96.30068819481207, 96.36633139227104, 96.42350449973532, 96.23928004235044, 96.42773954473266, 96.73901535203811, 96.37691900476443, 96.52302805717311, 96.3705664372684, 96.49973530968767, 96.51244044467973, 96.60349391212281, 96.67125463208046, 96.48914769719428, 96.57173107464267, 96.56749602964531, 96.78771836950767, 96.71995764955003, 96.53361566966649, 96.87030174695606, 96.88300688194812, 96.74748544203283, 96.76866066701959, 96.74113287453679, 96.9380624669137, 96.94229751191106, 96.8406564319746, 96.87877183695076, 96.91476971942826, 96.91688724192694, 97.1371095817893, 96.88935944944416, 96.98676548438327, 97.05876124933827, 97.13922710428798, 97.00370566437269, 97.03970354685019, 97.0926416093171, 97.23451561672843, 97.21334039174167, 97.25145579671783, 97.10322922181048, 97.23451561672843, 97.31286394917946, 97.2641609317099, 97.27898358920064, 97.39544732662785, 97.24510322922181, 97.35097935415564, 97.34250926416094, 97.571201694018, 97.43779777660137, 97.34250926416094, 97.4907358390683, 97.58178930651138, 97.46109052408681, 97.51402858655374, 97.66225516146109, 97.62202223398624, 97.39544732662785, 97.7787188988883, 97.65802011646373, 97.61990471148756, 97.76177871889888, 97.68554790894653, 97.77236633139228, 97.82530439385918, 97.82953943885654, 97.83377448385389, 97.74483853890948, 97.76389624139756, 97.88671254632081, 97.9692959237692, 97.96294335627316, 97.99682371625198, 97.99258867125464, 97.98411858125992, 98.05611434621493, 98.05187930121758, 98.05399682371625, 98.06670195870831, 98.01588141874008, 98.06034939121228, 98.22763366860772, 98.26363155108523, 98.1577554261514, 98.23186871360508, 98.25304393859184, 98.31233456855479, 98.28268925357332, 98.30174695606141, 98.356802541027, 98.35256749602965, 98.40762308099524, 98.32715722604553, 98.36739015352038, 98.4647961884595, 98.37374272101641, 98.43303335097936, 98.47114875595553, 98.45420857596612, 98.54737956590789, 98.45209105346744, 98.500794070937, 98.6553732133404, 98.65749073583906, 98.59396506087877, 98.72313393329804, 98.67866596082584, 98.65960825833774, 98.68713605082054, 98.7358390682901, 98.68925357331922, 98.77183695076761, 98.81418740074113, 98.69772366331392, 98.85865537321334, 98.8438327157226, 98.83748014822658, 98.86077289571202, 98.80571731074643, 98.9793541556379, 99.00052938062467, 98.90312334568554, 98.96029645314981, 99.04923239809423, 98.91371095817892, 99.03652726310217, 99.00899947061937, 99.00052938062467, 99.14663843303335, 99.13605082053996, 99.070407623081, 99.1064055055585, 99.11275807305452, 99.16357861302276, 99.1508734780307, 99.20169401799895, 99.17416622551615, 99.28427739544733, 99.214399152991, 99.23133933298041, 99.214399152991, 99.29486500794071, 99.29062996294336, 99.31815775542616, 99.29698253043938, 99.30333509793542, 99.3499205929063, 99.36050820539968, 99.31604023292748, 99.38803599788248, 99.41344626786659, 99.40074113287454, 99.33298041291688, 99.45367919534145, 99.37533086289042, 99.44732662784543, 99.37533086289042, 99.41979883536263, 99.3859184753838, 99.4219163578613, 99.43038644785601, 99.47697194282689, 99.42826892535733, 99.47485442032821, 99.50661725780836, 99.50661725780836, 99.50026469031233, 99.51085230280572, 99.48332451032292, 99.56379036527264, 99.53838009528852, 99.57437797776602, 99.55955532027528, 99.52779248279514, 99.50238221281101, 99.50661725780836, 99.55955532027528, 99.52779248279514, 99.54685018528322, 99.57861302276336, 99.51508734780307, 99.52567496029646, 99.52567496029646, 99.51932239280042, 99.56802541026998, 99.55955532027528, 99.51720487030175, 99.53414505029116, 99.57014293276866]\n",
            "test_loss_list_auto = [0.854697944048573, 0.6584666724882874, 0.44419189148089466, 0.368239817227803, 0.4940592580858399, 0.3690771728607954, 0.3589504133982986, 0.38058222209413844, 0.34228865349409626, 0.335723892000376, 0.3109845759383604, 0.29928872741612733, 0.31392319905845556, 0.27485897220378996, 0.29255017404462774, 0.29240670237763255, 0.29151450864532413, 0.31001000624953534, 0.27774785678176317, 0.2716419890376867, 0.26590198022769945, 0.2805121213416843, 0.2706572686632474, 0.2468469972703971, 0.26480546842018765, 0.2728810971054961, 0.2413128849995487, 0.2660955771365586, 0.2647931940634461, 0.2645452043370289, 0.25069177731433334, 0.2604605190309824, 0.2517391454574524, 0.24074612459277406, 0.24114863044929272, 0.23866420425474644, 0.2425372660817469, 0.23938925549680112, 0.2456162325015255, 0.24643426432329066, 0.2441049038487322, 0.2694204639205161, 0.2634314925720294, 0.25292548164725304, 0.23815971517971918, 0.23908892777912757, 0.25206760996404814, 0.24280147584995218, 0.2520727869488445, 0.24506068306372447, 0.24997088634500317, 0.2430651504546404, 0.23414121728901768, 0.22213738378794753, 0.247228968632864, 0.2360824038658072, 0.23799972842429198, 0.23885504494183787, 0.2358027821151065, 0.24082540213039108, 0.2557054047710171, 0.23378647274027267, 0.23469267157362958, 0.23138651949371777, 0.24428480978616895, 0.24390636994412132, 0.2296548501095351, 0.2494744443718125, 0.23908411262228207, 0.25455430325339823, 0.23804409528041587, 0.23210003986662509, 0.2522754323687039, 0.24094025425466836, 0.2353047978352098, 0.22578220312282735, 0.22642643833715542, 0.23648040128104827, 0.234295295368807, 0.23118343532961957, 0.243452065167766, 0.2479072526535567, 0.23031508502568684, 0.2307430071865811, 0.24165107517996254, 0.24791739840863966, 0.2356352105225418, 0.24240106997974947, 0.23156126701802598, 0.2280945602950512, 0.23597690470370591, 0.22922540408577405, 0.23591952800166374, 0.2404799498617649, 0.2431817608063712, 0.2453049985947562, 0.23956574037598044, 0.2583299451964159, 0.2395639850338008, 0.23400587027928993, 0.23422996013187894, 0.23995153374020375, 0.2439286811825107, 0.2674359942183775, 0.22688387820095407, 0.23382534689324744, 0.23335401259143562, 0.22591980135835268, 0.24091479602251567, 0.25041070262737136, 0.23992513426963022, 0.246772261190356, 0.2418343970077295, 0.23178920693987726, 0.24677032612118066, 0.2287094014532426, 0.23909340462848253, 0.23326664954862175, 0.23168033502046384, 0.2456203530743426, 0.235371302448067, 0.23657854840013326, 0.23801417038867287, 0.24392543954080811, 0.23823796327718916, 0.24473854583487206, 0.2388810735999369, 0.24342903593445525, 0.23894497259136507, 0.24675732284930407, 0.23822559116809977, 0.24382309390998938, 0.233994769516821, 0.2258786432828535, 0.23459827412358103, 0.2277108756277491, 0.22413930849300004, 0.23876028571862215, 0.2331210555435688, 0.23749786916681948, 0.23940081419605835, 0.23199230918259012, 0.2248807309432795, 0.2320321638356237, 0.24623563007323765, 0.2446849833370424, 0.21816706947763176, 0.23581119864156433, 0.23456720739383907, 0.23432794914526098, 0.23122229463621682, 0.23320124464511288, 0.2409158118945711, 0.23544718715928348, 0.23912205432048617, 0.24141824647199875, 0.24010024325666474, 0.23027214853494776, 0.23587687485212205, 0.23447171204230366, 0.23573052238526881, 0.22679930932673753, 0.23788673353984074, 0.2422654035811623, 0.23406771814231486, 0.24568954926422415, 0.23444239714858578, 0.24608345859337086, 0.23077788804749064, 0.24224108709570238, 0.24521961490459301, 0.23251463422624796, 0.23764923413959788, 0.23524608099650518, 0.24019971548342237, 0.24640236853384503, 0.2417017009072736, 0.23782137799642833, 0.2321096704633651, 0.23188522421554023, 0.2366727679310476, 0.23386354640345364, 0.23581221812934267, 0.2355241840340051, 0.24134659725150057, 0.23631648153212725, 0.24496783960756718, 0.23486665660040637, 0.24513840322912323, 0.24078461396343567, 0.23950668804201425, 0.23140845239600716, 0.23399199486034467, 0.24893675214957958, 0.24671873864809088, 0.2451156622127575, 0.24195303742353821, 0.23557983760667198, 0.2421512438678274, 0.2412950030948017, 0.24542047667737102, 0.24159109972271264, 0.24468982922753282, 0.23934249762518733, 0.23897204303420058, 0.24556210668136677, 0.2388775425725708, 0.23601807960692575, 0.22987533620029105, 0.246987152406398, 0.24375060487392486, 0.24201714165289612, 0.24381259923764304, 0.23624742922245287, 0.2411629831162738, 0.23951337353655083, 0.24577452823081436, 0.2465735137937408, 0.23719464980211913, 0.2379434087185883, 0.24825760760508916, 0.24059011924135335, 0.2505442435287085, 0.24614961039932334, 0.24116307924337246, 0.24537831190608295, 0.24592128061853788, 0.2437269031161479, 0.24327192477443638, 0.2411747233686494, 0.24220603216877756, 0.2415566837743801, 0.24126598647996492, 0.23794463349908007, 0.2413742526744803, 0.24037200650748083, 0.2412653976467018, 0.24241722687421477, 0.2416582004368013, 0.24397952188098548, 0.24545666710564903, 0.24501819806356057, 0.238878321870431, 0.248782909119173, 0.24211586062230317, 0.24884491410179466, 0.24774682367512701, 0.2402858989331506, 0.2464034580169063, 0.2420361806255053, 0.24698023514493422, 0.24638258141702882, 0.2440691111819344, 0.24283416516275383, 0.25251583178874615, 0.24522095931438254, 0.2505062839162408, 0.24517181726610837, 0.2511264611327765, 0.24910575721193762, 0.24469954134238994, 0.2462432504467228, 0.2518011958374843, 0.24128046771511436, 0.24614255446210212, 0.23931376788509534, 0.23999540297789315, 0.23831663229594044, 0.24262406715356252, 0.24283856312360833, 0.24234587159555623, 0.24248098625856288, 0.2431600397525757, 0.2497850547763793, 0.2420718543523667, 0.2455297116664987, 0.2453975499078047, 0.25367831595826384, 0.24988036927328827, 0.2441944194884569, 0.2475952035382244, 0.24913023439619472, 0.2496498769934417, 0.2486914823233497, 0.2396043237463078, 0.24878114394332265, 0.250280501229652, 0.24539401237944178, 0.24746752302984104, 0.24443048573391257, 0.24588232408916832, 0.24387701142433227, 0.24820849407172085, 0.24091276705411135, 0.24957285682195982, 0.2468200375179888, 0.2454140442880053, 0.24415877538130565, 0.24864615638758622, 0.2471419099864422]\n",
            "test_acc_list_auto = [72.76044867854948, 79.30623847572218, 86.36677934849416, 88.89827904118009, 84.18484941610326, 88.76382913337432, 89.1479717271051, 88.55255070682237, 89.66656422864168, 90.23509526736325, 90.95344191763982, 90.95728334357713, 90.68070067609096, 91.9061155500922, 91.58343577135832, 91.4259373079287, 91.60264290104487, 91.08789182544561, 91.74093423478796, 92.12891825445605, 92.2979409956976, 91.75245851259987, 92.26336816226183, 92.83574062692071, 92.39781807006761, 92.07513829133374, 93.09311616472034, 92.47464658881377, 92.490012292563, 92.46696373693915, 92.9279348494161, 92.42086662569146, 92.87031346035648, 93.23909035033805, 93.34665027658266, 93.1661032575292, 93.21220036877689, 93.28134603564843, 93.1238475722188, 93.08927473878303, 93.13153042409343, 92.52842655193608, 92.76659496004918, 92.74738783036263, 93.25829748002458, 93.3581745543946, 92.96250768285188, 93.02397049784881, 93.00092194222495, 92.85878918254456, 92.7819606637984, 93.14689612784265, 93.42732022126613, 93.8421942224954, 93.05470190534726, 93.23909035033805, 93.4580516287646, 93.38890596189306, 93.30055316533497, 93.13921327596803, 92.91256914566686, 93.53488014751076, 93.4618930547019, 93.53872157344806, 93.24293177627536, 93.1737861094038, 93.66548862937923, 93.14689612784265, 93.36585740626921, 92.81653349723418, 93.38506453595575, 93.57329440688383, 92.79348494161033, 93.21988322065151, 93.4580516287646, 93.79609711124769, 93.82682851874615, 93.49262446220037, 93.3159188690842, 93.700061462815, 93.46573448063921, 93.10848186846958, 93.70390288875231, 93.63859864781807, 93.38506453595575, 93.26213890596189, 93.41963736939152, 93.31207744314689, 93.75384142593731, 93.80762138905962, 93.58481868469576, 93.6078672403196, 93.4618930547019, 93.40043023970497, 93.2160417947142, 93.10848186846958, 93.49262446220037, 92.96634910878919, 93.53103872157345, 93.48110018438844, 93.63475722188076, 93.46957590657652, 93.35433312845728, 92.67440073755378, 93.82298709280884, 93.799938537185, 93.73463429625077, 93.95743700061463, 93.4618930547019, 93.20835894283958, 93.50799016594961, 93.34280885064535, 93.38890596189306, 93.69622003687769, 93.45036877688999, 93.84987707437, 93.61170866625692, 93.66164720344192, 93.75768285187462, 93.5041487400123, 93.6040258143823, 93.72311001843885, 93.53488014751076, 93.51951444376152, 93.82682851874615, 93.5579287031346, 93.71542716656423, 93.55024585125999, 93.5579287031346, 93.59250153657038, 93.62323294406883, 93.5540872771973, 93.88829133374308, 94.15719114935465, 93.73463429625077, 94.05731407498463, 94.15334972341734, 93.62323294406883, 94.02658266748617, 93.88060848186846, 93.67701290719116, 93.68469575906576, 94.18792255685311, 93.92670559311617, 93.56561155500921, 93.63475722188076, 94.15719114935465, 93.70390288875231, 93.85371850030731, 93.91518131530424, 93.95359557467732, 93.83451137062077, 93.81146281499693, 93.93054701905348, 93.87292562999386, 93.68085433312845, 93.7077443146896, 94.11877688998156, 93.98432698217579, 94.05731407498463, 94.02658266748617, 94.08420405654579, 93.91518131530424, 93.73079287031346, 93.8921327596804, 93.91902274124155, 93.9459127228027, 93.77304855562384, 94.04578979717272, 93.83835279655808, 94.04578979717272, 94.24170251997542, 94.0918869084204, 94.06115550092194, 93.93822987092808, 93.83066994468346, 94.01505838967425, 94.1379840196681, 94.32237246465888, 94.20712968653964, 94.1917639827904, 93.97664413030117, 94.23786109403811, 94.16103257529196, 94.22633681622618, 94.22249539028887, 94.01121696373694, 94.2340196681008, 94.13414259373079, 94.09956976029503, 94.19944683466503, 94.27627535341118, 94.36846957590657, 93.9958512599877, 94.07267977873387, 94.02274124154886, 94.25322679778733, 94.31084818684695, 94.11109403810694, 94.24170251997542, 94.20328826060233, 94.25706822372464, 94.19944683466503, 94.20328826060233, 94.2839582052858, 94.42224953902888, 94.2340196681008, 94.31468961278426, 94.45298094652735, 94.15334972341734, 94.11493546404425, 94.29932390903504, 94.20328826060233, 94.51060233558697, 94.50676090964966, 94.3300553165335, 94.30700676090964, 94.3262138905962, 94.53749231714812, 94.4299323909035, 94.29548248309773, 94.42609096496619, 94.38767670559312, 94.49907805777505, 94.42224953902888, 94.31084818684695, 94.40304240934235, 94.42609096496619, 94.39151813153042, 94.39151813153042, 94.55669944683467, 94.51060233558697, 94.61816226183159, 94.64121081745544, 94.47602950215119, 94.53749231714812, 94.4721880762139, 94.42224953902888, 94.62968653964352, 94.49523663183774, 94.55285802089736, 94.6258451137062, 94.66425937307929, 94.43377381684081, 94.64889366933005, 94.57974800245852, 94.4721880762139, 94.59127228027043, 94.54901659496005, 94.65657652120467, 94.7180393362016, 94.69883220651506, 94.58743085433314, 94.67194222495391, 94.54133374308543, 94.80255070682237, 94.50291948371235, 94.74108789182544, 94.62968653964352, 94.58358942839583, 94.87169637369392, 94.64889366933005, 94.71035648432698, 94.64889366933005, 94.67194222495391, 94.76029502151198, 94.80255070682237, 94.84480639213275, 94.76029502151198, 94.72572218807622, 94.77181929932391, 94.73340503995082, 94.79870928088506, 94.72572218807622, 94.7679778733866, 94.79870928088506, 94.77566072526122, 94.6681007990166, 94.69883220651506, 94.91779348494161, 94.89474492931777, 94.83328211432084, 94.7180393362016, 94.74108789182544, 94.95236631837739, 94.66041794714198, 94.80639213275968, 94.89474492931777, 94.76029502151198, 94.84480639213275, 94.79870928088506, 94.82559926244622, 94.79486785494775, 94.93315918869084, 94.77181929932391, 94.73724646588813, 94.82944068838353, 94.86017209588199, 94.68346650276582, 94.76029502151198]\n"
          ]
        }
      ]
    }
  ]
}