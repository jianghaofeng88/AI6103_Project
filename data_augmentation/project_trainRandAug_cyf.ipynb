{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "4d96196040e641699e8a78db79cf7b4e",
            "c3c542fe55a24623ba09e0f6cbcf6857",
            "dcc6414fe83041d8b5ebd3934cb1ca90",
            "92dff67c5c9049298e504f042450a0fc",
            "e13f87e33fad4a1e95fb438dd982ee13",
            "fa88d9d680484321a6889b511696efff",
            "78da9a8381fe41d6b4341e3603daf12e",
            "012c4fccfa124eb294393069e02f09d2",
            "969dedb7d7e74432919aaee552cc4cde",
            "84d2ff23e4634816b1c3291e34c4661b",
            "1878c24981c347adb95e0e89c72fe9f7",
            "a7a1a99f011e4610ad5ac35d32b324fd",
            "a57bfb0e83374f4493cc9062e20a718b",
            "5ae2aba4cc914176a5ccfc663eb6310a",
            "722efa37812c450c8997c8a0b916a30c",
            "f51e75cc2ff347969ebd105e903a8952",
            "bc3ee902af6e47b7b518a1aabaf7ba85",
            "b19fb9dad8774b1c8161b72430accf56",
            "bcc36657b3d44ea8a74b5ee30f4677d9",
            "d563a2f339ec4d0e896caf5b765d2be5",
            "02696ee9f9ce4417ae638d5844c2ec73",
            "5c3384e0e4044a3d82e3e66f3682220a"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "6c2947f4-6216-4d3e-c8ce-740013f7732e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./data\\train_32x32.mat\n",
            "Using downloaded and verified file: ./data\\test_32x32.mat\n"
          ]
        }
      ],
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandAugment(),\n",
        "    transforms.ToTensor(),\n",
        "    \n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "dccde39f-d470-4cdd-b75e-19839fdf9548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "73257 26032\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))\n",
        "print(trainset[4][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "outputs": [],
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "outputs": [],
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "01b301a0-525f-4c79-c065-9926ced8467e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnQAAAFECAYAAACu+6P/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA9JElEQVR4nO3de3hU1dn38e8NIUVAqiggEJBDUQKBBBIEWwtaGglqUQQPVB+CgJS2tChaKra0FfURfVChFqQoKLUVrAdqLkQOgkL15RQgIIIohwgBBERohHAKud8/ZkgJxwFnMkzy+1zXXMxehz33As3cWXvvtczdEREREZHYVSHaAYiIiIjIt6OETkRERCTGKaETERERiXFK6ERERERinBI6ERERkRinhE5EREQkximhkxOY2UQz22Fmq6Idi4iIiJyZEjo5mZeBjGgHISIiIqFRQicncPf5wNfRjkNERERCo4ROREREJMYpoRMRERGJcUroRERERGKcEjoRERGRGBcX7QDC6dJLL/WGDRtGO4yYt2HDBuLi4igsLCQ+Pt7r1q3LpZdeGu2wREREypSlS5d+5e41w3GuMpXQNWzYkOzs7GiHISIiInJGZvZFuM6lS64iIiIiMU4JnYiIiEiMU0InIiIiEuOU0MkJ+vTpQ61atUhKSiouy8nJoX379qSkpJCWlsbixYtP2nf06NEkJSXRokULRo0aVVz+9ddfk56eTtOmTUlPT2f37t0A7Nq1i+uuu45q1aoxcODA4vYHDx4kIyODpKQkxo4dW1zev39/li9fHuYRi4iIxDYldHKC3r17M2PGjBJlQ4YM4Y9//CM5OTkMHz6cIUOGnNBv1apVvPDCCyxevJgVK1Ywbdo0Pv/8cwBGjBhBp06d+Pzzz+nUqRMjRowAoHLlyjz66KOMHDmyxLlmzpxJamoqK1euZPz48QCsWLGCoqIiWrduHYlhi4iIxCwldHKCDh06UKNGjRJlZkZ+fj4A//nPf6hbt+4J/dasWUP79u2pUqUKcXFxdOzYkalTpwLw9ttvk5mZCUBmZib/+te/AKhatSrXXHMNlStXLnGuSpUqsX//fgoLC4vLhg0bxvDhw8M2ThERkbJCCZ2EZNSoUfzmN7+hfv36PPjggzzxxBMntElKSmL+/Pns2rWLgoICpk+fzubNmwHYvn07derUAaBOnTrs2LHjtJ+Xnp7Ol19+Sbt27RgyZAhZWVmkpqaeNJEUEREp78rUOnQSOc8//zzPPvss3bt355///Cd9+/blvffeK9EmMTGR3/72t6Snp1OtWjWSk5OJizu3/8Ti4uJ49dVXATh8+DCdO3cmKyuLwYMHs2nTJnr16kXXrl2/9bhERETKAs3QCQCbdhWQ/sw8mgydTvoz89iye3+J+kmTJnHrrbcCcNttt53yoYi+ffuybNky5s+fT40aNWjatCkAtWvXZtu2bQBs27aNWrVqhRzb2LFjyczMZMGCBcTHx/Paa6/x2GOPncswRUREyiQldAJA30lLWL9zL0fcWb9zL799c0WJ+rp16zJv3jwA5s6dW5yoHe/opdRNmzbx1ltv0bNnTwC6du3KpEmTgEByePPNN4cU1+7du5k2bRq9evWioKCAChUqYGYcOHDgnMYpIiJSFpm7RzuGsElLS3Nt/XVumgydzpHgfws7s57i4KaPsYPfULt2bR555BGuvPJKBg0aRGFhIZUrV2bs2LGkpqaydetW+vXrx/Tp0wH44Q9/yK5du6hUqRLPPPMMnTp1AgLLk9x+++1s2rSJBg0a8Prrrxc/eNGwYUPy8/M5dOgQF110EbNmzaJ58+YA3H///dxyyy107NiRAwcO0LVrV7Zs2cKAAQP41a9+FYW/KRERkfAws6XunhaWcymhE4D0Z+axfudeihwqGDSpWY3ZgztGOywREZEyK5wJnS65CgATMtvSpGY1KprRpGY1JmS2jXZIIiIiEiI95SoANLikimbkREREYpRm6ERERERinBI6ERERkRinhE5EREQkximhExEREYlxSuhEREREYpwSOhEREZEYp4ROREREJMYpoRMRERGJcUroRERERGKcEjoRERGRGBfRhM7MMsxsrZmtM7OHTlLfzMwWmNlBM3vwuLr7zewTM1tlZpPNrHIkYxURERGJVRFL6MysIjAG6AI0B3qaWfPjmn0N/BoYeVzfesHyNHdPAioCd0YqVhEREZFYFskZuquAde6+wd0PAVOAm49t4O473H0JcPgk/eOAC8wsDqgCbI1grCIiIiIxK5IJXT1g8zHHecGyM3L3LQRm7TYB24D/uPussEcoIiIiUgZEMqGzk5R5SB3NLiYwm9cIqAtUNbO7T9G2v5llm1n2zp07zzlYERERkVgVyYQuD6h/zHECoV82/TGw0d13uvth4C3g+ydr6O7j3T3N3dNq1qz5rQIWERERiUWRTOiWAE3NrJGZxRN4qCErxL6bgPZmVsXMDOgErIlQnCIiIiIxLS5SJ3b3QjMbCMwk8JTqRHf/xMwGBOvHmdllQDZQHSgys/uA5u6+yMzeAJYBhcByYHykYhURERGJZeYe0m1tMSEtLc2zs7OjHYaIiIjIGZnZUndPC8e5tFOEiIiISIxTQiciIiIS45TQiYiIiMQ4JXQiIiIiMU4JnYiIiEiMU0InIiIiEuOU0ImIiIjEOCV0IiIiIjFOCZ2IiIhIjFNCJyIiIhLjlNCJiIiIxDgldCIiIiIxTgmdiIiISIxTQiciIiIS45TQiYiIiMQ4JXQiIiIiMU4JnYiIiEiMU0InIiIiEuOU0ImIiIjEOCV0IiIiIjFOCZ2IiIhIjFNCJyIiIhLjlNCJiIiIxDgldCIiIiIxTgmdiIiISIxTQiciIiIS45TQiYiIiMQ4JXQiIiIiMU4JnYiIiEiMU0InIiIiEuMimtCZWYaZrTWzdWb20Enqm5nZAjM7aGYPHld3kZm9YWafmtkaM7s6krGKiIiIxKq4SJ3YzCoCY4B0IA9YYmZZ7r76mGZfA78GbjnJKUYDM9y9h5nFA1UiFauIiIhILIvkDN1VwDp33+Duh4ApwM3HNnD3He6+BDh8bLmZVQc6ABOC7Q65+54IxioiIiISsyKZ0NUDNh9znBcsC0VjYCfwkpktN7MXzaxquAMUERERKQsimdDZSco8xL5xQBvgeXdvDewDTrgHD8DM+ptZtpll79y589wiFREREYlhkUzo8oD6xxwnAFvPom+euy8KHr9BIME7gbuPd/c0d0+rWbPmOQcrIiIiEqsimdAtAZqaWaPgQw13AlmhdHT3L4HNZnZlsKgTsPo0XURERETKrYg95eruhWY2EJgJVAQmuvsnZjYgWD/OzC4DsoHqQJGZ3Qc0d/d84FfAP4LJ4AbgnkjFKiIiIhLLIpbQAbj7dGD6cWXjjnn/JYFLsSfrmwOkRTI+ERERkbJAO0WIiIiIxDgldCIiIiIxTgmdiIiISIxTQiciIiIS45TQSbmzefNmrrvuOhITE2nRogWjR4+OdkgiIiLfSkSfchU5H8XFxfH000/Tpk0bvvnmG1JTU0lPT6d58+bRDk1EROScaIZOyp06derQpk1g45ELL7yQxMREtmzZEuWoREREzp0SOinXcnNzWb58Oe3atYt2KCIiIudMCZ2UW3v37qV79+6MGjWK6tWrRzscERGRc6aETsqlw4cP0717d+666y5uvfXWaIcjIiLyrSihk3LH3enbty+JiYkMHjw42uGIiIh8a0ropNz56KOPeOWVV5g7dy4pKSmkpKQwffr0M3cUERE5T2nZEil3rrnmGtw92mGIiIiEjWboRERERGKcEjoRERGRGKeETkRERCTGKaGTcqlPnz7UqlWLpKSkE+pGjhyJmfHVV1+dtG/Dhg1p2bIlKSkppKWlnbH/4sWLix++SE5OZurUqQAcPHiQjIwMkpKSGDt2bHH//v37s3z58nAMU0REygkldFIu9e7dmxkzZpxQvnnzZmbPnk2DBg1O2//9998nJyeH7OzsM/ZPSkoiOzubnJwcZsyYwc9+9jMKCwuZOXMmqamprFy5kvHjxwOwYsUKioqKaN26dRhGKSIi5YUSOimXOnToQI0aNU4ov//++3nqqacws3M678n6V6lShbi4wAPlBw4cKK6rVKkS+/fvp7CwsLjtsGHDGD58+Dl9toiIlF9K6ESCsrKyqFevHsnJyadtZ2Zcf/31pKamFs+snan/okWLaNGiBS1btmTcuHHExcWRnp7Ol19+Sbt27RgyZAhZWVmkpqZSt27dsI9NRETKNq1DJwIUFBTw+OOPM2vWrDO2/eijj6hbty47duwgPT2dZs2akZaWdtr+7dq145NPPmHNmjVkZmbSpUsXKleuzKuvvgoEtiLr3LkzWVlZDB48mE2bNtGrVy+6du0a1nGKiEjZpBk6KTc27Sog/Zl5NBk6nfRn5rFl9/7iuvXr17Nx40aSk5Np2LAheXl5tGnThi+//PKE8xydQatVqxbdunVj8eLFIfdPTEykatWqrFq1qkT52LFjyczMZMGCBcTHx/Paa6/x2GOPReBvQUREyiIldFJu9J20hPU793LEnfU79/LbN1cU17Vs2ZIdO3aQm5tLbm4uCQkJLFu2jMsuu6zEOfbt28c333xT/H7WrFkkJSWdtv/GjRuL75P74osvWLt2LQ0bNiw+5+7du5k2bRq9evWioKCAChUqYGYcOHAg8n8pIiJSJiihk3Jjw859FAV3/Nr+9lMsHPUL1q5dS0JCAhMmTDhlv61bt3LDDTcE+m3fzjXXXENycjJXXXUVN954IxkZGaf93A8//JDk5GRSUlLo1q0bY8eO5dJLLy2uHz58OL///e8xMzp37kx2djYtW7bk3nvv/faDFhGRcsHK0p6WaWlpfvwyEiJHpT8zj/U791LkUMGgSc1qzB7cMdphiYhIOWVmS939xAVNz4Fm6KTcmJDZliY1q1HRjCY1qzEhs220QxIREQkLPeUq5UaDS6poRk5ERMokzdCJiIiIxDgldCIiIiIxTgmdiIiISIyLaEJnZhlmttbM1pnZQyepb2ZmC8zsoJk9eJL6ima23MymRTJOERERkVgWsYTOzCoCY4AuQHOgp5k1P67Z18CvgZGnOM0gYE2kYhQREREpCyI5Q3cVsM7dN7j7IWAKcPOxDdx9h7svAQ4f39nMEoAbgRcjGKOIiIhIzItkQlcP2HzMcV6wLFSjgCFAURhjEhERESlzIpnQ2UnKQtqWwsxuAna4+9IQ2vY3s2wzy965c+fZxigiIiIS8yKZ0OUB9Y85TgC2htj3B0BXM8slcKn2R2b295M1dPfx7p7m7mk1a9b8NvGKiIiIxKRIJnRLgKZm1sjM4oE7gaxQOrr7UHdPcPeGwX5z3f3uyIUqIiIiErsitvWXuxea2UBgJlARmOjun5jZgGD9ODO7DMgGqgNFZnYf0Nzd8yMVl4iIiEhZY+4h3dYWE9LS0jw7OzvaYYiIiIickZktdfe0cJwr5Bk6M/s+0PDYPu7+t3AEISIiIiLnLqSEzsxeAZoAOcCRYLEDSuhEREREoizUGbo0Ave2lZ3rsyIiIiJlRKhPua4CLotkICIiIiJybkKdobsUWG1mi4GDRwvdvWtEohIRERGRkIWa0P0pkkGIiIiIyLkLKaFz93lmVhtoGyxa7O47IheWiIiIiIQqpHvozOx2YDFwG3A7sMjMekQyMBEREREJTaiXXH8HtD06K2dmNYH3gDciFZiIiIiIhCbUp1wrHHeJdddZ9BURERGRCAp1hm6Gmc0EJgeP7wCmRyYkERERETkboT4U8Rsz6w78ADBgvLtPjWhkIiIiIhKSkPdydfc3gTcjGIuIiIiInIPTJnRm9qG7X2Nm3xDYu7W4CnB3rx7R6ERERETkjE6b0Ln7NcE/LyydcERERETkbIW6Dt0roZSJiIiISOkLdemRFscemFkckBr+cERERETkbJ02oTOzocH751qZWX7w9Q2wHXi7VCIUERERkdM6bULn7k8A3wX+5u7Vg68L3f0Sdx9aOiGKiIiIyOmc8ZKruxcByaUQi4iIiIicg1DvoVtoZm0jGomIiIiInJNQFxa+DviZmX0B7OO/69C1ilhkIiIiIhKSUBO6LhGNQkTC6sCBA3To0IGDBw9SWFhIjx49eOSRR6IdloiIREioe7l+YWbJwA+DRf929xWRC0tEvo3vfOc7zJ07l2rVqnH48GGuueYaunTpQvv27aMdmoiIRECoCwsPAv4B1Aq+/m5mv4pkYCJy7syMatWqAXD48GEOHz6MmUU5KhERiZRQH4roC7Rz9z+4+x+A9sC9kQtLRL6tI0eOkJKSQq1atUhPT6ddu3bRDklERCIk1ITOgCPHHB8JlonIeapixYrk5OSQl5fH4sWLWbVqVbRDEhGRCAn1oYiXgEVmNpVAInczMCFiUYlI2Fx00UVce+21zJgxg6SkpGiHIyIiERDSDJ27PwPcA3wN7ALucfdREYxLRL6FnTt3smfPHgD279/Pe++9R7NmzaIblIiIREyoM3RHGVCELreKnNe2bdtGZmYmR44coaioiNtvv52bbrop2mGJiEiEhJTQmdkfgNuANwkkcy+Z2evu/tgZ+mUAo4GKwIvuPuK4+mYELue2AX7n7iOD5fWBvwGXEUggx7v76LMZmEh51qpVK5YvXx7tMEREpJSEOkPXE2jt7gcAzGwEsAw4ZUJnZhWBMUA6kAcsMbMsd199TLOvgV8DtxzXvRB4wN2XmdmFwFIzm31cXxEREREh9Kdcc4HKxxx/B1h/hj5XAevcfYO7HwKmEHiYopi773D3JcDh48q3ufuy4PtvgDVAvRBjFRERESlXQk3oDgKfmNnLZvYSsArYa2Z/NrM/n6JPPWDzMcd5nENSZmYNgdbAorPtK1Je9enTh1q1apV4qvX111+nRYsWVKhQgezs7FP23bNnDz169KBZs2YkJiayYMGCEvUjR47EzPjqq6+AwMLFmZmZtGzZksTERJ544gkADh48SEZGBklJSYwdO7a4f//+/XU5WEQkzEJN6KYCDwPvAx8AvwPeBZYGXydzsgcn/GyCM7NqBO7bu8/d80/Rpr+ZZZtZ9s6dO8/m9CJlVu/evZkxY0aJsqSkJN566y06dOhw2r6DBg0iIyODTz/9lBUrVpCYmFhct3nzZmbPnk2DBg2Ky15//XUOHjzIxx9/zNKlS/nrX/9Kbm4uM2fOJDU1lZUrVzJ+/HgAVqxYQVFREa1btw7jaEVEJNS9XCeZWTxwRbBorbsfPl0fAjNy9Y85TgC2hhqYmVUikMz9w93fOk1s44HxAGlpaWeVMIqUVR06dCA3N7dE2bGJ2ank5+czf/58Xn75ZQDi4+OJj48vrr///vt56qmnuPnm/949YWbs27ePwsJC9u/fT3x8PNWrV6dSpUrs37+fwsLC4rbDhg1j3Lhx325wIiJyglD3cr0W+JzAQw5jgc/M7PS/5sMSoKmZNQomg3cCWSF+nhFYuHhNcA08ESkFGzZsoGbNmtxzzz20bt2afv36sW/fPgCysrKoV68eycnJJfr06NGDqlWrUqdOHRo0aMCDDz5IjRo1SE9P58svv6Rdu3YMGTKErKwsUlNTqVu3bjSGJiJSpoX6lOvTwPXuvhbAzK4AJgOpp+rg7oVmNhCYSWDZkonu/omZDQjWjzOzy4BsoDpQZGb3Ac2BVsD/AB+bWU7wlA+7+/SzHJ+InIXCwkKWLVvGc889R7t27Rg0aBAjRoxg6NChPP7448yaNeuEPosXL6ZixYps3bqV3bt388Mf/pAf//jHNG7cmFdffRUI3GfXuXNnsrKyGDx4MJs2baJXr1507dq1tIcoIlImhZrQVTqazAG4+2fBS6KnFUzAph9XNu6Y918SuBR7vA/R4sUipS4hIYGEhATatWsHBGbfRowYwfr169m4cWPx7FxeXh5t2rRh8eLFvPrqq2RkZFCpUiVq1arFD37wA7Kzs2ncuHHxeceOHUtmZiYLFiwgPj6e1157jauvvloJnYhImIT6UMRSM5tgZtcGXy9w6ochRCQKNu0qIP2ZeTQZOp30Z+axZff+sz7HZZddRv369Vm7NvD725w5c2jevDktW7Zkx44d5ObmkpubS0JCAsuWLeOyyy6jQYMGzJ07F3dn3759LFy4sMQ2Y7t372batGn06tWLgoICKlSogJlx4MCBsI1dRKS8CzWhGwB8QmAR4EHA6mCZiJwn+k5awvqdeznizoIX/8CPOl7D2rVrSUhIYMKECUydOpWEhAQWLFjAjTfeSOfOnQHYunUrN9xwQ/F5nnvuOe666y5atWpFTk4ODz/88Gk/95e//CV79+4lKSmJtm3bcs8999CqVavi+uHDh/P73/8eM6Nz585kZ2fTsmVL7r333sj8RYiIlEPmfvoHQ82sArDS3ZNO2/A8kJaW5qdbX0ukLGsydDpHjvn/uaIZ65+44TQ9REQkmsxsqbunheNcZ5yhc/ciYIWZNThTWxGJnsY1q1IheOdpBQsci4hI+RDqJdc6BHaKmGNmWUdfkQxMRM7OhMy2NKlZjYpmNKlZjQmZbaMdkoiIlJJQn3J9JKJRiMi31uCSKswe3DHaYYiISBScNqEzs8oEHn74HvAxMMHdC0/XR0RERERK15kuuU4C0ggkc10ILDAsIiIiIueRM11ybe7uLQHMbAKwOPIhiYiIiMjZONMM3eGjb3SpVUREROT8dKaELtnM8oOvb4BWR9+bWX5pBCgicipHjhyhdevW3HTTTdEORUQkqk57ydXdK5ZWICIiZ2v06NEkJiaSn6/fL0WkfAt1HToRkfNKXl4e77zzDv369Yt2KCIiUaeETkRi0n333cdTTz1FhQr6MSYiop+EIhJzpk2bRq1atUhNTY12KCIi5wUldCIScz766COysrJo2LAhd955J3PnzuXuu++OdlgiIlFj7h7tGMImLS3Ns7Ozox2GiJSiDz74gJEjRzJt2rRohyIiclbMbKm7p4XjXJqhExEREYlxZ9opQkTkvHbttddy7bXXRjsMEZGo0gydiIiISIxTQiciIiIS45TQiYiIiMQ4JXQiEpP69OlDrVq1SEpKKi77+uuvSU9Pp2nTpqSnp7N79+4T+m3evJnrrruOxMREWrRowejRo4vrfvOb39CsWTNatWpFt27d2LNnDwCLFy8mJSWFlJQUkpOTmTp1KgAHDx4kIyODpKQkxo4dW3ye/v37s3z58giNXETkREroRCQm9e7dmxkzZpQoGzFiBJ06deLzzz+nU6dOjBgx4oR+cXFxPP3006xZs4aFCxcyZswYVq9eDUB6ejqrVq1i5cqVXHHFFTzxxBMAJCUlkZ2dTU5ODjNmzOBnP/sZhYWFzJw5k9TUVFauXMn48eMBWLFiBUVFRbRu3TrCfwMiIv+lhE5EYlKHDh2oUaNGibK3336bzMxMADIzM/nXv/51Qr86derQpk0bAC688EISExPZsmULANdffz1xcYGH/9u3b09eXh4AVapUKS4/cOAAZgZApUqV2L9/P4WFhcXnHzZsGMOHDw/jSEVEzkwJnYiUGdu3b6dOnTpAIHHbsWPHadvn5uayfPly2rVrd0LdxIkT6dKlS/HxokWLaNGiBS1btmTcuHHExcWRnp7Ol19+Sbt27RgyZAhZWVmkpqZSt27d8A5MROQMtA6diJRLe/fupXv37owaNYrq1auXqHv88ceJi4vjrrvuKi5r164dn3zyCWvWrCEzM5MuXbpQuXJlXn31VQAOHz5M586dycrKYvDgwWzatIlevXrRtWvXUh2XiJRPSuhEJCZs2lVA30lL2LBzH41rVmVCZtsT2tSuXZtt27ZRp04dtm3bRq1atU56rsOHD9O9e3fuuusubr311hJ1kyZNYtq0acyZM6f40uqxEhMTqVq1KqtWrSIt7b879owdO5bMzEwWLFhAfHw8r732GldfffW3SugaNmzIhRdeSMWKFYmLi0NbG4rIqeiSq4jEhL6TlrB+516OuLN+5176TlpyQpuuXbsyadIkIJCY3XzzzSe0cXf69u1LYmIigwcPLlE3Y8YMnnzySbKysqhSpUpx+caNG4vvk/viiy9Yu3YtDRs2LK7fvXs306ZNo1evXhQUFFChQgXMjAMHDnzrcb///vvk5OQomROR01JCJyIxYcPOfRR54H2Rw4IX/8DVV1/N2rVrSUhIYMKECTz00EPMnj2bpk2bMnv2bB566CEAtm7dyg033ADARx99xCuvvMLcuXOLlyKZPn06AAMHDuSbb74hPT2dlJQUBgwYAMCHH35IcnIyKSkpdOvWjbFjx3LppZcWxzZ8+HB+//vfY2Z07tyZ7OxsWrZsyb333luKf0MiUp6Zu0fu5GYZwGigIvCiu484rr4Z8BLQBvidu48Mte/JpKWluX6LFSmb0p+Zx/qdeylyqGDQpGY1Zg/uGO2wIqpRo0ZcfPHFmBk/+9nP6N+/f7RDEpEwMrOl7p525pZnFrEZOjOrCIwBugDNgZ5m1vy4Zl8DvwZGnkNfESlHJmS2pUnNalQ0o0nNaie9h66s+eijj1i2bBnvvvsuY8aMYf78+dEOSUTOU5F8KOIqYJ27bwAwsynAzcDqow3cfQeww8xuPNu+IlK+NLikSpmfkTve0eVPatWqRbdu3Vi8eDEdOnSIclQicj6K5D109YDNxxznBcsi3VdEJObt27ePb775pvj9rFmzSmxzJiJyrEjO0J34vD+EesNeyH3NrD/QH6BBgwYhnl5E5Py2fft2unXrBkBhYSE//elPycjIiHJUInK+imRClwfUP+Y4Adga7r7uPh4YD4GHIs4+TBGR80/jxo1ZsWJFtMMQkRgRyUuuS4CmZtbIzOKBO4GsUugrIiIiUq5EbIbO3QvNbCAwk8DSIxPd/RMzGxCsH2dmlwHZQHWgyMzuA5q7e/7J+kYqVhEREZFYFtGFhd19urtf4e5N3P3xYNk4dx8XfP+luye4e3V3vyj4Pv9UfUVEyovRo0eTlJREixYtGDVq1An1b7/9Nq1atSIlJYW0tDQ+/PBDANauXVu8YHJKSgrVq1cv7r9ixQquvvpqWrZsyU9+8hPy8/OBwPIorVq1om3btqxbtw6APXv20LlzZyK5VqmIhE9EFxYubVpYWETKglWrVnHnnXeyePFi4uPjycjI4Pnnn6dp06bFbfbu3UvVqlUxM1auXMntt9/Op59+WuI8R44coV69eixatIjLL7+ctm3bMnLkSDp27MjEiRPZuHEjjz76KLfeeitPPvkkubm5zJgxg6effpoHHniArl270rFj+VoqRqQ0xcTCwiIicm7WrFlD+/btqVKlCnFxcXTs2JGpU6eWaFOtWjXMAgsC7Nu3r/j9sebMmUOTJk24/PLLgcDs3dF17NLT03nzzTcBqFSpEvv376egoIBKlSqxfv16tmzZomROJIYooRMROc8kJSUxf/58du3aRUFBAdOnT2fz5s0ntJs6dSrNmjXjxhtvZOLEiSfUT5kyhZ49e5Y4b1ZW4Pmy119/vficQ4cOpX///owaNYqBAwfyu9/9jkcffTRCoxORSFBCJyJynklMTOS3v/0t6enpZGRkkJycTFzcic+wdevWjU8//ZR//etfDBs2rETdoUOHyMrK4rbbbisumzhxImPGjCE1NZVvvvmG+Ph4AFJSUli4cCHvv/8+GzZsoG7durg7d9xxB3fffTfbt2+P7IBF5FtTQicich7q27cvy5YtY/78+dSoUaPE/XPH69ChA+vXr+err74qLnv33Xdp06YNtWvXLi5r1qwZs2bNYunSpfTs2ZMmTZqUOI+789hjjzFs2DAeeeQRHnnkEe6++27+/Oc/h3+AIhJWkVxYWEREzsKmXQX0nbSEDTv3kVD5EK8MvB72fcVbb73FggULSrRdt24dTZo0wcxYtmwZhw4d4pJLLimunzx5conLrQA7duygVq1aFBUV8dhjjzFgwIAS9ZMmTeLGG2/k4osvpqCggAoVKlChQgUKCgoiN2gRCQsldCIi54m+k5awfudeihwWvfg7mj9/H01qf5cxY8Zw8cUXM27cOAAGDBjAm2++yd/+9jcqVarEBRdcwGuvvVb8YERBQQGzZ8/mr3/9a4nzT548mTFjxgBw6623cs899xTXFRQUMGnSJGbNmgXA4MGD6d69O/Hx8UyePLk0hi8i34KWLREROU80GTqdI8f8TK5oxvonbohiRCISSVq2RESkDGpcsyoVgquPVLDAsYhIKJTQiYicJyZktqVJzWpUNKNJzWpMyGwb7ZBEJEboHjoRkfNEg0uqMHuwFvMVkbOnGToRERGRGKeETkRERCTGKaETERERiXFK6ERERERinBI6ERERkRinhE5ERKJiz5499OjRg2bNmpGYmHjC9mYiEjotWyIiIlExaNAgMjIyeOONNzh06JD2jBX5FpTQiYhIqcvPz2f+/Pm8/PLLAMTHxxMfHx/doERimC65iohIqduwYQM1a9bknnvuoXXr1vTr1499+/ZFOyyRmKWETkRESl1hYSHLli3j5z//OcuXL6dq1aqMGDEi2mGJxCwldCIiUuoSEhJISEigXbt2APTo0YNly5ZFOSqR2KWETkRESt1ll11G/fr1Wbt2LQBz5syhefPmUY5KJHbpoQgREYmK5557jrvuuotDhw7RuHFjXnrppWiHJBKzlNCJiEhUpKSkkJ2dHe0wRMoEXXIVEREJs7Vr15KSklL8ql69OqNGjYp2WFKGaYZOREQkzK688kpycnIAOHLkCPXq1aNbt27RDUrKNM3QiYhIqQtlBmv37t1069aNVq1acdVVV7Fq1aoS9UeOHKF169bcdNNNxWV33HFH8TkbNmxISkoKAB999BGtWrWibdu2rFu3DghsPda5c2fcPaJjnTNnDk2aNOHyyy+P6OdI+aYZOhERKXWhzGD97//+LykpKUydOpVPP/2UX/7yl8yZM6e4fvTo0SQmJpKfn19c9tprrxW/f+CBB/jud78LwNNPP82bb75Jbm4uzz//PE8//TSPPvooDz/8MGYWwZHClClT6NmzZ0Q/Q0QzdCIiElWnmsFavXo1nTp1AqBZs2bk5uayfft2APLy8njnnXfo16/fSc/p7vzzn/8sTqQqVarE/v37KSgooFKlSqxfv54tW7bQsWPHCI4MDh06RFZWFrfddltEP0ckogmdmWWY2VozW2dmD52k3szsz8H6lWbW5pi6+83sEzNbZWaTzaxyJGMVEZHoONUMVnJyMm+99RYAixcv5osvviAvLw+A++67j6eeeooKFU7+Nfbvf/+b2rVr07RpUwCGDh1K//79GTVqFAMHDuR3v/sdjz76aIRG9F/vvvsubdq0oXbt2hH/LCnfIpbQmVlFYAzQBWgO9DSz41eN7AI0Db76A88H+9YDfg2kuXsSUBG4M1KxiohIdJxuBuuhhx5i9+7dpKSk8Nxzz9G6dWvi4uKYNm0atWrVIjU19ZTnnTx5cokkMSUlhYULF/L++++zYcMG6tati7tzxx13cPfddxfP/IXb8XGIREok76G7Cljn7hsAzGwKcDOw+pg2NwN/88AdqQvN7CIzq3NMbBeY2WGgCrA1grGKiEgUnG4Gq3r16sWLDbs7jRo1olGjRkyZMoWsrCymT5/OgQMHyM/P5+677+bvf/87ENgn9q233mLp0qUnnNPdeeyxx3jttdcYOHAgjzzyCLm5ufz5z3/m8ccfD+vYCgoKmD17Nn/961/Del6Rk4nkJdd6wOZjjvOCZWds4+5bgJHAJmAb8B93nxXBWEVEJMI27Sog/Zl5NBk6nfRn5rFpV8FpZ7D27NnDoUOHAHjxxRfp0KED1atX54knniAvL4/c3FymTJnCj370o+JkDuC9996jWbNmJCQknHDOSZMmceONN3LxxRdTUFBAhQoVqFChAgUFBWEfb5UqVdi1a1fxgxkikRTJGbqTPTZ0/LPhJ21jZhcTmL1rBOwBXjezu93978c3NrP+BC7X0qBBg28VsIiIRE7fSUtYv3MvRQ7rd+6l9wv/ZsVxM1jjxo0DYMCAAaxZs4ZevXpRsWJFmjdvzoQJE0L6nFPdk1dQUMCkSZOYNSswPzB48GC6d+9OfHw8kydPDsMIRaLHIrX+jpldDfzJ3TsHj4cCuPsTx7T5K/CBu08OHq8FrgWuATLcvW+wvBfQ3t1/cbrPTEtLc20jIyJyfmoydDpHjvnOqWjG+iduiGJEItFlZkvdPS0c54rkJdclQFMza2Rm8QQeasg6rk0W0Cv4tGt7ApdWtxG41NrezKpYYIGgTsCaCMYqIiIR1rhmVSoEr8tUsMCxiIRHxBI6dy8EBgIzCSRj/3T3T8xsgJkNCDabDmwA1gEvAL8I9l0EvAEsAz4Oxjk+UrGeixkzZnDllVfyve99jxEjRkQ7HBGR896EzLY0qVmNimY0qVmNCZltox1SxJzNXq5LliyhYsWKvPHGGyXKT7YTxp/+9Cfq1atXfN7p06cD0dkJ49lnn6VFixYkJSXRs2dPDhw4EJHPkdBE7JJrNJTWJdcjR45wxRVXMHv2bBISEmjbti2TJ0+mefPjV2UREZHy7uhOGIsWLTph8eQjR46Qnp5O5cqV6dOnDz169Ciue+aZZ8jOziY/P59p06YBgYSuWrVqPPjggyXOc+utt/Lkk0+Sm5vLjBkzePrpp3nggQfo2rVrRBZP3rJlC9dccw2rV6/mggsu4Pbbb+eGG26gd+/eYf+ssixWLrmWWYsXL+Z73/sejRs3Jj4+njvvvJO333472mGJiMh56HR7uT733HN0796dWrVqlSg/004Yx4vGThiFhYXs37+fwsJCCgoKqFu3bsQ+S85MCd052LJlC/Xr1y8+TkhIYMuWLVGMSEREzleneup2y5YtTJ06lQEDBpxQd7qdMP7yl7/QqlUr+vTpw+7du4HS3wmjXr16PPjggzRo0IA6derw3e9+l+uvvz5inydnpoTuHJzsMnWkN3cWEZHYc7qdMO677z6efPJJKlasWKL8dDth/PznP2f9+vXk5ORQp04dHnjgAaD0d8LYvXs3b7/9Nhs3bmTr1q3s27evxFqAUvoiuQ5dmZWQkMDmzf9dDzkvL09TzSIicoLT7YSRnZ3NnXcGdrX86quvmD59OnFxcSxatOiUO2Ece5577723xAMTUHo7Ybz33ns0atSImjVrAoF7+P7f//t/3H333WH7DDk7SujOQdu2bfn888/ZuHEj9erVY8qUKbz66qvRDktERKJo064C+k5awoad+2hcsyoTMtuedieMjRs3Fr/v3bs3N910E7fccgu33HILTzwRWLL1gw8+YOTIkcWzX9u2baNOncAOmVOnTiUpKanEOUtrJ4wGDRqwcOFCCgoKuOCCC5gzZw5paWG5t1/OkRK6cxAXF8df/vIXOnfuzJEjR+jTpw8tWrSIdlgiIhJFZ7sTxrkYMmQIOTk5mBkNGzYsce7S3AmjXbt29OjRgzZt2hAXF0fr1q3p379/WD9Dzo6WLREREQkD7YQhZ0vLloiIiJxntBOGRJMSOhERkTAoTzthyPlH99CJiIiEQYNLqjB7cOQW8hU5Hc3QnaM9e/bQo0cPmjVrRmJiIgsWLChR/5///Ief/OQnJCcn06JFC1566aUS9Sfboy8nJ4f27duTkpJCWloaixcvBqKzR5+IiMiZROK7EAI7aFx55ZW0aNGCIUOGANH5Lhw9ejRJSUm0aNHilHvxnjfcvcy8UlNTvbT06tXLX3jhBXd3P3jwoO/evbtE/eOPP+5Dhgxxd/cdO3b4xRdf7AcPHiyuf/rpp71nz55+4403Fpelp6f79OnT3d39nXfe8Y4dO7q7e7du3fyzzz7zWbNm+eDBg93dffDgwf7BBx9EangiIiJnFInvwrlz53qnTp38wIED7u6+fft2dy/978KPP/7YW7Ro4fv27fPDhw97p06d/LPPPgvrZwDZHqYcSDN05yA/P5/58+fTt29fAOLj47noootKtDEzvvnmG9ydvXv3UqNGDeLiAle4T7VHn5mRn58PBH6rObpYcTT26BMRETmdSH0XPv/88zz00EN85zvfASje57a0vwvXrFlD+/btqVKlCnFxcXTs2JGpU6dG5LPCIlyZ4fnwKq0ZuuXLl3vbtm09MzPTU1JSvG/fvr53794SbfLz8/3aa6/1yy67zKtWrerTpk0rruvevbtnZ2f7+++/X+K3ktWrV3v9+vU9ISHB69at67m5ucWf165dO7/22mt98+bNfscdd4T9twQREZGzEanvwuTkZP/DH/7gV111lXfo0MEXL15c/Hml+V24evVqb9q0qX/11Ve+b98+b9++vQ8cODCsn4Fm6KKrsLCQZcuW8fOf/5zly5dTtWpVRowYUaLNzJkzSUlJYevWreTk5DBw4EDy8/NPu0ff888/z7PPPsvmzZt59tlni3/rKe09+kRERM4kUt+FhYWF7N69m4ULF/J///d/3H777bh7qX8XJiYm8tvf/pb09HQyMjJITk4unl08L4UrMzwfXpGcofviq33+46c/8MYPveM//OObnlC/QXHd/Pnz/YYbbijR/oYbbvD58+cXH1933XW+aNEif+ihh7xevXp++eWXe+3atf2CCy7wu+66y93dq1ev7kVFRe7uXlRU5BdeeGGJcxYVFXl6erp//fXX/tOf/tTXrFnj7777rj/88MORGraIiEix0vgu7Ny5s7///vvFfRo3buw7duwoPo7Wd+HQoUN9zJgxYT0nmqErfUe3dDniTt7B77A37rusXbsWgDlz5tC8efMS7Rs0aMCcOXMA2L59O2vXrqVx48Y88cQT5OXlkZuby5QpU/jRj35UvEdf3bp1mTdvHgBz586ladOmJc5ZWnv0iYiInExpfBfecsstzJ07F4DPPvuMQ4cOcemllxafszS/C3fs2AHApk2beOutt065L+/54DyeOzy/bNi5j6LgU9FFDlWvvZe77rqLQ4cO0bhxY1566aUSe/QNGzaM3r1707JlS9ydJ598ssR/kCfzwgsvMGjQIAoLC6lcuTLjx48vrivNPfpEREROpjS+C/v06UOfPn1ISkoiPj6eSZMmYRbYgqO0vwu7d+/Orl27qFSpEmPGjOHiiy8O+2eEi/ZyDVH6M/OKN12uYNCkZjUtICkiIuWKvgvDS3u5RoG2dBERkfJO34XnL83QiYiIiESBZuhEREREpJgSOhEREZGTONNetf/4xz9o1aoVrVq14vvf/z4rVqworuvTpw+1atUiKSmpRJ8VK1Zw9dVX07JlS4DvmVl1ADP7gZmtNLMlZva9YNlFZjbTjj4VchpK6EREREROYtCgQWRkZPDpp5+yYsUKEhMTS9Q3atSIefPmsXLlSoYNG0b//v2L63r37s2MGTNOOGe/fv0YMWIEH3/8McBu4DfBqgeA7sDDwM+DZcOA//UQ7o9TQiciIiJynFD2qv3+979fvJRJ+/btycvLK67r0KEDNWrUOOG8a9eupUOHDsUfQyCJAzgMXABUAQ6bWROgnrvPCyVeJXQiIiIix9mwYQM1a9bknnvuoXXr1vTr1499+/adsv2ECRPo0qXLGc+blJREVlbW0cMaQP3g+yeA8cB9wF+AxwnM0IVECZ2IiIjIcULZq/ao999/nwkTJvDkk0+e8bwTJ05kzJgxR/exrQAcAnD3HHdv7+7XAY2BrYCZ2Wtm9nczq32682qnCBERERFg064C+k5awoad+6j3nQPUqVuPdu3aAdCjR4+TJnQrV66kX79+vPvuu1xyySVn/IxmzZoV73RhZl8D+4+tDz4A8XvgDgIzdX8EGgK/Bn53qvNqhk5ERESEs9+rdtOmTdx666288sorXHHFFSF9xtH9YYuKigDqAOOOa5IJvOPuuwncT1cUfFU53Xk1QyciIiLC2e9VO3z4cHbt2sUvfvELAOLi4ji6wUHPnj354IMP+Oqrr0hISOCRRx6hb9++TJ48mTFjxhz9yMPAS0cPzKwKgYTu+mDRM8CbBC7L9jxd7BHdKcLMMoDRQEXgRXcfcVy9BetvAAqA3u6+LFh3EfAikAQ40MfdSy4AcxztFCEiIiLnqrT3qo2JnSLMrCIwBugCNAd6mlnz45p1AZoGX/2B54+pGw3McPdmQDKwJlKxioiIiMTyXrWRvOR6FbDO3TcAmNkU4GZg9TFtbgb+Flwwb2FwReQ6wD6gA9AbwN0PEXwKRERERCQSGlxSJaIzcpEUyYci6gGbjznOC5aF0qYxsBN4ycyWm9mLZlY1grGKiIiIxKxIJnQn23fs+Bv2TtUmDmgDPO/urQnM2D100g8x629m2WaWvXPnzm8Tr4iIiEhMimRCl8d/Vz8GSCCwSF4obfKAPHdfFCx/g0CCdwJ3H+/uae6eVrNmzbAELiIiIhJLIpnQLQGamlkjM4sH7gSyjmuTBfSygPbAf9x9m7t/CWw2syuD7TpR8t47EREREQmK2EMR7l5oZgOBmQSWLZno7p+Y2YBg/ThgOoElS9YRWLbknmNO8SvgH8FkcMNxdSIiIiISFNF16Eqb1qETERGRWBET69CJiIiISOlQQiciIiIS45TQiYiIiMS4MnUPnZntBL6I8MdcCnwV4c+IprI+Pij7Y9T4Yl9ZH6PGF/vK+hhLa3yXu3tY1lwrUwldaTCz7HDdwHg+Kuvjg7I/Ro0v9pX1MWp8sa+sjzEWx6dLriIiIiIxTgmdiIiISIxTQnf2xkc7gAgr6+ODsj9GjS/2lfUxanyxr6yPMebGp3voRERERGKcZuhEREREYpwSuhCZWYaZrTWzdWb2ULTjCTczm2hmO8xsVbRjiQQzq29m75vZGjP7xMwGRTumcDOzyma22MxWBMf4SLRjigQzq2hmy81sWrRjCTczyzWzj80sx8zK5D6GZnaRmb1hZp8G/3+8OtoxhYuZXRn8tzv6yjez+6IdVziZ2f3Bny+rzGyymVWOdkzhZmaDguP7JJb+/XTJNQRmVhH4DEgH8oAlQE93Xx3VwMLIzDoAe4G/uXtStOMJNzOrA9Rx92VmdiGwFLiljP0bGlDV3feaWSXgQ2CQuy+McmhhZWaDgTSgurvfFO14wsnMcoE0dy+z63uZ2STg3+7+opnFA1XcfU+Uwwq74PfGFqCdu0d6fdRSYWb1CPxcae7u+83sn8B0d385upGFj5klAVOAq4BDwAzg5+7+eVQDC4Fm6EJzFbDO3Te4+yEC/9g3RzmmsHL3+cDX0Y4jUtx9m7svC77/BlgD1ItuVOHlAXuDh5WCrzL1G5uZJQA3Ai9GOxY5e2ZWHegATABw90NlMZkL6gSsLyvJ3DHigAvMLA6oAmyNcjzhlggsdPcCdy8E5gHdohxTSJTQhaYesPmY4zzKWDJQnphZQ6A1sCjKoYRd8HJkDrADmO3uZW2Mo4AhQFGU44gUB2aZ2VIz6x/tYCKgMbATeCl42fxFM6sa7aAi5E5gcrSDCCd33wKMBDYB24D/uPus6EYVdquADmZ2iZlVAW4A6kc5ppAooQuNnaSsTM18lBdmVg14E7jP3fOjHU+4ufsRd08BEoCrgpcPygQzuwnY4e5Lox1LBP3A3dsAXYBfBm+FKEvigDbA8+7eGtgHlMV7kuOBrsDr0Y4lnMzsYgJXpxoBdYGqZnZ3dKMKL3dfAzwJzCZwuXUFUBjVoEKkhC40eZTM0BMoe9PMZV7wvrI3gX+4+1vRjieSgpexPgAyohtJWP0A6Bq8z2wK8CMz+3t0Qwovd98a/HMHMJXA7R5lSR6Qd8zM8RsEEryypguwzN23RzuQMPsxsNHdd7r7YeAt4PtRjins3H2Cu7dx9w4EbkU67++fAyV0oVoCNDWzRsHfvO4EsqIck5yF4AMDE4A17v5MtOOJBDOraWYXBd9fQOCH76dRDSqM3H2ouye4e0MC/w/OdfcyMztgZlWDD+wQvAx5PYHLP2WGu38JbDazK4NFnYAy82DSMXpSxi63Bm0C2ptZleDP1E4E7kcuU8ysVvDPBsCtxMi/ZVy0A4gF7l5oZgOBmUBFYKK7fxLlsMLKzCYD1wKXmlke8Ed3nxDdqMLqB8D/AB8H7zEDeNjdp0cvpLCrA0wKPl1XAfinu5e5pT3KsNrA1MD3JHHAq+4+I7ohRcSvgH8EfzneANwT5XjCKnjfVTrws2jHEm7uvsjM3gCWEbgMuZwY3FEhBG+a2SXAYeCX7r472gGFQsuWiIiIiMQ4XXIVERERiXFK6ERERERinBI6ERERkRinhE5EREQkximhExEREYlxSuhEpMwys8vMbIqZrTez1WY23cyuMLMytb6biIjWoRORMim48OlUYJK73xksSyGw3puISJmiGToRKauuAw67+7ijBe6eA2w+emxmDc3s32a2LPj6frC8jpnNN7McM1tlZj80s4pm9nLw+GMzuz/YtomZzTCzpcFzNQuW3xZsu8LM5pfqyEWk3NEMnYiUVUnA0jO02QGku/sBM2tKYIufNOCnwEx3fzy480YVIAWo5+5JAEe3WSOwUv4Ad//czNoBY4EfAX8AOrv7lmPaiohEhBI6ESnPKgF/CV6KPQJcESxfAkw0s0rAv9w9x8w2AI3N7DngHWCWmVUjsDn568EtuwC+E/zzI+BlM/sngU3MRUQiRpdcRaSs+gRIPUOb+4HtQDKBmbl4AHefD3QAtgCvmFmv4H6OycAHwC+BFwn8DN3j7inHvBKD5xgA/B6oD+QE94YUEYkIJXQiUlbNBb5jZvceLTCztsDlx7T5LrDN3YuA/wEqBttdDuxw9xeACUAbM7sUqODubwLDgDbung9sNLPbgv3MzJKD75u4+yJ3/wPwFYHETkQkIpTQiUiZ5O4OdAPSg8uWfAL8Cdh6TLOxQKaZLSRwuXVfsPxaArNqy4HuwGigHvCBmeUALwNDg23vAvqa2QoCs4I3B8v/L/jwxCpgPrAiAsMUEQHAAj/zRERERCRWaYZOREREJMYpoRMRERGJcUroRERERGKcEjoRERGRGKeETkRERCTGKaETERERiXFK6ERERERinBI6ERERkRj3/wGzYEdQjiHG/wAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "9f6d7183-e99e-4f93-e660-af58e8de94b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(class_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArgupDVRwB8i",
        "outputId": "84131c1f-7e68-4cff-b306-401f9d21337f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3751, accuracy : 16.53\n",
            "iteration : 100, loss : 2.3005, accuracy : 17.37\n",
            "iteration : 150, loss : 2.1694, accuracy : 22.66\n",
            "iteration : 200, loss : 1.9115, accuracy : 32.52\n",
            "iteration : 250, loss : 1.6752, accuracy : 41.33\n",
            "iteration : 300, loss : 1.4923, accuracy : 48.11\n",
            "iteration : 350, loss : 1.3504, accuracy : 53.24\n",
            "Epoch :   1, training loss : 1.3080, training accuracy : 54.77, test loss : 1.2243, test accuracy : 63.28\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.4546, accuracy : 86.06\n",
            "iteration : 100, loss : 0.4424, accuracy : 86.60\n",
            "iteration : 150, loss : 0.4364, accuracy : 86.78\n",
            "iteration : 200, loss : 0.4349, accuracy : 86.75\n",
            "iteration : 250, loss : 0.4229, accuracy : 87.09\n",
            "iteration : 300, loss : 0.4164, accuracy : 87.30\n",
            "iteration : 350, loss : 0.4143, accuracy : 87.34\n",
            "Epoch :   2, training loss : 0.4128, training accuracy : 87.39, test loss : 0.4767, test accuracy : 85.48\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.3723, accuracy : 89.20\n",
            "iteration : 100, loss : 0.3607, accuracy : 89.29\n",
            "iteration : 150, loss : 0.3561, accuracy : 89.35\n",
            "iteration : 200, loss : 0.3618, accuracy : 89.12\n",
            "iteration : 250, loss : 0.3650, accuracy : 88.91\n",
            "iteration : 300, loss : 0.3653, accuracy : 88.91\n",
            "iteration : 350, loss : 0.3632, accuracy : 88.98\n",
            "Epoch :   3, training loss : 0.3641, training accuracy : 88.97, test loss : 0.4852, test accuracy : 85.78\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3227, accuracy : 90.17\n",
            "iteration : 100, loss : 0.3312, accuracy : 90.18\n",
            "iteration : 150, loss : 0.3295, accuracy : 90.21\n",
            "iteration : 200, loss : 0.3316, accuracy : 90.11\n",
            "iteration : 250, loss : 0.3324, accuracy : 90.07\n",
            "iteration : 300, loss : 0.3377, accuracy : 89.89\n",
            "iteration : 350, loss : 0.3366, accuracy : 89.93\n",
            "Epoch :   4, training loss : 0.3362, training accuracy : 89.91, test loss : 0.4449, test accuracy : 86.67\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.3160, accuracy : 90.36\n",
            "iteration : 100, loss : 0.3220, accuracy : 90.27\n",
            "iteration : 150, loss : 0.3220, accuracy : 90.24\n",
            "iteration : 200, loss : 0.3253, accuracy : 90.25\n",
            "iteration : 250, loss : 0.3239, accuracy : 90.20\n",
            "iteration : 300, loss : 0.3219, accuracy : 90.32\n",
            "iteration : 350, loss : 0.3214, accuracy : 90.33\n",
            "Epoch :   5, training loss : 0.3223, training accuracy : 90.34, test loss : 0.3845, test accuracy : 88.43\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.3220, accuracy : 90.05\n",
            "iteration : 100, loss : 0.3138, accuracy : 90.45\n",
            "iteration : 150, loss : 0.3092, accuracy : 90.66\n",
            "iteration : 200, loss : 0.3115, accuracy : 90.55\n",
            "iteration : 250, loss : 0.3101, accuracy : 90.62\n",
            "iteration : 300, loss : 0.3095, accuracy : 90.67\n",
            "iteration : 350, loss : 0.3112, accuracy : 90.61\n",
            "Epoch :   6, training loss : 0.3105, training accuracy : 90.63, test loss : 0.3909, test accuracy : 88.22\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.2839, accuracy : 91.55\n",
            "iteration : 100, loss : 0.2922, accuracy : 91.23\n",
            "iteration : 150, loss : 0.3018, accuracy : 90.97\n",
            "iteration : 200, loss : 0.3002, accuracy : 90.98\n",
            "iteration : 250, loss : 0.3009, accuracy : 90.94\n",
            "iteration : 300, loss : 0.2974, accuracy : 91.06\n",
            "iteration : 350, loss : 0.2995, accuracy : 91.07\n",
            "Epoch :   7, training loss : 0.2985, training accuracy : 91.13, test loss : 0.3331, test accuracy : 89.89\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.2792, accuracy : 91.69\n",
            "iteration : 100, loss : 0.2805, accuracy : 91.62\n",
            "iteration : 150, loss : 0.2829, accuracy : 91.64\n",
            "iteration : 200, loss : 0.2843, accuracy : 91.58\n",
            "iteration : 250, loss : 0.2825, accuracy : 91.64\n",
            "iteration : 300, loss : 0.2832, accuracy : 91.66\n",
            "iteration : 350, loss : 0.2855, accuracy : 91.58\n",
            "Epoch :   8, training loss : 0.2846, training accuracy : 91.62, test loss : 0.3330, test accuracy : 90.08\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.2685, accuracy : 92.34\n",
            "iteration : 100, loss : 0.2723, accuracy : 92.12\n",
            "iteration : 150, loss : 0.2736, accuracy : 91.96\n",
            "iteration : 200, loss : 0.2694, accuracy : 92.03\n",
            "iteration : 250, loss : 0.2710, accuracy : 91.97\n",
            "iteration : 300, loss : 0.2720, accuracy : 91.93\n",
            "iteration : 350, loss : 0.2719, accuracy : 91.96\n",
            "Epoch :   9, training loss : 0.2726, training accuracy : 91.95, test loss : 0.3105, test accuracy : 90.70\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2523, accuracy : 92.78\n",
            "iteration : 100, loss : 0.2483, accuracy : 92.70\n",
            "iteration : 150, loss : 0.2557, accuracy : 92.49\n",
            "iteration : 200, loss : 0.2588, accuracy : 92.36\n",
            "iteration : 250, loss : 0.2621, accuracy : 92.27\n",
            "iteration : 300, loss : 0.2605, accuracy : 92.29\n",
            "iteration : 350, loss : 0.2626, accuracy : 92.19\n",
            "Epoch :  10, training loss : 0.2629, training accuracy : 92.19, test loss : 0.2816, test accuracy : 91.61\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.2713, accuracy : 92.06\n",
            "iteration : 100, loss : 0.2568, accuracy : 92.55\n",
            "iteration : 150, loss : 0.2555, accuracy : 92.59\n",
            "iteration : 200, loss : 0.2552, accuracy : 92.57\n",
            "iteration : 250, loss : 0.2575, accuracy : 92.48\n",
            "iteration : 300, loss : 0.2563, accuracy : 92.50\n",
            "iteration : 350, loss : 0.2550, accuracy : 92.55\n",
            "Epoch :  11, training loss : 0.2557, training accuracy : 92.53, test loss : 0.2945, test accuracy : 91.36\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.2294, accuracy : 93.08\n",
            "iteration : 100, loss : 0.2397, accuracy : 92.89\n",
            "iteration : 150, loss : 0.2466, accuracy : 92.71\n",
            "iteration : 200, loss : 0.2476, accuracy : 92.68\n",
            "iteration : 250, loss : 0.2454, accuracy : 92.71\n",
            "iteration : 300, loss : 0.2464, accuracy : 92.70\n",
            "iteration : 350, loss : 0.2471, accuracy : 92.70\n",
            "Epoch :  12, training loss : 0.2472, training accuracy : 92.70, test loss : 0.2779, test accuracy : 91.83\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.2250, accuracy : 93.14\n",
            "iteration : 100, loss : 0.2176, accuracy : 93.54\n",
            "iteration : 150, loss : 0.2294, accuracy : 93.26\n",
            "iteration : 200, loss : 0.2283, accuracy : 93.28\n",
            "iteration : 250, loss : 0.2297, accuracy : 93.22\n",
            "iteration : 300, loss : 0.2324, accuracy : 93.14\n",
            "iteration : 350, loss : 0.2328, accuracy : 93.11\n",
            "Epoch :  13, training loss : 0.2331, training accuracy : 93.09, test loss : 0.3032, test accuracy : 91.17\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.2125, accuracy : 93.62\n",
            "iteration : 100, loss : 0.2082, accuracy : 93.71\n",
            "iteration : 150, loss : 0.2158, accuracy : 93.53\n",
            "iteration : 200, loss : 0.2215, accuracy : 93.47\n",
            "iteration : 250, loss : 0.2291, accuracy : 93.36\n",
            "iteration : 300, loss : 0.2286, accuracy : 93.36\n",
            "iteration : 350, loss : 0.2315, accuracy : 93.27\n",
            "Epoch :  14, training loss : 0.2307, training accuracy : 93.29, test loss : 0.2831, test accuracy : 91.79\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.2051, accuracy : 94.08\n",
            "iteration : 100, loss : 0.2192, accuracy : 93.62\n",
            "iteration : 150, loss : 0.2208, accuracy : 93.49\n",
            "iteration : 200, loss : 0.2233, accuracy : 93.46\n",
            "iteration : 250, loss : 0.2241, accuracy : 93.42\n",
            "iteration : 300, loss : 0.2279, accuracy : 93.34\n",
            "iteration : 350, loss : 0.2283, accuracy : 93.34\n",
            "Epoch :  15, training loss : 0.2283, training accuracy : 93.35, test loss : 0.2758, test accuracy : 91.81\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.2141, accuracy : 93.97\n",
            "iteration : 100, loss : 0.2158, accuracy : 93.77\n",
            "iteration : 150, loss : 0.2191, accuracy : 93.81\n",
            "iteration : 200, loss : 0.2167, accuracy : 93.82\n",
            "iteration : 250, loss : 0.2162, accuracy : 93.87\n",
            "iteration : 300, loss : 0.2156, accuracy : 93.82\n",
            "iteration : 350, loss : 0.2140, accuracy : 93.84\n",
            "Epoch :  16, training loss : 0.2151, training accuracy : 93.77, test loss : 0.2620, test accuracy : 92.49\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.2065, accuracy : 94.20\n",
            "iteration : 100, loss : 0.2049, accuracy : 94.11\n",
            "iteration : 150, loss : 0.2158, accuracy : 93.76\n",
            "iteration : 200, loss : 0.2138, accuracy : 93.81\n",
            "iteration : 250, loss : 0.2158, accuracy : 93.77\n",
            "iteration : 300, loss : 0.2151, accuracy : 93.80\n",
            "iteration : 350, loss : 0.2144, accuracy : 93.85\n",
            "Epoch :  17, training loss : 0.2153, training accuracy : 93.83, test loss : 0.3133, test accuracy : 91.03\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.2063, accuracy : 93.95\n",
            "iteration : 100, loss : 0.2024, accuracy : 94.17\n",
            "iteration : 150, loss : 0.1980, accuracy : 94.18\n",
            "iteration : 200, loss : 0.1992, accuracy : 94.16\n",
            "iteration : 250, loss : 0.1997, accuracy : 94.16\n",
            "iteration : 300, loss : 0.2025, accuracy : 94.11\n",
            "iteration : 350, loss : 0.2045, accuracy : 94.03\n",
            "Epoch :  18, training loss : 0.2038, training accuracy : 94.05, test loss : 0.2692, test accuracy : 92.31\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.1876, accuracy : 94.44\n",
            "iteration : 100, loss : 0.1918, accuracy : 94.30\n",
            "iteration : 150, loss : 0.1942, accuracy : 94.22\n",
            "iteration : 200, loss : 0.2001, accuracy : 94.07\n",
            "iteration : 250, loss : 0.2045, accuracy : 93.89\n",
            "iteration : 300, loss : 0.2032, accuracy : 93.94\n",
            "iteration : 350, loss : 0.2017, accuracy : 94.00\n",
            "Epoch :  19, training loss : 0.2022, training accuracy : 94.04, test loss : 0.2539, test accuracy : 92.62\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.1891, accuracy : 94.50\n",
            "iteration : 100, loss : 0.2009, accuracy : 94.27\n",
            "iteration : 150, loss : 0.1957, accuracy : 94.36\n",
            "iteration : 200, loss : 0.1935, accuracy : 94.37\n",
            "iteration : 250, loss : 0.1935, accuracy : 94.37\n",
            "iteration : 300, loss : 0.1957, accuracy : 94.25\n",
            "iteration : 350, loss : 0.1996, accuracy : 94.17\n",
            "Epoch :  20, training loss : 0.2004, training accuracy : 94.14, test loss : 0.2555, test accuracy : 92.65\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.1858, accuracy : 94.89\n",
            "iteration : 100, loss : 0.1838, accuracy : 94.94\n",
            "iteration : 150, loss : 0.1861, accuracy : 94.69\n",
            "iteration : 200, loss : 0.1848, accuracy : 94.68\n",
            "iteration : 250, loss : 0.1859, accuracy : 94.69\n",
            "iteration : 300, loss : 0.1841, accuracy : 94.72\n",
            "iteration : 350, loss : 0.1867, accuracy : 94.63\n",
            "Epoch :  21, training loss : 0.1882, training accuracy : 94.57, test loss : 0.2524, test accuracy : 92.92\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.1716, accuracy : 94.70\n",
            "iteration : 100, loss : 0.1771, accuracy : 94.79\n",
            "iteration : 150, loss : 0.1807, accuracy : 94.67\n",
            "iteration : 200, loss : 0.1839, accuracy : 94.65\n",
            "iteration : 250, loss : 0.1891, accuracy : 94.49\n",
            "iteration : 300, loss : 0.1917, accuracy : 94.43\n",
            "iteration : 350, loss : 0.1911, accuracy : 94.47\n",
            "Epoch :  22, training loss : 0.1920, training accuracy : 94.45, test loss : 0.2694, test accuracy : 92.36\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.1706, accuracy : 95.11\n",
            "iteration : 100, loss : 0.1787, accuracy : 95.05\n",
            "iteration : 150, loss : 0.1825, accuracy : 94.86\n",
            "iteration : 200, loss : 0.1839, accuracy : 94.73\n",
            "iteration : 250, loss : 0.1846, accuracy : 94.76\n",
            "iteration : 300, loss : 0.1864, accuracy : 94.66\n",
            "iteration : 350, loss : 0.1877, accuracy : 94.61\n",
            "Epoch :  23, training loss : 0.1873, training accuracy : 94.62, test loss : 0.2469, test accuracy : 93.00\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.1661, accuracy : 95.20\n",
            "iteration : 100, loss : 0.1774, accuracy : 94.70\n",
            "iteration : 150, loss : 0.1777, accuracy : 94.71\n",
            "iteration : 200, loss : 0.1795, accuracy : 94.67\n",
            "iteration : 250, loss : 0.1828, accuracy : 94.57\n",
            "iteration : 300, loss : 0.1850, accuracy : 94.49\n",
            "iteration : 350, loss : 0.1849, accuracy : 94.53\n",
            "Epoch :  24, training loss : 0.1846, training accuracy : 94.52, test loss : 0.2430, test accuracy : 93.07\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.1646, accuracy : 95.17\n",
            "iteration : 100, loss : 0.1658, accuracy : 95.24\n",
            "iteration : 150, loss : 0.1712, accuracy : 95.07\n",
            "iteration : 200, loss : 0.1712, accuracy : 95.10\n",
            "iteration : 250, loss : 0.1721, accuracy : 95.00\n",
            "iteration : 300, loss : 0.1710, accuracy : 95.06\n",
            "iteration : 350, loss : 0.1740, accuracy : 94.94\n",
            "Epoch :  25, training loss : 0.1777, training accuracy : 94.82, test loss : 0.2985, test accuracy : 91.64\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.1702, accuracy : 95.45\n",
            "iteration : 100, loss : 0.1702, accuracy : 95.32\n",
            "iteration : 150, loss : 0.1650, accuracy : 95.36\n",
            "iteration : 200, loss : 0.1691, accuracy : 95.22\n",
            "iteration : 250, loss : 0.1724, accuracy : 95.17\n",
            "iteration : 300, loss : 0.1740, accuracy : 95.11\n",
            "iteration : 350, loss : 0.1766, accuracy : 95.00\n",
            "Epoch :  26, training loss : 0.1765, training accuracy : 94.98, test loss : 0.2367, test accuracy : 93.31\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.1508, accuracy : 95.53\n",
            "iteration : 100, loss : 0.1605, accuracy : 95.23\n",
            "iteration : 150, loss : 0.1611, accuracy : 95.32\n",
            "iteration : 200, loss : 0.1635, accuracy : 95.22\n",
            "iteration : 250, loss : 0.1683, accuracy : 95.12\n",
            "iteration : 300, loss : 0.1700, accuracy : 95.03\n",
            "iteration : 350, loss : 0.1700, accuracy : 95.05\n",
            "Epoch :  27, training loss : 0.1703, training accuracy : 95.04, test loss : 0.2460, test accuracy : 93.09\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.1780, accuracy : 94.81\n",
            "iteration : 100, loss : 0.1746, accuracy : 94.91\n",
            "iteration : 150, loss : 0.1725, accuracy : 94.96\n",
            "iteration : 200, loss : 0.1761, accuracy : 94.91\n",
            "iteration : 250, loss : 0.1720, accuracy : 95.04\n",
            "iteration : 300, loss : 0.1733, accuracy : 95.00\n",
            "iteration : 350, loss : 0.1717, accuracy : 95.04\n",
            "Epoch :  28, training loss : 0.1724, training accuracy : 95.01, test loss : 0.2627, test accuracy : 92.47\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.1606, accuracy : 95.59\n",
            "iteration : 100, loss : 0.1568, accuracy : 95.63\n",
            "iteration : 150, loss : 0.1597, accuracy : 95.45\n",
            "iteration : 200, loss : 0.1623, accuracy : 95.38\n",
            "iteration : 250, loss : 0.1634, accuracy : 95.30\n",
            "iteration : 300, loss : 0.1655, accuracy : 95.22\n",
            "iteration : 350, loss : 0.1676, accuracy : 95.12\n",
            "Epoch :  29, training loss : 0.1696, training accuracy : 95.08, test loss : 0.2418, test accuracy : 93.22\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.1527, accuracy : 95.61\n",
            "iteration : 100, loss : 0.1527, accuracy : 95.73\n",
            "iteration : 150, loss : 0.1570, accuracy : 95.55\n",
            "iteration : 200, loss : 0.1556, accuracy : 95.62\n",
            "iteration : 250, loss : 0.1600, accuracy : 95.46\n",
            "iteration : 300, loss : 0.1632, accuracy : 95.33\n",
            "iteration : 350, loss : 0.1656, accuracy : 95.26\n",
            "Epoch :  30, training loss : 0.1653, training accuracy : 95.28, test loss : 0.2399, test accuracy : 93.04\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.1382, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1473, accuracy : 95.81\n",
            "iteration : 150, loss : 0.1522, accuracy : 95.67\n",
            "iteration : 200, loss : 0.1546, accuracy : 95.55\n",
            "iteration : 250, loss : 0.1601, accuracy : 95.41\n",
            "iteration : 300, loss : 0.1599, accuracy : 95.41\n",
            "iteration : 350, loss : 0.1617, accuracy : 95.36\n",
            "Epoch :  31, training loss : 0.1618, training accuracy : 95.37, test loss : 0.2373, test accuracy : 93.45\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.1503, accuracy : 95.77\n",
            "iteration : 100, loss : 0.1476, accuracy : 95.69\n",
            "iteration : 150, loss : 0.1526, accuracy : 95.49\n",
            "iteration : 200, loss : 0.1569, accuracy : 95.49\n",
            "iteration : 250, loss : 0.1597, accuracy : 95.39\n",
            "iteration : 300, loss : 0.1628, accuracy : 95.29\n",
            "iteration : 350, loss : 0.1630, accuracy : 95.27\n",
            "Epoch :  32, training loss : 0.1627, training accuracy : 95.29, test loss : 0.2383, test accuracy : 93.35\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.1406, accuracy : 95.83\n",
            "iteration : 100, loss : 0.1441, accuracy : 95.80\n",
            "iteration : 150, loss : 0.1485, accuracy : 95.66\n",
            "iteration : 200, loss : 0.1534, accuracy : 95.58\n",
            "iteration : 250, loss : 0.1587, accuracy : 95.41\n",
            "iteration : 300, loss : 0.1571, accuracy : 95.44\n",
            "iteration : 350, loss : 0.1572, accuracy : 95.45\n",
            "Epoch :  33, training loss : 0.1599, training accuracy : 95.37, test loss : 0.2611, test accuracy : 92.45\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.1485, accuracy : 95.72\n",
            "iteration : 100, loss : 0.1462, accuracy : 95.93\n",
            "iteration : 150, loss : 0.1492, accuracy : 95.84\n",
            "iteration : 200, loss : 0.1519, accuracy : 95.66\n",
            "iteration : 250, loss : 0.1505, accuracy : 95.67\n",
            "iteration : 300, loss : 0.1521, accuracy : 95.67\n",
            "iteration : 350, loss : 0.1543, accuracy : 95.60\n",
            "Epoch :  34, training loss : 0.1552, training accuracy : 95.58, test loss : 0.2243, test accuracy : 93.64\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.1348, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1505, accuracy : 95.63\n",
            "iteration : 150, loss : 0.1526, accuracy : 95.58\n",
            "iteration : 200, loss : 0.1480, accuracy : 95.78\n",
            "iteration : 250, loss : 0.1516, accuracy : 95.62\n",
            "iteration : 300, loss : 0.1509, accuracy : 95.65\n",
            "iteration : 350, loss : 0.1539, accuracy : 95.61\n",
            "Epoch :  35, training loss : 0.1553, training accuracy : 95.58, test loss : 0.2430, test accuracy : 93.44\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.1315, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1452, accuracy : 95.63\n",
            "iteration : 150, loss : 0.1525, accuracy : 95.52\n",
            "iteration : 200, loss : 0.1501, accuracy : 95.55\n",
            "iteration : 250, loss : 0.1510, accuracy : 95.56\n",
            "iteration : 300, loss : 0.1549, accuracy : 95.48\n",
            "iteration : 350, loss : 0.1571, accuracy : 95.47\n",
            "Epoch :  36, training loss : 0.1585, training accuracy : 95.43, test loss : 0.2620, test accuracy : 92.70\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.1421, accuracy : 96.08\n",
            "iteration : 100, loss : 0.1447, accuracy : 96.02\n",
            "iteration : 150, loss : 0.1450, accuracy : 95.91\n",
            "iteration : 200, loss : 0.1456, accuracy : 95.92\n",
            "iteration : 250, loss : 0.1478, accuracy : 95.81\n",
            "iteration : 300, loss : 0.1493, accuracy : 95.78\n",
            "iteration : 350, loss : 0.1512, accuracy : 95.70\n",
            "Epoch :  37, training loss : 0.1528, training accuracy : 95.65, test loss : 0.2360, test accuracy : 93.63\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.1389, accuracy : 96.05\n",
            "iteration : 100, loss : 0.1456, accuracy : 95.80\n",
            "iteration : 150, loss : 0.1459, accuracy : 95.66\n",
            "iteration : 200, loss : 0.1469, accuracy : 95.63\n",
            "iteration : 250, loss : 0.1500, accuracy : 95.59\n",
            "iteration : 300, loss : 0.1511, accuracy : 95.56\n",
            "iteration : 350, loss : 0.1528, accuracy : 95.50\n",
            "Epoch :  38, training loss : 0.1545, training accuracy : 95.46, test loss : 0.2473, test accuracy : 93.02\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.1367, accuracy : 95.92\n",
            "iteration : 100, loss : 0.1395, accuracy : 95.91\n",
            "iteration : 150, loss : 0.1444, accuracy : 95.76\n",
            "iteration : 200, loss : 0.1473, accuracy : 95.68\n",
            "iteration : 250, loss : 0.1487, accuracy : 95.70\n",
            "iteration : 300, loss : 0.1470, accuracy : 95.74\n",
            "iteration : 350, loss : 0.1496, accuracy : 95.65\n",
            "Epoch :  39, training loss : 0.1508, training accuracy : 95.62, test loss : 0.2386, test accuracy : 93.29\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.1441, accuracy : 95.95\n",
            "iteration : 100, loss : 0.1414, accuracy : 95.88\n",
            "iteration : 150, loss : 0.1401, accuracy : 95.92\n",
            "iteration : 200, loss : 0.1406, accuracy : 95.92\n",
            "iteration : 250, loss : 0.1433, accuracy : 95.91\n",
            "iteration : 300, loss : 0.1465, accuracy : 95.76\n",
            "iteration : 350, loss : 0.1519, accuracy : 95.62\n",
            "Epoch :  40, training loss : 0.1518, training accuracy : 95.60, test loss : 0.2304, test accuracy : 93.36\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.1353, accuracy : 96.36\n",
            "iteration : 100, loss : 0.1392, accuracy : 96.16\n",
            "iteration : 150, loss : 0.1482, accuracy : 95.92\n",
            "iteration : 200, loss : 0.1480, accuracy : 95.88\n",
            "iteration : 250, loss : 0.1497, accuracy : 95.83\n",
            "iteration : 300, loss : 0.1521, accuracy : 95.70\n",
            "iteration : 350, loss : 0.1500, accuracy : 95.72\n",
            "Epoch :  41, training loss : 0.1502, training accuracy : 95.74, test loss : 0.2345, test accuracy : 93.29\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.1316, accuracy : 96.25\n",
            "iteration : 100, loss : 0.1341, accuracy : 96.09\n",
            "iteration : 150, loss : 0.1368, accuracy : 95.99\n",
            "iteration : 200, loss : 0.1387, accuracy : 96.00\n",
            "iteration : 250, loss : 0.1396, accuracy : 95.97\n",
            "iteration : 300, loss : 0.1422, accuracy : 95.92\n",
            "iteration : 350, loss : 0.1454, accuracy : 95.80\n",
            "Epoch :  42, training loss : 0.1461, training accuracy : 95.80, test loss : 0.2340, test accuracy : 93.45\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.1327, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1378, accuracy : 96.10\n",
            "iteration : 150, loss : 0.1453, accuracy : 95.89\n",
            "iteration : 200, loss : 0.1452, accuracy : 95.92\n",
            "iteration : 250, loss : 0.1474, accuracy : 95.80\n",
            "iteration : 300, loss : 0.1478, accuracy : 95.76\n",
            "iteration : 350, loss : 0.1491, accuracy : 95.75\n",
            "Epoch :  43, training loss : 0.1499, training accuracy : 95.74, test loss : 0.2395, test accuracy : 93.43\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.1335, accuracy : 96.27\n",
            "iteration : 100, loss : 0.1346, accuracy : 96.16\n",
            "iteration : 150, loss : 0.1401, accuracy : 95.99\n",
            "iteration : 200, loss : 0.1424, accuracy : 95.90\n",
            "iteration : 250, loss : 0.1409, accuracy : 95.96\n",
            "iteration : 300, loss : 0.1441, accuracy : 95.88\n",
            "iteration : 350, loss : 0.1447, accuracy : 95.86\n",
            "Epoch :  44, training loss : 0.1455, training accuracy : 95.84, test loss : 0.2479, test accuracy : 93.21\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.1424, accuracy : 96.05\n",
            "iteration : 100, loss : 0.1464, accuracy : 95.67\n",
            "iteration : 150, loss : 0.1416, accuracy : 95.82\n",
            "iteration : 200, loss : 0.1425, accuracy : 95.87\n",
            "iteration : 250, loss : 0.1394, accuracy : 95.96\n",
            "iteration : 300, loss : 0.1389, accuracy : 96.02\n",
            "iteration : 350, loss : 0.1398, accuracy : 95.99\n",
            "Epoch :  45, training loss : 0.1404, training accuracy : 95.97, test loss : 0.2424, test accuracy : 93.40\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.1415, accuracy : 95.97\n",
            "iteration : 100, loss : 0.1377, accuracy : 95.99\n",
            "iteration : 150, loss : 0.1390, accuracy : 96.05\n",
            "iteration : 200, loss : 0.1430, accuracy : 95.80\n",
            "iteration : 250, loss : 0.1422, accuracy : 95.87\n",
            "iteration : 300, loss : 0.1447, accuracy : 95.84\n",
            "iteration : 350, loss : 0.1452, accuracy : 95.81\n",
            "Epoch :  46, training loss : 0.1454, training accuracy : 95.81, test loss : 0.2396, test accuracy : 93.39\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.1292, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1284, accuracy : 96.24\n",
            "iteration : 150, loss : 0.1326, accuracy : 96.14\n",
            "iteration : 200, loss : 0.1342, accuracy : 96.12\n",
            "iteration : 250, loss : 0.1375, accuracy : 96.03\n",
            "iteration : 300, loss : 0.1389, accuracy : 95.98\n",
            "iteration : 350, loss : 0.1396, accuracy : 95.93\n",
            "Epoch :  47, training loss : 0.1405, training accuracy : 95.89, test loss : 0.2536, test accuracy : 92.97\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.1218, accuracy : 96.31\n",
            "iteration : 100, loss : 0.1288, accuracy : 96.02\n",
            "iteration : 150, loss : 0.1328, accuracy : 95.95\n",
            "iteration : 200, loss : 0.1369, accuracy : 95.86\n",
            "iteration : 250, loss : 0.1394, accuracy : 95.82\n",
            "iteration : 300, loss : 0.1434, accuracy : 95.74\n",
            "iteration : 350, loss : 0.1436, accuracy : 95.77\n",
            "Epoch :  48, training loss : 0.1437, training accuracy : 95.78, test loss : 0.2292, test accuracy : 93.51\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.1042, accuracy : 96.91\n",
            "iteration : 100, loss : 0.1193, accuracy : 96.42\n",
            "iteration : 150, loss : 0.1305, accuracy : 96.15\n",
            "iteration : 200, loss : 0.1335, accuracy : 96.08\n",
            "iteration : 250, loss : 0.1348, accuracy : 96.08\n",
            "iteration : 300, loss : 0.1356, accuracy : 96.05\n",
            "iteration : 350, loss : 0.1381, accuracy : 95.99\n",
            "Epoch :  49, training loss : 0.1381, training accuracy : 95.99, test loss : 0.2559, test accuracy : 92.77\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.1277, accuracy : 96.11\n",
            "iteration : 100, loss : 0.1331, accuracy : 96.08\n",
            "iteration : 150, loss : 0.1326, accuracy : 96.10\n",
            "iteration : 200, loss : 0.1340, accuracy : 96.07\n",
            "iteration : 250, loss : 0.1348, accuracy : 96.05\n",
            "iteration : 300, loss : 0.1381, accuracy : 95.99\n",
            "iteration : 350, loss : 0.1409, accuracy : 95.86\n",
            "Epoch :  50, training loss : 0.1406, training accuracy : 95.86, test loss : 0.2396, test accuracy : 93.39\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.1355, accuracy : 96.19\n",
            "iteration : 100, loss : 0.1307, accuracy : 96.23\n",
            "iteration : 150, loss : 0.1340, accuracy : 96.06\n",
            "iteration : 200, loss : 0.1364, accuracy : 96.02\n",
            "iteration : 250, loss : 0.1361, accuracy : 96.03\n",
            "iteration : 300, loss : 0.1381, accuracy : 95.96\n",
            "iteration : 350, loss : 0.1389, accuracy : 95.94\n",
            "Epoch :  51, training loss : 0.1391, training accuracy : 95.95, test loss : 0.2308, test accuracy : 93.73\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.1158, accuracy : 96.77\n",
            "iteration : 100, loss : 0.1254, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1318, accuracy : 96.21\n",
            "iteration : 200, loss : 0.1314, accuracy : 96.22\n",
            "iteration : 250, loss : 0.1378, accuracy : 96.08\n",
            "iteration : 300, loss : 0.1363, accuracy : 96.14\n",
            "iteration : 350, loss : 0.1372, accuracy : 96.10\n",
            "Epoch :  52, training loss : 0.1370, training accuracy : 96.10, test loss : 0.2284, test accuracy : 93.69\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.1245, accuracy : 96.56\n",
            "iteration : 100, loss : 0.1242, accuracy : 96.46\n",
            "iteration : 150, loss : 0.1270, accuracy : 96.35\n",
            "iteration : 200, loss : 0.1314, accuracy : 96.22\n",
            "iteration : 250, loss : 0.1330, accuracy : 96.21\n",
            "iteration : 300, loss : 0.1335, accuracy : 96.14\n",
            "iteration : 350, loss : 0.1344, accuracy : 96.09\n",
            "Epoch :  53, training loss : 0.1334, training accuracy : 96.12, test loss : 0.2475, test accuracy : 93.28\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.1275, accuracy : 96.23\n",
            "iteration : 100, loss : 0.1231, accuracy : 96.35\n",
            "iteration : 150, loss : 0.1216, accuracy : 96.50\n",
            "iteration : 200, loss : 0.1257, accuracy : 96.39\n",
            "iteration : 250, loss : 0.1256, accuracy : 96.37\n",
            "iteration : 300, loss : 0.1304, accuracy : 96.22\n",
            "iteration : 350, loss : 0.1307, accuracy : 96.22\n",
            "Epoch :  54, training loss : 0.1321, training accuracy : 96.19, test loss : 0.2438, test accuracy : 93.27\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.1357, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1322, accuracy : 96.13\n",
            "iteration : 150, loss : 0.1310, accuracy : 96.18\n",
            "iteration : 200, loss : 0.1359, accuracy : 96.04\n",
            "iteration : 250, loss : 0.1348, accuracy : 96.09\n",
            "iteration : 300, loss : 0.1339, accuracy : 96.03\n",
            "iteration : 350, loss : 0.1354, accuracy : 96.01\n",
            "Epoch :  55, training loss : 0.1356, training accuracy : 96.00, test loss : 0.2348, test accuracy : 93.51\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.1224, accuracy : 96.52\n",
            "iteration : 100, loss : 0.1247, accuracy : 96.44\n",
            "iteration : 150, loss : 0.1180, accuracy : 96.71\n",
            "iteration : 200, loss : 0.1223, accuracy : 96.50\n",
            "iteration : 250, loss : 0.1242, accuracy : 96.48\n",
            "iteration : 300, loss : 0.1266, accuracy : 96.42\n",
            "iteration : 350, loss : 0.1300, accuracy : 96.29\n",
            "Epoch :  56, training loss : 0.1313, training accuracy : 96.27, test loss : 0.2408, test accuracy : 93.51\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.1181, accuracy : 96.69\n",
            "iteration : 100, loss : 0.1229, accuracy : 96.54\n",
            "iteration : 150, loss : 0.1221, accuracy : 96.51\n",
            "iteration : 200, loss : 0.1237, accuracy : 96.44\n",
            "iteration : 250, loss : 0.1238, accuracy : 96.39\n",
            "iteration : 300, loss : 0.1258, accuracy : 96.33\n",
            "iteration : 350, loss : 0.1297, accuracy : 96.25\n",
            "Epoch :  57, training loss : 0.1314, training accuracy : 96.20, test loss : 0.2303, test accuracy : 93.75\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.1232, accuracy : 96.45\n",
            "iteration : 100, loss : 0.1234, accuracy : 96.47\n",
            "iteration : 150, loss : 0.1233, accuracy : 96.48\n",
            "iteration : 200, loss : 0.1222, accuracy : 96.49\n",
            "iteration : 250, loss : 0.1268, accuracy : 96.36\n",
            "iteration : 300, loss : 0.1277, accuracy : 96.35\n",
            "iteration : 350, loss : 0.1304, accuracy : 96.25\n",
            "Epoch :  58, training loss : 0.1319, training accuracy : 96.22, test loss : 0.2319, test accuracy : 93.76\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.1231, accuracy : 96.39\n",
            "iteration : 100, loss : 0.1192, accuracy : 96.42\n",
            "iteration : 150, loss : 0.1228, accuracy : 96.33\n",
            "iteration : 200, loss : 0.1275, accuracy : 96.18\n",
            "iteration : 250, loss : 0.1309, accuracy : 96.10\n",
            "iteration : 300, loss : 0.1299, accuracy : 96.13\n",
            "iteration : 350, loss : 0.1304, accuracy : 96.11\n",
            "Epoch :  59, training loss : 0.1304, training accuracy : 96.11, test loss : 0.2309, test accuracy : 93.63\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.1217, accuracy : 96.33\n",
            "iteration : 100, loss : 0.1195, accuracy : 96.50\n",
            "iteration : 150, loss : 0.1219, accuracy : 96.45\n",
            "iteration : 200, loss : 0.1240, accuracy : 96.41\n",
            "iteration : 250, loss : 0.1237, accuracy : 96.40\n",
            "iteration : 300, loss : 0.1261, accuracy : 96.34\n",
            "iteration : 350, loss : 0.1277, accuracy : 96.29\n",
            "Epoch :  60, training loss : 0.1286, training accuracy : 96.28, test loss : 0.2373, test accuracy : 93.40\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.1033, accuracy : 96.92\n",
            "iteration : 100, loss : 0.1134, accuracy : 96.68\n",
            "iteration : 150, loss : 0.1205, accuracy : 96.52\n",
            "iteration : 200, loss : 0.1243, accuracy : 96.41\n",
            "iteration : 250, loss : 0.1273, accuracy : 96.30\n",
            "iteration : 300, loss : 0.1258, accuracy : 96.35\n",
            "iteration : 350, loss : 0.1275, accuracy : 96.27\n",
            "Epoch :  61, training loss : 0.1288, training accuracy : 96.25, test loss : 0.2374, test accuracy : 93.49\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.0998, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1132, accuracy : 96.80\n",
            "iteration : 150, loss : 0.1166, accuracy : 96.66\n",
            "iteration : 200, loss : 0.1192, accuracy : 96.51\n",
            "iteration : 250, loss : 0.1206, accuracy : 96.49\n",
            "iteration : 300, loss : 0.1246, accuracy : 96.39\n",
            "iteration : 350, loss : 0.1285, accuracy : 96.29\n",
            "Epoch :  62, training loss : 0.1286, training accuracy : 96.28, test loss : 0.2375, test accuracy : 93.58\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.1072, accuracy : 96.91\n",
            "iteration : 100, loss : 0.1099, accuracy : 96.87\n",
            "iteration : 150, loss : 0.1089, accuracy : 96.81\n",
            "iteration : 200, loss : 0.1149, accuracy : 96.67\n",
            "iteration : 250, loss : 0.1177, accuracy : 96.60\n",
            "iteration : 300, loss : 0.1204, accuracy : 96.53\n",
            "iteration : 350, loss : 0.1220, accuracy : 96.50\n",
            "Epoch :  63, training loss : 0.1229, training accuracy : 96.47, test loss : 0.2297, test accuracy : 93.57\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.1144, accuracy : 96.53\n",
            "iteration : 100, loss : 0.1187, accuracy : 96.44\n",
            "iteration : 150, loss : 0.1231, accuracy : 96.33\n",
            "iteration : 200, loss : 0.1228, accuracy : 96.30\n",
            "iteration : 250, loss : 0.1246, accuracy : 96.27\n",
            "iteration : 300, loss : 0.1237, accuracy : 96.26\n",
            "iteration : 350, loss : 0.1257, accuracy : 96.24\n",
            "Epoch :  64, training loss : 0.1276, training accuracy : 96.19, test loss : 0.2656, test accuracy : 92.60\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.1218, accuracy : 96.41\n",
            "iteration : 100, loss : 0.1211, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1187, accuracy : 96.47\n",
            "iteration : 200, loss : 0.1230, accuracy : 96.39\n",
            "iteration : 250, loss : 0.1259, accuracy : 96.36\n",
            "iteration : 300, loss : 0.1263, accuracy : 96.37\n",
            "iteration : 350, loss : 0.1273, accuracy : 96.32\n",
            "Epoch :  65, training loss : 0.1276, training accuracy : 96.32, test loss : 0.2327, test accuracy : 93.70\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.1204, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1198, accuracy : 96.55\n",
            "iteration : 150, loss : 0.1203, accuracy : 96.57\n",
            "iteration : 200, loss : 0.1196, accuracy : 96.59\n",
            "iteration : 250, loss : 0.1212, accuracy : 96.50\n",
            "iteration : 300, loss : 0.1244, accuracy : 96.38\n",
            "iteration : 350, loss : 0.1247, accuracy : 96.38\n",
            "Epoch :  66, training loss : 0.1241, training accuracy : 96.41, test loss : 0.2347, test accuracy : 93.59\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.1068, accuracy : 96.78\n",
            "iteration : 100, loss : 0.1091, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1094, accuracy : 96.64\n",
            "iteration : 200, loss : 0.1177, accuracy : 96.47\n",
            "iteration : 250, loss : 0.1206, accuracy : 96.42\n",
            "iteration : 300, loss : 0.1213, accuracy : 96.42\n",
            "iteration : 350, loss : 0.1212, accuracy : 96.44\n",
            "Epoch :  67, training loss : 0.1226, training accuracy : 96.40, test loss : 0.2371, test accuracy : 93.65\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.1083, accuracy : 96.72\n",
            "iteration : 100, loss : 0.1072, accuracy : 96.75\n",
            "iteration : 150, loss : 0.1091, accuracy : 96.80\n",
            "iteration : 200, loss : 0.1145, accuracy : 96.66\n",
            "iteration : 250, loss : 0.1191, accuracy : 96.55\n",
            "iteration : 300, loss : 0.1210, accuracy : 96.50\n",
            "iteration : 350, loss : 0.1234, accuracy : 96.42\n",
            "Epoch :  68, training loss : 0.1236, training accuracy : 96.43, test loss : 0.2428, test accuracy : 93.31\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.1173, accuracy : 96.67\n",
            "iteration : 100, loss : 0.1093, accuracy : 96.87\n",
            "iteration : 150, loss : 0.1111, accuracy : 96.83\n",
            "iteration : 200, loss : 0.1132, accuracy : 96.80\n",
            "iteration : 250, loss : 0.1159, accuracy : 96.75\n",
            "iteration : 300, loss : 0.1167, accuracy : 96.71\n",
            "iteration : 350, loss : 0.1168, accuracy : 96.70\n",
            "Epoch :  69, training loss : 0.1168, training accuracy : 96.70, test loss : 0.2379, test accuracy : 93.60\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.1239, accuracy : 96.27\n",
            "iteration : 100, loss : 0.1255, accuracy : 96.34\n",
            "iteration : 150, loss : 0.1261, accuracy : 96.40\n",
            "iteration : 200, loss : 0.1265, accuracy : 96.34\n",
            "iteration : 250, loss : 0.1256, accuracy : 96.34\n",
            "iteration : 300, loss : 0.1245, accuracy : 96.31\n",
            "iteration : 350, loss : 0.1248, accuracy : 96.27\n",
            "Epoch :  70, training loss : 0.1264, training accuracy : 96.21, test loss : 0.2409, test accuracy : 93.53\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.0996, accuracy : 97.05\n",
            "iteration : 100, loss : 0.1100, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1137, accuracy : 96.66\n",
            "iteration : 200, loss : 0.1149, accuracy : 96.61\n",
            "iteration : 250, loss : 0.1142, accuracy : 96.64\n",
            "iteration : 300, loss : 0.1146, accuracy : 96.60\n",
            "iteration : 350, loss : 0.1166, accuracy : 96.54\n",
            "Epoch :  71, training loss : 0.1173, training accuracy : 96.52, test loss : 0.2335, test accuracy : 93.72\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.1188, accuracy : 96.58\n",
            "iteration : 100, loss : 0.1183, accuracy : 96.64\n",
            "iteration : 150, loss : 0.1205, accuracy : 96.51\n",
            "iteration : 200, loss : 0.1173, accuracy : 96.60\n",
            "iteration : 250, loss : 0.1193, accuracy : 96.59\n",
            "iteration : 300, loss : 0.1202, accuracy : 96.54\n",
            "iteration : 350, loss : 0.1198, accuracy : 96.52\n",
            "Epoch :  72, training loss : 0.1202, training accuracy : 96.52, test loss : 0.2394, test accuracy : 93.65\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.1189, accuracy : 96.47\n",
            "iteration : 100, loss : 0.1084, accuracy : 96.86\n",
            "iteration : 150, loss : 0.1109, accuracy : 96.84\n",
            "iteration : 200, loss : 0.1153, accuracy : 96.71\n",
            "iteration : 250, loss : 0.1209, accuracy : 96.53\n",
            "iteration : 300, loss : 0.1203, accuracy : 96.51\n",
            "iteration : 350, loss : 0.1200, accuracy : 96.50\n",
            "Epoch :  73, training loss : 0.1198, training accuracy : 96.52, test loss : 0.2361, test accuracy : 93.70\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.1139, accuracy : 96.97\n",
            "iteration : 100, loss : 0.1165, accuracy : 96.66\n",
            "iteration : 150, loss : 0.1159, accuracy : 96.61\n",
            "iteration : 200, loss : 0.1129, accuracy : 96.73\n",
            "iteration : 250, loss : 0.1154, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1171, accuracy : 96.61\n",
            "iteration : 350, loss : 0.1178, accuracy : 96.59\n",
            "Epoch :  74, training loss : 0.1182, training accuracy : 96.55, test loss : 0.2507, test accuracy : 93.40\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.0973, accuracy : 97.30\n",
            "iteration : 100, loss : 0.1041, accuracy : 97.09\n",
            "iteration : 150, loss : 0.1045, accuracy : 97.05\n",
            "iteration : 200, loss : 0.1065, accuracy : 96.89\n",
            "iteration : 250, loss : 0.1100, accuracy : 96.80\n",
            "iteration : 300, loss : 0.1107, accuracy : 96.79\n",
            "iteration : 350, loss : 0.1124, accuracy : 96.74\n",
            "Epoch :  75, training loss : 0.1141, training accuracy : 96.71, test loss : 0.2448, test accuracy : 93.34\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.1194, accuracy : 96.59\n",
            "iteration : 100, loss : 0.1168, accuracy : 96.62\n",
            "iteration : 150, loss : 0.1182, accuracy : 96.57\n",
            "iteration : 200, loss : 0.1188, accuracy : 96.51\n",
            "iteration : 250, loss : 0.1210, accuracy : 96.48\n",
            "iteration : 300, loss : 0.1227, accuracy : 96.43\n",
            "iteration : 350, loss : 0.1223, accuracy : 96.44\n",
            "Epoch :  76, training loss : 0.1216, training accuracy : 96.47, test loss : 0.2369, test accuracy : 93.70\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.0957, accuracy : 97.23\n",
            "iteration : 100, loss : 0.0987, accuracy : 97.12\n",
            "iteration : 150, loss : 0.1050, accuracy : 97.04\n",
            "iteration : 200, loss : 0.1080, accuracy : 96.89\n",
            "iteration : 250, loss : 0.1087, accuracy : 96.87\n",
            "iteration : 300, loss : 0.1116, accuracy : 96.77\n",
            "iteration : 350, loss : 0.1123, accuracy : 96.71\n",
            "Epoch :  77, training loss : 0.1135, training accuracy : 96.68, test loss : 0.2366, test accuracy : 93.68\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.1051, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1036, accuracy : 96.84\n",
            "iteration : 150, loss : 0.1051, accuracy : 96.83\n",
            "iteration : 200, loss : 0.1090, accuracy : 96.77\n",
            "iteration : 250, loss : 0.1156, accuracy : 96.59\n",
            "iteration : 300, loss : 0.1179, accuracy : 96.53\n",
            "iteration : 350, loss : 0.1172, accuracy : 96.56\n",
            "Epoch :  78, training loss : 0.1176, training accuracy : 96.55, test loss : 0.2328, test accuracy : 93.80\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.1053, accuracy : 96.97\n",
            "iteration : 100, loss : 0.1110, accuracy : 96.80\n",
            "iteration : 150, loss : 0.1144, accuracy : 96.69\n",
            "iteration : 200, loss : 0.1141, accuracy : 96.73\n",
            "iteration : 250, loss : 0.1160, accuracy : 96.64\n",
            "iteration : 300, loss : 0.1159, accuracy : 96.65\n",
            "iteration : 350, loss : 0.1154, accuracy : 96.69\n",
            "Epoch :  79, training loss : 0.1169, training accuracy : 96.64, test loss : 0.2526, test accuracy : 93.23\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.0987, accuracy : 97.08\n",
            "iteration : 100, loss : 0.1038, accuracy : 97.00\n",
            "iteration : 150, loss : 0.1089, accuracy : 96.85\n",
            "iteration : 200, loss : 0.1131, accuracy : 96.77\n",
            "iteration : 250, loss : 0.1120, accuracy : 96.79\n",
            "iteration : 300, loss : 0.1116, accuracy : 96.81\n",
            "iteration : 350, loss : 0.1147, accuracy : 96.69\n",
            "Epoch :  80, training loss : 0.1153, training accuracy : 96.67, test loss : 0.2294, test accuracy : 93.79\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.1065, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1125, accuracy : 96.66\n",
            "iteration : 150, loss : 0.1123, accuracy : 96.68\n",
            "iteration : 200, loss : 0.1122, accuracy : 96.68\n",
            "iteration : 250, loss : 0.1125, accuracy : 96.71\n",
            "iteration : 300, loss : 0.1137, accuracy : 96.64\n",
            "iteration : 350, loss : 0.1143, accuracy : 96.64\n",
            "Epoch :  81, training loss : 0.1139, training accuracy : 96.67, test loss : 0.2379, test accuracy : 93.73\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.1087, accuracy : 96.67\n",
            "iteration : 100, loss : 0.1048, accuracy : 96.80\n",
            "iteration : 150, loss : 0.1092, accuracy : 96.70\n",
            "iteration : 200, loss : 0.1075, accuracy : 96.75\n",
            "iteration : 250, loss : 0.1100, accuracy : 96.74\n",
            "iteration : 300, loss : 0.1100, accuracy : 96.71\n",
            "iteration : 350, loss : 0.1102, accuracy : 96.70\n",
            "Epoch :  82, training loss : 0.1113, training accuracy : 96.68, test loss : 0.2414, test accuracy : 93.53\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.0929, accuracy : 97.45\n",
            "iteration : 100, loss : 0.1026, accuracy : 97.08\n",
            "iteration : 150, loss : 0.1004, accuracy : 97.07\n",
            "iteration : 200, loss : 0.1013, accuracy : 97.04\n",
            "iteration : 250, loss : 0.1027, accuracy : 97.02\n",
            "iteration : 300, loss : 0.1054, accuracy : 96.96\n",
            "iteration : 350, loss : 0.1084, accuracy : 96.81\n",
            "Epoch :  83, training loss : 0.1084, training accuracy : 96.81, test loss : 0.2282, test accuracy : 93.94\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.1111, accuracy : 96.77\n",
            "iteration : 100, loss : 0.1104, accuracy : 96.79\n",
            "iteration : 150, loss : 0.1054, accuracy : 96.98\n",
            "iteration : 200, loss : 0.1123, accuracy : 96.79\n",
            "iteration : 250, loss : 0.1122, accuracy : 96.76\n",
            "iteration : 300, loss : 0.1129, accuracy : 96.73\n",
            "iteration : 350, loss : 0.1132, accuracy : 96.73\n",
            "Epoch :  84, training loss : 0.1122, training accuracy : 96.75, test loss : 0.2306, test accuracy : 93.81\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.0903, accuracy : 97.30\n",
            "iteration : 100, loss : 0.1004, accuracy : 97.06\n",
            "iteration : 150, loss : 0.0975, accuracy : 97.07\n",
            "iteration : 200, loss : 0.0993, accuracy : 97.07\n",
            "iteration : 250, loss : 0.1040, accuracy : 96.95\n",
            "iteration : 300, loss : 0.1065, accuracy : 96.86\n",
            "iteration : 350, loss : 0.1079, accuracy : 96.81\n",
            "Epoch :  85, training loss : 0.1090, training accuracy : 96.79, test loss : 0.2391, test accuracy : 93.67\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.1058, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1092, accuracy : 96.77\n",
            "iteration : 150, loss : 0.1055, accuracy : 96.90\n",
            "iteration : 200, loss : 0.1064, accuracy : 96.86\n",
            "iteration : 250, loss : 0.1089, accuracy : 96.77\n",
            "iteration : 300, loss : 0.1098, accuracy : 96.77\n",
            "iteration : 350, loss : 0.1101, accuracy : 96.77\n",
            "Epoch :  86, training loss : 0.1105, training accuracy : 96.76, test loss : 0.2326, test accuracy : 93.82\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.1046, accuracy : 96.73\n",
            "iteration : 100, loss : 0.1030, accuracy : 96.95\n",
            "iteration : 150, loss : 0.1047, accuracy : 96.94\n",
            "iteration : 200, loss : 0.1084, accuracy : 96.88\n",
            "iteration : 250, loss : 0.1062, accuracy : 96.91\n",
            "iteration : 300, loss : 0.1088, accuracy : 96.87\n",
            "iteration : 350, loss : 0.1101, accuracy : 96.83\n",
            "Epoch :  87, training loss : 0.1101, training accuracy : 96.86, test loss : 0.2537, test accuracy : 93.22\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.1079, accuracy : 96.77\n",
            "iteration : 100, loss : 0.0977, accuracy : 97.04\n",
            "iteration : 150, loss : 0.0999, accuracy : 97.02\n",
            "iteration : 200, loss : 0.1052, accuracy : 96.83\n",
            "iteration : 250, loss : 0.1092, accuracy : 96.74\n",
            "iteration : 300, loss : 0.1098, accuracy : 96.75\n",
            "iteration : 350, loss : 0.1101, accuracy : 96.75\n",
            "Epoch :  88, training loss : 0.1104, training accuracy : 96.75, test loss : 0.2510, test accuracy : 93.32\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.0958, accuracy : 97.06\n",
            "iteration : 100, loss : 0.1036, accuracy : 96.83\n",
            "iteration : 150, loss : 0.1048, accuracy : 96.84\n",
            "iteration : 200, loss : 0.1057, accuracy : 96.86\n",
            "iteration : 250, loss : 0.1039, accuracy : 96.91\n",
            "iteration : 300, loss : 0.1063, accuracy : 96.83\n",
            "iteration : 350, loss : 0.1056, accuracy : 96.85\n",
            "Epoch :  89, training loss : 0.1066, training accuracy : 96.80, test loss : 0.2410, test accuracy : 93.64\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.1016, accuracy : 96.94\n",
            "iteration : 100, loss : 0.1008, accuracy : 97.06\n",
            "iteration : 150, loss : 0.1057, accuracy : 96.88\n",
            "iteration : 200, loss : 0.1040, accuracy : 96.91\n",
            "iteration : 250, loss : 0.1046, accuracy : 96.90\n",
            "iteration : 300, loss : 0.1078, accuracy : 96.79\n",
            "iteration : 350, loss : 0.1075, accuracy : 96.80\n",
            "Epoch :  90, training loss : 0.1073, training accuracy : 96.82, test loss : 0.2343, test accuracy : 93.76\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.0935, accuracy : 97.23\n",
            "iteration : 100, loss : 0.0998, accuracy : 97.06\n",
            "iteration : 150, loss : 0.0999, accuracy : 97.08\n",
            "iteration : 200, loss : 0.1006, accuracy : 97.05\n",
            "iteration : 250, loss : 0.1030, accuracy : 97.02\n",
            "iteration : 300, loss : 0.1042, accuracy : 96.97\n",
            "iteration : 350, loss : 0.1059, accuracy : 96.91\n",
            "Epoch :  91, training loss : 0.1064, training accuracy : 96.90, test loss : 0.2497, test accuracy : 93.34\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.1002, accuracy : 97.00\n",
            "iteration : 100, loss : 0.1018, accuracy : 96.91\n",
            "iteration : 150, loss : 0.0997, accuracy : 96.99\n",
            "iteration : 200, loss : 0.0983, accuracy : 97.08\n",
            "iteration : 250, loss : 0.0990, accuracy : 97.07\n",
            "iteration : 300, loss : 0.1020, accuracy : 96.97\n",
            "iteration : 350, loss : 0.1045, accuracy : 96.90\n",
            "Epoch :  92, training loss : 0.1053, training accuracy : 96.87, test loss : 0.2255, test accuracy : 93.93\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.0887, accuracy : 97.50\n",
            "iteration : 100, loss : 0.0882, accuracy : 97.50\n",
            "iteration : 150, loss : 0.0900, accuracy : 97.43\n",
            "iteration : 200, loss : 0.0945, accuracy : 97.25\n",
            "iteration : 250, loss : 0.0983, accuracy : 97.15\n",
            "iteration : 300, loss : 0.1007, accuracy : 97.07\n",
            "iteration : 350, loss : 0.1023, accuracy : 97.01\n",
            "Epoch :  93, training loss : 0.1032, training accuracy : 97.00, test loss : 0.2557, test accuracy : 93.02\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.0879, accuracy : 97.44\n",
            "iteration : 100, loss : 0.0963, accuracy : 97.20\n",
            "iteration : 150, loss : 0.0971, accuracy : 97.17\n",
            "iteration : 200, loss : 0.0964, accuracy : 97.16\n",
            "iteration : 250, loss : 0.1009, accuracy : 97.01\n",
            "iteration : 300, loss : 0.1045, accuracy : 96.94\n",
            "iteration : 350, loss : 0.1059, accuracy : 96.91\n",
            "Epoch :  94, training loss : 0.1049, training accuracy : 96.94, test loss : 0.2659, test accuracy : 92.99\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.1104, accuracy : 96.66\n",
            "iteration : 100, loss : 0.1025, accuracy : 97.06\n",
            "iteration : 150, loss : 0.1013, accuracy : 97.12\n",
            "iteration : 200, loss : 0.1048, accuracy : 97.02\n",
            "iteration : 250, loss : 0.1050, accuracy : 96.95\n",
            "iteration : 300, loss : 0.1047, accuracy : 96.91\n",
            "iteration : 350, loss : 0.1064, accuracy : 96.87\n",
            "Epoch :  95, training loss : 0.1070, training accuracy : 96.84, test loss : 0.2277, test accuracy : 93.98\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.0957, accuracy : 97.31\n",
            "iteration : 100, loss : 0.0916, accuracy : 97.41\n",
            "iteration : 150, loss : 0.0947, accuracy : 97.30\n",
            "iteration : 200, loss : 0.0953, accuracy : 97.28\n",
            "iteration : 250, loss : 0.0957, accuracy : 97.30\n",
            "iteration : 300, loss : 0.0951, accuracy : 97.30\n",
            "iteration : 350, loss : 0.0980, accuracy : 97.19\n",
            "Epoch :  96, training loss : 0.1002, training accuracy : 97.12, test loss : 0.2438, test accuracy : 93.38\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.0967, accuracy : 97.09\n",
            "iteration : 100, loss : 0.0939, accuracy : 97.21\n",
            "iteration : 150, loss : 0.0971, accuracy : 97.08\n",
            "iteration : 200, loss : 0.0996, accuracy : 96.97\n",
            "iteration : 250, loss : 0.1018, accuracy : 96.92\n",
            "iteration : 300, loss : 0.1042, accuracy : 96.85\n",
            "iteration : 350, loss : 0.1042, accuracy : 96.85\n",
            "Epoch :  97, training loss : 0.1044, training accuracy : 96.84, test loss : 0.2371, test accuracy : 93.92\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.0881, accuracy : 97.39\n",
            "iteration : 100, loss : 0.0894, accuracy : 97.26\n",
            "iteration : 150, loss : 0.0906, accuracy : 97.26\n",
            "iteration : 200, loss : 0.0973, accuracy : 97.10\n",
            "iteration : 250, loss : 0.1015, accuracy : 96.98\n",
            "iteration : 300, loss : 0.1024, accuracy : 96.99\n",
            "iteration : 350, loss : 0.1030, accuracy : 96.92\n",
            "Epoch :  98, training loss : 0.1033, training accuracy : 96.92, test loss : 0.2436, test accuracy : 93.76\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.0965, accuracy : 97.39\n",
            "iteration : 100, loss : 0.0916, accuracy : 97.43\n",
            "iteration : 150, loss : 0.0978, accuracy : 97.29\n",
            "iteration : 200, loss : 0.0985, accuracy : 97.23\n",
            "iteration : 250, loss : 0.0997, accuracy : 97.16\n",
            "iteration : 300, loss : 0.1005, accuracy : 97.11\n",
            "iteration : 350, loss : 0.1013, accuracy : 97.04\n",
            "Epoch :  99, training loss : 0.1025, training accuracy : 97.02, test loss : 0.2400, test accuracy : 93.75\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.0834, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0872, accuracy : 97.51\n",
            "iteration : 150, loss : 0.0937, accuracy : 97.30\n",
            "iteration : 200, loss : 0.0961, accuracy : 97.25\n",
            "iteration : 250, loss : 0.0960, accuracy : 97.26\n",
            "iteration : 300, loss : 0.0981, accuracy : 97.17\n",
            "iteration : 350, loss : 0.0998, accuracy : 97.10\n",
            "Epoch : 100, training loss : 0.1001, training accuracy : 97.10, test loss : 0.2302, test accuracy : 93.92\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.0868, accuracy : 97.42\n",
            "iteration : 100, loss : 0.0938, accuracy : 97.27\n",
            "iteration : 150, loss : 0.1003, accuracy : 97.05\n",
            "iteration : 200, loss : 0.0979, accuracy : 97.09\n",
            "iteration : 250, loss : 0.0971, accuracy : 97.16\n",
            "iteration : 300, loss : 0.0972, accuracy : 97.12\n",
            "iteration : 350, loss : 0.0972, accuracy : 97.11\n",
            "Epoch : 101, training loss : 0.0978, training accuracy : 97.10, test loss : 0.2313, test accuracy : 94.02\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.0994, accuracy : 97.11\n",
            "iteration : 100, loss : 0.0938, accuracy : 97.24\n",
            "iteration : 150, loss : 0.0968, accuracy : 97.14\n",
            "iteration : 200, loss : 0.0970, accuracy : 97.12\n",
            "iteration : 250, loss : 0.0976, accuracy : 97.13\n",
            "iteration : 300, loss : 0.0964, accuracy : 97.17\n",
            "iteration : 350, loss : 0.0983, accuracy : 97.11\n",
            "Epoch : 102, training loss : 0.0990, training accuracy : 97.10, test loss : 0.2325, test accuracy : 93.90\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.0881, accuracy : 97.58\n",
            "iteration : 100, loss : 0.0921, accuracy : 97.39\n",
            "iteration : 150, loss : 0.0899, accuracy : 97.43\n",
            "iteration : 200, loss : 0.0935, accuracy : 97.32\n",
            "iteration : 250, loss : 0.0964, accuracy : 97.26\n",
            "iteration : 300, loss : 0.0985, accuracy : 97.18\n",
            "iteration : 350, loss : 0.0996, accuracy : 97.15\n",
            "Epoch : 103, training loss : 0.0996, training accuracy : 97.13, test loss : 0.2471, test accuracy : 93.57\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.0920, accuracy : 97.17\n",
            "iteration : 100, loss : 0.0929, accuracy : 97.19\n",
            "iteration : 150, loss : 0.0933, accuracy : 97.18\n",
            "iteration : 200, loss : 0.0913, accuracy : 97.20\n",
            "iteration : 250, loss : 0.0924, accuracy : 97.18\n",
            "iteration : 300, loss : 0.0935, accuracy : 97.14\n",
            "iteration : 350, loss : 0.0957, accuracy : 97.08\n",
            "Epoch : 104, training loss : 0.0966, training accuracy : 97.07, test loss : 0.2455, test accuracy : 93.57\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.0862, accuracy : 97.28\n",
            "iteration : 100, loss : 0.0966, accuracy : 97.20\n",
            "iteration : 150, loss : 0.0982, accuracy : 97.15\n",
            "iteration : 200, loss : 0.0985, accuracy : 97.20\n",
            "iteration : 250, loss : 0.0968, accuracy : 97.21\n",
            "iteration : 300, loss : 0.0977, accuracy : 97.16\n",
            "iteration : 350, loss : 0.0980, accuracy : 97.17\n",
            "Epoch : 105, training loss : 0.0982, training accuracy : 97.18, test loss : 0.2385, test accuracy : 93.87\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.0827, accuracy : 97.61\n",
            "iteration : 100, loss : 0.0868, accuracy : 97.44\n",
            "iteration : 150, loss : 0.0891, accuracy : 97.35\n",
            "iteration : 200, loss : 0.0946, accuracy : 97.16\n",
            "iteration : 250, loss : 0.0965, accuracy : 97.11\n",
            "iteration : 300, loss : 0.0956, accuracy : 97.18\n",
            "iteration : 350, loss : 0.0958, accuracy : 97.21\n",
            "Epoch : 106, training loss : 0.0958, training accuracy : 97.20, test loss : 0.2348, test accuracy : 93.90\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.0815, accuracy : 97.56\n",
            "iteration : 100, loss : 0.0874, accuracy : 97.29\n",
            "iteration : 150, loss : 0.0883, accuracy : 97.34\n",
            "iteration : 200, loss : 0.0913, accuracy : 97.23\n",
            "iteration : 250, loss : 0.0945, accuracy : 97.11\n",
            "iteration : 300, loss : 0.0958, accuracy : 97.10\n",
            "iteration : 350, loss : 0.0978, accuracy : 97.05\n",
            "Epoch : 107, training loss : 0.0983, training accuracy : 97.03, test loss : 0.2396, test accuracy : 93.59\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.0906, accuracy : 97.38\n",
            "iteration : 100, loss : 0.0856, accuracy : 97.42\n",
            "iteration : 150, loss : 0.0891, accuracy : 97.35\n",
            "iteration : 200, loss : 0.0941, accuracy : 97.21\n",
            "iteration : 250, loss : 0.0970, accuracy : 97.09\n",
            "iteration : 300, loss : 0.1002, accuracy : 97.02\n",
            "iteration : 350, loss : 0.1003, accuracy : 97.03\n",
            "Epoch : 108, training loss : 0.1001, training accuracy : 97.02, test loss : 0.2364, test accuracy : 93.77\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.0867, accuracy : 97.50\n",
            "iteration : 100, loss : 0.0875, accuracy : 97.35\n",
            "iteration : 150, loss : 0.0855, accuracy : 97.39\n",
            "iteration : 200, loss : 0.0918, accuracy : 97.23\n",
            "iteration : 250, loss : 0.0930, accuracy : 97.20\n",
            "iteration : 300, loss : 0.0944, accuracy : 97.18\n",
            "iteration : 350, loss : 0.0945, accuracy : 97.15\n",
            "Epoch : 109, training loss : 0.0948, training accuracy : 97.14, test loss : 0.2366, test accuracy : 93.67\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.0774, accuracy : 97.70\n",
            "iteration : 100, loss : 0.0831, accuracy : 97.62\n",
            "iteration : 150, loss : 0.0850, accuracy : 97.53\n",
            "iteration : 200, loss : 0.0885, accuracy : 97.39\n",
            "iteration : 250, loss : 0.0931, accuracy : 97.28\n",
            "iteration : 300, loss : 0.0945, accuracy : 97.23\n",
            "iteration : 350, loss : 0.0952, accuracy : 97.20\n",
            "Epoch : 110, training loss : 0.0960, training accuracy : 97.19, test loss : 0.2502, test accuracy : 93.46\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.0947, accuracy : 97.38\n",
            "iteration : 100, loss : 0.0892, accuracy : 97.40\n",
            "iteration : 150, loss : 0.0873, accuracy : 97.49\n",
            "iteration : 200, loss : 0.0870, accuracy : 97.48\n",
            "iteration : 250, loss : 0.0890, accuracy : 97.42\n",
            "iteration : 300, loss : 0.0905, accuracy : 97.37\n",
            "iteration : 350, loss : 0.0917, accuracy : 97.36\n",
            "Epoch : 111, training loss : 0.0937, training accuracy : 97.28, test loss : 0.2523, test accuracy : 93.46\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.0838, accuracy : 97.64\n",
            "iteration : 100, loss : 0.0836, accuracy : 97.54\n",
            "iteration : 150, loss : 0.0848, accuracy : 97.53\n",
            "iteration : 200, loss : 0.0895, accuracy : 97.38\n",
            "iteration : 250, loss : 0.0917, accuracy : 97.32\n",
            "iteration : 300, loss : 0.0950, accuracy : 97.21\n",
            "iteration : 350, loss : 0.0950, accuracy : 97.20\n",
            "Epoch : 112, training loss : 0.0952, training accuracy : 97.20, test loss : 0.2339, test accuracy : 93.86\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.0829, accuracy : 97.53\n",
            "iteration : 100, loss : 0.0842, accuracy : 97.53\n",
            "iteration : 150, loss : 0.0854, accuracy : 97.50\n",
            "iteration : 200, loss : 0.0859, accuracy : 97.48\n",
            "iteration : 250, loss : 0.0886, accuracy : 97.39\n",
            "iteration : 300, loss : 0.0892, accuracy : 97.35\n",
            "iteration : 350, loss : 0.0909, accuracy : 97.31\n",
            "Epoch : 113, training loss : 0.0916, training accuracy : 97.29, test loss : 0.2236, test accuracy : 94.15\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.0828, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0854, accuracy : 97.51\n",
            "iteration : 150, loss : 0.0857, accuracy : 97.49\n",
            "iteration : 200, loss : 0.0858, accuracy : 97.45\n",
            "iteration : 250, loss : 0.0887, accuracy : 97.38\n",
            "iteration : 300, loss : 0.0889, accuracy : 97.36\n",
            "iteration : 350, loss : 0.0891, accuracy : 97.38\n",
            "Epoch : 114, training loss : 0.0890, training accuracy : 97.36, test loss : 0.2412, test accuracy : 93.92\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0843, accuracy : 97.47\n",
            "iteration : 100, loss : 0.0827, accuracy : 97.54\n",
            "iteration : 150, loss : 0.0845, accuracy : 97.50\n",
            "iteration : 200, loss : 0.0867, accuracy : 97.44\n",
            "iteration : 250, loss : 0.0905, accuracy : 97.34\n",
            "iteration : 300, loss : 0.0908, accuracy : 97.31\n",
            "iteration : 350, loss : 0.0936, accuracy : 97.24\n",
            "Epoch : 115, training loss : 0.0937, training accuracy : 97.24, test loss : 0.2379, test accuracy : 93.80\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0836, accuracy : 97.58\n",
            "iteration : 100, loss : 0.0943, accuracy : 97.41\n",
            "iteration : 150, loss : 0.0914, accuracy : 97.38\n",
            "iteration : 200, loss : 0.0893, accuracy : 97.44\n",
            "iteration : 250, loss : 0.0892, accuracy : 97.44\n",
            "iteration : 300, loss : 0.0896, accuracy : 97.40\n",
            "iteration : 350, loss : 0.0915, accuracy : 97.32\n",
            "Epoch : 116, training loss : 0.0913, training accuracy : 97.30, test loss : 0.2589, test accuracy : 93.35\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.0820, accuracy : 97.75\n",
            "iteration : 100, loss : 0.0763, accuracy : 97.80\n",
            "iteration : 150, loss : 0.0832, accuracy : 97.52\n",
            "iteration : 200, loss : 0.0853, accuracy : 97.47\n",
            "iteration : 250, loss : 0.0873, accuracy : 97.41\n",
            "iteration : 300, loss : 0.0889, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0887, accuracy : 97.40\n",
            "Epoch : 117, training loss : 0.0886, training accuracy : 97.41, test loss : 0.2408, test accuracy : 93.74\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.0814, accuracy : 97.61\n",
            "iteration : 100, loss : 0.0818, accuracy : 97.62\n",
            "iteration : 150, loss : 0.0832, accuracy : 97.66\n",
            "iteration : 200, loss : 0.0852, accuracy : 97.59\n",
            "iteration : 250, loss : 0.0863, accuracy : 97.55\n",
            "iteration : 300, loss : 0.0858, accuracy : 97.54\n",
            "iteration : 350, loss : 0.0848, accuracy : 97.59\n",
            "Epoch : 118, training loss : 0.0852, training accuracy : 97.58, test loss : 0.2473, test accuracy : 93.72\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.0703, accuracy : 97.95\n",
            "iteration : 100, loss : 0.0743, accuracy : 97.75\n",
            "iteration : 150, loss : 0.0768, accuracy : 97.71\n",
            "iteration : 200, loss : 0.0835, accuracy : 97.51\n",
            "iteration : 250, loss : 0.0844, accuracy : 97.47\n",
            "iteration : 300, loss : 0.0883, accuracy : 97.35\n",
            "iteration : 350, loss : 0.0883, accuracy : 97.35\n",
            "Epoch : 119, training loss : 0.0900, training accuracy : 97.33, test loss : 0.2456, test accuracy : 93.64\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.0763, accuracy : 97.73\n",
            "iteration : 100, loss : 0.0767, accuracy : 97.73\n",
            "iteration : 150, loss : 0.0803, accuracy : 97.65\n",
            "iteration : 200, loss : 0.0809, accuracy : 97.61\n",
            "iteration : 250, loss : 0.0839, accuracy : 97.51\n",
            "iteration : 300, loss : 0.0864, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0896, accuracy : 97.30\n",
            "Epoch : 120, training loss : 0.0904, training accuracy : 97.28, test loss : 0.2359, test accuracy : 93.83\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.0804, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0831, accuracy : 97.55\n",
            "iteration : 150, loss : 0.0851, accuracy : 97.48\n",
            "iteration : 200, loss : 0.0827, accuracy : 97.59\n",
            "iteration : 250, loss : 0.0843, accuracy : 97.51\n",
            "iteration : 300, loss : 0.0853, accuracy : 97.48\n",
            "iteration : 350, loss : 0.0878, accuracy : 97.43\n",
            "Epoch : 121, training loss : 0.0888, training accuracy : 97.41, test loss : 0.2338, test accuracy : 93.98\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.0786, accuracy : 97.62\n",
            "iteration : 100, loss : 0.0800, accuracy : 97.63\n",
            "iteration : 150, loss : 0.0810, accuracy : 97.60\n",
            "iteration : 200, loss : 0.0809, accuracy : 97.66\n",
            "iteration : 250, loss : 0.0832, accuracy : 97.58\n",
            "iteration : 300, loss : 0.0847, accuracy : 97.50\n",
            "iteration : 350, loss : 0.0864, accuracy : 97.42\n",
            "Epoch : 122, training loss : 0.0874, training accuracy : 97.40, test loss : 0.2499, test accuracy : 93.44\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.0802, accuracy : 97.61\n",
            "iteration : 100, loss : 0.0844, accuracy : 97.48\n",
            "iteration : 150, loss : 0.0885, accuracy : 97.33\n",
            "iteration : 200, loss : 0.0876, accuracy : 97.39\n",
            "iteration : 250, loss : 0.0865, accuracy : 97.38\n",
            "iteration : 300, loss : 0.0872, accuracy : 97.37\n",
            "iteration : 350, loss : 0.0870, accuracy : 97.35\n",
            "Epoch : 123, training loss : 0.0871, training accuracy : 97.34, test loss : 0.2427, test accuracy : 93.81\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.0757, accuracy : 97.77\n",
            "iteration : 100, loss : 0.0806, accuracy : 97.56\n",
            "iteration : 150, loss : 0.0844, accuracy : 97.42\n",
            "iteration : 200, loss : 0.0853, accuracy : 97.44\n",
            "iteration : 250, loss : 0.0857, accuracy : 97.47\n",
            "iteration : 300, loss : 0.0873, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0864, accuracy : 97.42\n",
            "Epoch : 124, training loss : 0.0861, training accuracy : 97.44, test loss : 0.2322, test accuracy : 94.04\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.0725, accuracy : 98.09\n",
            "iteration : 100, loss : 0.0749, accuracy : 97.88\n",
            "iteration : 150, loss : 0.0766, accuracy : 97.76\n",
            "iteration : 200, loss : 0.0779, accuracy : 97.71\n",
            "iteration : 250, loss : 0.0794, accuracy : 97.66\n",
            "iteration : 300, loss : 0.0814, accuracy : 97.61\n",
            "iteration : 350, loss : 0.0829, accuracy : 97.60\n",
            "Epoch : 125, training loss : 0.0835, training accuracy : 97.56, test loss : 0.2426, test accuracy : 93.68\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.0768, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0713, accuracy : 97.80\n",
            "iteration : 150, loss : 0.0762, accuracy : 97.64\n",
            "iteration : 200, loss : 0.0811, accuracy : 97.55\n",
            "iteration : 250, loss : 0.0828, accuracy : 97.54\n",
            "iteration : 300, loss : 0.0825, accuracy : 97.57\n",
            "iteration : 350, loss : 0.0841, accuracy : 97.52\n",
            "Epoch : 126, training loss : 0.0841, training accuracy : 97.51, test loss : 0.2510, test accuracy : 93.46\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0814, accuracy : 97.47\n",
            "iteration : 100, loss : 0.0894, accuracy : 97.25\n",
            "iteration : 150, loss : 0.0874, accuracy : 97.30\n",
            "iteration : 200, loss : 0.0857, accuracy : 97.35\n",
            "iteration : 250, loss : 0.0833, accuracy : 97.46\n",
            "iteration : 300, loss : 0.0826, accuracy : 97.49\n",
            "iteration : 350, loss : 0.0842, accuracy : 97.45\n",
            "Epoch : 127, training loss : 0.0851, training accuracy : 97.42, test loss : 0.2643, test accuracy : 93.50\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0810, accuracy : 97.53\n",
            "iteration : 100, loss : 0.0838, accuracy : 97.48\n",
            "iteration : 150, loss : 0.0834, accuracy : 97.49\n",
            "iteration : 200, loss : 0.0844, accuracy : 97.51\n",
            "iteration : 250, loss : 0.0817, accuracy : 97.60\n",
            "iteration : 300, loss : 0.0831, accuracy : 97.54\n",
            "iteration : 350, loss : 0.0848, accuracy : 97.48\n",
            "Epoch : 128, training loss : 0.0850, training accuracy : 97.47, test loss : 0.2361, test accuracy : 94.06\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.0839, accuracy : 97.41\n",
            "iteration : 100, loss : 0.0781, accuracy : 97.67\n",
            "iteration : 150, loss : 0.0795, accuracy : 97.66\n",
            "iteration : 200, loss : 0.0769, accuracy : 97.72\n",
            "iteration : 250, loss : 0.0765, accuracy : 97.69\n",
            "iteration : 300, loss : 0.0762, accuracy : 97.68\n",
            "iteration : 350, loss : 0.0791, accuracy : 97.58\n",
            "Epoch : 129, training loss : 0.0797, training accuracy : 97.59, test loss : 0.2427, test accuracy : 93.65\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.0692, accuracy : 97.98\n",
            "iteration : 100, loss : 0.0732, accuracy : 97.86\n",
            "iteration : 150, loss : 0.0768, accuracy : 97.73\n",
            "iteration : 200, loss : 0.0798, accuracy : 97.69\n",
            "iteration : 250, loss : 0.0786, accuracy : 97.72\n",
            "iteration : 300, loss : 0.0802, accuracy : 97.67\n",
            "iteration : 350, loss : 0.0832, accuracy : 97.59\n",
            "Epoch : 130, training loss : 0.0833, training accuracy : 97.58, test loss : 0.2402, test accuracy : 93.67\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.0781, accuracy : 97.84\n",
            "iteration : 100, loss : 0.0781, accuracy : 97.82\n",
            "iteration : 150, loss : 0.0778, accuracy : 97.79\n",
            "iteration : 200, loss : 0.0771, accuracy : 97.80\n",
            "iteration : 250, loss : 0.0779, accuracy : 97.76\n",
            "iteration : 300, loss : 0.0785, accuracy : 97.73\n",
            "iteration : 350, loss : 0.0792, accuracy : 97.67\n",
            "Epoch : 131, training loss : 0.0794, training accuracy : 97.67, test loss : 0.2393, test accuracy : 93.93\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.0753, accuracy : 97.75\n",
            "iteration : 100, loss : 0.0756, accuracy : 97.65\n",
            "iteration : 150, loss : 0.0780, accuracy : 97.59\n",
            "iteration : 200, loss : 0.0807, accuracy : 97.55\n",
            "iteration : 250, loss : 0.0820, accuracy : 97.52\n",
            "iteration : 300, loss : 0.0822, accuracy : 97.54\n",
            "iteration : 350, loss : 0.0817, accuracy : 97.53\n",
            "Epoch : 132, training loss : 0.0828, training accuracy : 97.50, test loss : 0.2476, test accuracy : 94.00\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.0719, accuracy : 97.86\n",
            "iteration : 100, loss : 0.0759, accuracy : 97.77\n",
            "iteration : 150, loss : 0.0768, accuracy : 97.71\n",
            "iteration : 200, loss : 0.0778, accuracy : 97.67\n",
            "iteration : 250, loss : 0.0807, accuracy : 97.56\n",
            "iteration : 300, loss : 0.0812, accuracy : 97.57\n",
            "iteration : 350, loss : 0.0821, accuracy : 97.55\n",
            "Epoch : 133, training loss : 0.0820, training accuracy : 97.54, test loss : 0.2386, test accuracy : 93.92\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0795, accuracy : 97.42\n",
            "iteration : 100, loss : 0.0736, accuracy : 97.69\n",
            "iteration : 150, loss : 0.0750, accuracy : 97.62\n",
            "iteration : 200, loss : 0.0739, accuracy : 97.68\n",
            "iteration : 250, loss : 0.0743, accuracy : 97.68\n",
            "iteration : 300, loss : 0.0753, accuracy : 97.68\n",
            "iteration : 350, loss : 0.0775, accuracy : 97.65\n",
            "Epoch : 134, training loss : 0.0783, training accuracy : 97.62, test loss : 0.2454, test accuracy : 93.85\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.0622, accuracy : 98.12\n",
            "iteration : 100, loss : 0.0669, accuracy : 98.09\n",
            "iteration : 150, loss : 0.0728, accuracy : 97.82\n",
            "iteration : 200, loss : 0.0770, accuracy : 97.68\n",
            "iteration : 250, loss : 0.0777, accuracy : 97.66\n",
            "iteration : 300, loss : 0.0776, accuracy : 97.66\n",
            "iteration : 350, loss : 0.0786, accuracy : 97.64\n",
            "Epoch : 135, training loss : 0.0791, training accuracy : 97.63, test loss : 0.2482, test accuracy : 93.80\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0763, accuracy : 97.55\n",
            "iteration : 100, loss : 0.0740, accuracy : 97.66\n",
            "iteration : 150, loss : 0.0707, accuracy : 97.82\n",
            "iteration : 200, loss : 0.0701, accuracy : 97.93\n",
            "iteration : 250, loss : 0.0740, accuracy : 97.81\n",
            "iteration : 300, loss : 0.0757, accuracy : 97.75\n",
            "iteration : 350, loss : 0.0767, accuracy : 97.73\n",
            "Epoch : 136, training loss : 0.0768, training accuracy : 97.73, test loss : 0.2520, test accuracy : 93.80\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0698, accuracy : 98.05\n",
            "iteration : 100, loss : 0.0669, accuracy : 98.04\n",
            "iteration : 150, loss : 0.0708, accuracy : 97.93\n",
            "iteration : 200, loss : 0.0722, accuracy : 97.87\n",
            "iteration : 250, loss : 0.0750, accuracy : 97.76\n",
            "iteration : 300, loss : 0.0780, accuracy : 97.65\n",
            "iteration : 350, loss : 0.0782, accuracy : 97.66\n",
            "Epoch : 137, training loss : 0.0781, training accuracy : 97.66, test loss : 0.2407, test accuracy : 93.93\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.0709, accuracy : 98.02\n",
            "iteration : 100, loss : 0.0748, accuracy : 97.84\n",
            "iteration : 150, loss : 0.0741, accuracy : 97.81\n",
            "iteration : 200, loss : 0.0740, accuracy : 97.83\n",
            "iteration : 250, loss : 0.0757, accuracy : 97.78\n",
            "iteration : 300, loss : 0.0771, accuracy : 97.74\n",
            "iteration : 350, loss : 0.0781, accuracy : 97.70\n",
            "Epoch : 138, training loss : 0.0782, training accuracy : 97.70, test loss : 0.2433, test accuracy : 93.57\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0679, accuracy : 98.08\n",
            "iteration : 100, loss : 0.0705, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0750, accuracy : 97.84\n",
            "iteration : 200, loss : 0.0736, accuracy : 97.84\n",
            "iteration : 250, loss : 0.0746, accuracy : 97.83\n",
            "iteration : 300, loss : 0.0753, accuracy : 97.79\n",
            "iteration : 350, loss : 0.0762, accuracy : 97.75\n",
            "Epoch : 139, training loss : 0.0760, training accuracy : 97.77, test loss : 0.2436, test accuracy : 93.85\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.0852, accuracy : 97.41\n",
            "iteration : 100, loss : 0.0783, accuracy : 97.67\n",
            "iteration : 150, loss : 0.0762, accuracy : 97.79\n",
            "iteration : 200, loss : 0.0777, accuracy : 97.79\n",
            "iteration : 250, loss : 0.0782, accuracy : 97.78\n",
            "iteration : 300, loss : 0.0775, accuracy : 97.77\n",
            "iteration : 350, loss : 0.0772, accuracy : 97.76\n",
            "Epoch : 140, training loss : 0.0765, training accuracy : 97.77, test loss : 0.2373, test accuracy : 94.08\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0613, accuracy : 98.16\n",
            "iteration : 100, loss : 0.0667, accuracy : 97.94\n",
            "iteration : 150, loss : 0.0690, accuracy : 97.83\n",
            "iteration : 200, loss : 0.0732, accuracy : 97.75\n",
            "iteration : 250, loss : 0.0729, accuracy : 97.79\n",
            "iteration : 300, loss : 0.0744, accuracy : 97.71\n",
            "iteration : 350, loss : 0.0756, accuracy : 97.69\n",
            "Epoch : 141, training loss : 0.0760, training accuracy : 97.69, test loss : 0.2504, test accuracy : 93.68\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0717, accuracy : 97.88\n",
            "iteration : 100, loss : 0.0706, accuracy : 97.91\n",
            "iteration : 150, loss : 0.0681, accuracy : 97.99\n",
            "iteration : 200, loss : 0.0706, accuracy : 97.91\n",
            "iteration : 250, loss : 0.0698, accuracy : 97.92\n",
            "iteration : 300, loss : 0.0714, accuracy : 97.85\n",
            "iteration : 350, loss : 0.0726, accuracy : 97.82\n",
            "Epoch : 142, training loss : 0.0730, training accuracy : 97.81, test loss : 0.2472, test accuracy : 93.83\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0680, accuracy : 97.88\n",
            "iteration : 100, loss : 0.0688, accuracy : 97.95\n",
            "iteration : 150, loss : 0.0703, accuracy : 97.95\n",
            "iteration : 200, loss : 0.0696, accuracy : 97.95\n",
            "iteration : 250, loss : 0.0716, accuracy : 97.88\n",
            "iteration : 300, loss : 0.0727, accuracy : 97.83\n",
            "iteration : 350, loss : 0.0723, accuracy : 97.87\n",
            "Epoch : 143, training loss : 0.0722, training accuracy : 97.87, test loss : 0.2410, test accuracy : 94.20\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0670, accuracy : 98.09\n",
            "iteration : 100, loss : 0.0677, accuracy : 98.02\n",
            "iteration : 150, loss : 0.0689, accuracy : 97.92\n",
            "iteration : 200, loss : 0.0711, accuracy : 97.84\n",
            "iteration : 250, loss : 0.0726, accuracy : 97.83\n",
            "iteration : 300, loss : 0.0757, accuracy : 97.72\n",
            "iteration : 350, loss : 0.0754, accuracy : 97.74\n",
            "Epoch : 144, training loss : 0.0755, training accuracy : 97.74, test loss : 0.2313, test accuracy : 94.24\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0669, accuracy : 98.16\n",
            "iteration : 100, loss : 0.0688, accuracy : 97.98\n",
            "iteration : 150, loss : 0.0675, accuracy : 97.98\n",
            "iteration : 200, loss : 0.0710, accuracy : 97.87\n",
            "iteration : 250, loss : 0.0700, accuracy : 97.91\n",
            "iteration : 300, loss : 0.0712, accuracy : 97.85\n",
            "iteration : 350, loss : 0.0707, accuracy : 97.87\n",
            "Epoch : 145, training loss : 0.0708, training accuracy : 97.86, test loss : 0.2410, test accuracy : 93.88\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0587, accuracy : 98.12\n",
            "iteration : 100, loss : 0.0683, accuracy : 97.90\n",
            "iteration : 150, loss : 0.0680, accuracy : 97.90\n",
            "iteration : 200, loss : 0.0664, accuracy : 97.96\n",
            "iteration : 250, loss : 0.0698, accuracy : 97.89\n",
            "iteration : 300, loss : 0.0698, accuracy : 97.90\n",
            "iteration : 350, loss : 0.0702, accuracy : 97.90\n",
            "Epoch : 146, training loss : 0.0705, training accuracy : 97.89, test loss : 0.2391, test accuracy : 94.00\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0634, accuracy : 98.08\n",
            "iteration : 100, loss : 0.0663, accuracy : 98.04\n",
            "iteration : 150, loss : 0.0670, accuracy : 98.06\n",
            "iteration : 200, loss : 0.0685, accuracy : 97.97\n",
            "iteration : 250, loss : 0.0693, accuracy : 97.92\n",
            "iteration : 300, loss : 0.0703, accuracy : 97.89\n",
            "iteration : 350, loss : 0.0720, accuracy : 97.83\n",
            "Epoch : 147, training loss : 0.0711, training accuracy : 97.85, test loss : 0.2396, test accuracy : 94.03\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0683, accuracy : 98.08\n",
            "iteration : 100, loss : 0.0674, accuracy : 98.08\n",
            "iteration : 150, loss : 0.0654, accuracy : 98.10\n",
            "iteration : 200, loss : 0.0683, accuracy : 98.04\n",
            "iteration : 250, loss : 0.0691, accuracy : 98.02\n",
            "iteration : 300, loss : 0.0689, accuracy : 98.01\n",
            "iteration : 350, loss : 0.0699, accuracy : 97.98\n",
            "Epoch : 148, training loss : 0.0703, training accuracy : 97.95, test loss : 0.2407, test accuracy : 94.09\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0691, accuracy : 97.98\n",
            "iteration : 100, loss : 0.0696, accuracy : 97.94\n",
            "iteration : 150, loss : 0.0723, accuracy : 97.87\n",
            "iteration : 200, loss : 0.0749, accuracy : 97.77\n",
            "iteration : 250, loss : 0.0751, accuracy : 97.73\n",
            "iteration : 300, loss : 0.0740, accuracy : 97.75\n",
            "iteration : 350, loss : 0.0728, accuracy : 97.79\n",
            "Epoch : 149, training loss : 0.0730, training accuracy : 97.79, test loss : 0.2401, test accuracy : 93.93\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0696, accuracy : 97.92\n",
            "iteration : 100, loss : 0.0679, accuracy : 97.97\n",
            "iteration : 150, loss : 0.0676, accuracy : 98.02\n",
            "iteration : 200, loss : 0.0689, accuracy : 97.93\n",
            "iteration : 250, loss : 0.0672, accuracy : 97.97\n",
            "iteration : 300, loss : 0.0674, accuracy : 97.98\n",
            "iteration : 350, loss : 0.0676, accuracy : 97.97\n",
            "Epoch : 150, training loss : 0.0675, training accuracy : 97.98, test loss : 0.2437, test accuracy : 93.88\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0510, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0537, accuracy : 98.34\n",
            "iteration : 150, loss : 0.0575, accuracy : 98.27\n",
            "iteration : 200, loss : 0.0599, accuracy : 98.16\n",
            "iteration : 250, loss : 0.0624, accuracy : 98.08\n",
            "iteration : 300, loss : 0.0653, accuracy : 98.00\n",
            "iteration : 350, loss : 0.0660, accuracy : 97.97\n",
            "Epoch : 151, training loss : 0.0666, training accuracy : 97.95, test loss : 0.2446, test accuracy : 93.84\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0631, accuracy : 97.95\n",
            "iteration : 100, loss : 0.0640, accuracy : 98.06\n",
            "iteration : 150, loss : 0.0658, accuracy : 98.03\n",
            "iteration : 200, loss : 0.0652, accuracy : 98.07\n",
            "iteration : 250, loss : 0.0667, accuracy : 98.02\n",
            "iteration : 300, loss : 0.0668, accuracy : 98.02\n",
            "iteration : 350, loss : 0.0670, accuracy : 98.02\n",
            "Epoch : 152, training loss : 0.0663, training accuracy : 98.03, test loss : 0.2543, test accuracy : 94.11\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0652, accuracy : 98.09\n",
            "iteration : 100, loss : 0.0635, accuracy : 98.11\n",
            "iteration : 150, loss : 0.0635, accuracy : 98.14\n",
            "iteration : 200, loss : 0.0634, accuracy : 98.14\n",
            "iteration : 250, loss : 0.0622, accuracy : 98.17\n",
            "iteration : 300, loss : 0.0631, accuracy : 98.17\n",
            "iteration : 350, loss : 0.0646, accuracy : 98.11\n",
            "Epoch : 153, training loss : 0.0651, training accuracy : 98.10, test loss : 0.2477, test accuracy : 94.03\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0647, accuracy : 98.05\n",
            "iteration : 100, loss : 0.0624, accuracy : 98.14\n",
            "iteration : 150, loss : 0.0612, accuracy : 98.20\n",
            "iteration : 200, loss : 0.0636, accuracy : 98.09\n",
            "iteration : 250, loss : 0.0631, accuracy : 98.10\n",
            "iteration : 300, loss : 0.0648, accuracy : 98.05\n",
            "iteration : 350, loss : 0.0649, accuracy : 98.04\n",
            "Epoch : 154, training loss : 0.0646, training accuracy : 98.05, test loss : 0.2446, test accuracy : 94.20\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0643, accuracy : 97.91\n",
            "iteration : 100, loss : 0.0638, accuracy : 98.01\n",
            "iteration : 150, loss : 0.0623, accuracy : 98.09\n",
            "iteration : 200, loss : 0.0630, accuracy : 98.08\n",
            "iteration : 250, loss : 0.0650, accuracy : 98.01\n",
            "iteration : 300, loss : 0.0678, accuracy : 97.93\n",
            "iteration : 350, loss : 0.0685, accuracy : 97.92\n",
            "Epoch : 155, training loss : 0.0686, training accuracy : 97.91, test loss : 0.2333, test accuracy : 94.16\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0643, accuracy : 98.11\n",
            "iteration : 100, loss : 0.0626, accuracy : 98.11\n",
            "iteration : 150, loss : 0.0621, accuracy : 98.14\n",
            "iteration : 200, loss : 0.0601, accuracy : 98.16\n",
            "iteration : 250, loss : 0.0617, accuracy : 98.13\n",
            "iteration : 300, loss : 0.0620, accuracy : 98.13\n",
            "iteration : 350, loss : 0.0637, accuracy : 98.08\n",
            "Epoch : 156, training loss : 0.0636, training accuracy : 98.08, test loss : 0.2446, test accuracy : 94.06\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0678, accuracy : 97.97\n",
            "iteration : 100, loss : 0.0637, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0630, accuracy : 98.08\n",
            "iteration : 200, loss : 0.0624, accuracy : 98.11\n",
            "iteration : 250, loss : 0.0627, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0642, accuracy : 98.01\n",
            "iteration : 350, loss : 0.0660, accuracy : 97.94\n",
            "Epoch : 157, training loss : 0.0658, training accuracy : 97.95, test loss : 0.2408, test accuracy : 94.12\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0561, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0622, accuracy : 98.21\n",
            "iteration : 150, loss : 0.0614, accuracy : 98.19\n",
            "iteration : 200, loss : 0.0614, accuracy : 98.20\n",
            "iteration : 250, loss : 0.0611, accuracy : 98.23\n",
            "iteration : 300, loss : 0.0615, accuracy : 98.20\n",
            "iteration : 350, loss : 0.0616, accuracy : 98.20\n",
            "Epoch : 158, training loss : 0.0614, training accuracy : 98.21, test loss : 0.2533, test accuracy : 93.89\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0520, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0550, accuracy : 98.27\n",
            "iteration : 150, loss : 0.0596, accuracy : 98.12\n",
            "iteration : 200, loss : 0.0626, accuracy : 98.05\n",
            "iteration : 250, loss : 0.0607, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0597, accuracy : 98.15\n",
            "iteration : 350, loss : 0.0603, accuracy : 98.15\n",
            "Epoch : 159, training loss : 0.0611, training accuracy : 98.12, test loss : 0.2727, test accuracy : 93.18\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0567, accuracy : 98.52\n",
            "iteration : 100, loss : 0.0609, accuracy : 98.30\n",
            "iteration : 150, loss : 0.0586, accuracy : 98.33\n",
            "iteration : 200, loss : 0.0589, accuracy : 98.30\n",
            "iteration : 250, loss : 0.0593, accuracy : 98.25\n",
            "iteration : 300, loss : 0.0600, accuracy : 98.22\n",
            "iteration : 350, loss : 0.0594, accuracy : 98.23\n",
            "Epoch : 160, training loss : 0.0590, training accuracy : 98.24, test loss : 0.2566, test accuracy : 93.98\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0609, accuracy : 98.28\n",
            "iteration : 100, loss : 0.0628, accuracy : 98.12\n",
            "iteration : 150, loss : 0.0620, accuracy : 98.18\n",
            "iteration : 200, loss : 0.0622, accuracy : 98.13\n",
            "iteration : 250, loss : 0.0625, accuracy : 98.14\n",
            "iteration : 300, loss : 0.0634, accuracy : 98.12\n",
            "iteration : 350, loss : 0.0641, accuracy : 98.09\n",
            "Epoch : 161, training loss : 0.0644, training accuracy : 98.09, test loss : 0.2470, test accuracy : 94.03\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0573, accuracy : 98.25\n",
            "iteration : 100, loss : 0.0594, accuracy : 98.19\n",
            "iteration : 150, loss : 0.0567, accuracy : 98.26\n",
            "iteration : 200, loss : 0.0614, accuracy : 98.11\n",
            "iteration : 250, loss : 0.0619, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0610, accuracy : 98.12\n",
            "iteration : 350, loss : 0.0624, accuracy : 98.08\n",
            "Epoch : 162, training loss : 0.0626, training accuracy : 98.07, test loss : 0.2541, test accuracy : 93.77\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0525, accuracy : 98.56\n",
            "iteration : 100, loss : 0.0546, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0541, accuracy : 98.43\n",
            "iteration : 200, loss : 0.0535, accuracy : 98.43\n",
            "iteration : 250, loss : 0.0535, accuracy : 98.43\n",
            "iteration : 300, loss : 0.0548, accuracy : 98.38\n",
            "iteration : 350, loss : 0.0556, accuracy : 98.36\n",
            "Epoch : 163, training loss : 0.0562, training accuracy : 98.35, test loss : 0.2573, test accuracy : 93.75\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0513, accuracy : 98.56\n",
            "iteration : 100, loss : 0.0541, accuracy : 98.46\n",
            "iteration : 150, loss : 0.0544, accuracy : 98.41\n",
            "iteration : 200, loss : 0.0566, accuracy : 98.35\n",
            "iteration : 250, loss : 0.0571, accuracy : 98.31\n",
            "iteration : 300, loss : 0.0576, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0597, accuracy : 98.22\n",
            "Epoch : 164, training loss : 0.0606, training accuracy : 98.19, test loss : 0.2492, test accuracy : 94.13\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0569, accuracy : 98.30\n",
            "iteration : 100, loss : 0.0548, accuracy : 98.30\n",
            "iteration : 150, loss : 0.0559, accuracy : 98.27\n",
            "iteration : 200, loss : 0.0557, accuracy : 98.29\n",
            "iteration : 250, loss : 0.0560, accuracy : 98.31\n",
            "iteration : 300, loss : 0.0562, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0563, accuracy : 98.30\n",
            "Epoch : 165, training loss : 0.0574, training accuracy : 98.27, test loss : 0.2446, test accuracy : 93.90\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0487, accuracy : 98.52\n",
            "iteration : 100, loss : 0.0519, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0537, accuracy : 98.44\n",
            "iteration : 200, loss : 0.0561, accuracy : 98.37\n",
            "iteration : 250, loss : 0.0569, accuracy : 98.30\n",
            "iteration : 300, loss : 0.0558, accuracy : 98.33\n",
            "iteration : 350, loss : 0.0556, accuracy : 98.31\n",
            "Epoch : 166, training loss : 0.0558, training accuracy : 98.31, test loss : 0.2400, test accuracy : 94.27\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0539, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0519, accuracy : 98.37\n",
            "iteration : 150, loss : 0.0506, accuracy : 98.41\n",
            "iteration : 200, loss : 0.0528, accuracy : 98.36\n",
            "iteration : 250, loss : 0.0540, accuracy : 98.33\n",
            "iteration : 300, loss : 0.0554, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0564, accuracy : 98.25\n",
            "Epoch : 167, training loss : 0.0566, training accuracy : 98.25, test loss : 0.2393, test accuracy : 94.16\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0507, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0557, accuracy : 98.28\n",
            "iteration : 150, loss : 0.0543, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0541, accuracy : 98.32\n",
            "iteration : 250, loss : 0.0536, accuracy : 98.35\n",
            "iteration : 300, loss : 0.0546, accuracy : 98.34\n",
            "iteration : 350, loss : 0.0552, accuracy : 98.31\n",
            "Epoch : 168, training loss : 0.0555, training accuracy : 98.29, test loss : 0.2425, test accuracy : 94.01\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0502, accuracy : 98.47\n",
            "iteration : 100, loss : 0.0532, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0554, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0547, accuracy : 98.38\n",
            "iteration : 250, loss : 0.0549, accuracy : 98.39\n",
            "iteration : 300, loss : 0.0567, accuracy : 98.31\n",
            "iteration : 350, loss : 0.0581, accuracy : 98.28\n",
            "Epoch : 169, training loss : 0.0582, training accuracy : 98.26, test loss : 0.2467, test accuracy : 94.11\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0468, accuracy : 98.44\n",
            "iteration : 100, loss : 0.0449, accuracy : 98.55\n",
            "iteration : 150, loss : 0.0475, accuracy : 98.49\n",
            "iteration : 200, loss : 0.0493, accuracy : 98.45\n",
            "iteration : 250, loss : 0.0520, accuracy : 98.42\n",
            "iteration : 300, loss : 0.0541, accuracy : 98.35\n",
            "iteration : 350, loss : 0.0547, accuracy : 98.33\n",
            "Epoch : 170, training loss : 0.0544, training accuracy : 98.35, test loss : 0.2441, test accuracy : 94.14\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0433, accuracy : 98.47\n",
            "iteration : 100, loss : 0.0449, accuracy : 98.52\n",
            "iteration : 150, loss : 0.0497, accuracy : 98.36\n",
            "iteration : 200, loss : 0.0519, accuracy : 98.35\n",
            "iteration : 250, loss : 0.0541, accuracy : 98.29\n",
            "iteration : 300, loss : 0.0549, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0553, accuracy : 98.29\n",
            "Epoch : 171, training loss : 0.0563, training accuracy : 98.28, test loss : 0.2471, test accuracy : 94.01\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0643, accuracy : 98.25\n",
            "iteration : 100, loss : 0.0551, accuracy : 98.40\n",
            "iteration : 150, loss : 0.0548, accuracy : 98.41\n",
            "iteration : 200, loss : 0.0541, accuracy : 98.40\n",
            "iteration : 250, loss : 0.0567, accuracy : 98.35\n",
            "iteration : 300, loss : 0.0561, accuracy : 98.36\n",
            "iteration : 350, loss : 0.0553, accuracy : 98.38\n",
            "Epoch : 172, training loss : 0.0548, training accuracy : 98.39, test loss : 0.2451, test accuracy : 94.26\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0587, accuracy : 98.27\n",
            "iteration : 100, loss : 0.0525, accuracy : 98.46\n",
            "iteration : 150, loss : 0.0534, accuracy : 98.40\n",
            "iteration : 200, loss : 0.0528, accuracy : 98.42\n",
            "iteration : 250, loss : 0.0522, accuracy : 98.46\n",
            "iteration : 300, loss : 0.0529, accuracy : 98.45\n",
            "iteration : 350, loss : 0.0544, accuracy : 98.41\n",
            "Epoch : 173, training loss : 0.0553, training accuracy : 98.38, test loss : 0.2447, test accuracy : 94.18\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0483, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0508, accuracy : 98.46\n",
            "iteration : 150, loss : 0.0521, accuracy : 98.47\n",
            "iteration : 200, loss : 0.0516, accuracy : 98.51\n",
            "iteration : 250, loss : 0.0511, accuracy : 98.53\n",
            "iteration : 300, loss : 0.0515, accuracy : 98.53\n",
            "iteration : 350, loss : 0.0526, accuracy : 98.49\n",
            "Epoch : 174, training loss : 0.0529, training accuracy : 98.48, test loss : 0.2468, test accuracy : 94.13\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0453, accuracy : 98.55\n",
            "iteration : 100, loss : 0.0499, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0470, accuracy : 98.57\n",
            "iteration : 200, loss : 0.0490, accuracy : 98.50\n",
            "iteration : 250, loss : 0.0502, accuracy : 98.48\n",
            "iteration : 300, loss : 0.0522, accuracy : 98.44\n",
            "iteration : 350, loss : 0.0533, accuracy : 98.40\n",
            "Epoch : 175, training loss : 0.0535, training accuracy : 98.39, test loss : 0.2419, test accuracy : 94.25\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0562, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0536, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0507, accuracy : 98.45\n",
            "iteration : 200, loss : 0.0511, accuracy : 98.43\n",
            "iteration : 250, loss : 0.0516, accuracy : 98.43\n",
            "iteration : 300, loss : 0.0515, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0522, accuracy : 98.40\n",
            "Epoch : 176, training loss : 0.0523, training accuracy : 98.41, test loss : 0.2395, test accuracy : 94.22\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0461, accuracy : 98.70\n",
            "iteration : 100, loss : 0.0476, accuracy : 98.58\n",
            "iteration : 150, loss : 0.0481, accuracy : 98.58\n",
            "iteration : 200, loss : 0.0486, accuracy : 98.59\n",
            "iteration : 250, loss : 0.0493, accuracy : 98.57\n",
            "iteration : 300, loss : 0.0506, accuracy : 98.53\n",
            "iteration : 350, loss : 0.0518, accuracy : 98.50\n",
            "Epoch : 177, training loss : 0.0523, training accuracy : 98.48, test loss : 0.2508, test accuracy : 94.08\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0465, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0455, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0469, accuracy : 98.57\n",
            "iteration : 200, loss : 0.0454, accuracy : 98.58\n",
            "iteration : 250, loss : 0.0452, accuracy : 98.58\n",
            "iteration : 300, loss : 0.0458, accuracy : 98.57\n",
            "iteration : 350, loss : 0.0473, accuracy : 98.52\n",
            "Epoch : 178, training loss : 0.0481, training accuracy : 98.50, test loss : 0.2495, test accuracy : 94.11\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0477, accuracy : 98.48\n",
            "iteration : 100, loss : 0.0487, accuracy : 98.49\n",
            "iteration : 150, loss : 0.0475, accuracy : 98.54\n",
            "iteration : 200, loss : 0.0476, accuracy : 98.55\n",
            "iteration : 250, loss : 0.0472, accuracy : 98.55\n",
            "iteration : 300, loss : 0.0474, accuracy : 98.56\n",
            "iteration : 350, loss : 0.0484, accuracy : 98.54\n",
            "Epoch : 179, training loss : 0.0488, training accuracy : 98.54, test loss : 0.2440, test accuracy : 94.31\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0415, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0467, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0477, accuracy : 98.58\n",
            "iteration : 200, loss : 0.0481, accuracy : 98.55\n",
            "iteration : 250, loss : 0.0495, accuracy : 98.48\n",
            "iteration : 300, loss : 0.0513, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0510, accuracy : 98.44\n",
            "Epoch : 180, training loss : 0.0509, training accuracy : 98.44, test loss : 0.2387, test accuracy : 94.27\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0475, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0476, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0481, accuracy : 98.54\n",
            "iteration : 200, loss : 0.0473, accuracy : 98.59\n",
            "iteration : 250, loss : 0.0497, accuracy : 98.51\n",
            "iteration : 300, loss : 0.0495, accuracy : 98.51\n",
            "iteration : 350, loss : 0.0488, accuracy : 98.55\n",
            "Epoch : 181, training loss : 0.0486, training accuracy : 98.56, test loss : 0.2394, test accuracy : 94.34\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0453, accuracy : 98.56\n",
            "iteration : 100, loss : 0.0465, accuracy : 98.62\n",
            "iteration : 150, loss : 0.0428, accuracy : 98.76\n",
            "iteration : 200, loss : 0.0444, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0460, accuracy : 98.60\n",
            "iteration : 300, loss : 0.0468, accuracy : 98.56\n",
            "iteration : 350, loss : 0.0475, accuracy : 98.53\n",
            "Epoch : 182, training loss : 0.0475, training accuracy : 98.54, test loss : 0.2437, test accuracy : 94.38\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0446, accuracy : 98.62\n",
            "iteration : 100, loss : 0.0418, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0419, accuracy : 98.71\n",
            "iteration : 200, loss : 0.0454, accuracy : 98.61\n",
            "iteration : 250, loss : 0.0455, accuracy : 98.60\n",
            "iteration : 300, loss : 0.0459, accuracy : 98.57\n",
            "iteration : 350, loss : 0.0457, accuracy : 98.57\n",
            "Epoch : 183, training loss : 0.0457, training accuracy : 98.57, test loss : 0.2481, test accuracy : 94.14\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0514, accuracy : 98.28\n",
            "iteration : 100, loss : 0.0506, accuracy : 98.39\n",
            "iteration : 150, loss : 0.0504, accuracy : 98.46\n",
            "iteration : 200, loss : 0.0484, accuracy : 98.52\n",
            "iteration : 250, loss : 0.0480, accuracy : 98.56\n",
            "iteration : 300, loss : 0.0489, accuracy : 98.52\n",
            "iteration : 350, loss : 0.0496, accuracy : 98.50\n",
            "Epoch : 184, training loss : 0.0502, training accuracy : 98.48, test loss : 0.2532, test accuracy : 93.93\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0455, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0455, accuracy : 98.70\n",
            "iteration : 150, loss : 0.0459, accuracy : 98.68\n",
            "iteration : 200, loss : 0.0467, accuracy : 98.62\n",
            "iteration : 250, loss : 0.0470, accuracy : 98.59\n",
            "iteration : 300, loss : 0.0467, accuracy : 98.62\n",
            "iteration : 350, loss : 0.0488, accuracy : 98.56\n",
            "Epoch : 185, training loss : 0.0491, training accuracy : 98.55, test loss : 0.2562, test accuracy : 93.98\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0487, accuracy : 98.44\n",
            "iteration : 100, loss : 0.0472, accuracy : 98.49\n",
            "iteration : 150, loss : 0.0464, accuracy : 98.55\n",
            "iteration : 200, loss : 0.0467, accuracy : 98.53\n",
            "iteration : 250, loss : 0.0460, accuracy : 98.55\n",
            "iteration : 300, loss : 0.0460, accuracy : 98.57\n",
            "iteration : 350, loss : 0.0468, accuracy : 98.56\n",
            "Epoch : 186, training loss : 0.0467, training accuracy : 98.55, test loss : 0.2330, test accuracy : 94.43\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0463, accuracy : 98.56\n",
            "iteration : 100, loss : 0.0465, accuracy : 98.61\n",
            "iteration : 150, loss : 0.0473, accuracy : 98.56\n",
            "iteration : 200, loss : 0.0465, accuracy : 98.61\n",
            "iteration : 250, loss : 0.0467, accuracy : 98.61\n",
            "iteration : 300, loss : 0.0485, accuracy : 98.53\n",
            "iteration : 350, loss : 0.0482, accuracy : 98.53\n",
            "Epoch : 187, training loss : 0.0483, training accuracy : 98.53, test loss : 0.2456, test accuracy : 94.09\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0398, accuracy : 98.83\n",
            "iteration : 100, loss : 0.0411, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0408, accuracy : 98.79\n",
            "iteration : 200, loss : 0.0403, accuracy : 98.79\n",
            "iteration : 250, loss : 0.0420, accuracy : 98.76\n",
            "iteration : 300, loss : 0.0433, accuracy : 98.74\n",
            "iteration : 350, loss : 0.0449, accuracy : 98.67\n",
            "Epoch : 188, training loss : 0.0456, training accuracy : 98.65, test loss : 0.2421, test accuracy : 94.37\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0415, accuracy : 98.66\n",
            "iteration : 100, loss : 0.0391, accuracy : 98.75\n",
            "iteration : 150, loss : 0.0395, accuracy : 98.79\n",
            "iteration : 200, loss : 0.0389, accuracy : 98.80\n",
            "iteration : 250, loss : 0.0400, accuracy : 98.77\n",
            "iteration : 300, loss : 0.0397, accuracy : 98.78\n",
            "iteration : 350, loss : 0.0398, accuracy : 98.78\n",
            "Epoch : 189, training loss : 0.0400, training accuracy : 98.80, test loss : 0.2466, test accuracy : 94.15\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0468, accuracy : 98.59\n",
            "iteration : 100, loss : 0.0426, accuracy : 98.67\n",
            "iteration : 150, loss : 0.0396, accuracy : 98.80\n",
            "iteration : 200, loss : 0.0399, accuracy : 98.76\n",
            "iteration : 250, loss : 0.0409, accuracy : 98.74\n",
            "iteration : 300, loss : 0.0424, accuracy : 98.69\n",
            "iteration : 350, loss : 0.0435, accuracy : 98.68\n",
            "Epoch : 190, training loss : 0.0433, training accuracy : 98.69, test loss : 0.2402, test accuracy : 94.19\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0406, accuracy : 98.78\n",
            "iteration : 100, loss : 0.0428, accuracy : 98.72\n",
            "iteration : 150, loss : 0.0449, accuracy : 98.68\n",
            "iteration : 200, loss : 0.0429, accuracy : 98.75\n",
            "iteration : 250, loss : 0.0428, accuracy : 98.72\n",
            "iteration : 300, loss : 0.0426, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0421, accuracy : 98.75\n",
            "Epoch : 191, training loss : 0.0422, training accuracy : 98.75, test loss : 0.2458, test accuracy : 94.07\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0401, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0382, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0397, accuracy : 98.82\n",
            "iteration : 200, loss : 0.0407, accuracy : 98.77\n",
            "iteration : 250, loss : 0.0394, accuracy : 98.83\n",
            "iteration : 300, loss : 0.0420, accuracy : 98.76\n",
            "iteration : 350, loss : 0.0427, accuracy : 98.72\n",
            "Epoch : 192, training loss : 0.0425, training accuracy : 98.73, test loss : 0.2535, test accuracy : 94.14\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0445, accuracy : 98.62\n",
            "iteration : 100, loss : 0.0430, accuracy : 98.68\n",
            "iteration : 150, loss : 0.0437, accuracy : 98.73\n",
            "iteration : 200, loss : 0.0411, accuracy : 98.81\n",
            "iteration : 250, loss : 0.0409, accuracy : 98.80\n",
            "iteration : 300, loss : 0.0411, accuracy : 98.78\n",
            "iteration : 350, loss : 0.0408, accuracy : 98.78\n",
            "Epoch : 193, training loss : 0.0406, training accuracy : 98.79, test loss : 0.2396, test accuracy : 94.41\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0427, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0418, accuracy : 98.71\n",
            "iteration : 150, loss : 0.0433, accuracy : 98.66\n",
            "iteration : 200, loss : 0.0431, accuracy : 98.67\n",
            "iteration : 250, loss : 0.0439, accuracy : 98.66\n",
            "iteration : 300, loss : 0.0451, accuracy : 98.65\n",
            "iteration : 350, loss : 0.0460, accuracy : 98.62\n",
            "Epoch : 194, training loss : 0.0463, training accuracy : 98.63, test loss : 0.2404, test accuracy : 94.21\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0350, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0354, accuracy : 98.91\n",
            "iteration : 150, loss : 0.0366, accuracy : 98.93\n",
            "iteration : 200, loss : 0.0380, accuracy : 98.87\n",
            "iteration : 250, loss : 0.0387, accuracy : 98.86\n",
            "iteration : 300, loss : 0.0383, accuracy : 98.86\n",
            "iteration : 350, loss : 0.0387, accuracy : 98.86\n",
            "Epoch : 195, training loss : 0.0384, training accuracy : 98.86, test loss : 0.2366, test accuracy : 94.43\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0328, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0366, accuracy : 99.00\n",
            "iteration : 150, loss : 0.0395, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0414, accuracy : 98.76\n",
            "iteration : 250, loss : 0.0417, accuracy : 98.75\n",
            "iteration : 300, loss : 0.0427, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0433, accuracy : 98.73\n",
            "Epoch : 196, training loss : 0.0430, training accuracy : 98.74, test loss : 0.2370, test accuracy : 94.24\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0364, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0344, accuracy : 98.99\n",
            "iteration : 150, loss : 0.0356, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0372, accuracy : 98.88\n",
            "iteration : 250, loss : 0.0397, accuracy : 98.79\n",
            "iteration : 300, loss : 0.0412, accuracy : 98.78\n",
            "iteration : 350, loss : 0.0420, accuracy : 98.74\n",
            "Epoch : 197, training loss : 0.0420, training accuracy : 98.74, test loss : 0.2357, test accuracy : 94.45\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0378, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0388, accuracy : 98.89\n",
            "iteration : 150, loss : 0.0391, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0382, accuracy : 98.93\n",
            "iteration : 250, loss : 0.0368, accuracy : 98.99\n",
            "iteration : 300, loss : 0.0381, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0391, accuracy : 98.91\n",
            "Epoch : 198, training loss : 0.0398, training accuracy : 98.88, test loss : 0.2613, test accuracy : 93.75\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0345, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0358, accuracy : 99.00\n",
            "iteration : 150, loss : 0.0355, accuracy : 98.98\n",
            "iteration : 200, loss : 0.0349, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0346, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0360, accuracy : 98.98\n",
            "iteration : 350, loss : 0.0373, accuracy : 98.94\n",
            "Epoch : 199, training loss : 0.0376, training accuracy : 98.93, test loss : 0.2469, test accuracy : 94.25\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0364, accuracy : 98.81\n",
            "iteration : 100, loss : 0.0385, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0409, accuracy : 98.73\n",
            "iteration : 200, loss : 0.0406, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0416, accuracy : 98.71\n",
            "iteration : 300, loss : 0.0410, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0415, accuracy : 98.74\n",
            "Epoch : 200, training loss : 0.0411, training accuracy : 98.75, test loss : 0.2470, test accuracy : 94.35\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0332, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0331, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0325, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0331, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0359, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0362, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0367, accuracy : 98.93\n",
            "Epoch : 201, training loss : 0.0368, training accuracy : 98.94, test loss : 0.2493, test accuracy : 94.18\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0368, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0364, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0359, accuracy : 98.91\n",
            "iteration : 200, loss : 0.0374, accuracy : 98.89\n",
            "iteration : 250, loss : 0.0386, accuracy : 98.86\n",
            "iteration : 300, loss : 0.0383, accuracy : 98.88\n",
            "iteration : 350, loss : 0.0379, accuracy : 98.90\n",
            "Epoch : 202, training loss : 0.0377, training accuracy : 98.91, test loss : 0.2531, test accuracy : 94.05\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0366, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0357, accuracy : 98.97\n",
            "iteration : 150, loss : 0.0386, accuracy : 98.87\n",
            "iteration : 200, loss : 0.0385, accuracy : 98.87\n",
            "iteration : 250, loss : 0.0390, accuracy : 98.86\n",
            "iteration : 300, loss : 0.0394, accuracy : 98.87\n",
            "iteration : 350, loss : 0.0391, accuracy : 98.87\n",
            "Epoch : 203, training loss : 0.0392, training accuracy : 98.86, test loss : 0.2422, test accuracy : 94.41\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0316, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0312, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0314, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0328, accuracy : 99.00\n",
            "iteration : 250, loss : 0.0340, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0349, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0355, accuracy : 98.94\n",
            "Epoch : 204, training loss : 0.0356, training accuracy : 98.95, test loss : 0.2507, test accuracy : 94.32\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0310, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0335, accuracy : 98.99\n",
            "iteration : 150, loss : 0.0338, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0344, accuracy : 98.95\n",
            "iteration : 250, loss : 0.0340, accuracy : 98.97\n",
            "iteration : 300, loss : 0.0340, accuracy : 98.96\n",
            "iteration : 350, loss : 0.0353, accuracy : 98.93\n",
            "Epoch : 205, training loss : 0.0354, training accuracy : 98.94, test loss : 0.2484, test accuracy : 94.29\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0310, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0318, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0349, accuracy : 98.98\n",
            "iteration : 200, loss : 0.0344, accuracy : 98.95\n",
            "iteration : 250, loss : 0.0349, accuracy : 98.92\n",
            "iteration : 300, loss : 0.0340, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0347, accuracy : 98.94\n",
            "Epoch : 206, training loss : 0.0346, training accuracy : 98.95, test loss : 0.2427, test accuracy : 94.42\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0313, accuracy : 99.06\n",
            "iteration : 100, loss : 0.0313, accuracy : 99.11\n",
            "iteration : 150, loss : 0.0323, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0326, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0330, accuracy : 99.04\n",
            "iteration : 300, loss : 0.0334, accuracy : 99.01\n",
            "iteration : 350, loss : 0.0342, accuracy : 98.97\n",
            "Epoch : 207, training loss : 0.0341, training accuracy : 98.97, test loss : 0.2484, test accuracy : 94.17\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0317, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0346, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0338, accuracy : 99.12\n",
            "iteration : 200, loss : 0.0345, accuracy : 99.07\n",
            "iteration : 250, loss : 0.0338, accuracy : 99.07\n",
            "iteration : 300, loss : 0.0338, accuracy : 99.06\n",
            "iteration : 350, loss : 0.0338, accuracy : 99.06\n",
            "Epoch : 208, training loss : 0.0341, training accuracy : 99.06, test loss : 0.2448, test accuracy : 94.48\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0273, accuracy : 99.08\n",
            "iteration : 100, loss : 0.0304, accuracy : 99.06\n",
            "iteration : 150, loss : 0.0346, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0361, accuracy : 98.91\n",
            "iteration : 250, loss : 0.0369, accuracy : 98.89\n",
            "iteration : 300, loss : 0.0360, accuracy : 98.90\n",
            "iteration : 350, loss : 0.0359, accuracy : 98.91\n",
            "Epoch : 209, training loss : 0.0364, training accuracy : 98.89, test loss : 0.2440, test accuracy : 94.37\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0326, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0366, accuracy : 98.90\n",
            "iteration : 150, loss : 0.0368, accuracy : 98.91\n",
            "iteration : 200, loss : 0.0360, accuracy : 98.93\n",
            "iteration : 250, loss : 0.0358, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0357, accuracy : 98.99\n",
            "iteration : 350, loss : 0.0349, accuracy : 99.00\n",
            "Epoch : 210, training loss : 0.0348, training accuracy : 99.02, test loss : 0.2367, test accuracy : 94.55\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0310, accuracy : 98.91\n",
            "iteration : 100, loss : 0.0325, accuracy : 98.97\n",
            "iteration : 150, loss : 0.0318, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0311, accuracy : 99.07\n",
            "iteration : 250, loss : 0.0307, accuracy : 99.10\n",
            "iteration : 300, loss : 0.0321, accuracy : 99.06\n",
            "iteration : 350, loss : 0.0331, accuracy : 99.05\n",
            "Epoch : 211, training loss : 0.0329, training accuracy : 99.06, test loss : 0.2372, test accuracy : 94.58\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0313, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0307, accuracy : 99.17\n",
            "iteration : 150, loss : 0.0299, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0311, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0311, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0320, accuracy : 99.07\n",
            "iteration : 350, loss : 0.0317, accuracy : 99.08\n",
            "Epoch : 212, training loss : 0.0318, training accuracy : 99.06, test loss : 0.2380, test accuracy : 94.47\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0298, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0298, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0315, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0318, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0322, accuracy : 99.02\n",
            "iteration : 300, loss : 0.0323, accuracy : 99.05\n",
            "iteration : 350, loss : 0.0321, accuracy : 99.07\n",
            "Epoch : 213, training loss : 0.0326, training accuracy : 99.06, test loss : 0.2437, test accuracy : 94.41\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0336, accuracy : 98.95\n",
            "iteration : 100, loss : 0.0332, accuracy : 99.03\n",
            "iteration : 150, loss : 0.0326, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0327, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0319, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0310, accuracy : 99.12\n",
            "iteration : 350, loss : 0.0305, accuracy : 99.14\n",
            "Epoch : 214, training loss : 0.0307, training accuracy : 99.13, test loss : 0.2449, test accuracy : 94.38\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0281, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0288, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0298, accuracy : 99.15\n",
            "iteration : 200, loss : 0.0317, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0307, accuracy : 99.14\n",
            "iteration : 300, loss : 0.0300, accuracy : 99.15\n",
            "iteration : 350, loss : 0.0298, accuracy : 99.14\n",
            "Epoch : 215, training loss : 0.0301, training accuracy : 99.13, test loss : 0.2370, test accuracy : 94.58\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0305, accuracy : 99.03\n",
            "iteration : 100, loss : 0.0296, accuracy : 99.15\n",
            "iteration : 150, loss : 0.0301, accuracy : 99.16\n",
            "iteration : 200, loss : 0.0303, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0322, accuracy : 99.06\n",
            "iteration : 300, loss : 0.0320, accuracy : 99.06\n",
            "iteration : 350, loss : 0.0320, accuracy : 99.06\n",
            "Epoch : 216, training loss : 0.0317, training accuracy : 99.07, test loss : 0.2402, test accuracy : 94.50\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0330, accuracy : 99.03\n",
            "iteration : 100, loss : 0.0308, accuracy : 99.04\n",
            "iteration : 150, loss : 0.0319, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0310, accuracy : 99.07\n",
            "iteration : 250, loss : 0.0306, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0303, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0305, accuracy : 99.09\n",
            "Epoch : 217, training loss : 0.0308, training accuracy : 99.08, test loss : 0.2483, test accuracy : 94.39\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0255, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0264, accuracy : 99.32\n",
            "iteration : 150, loss : 0.0268, accuracy : 99.29\n",
            "iteration : 200, loss : 0.0268, accuracy : 99.26\n",
            "iteration : 250, loss : 0.0272, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0266, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0268, accuracy : 99.25\n",
            "Epoch : 218, training loss : 0.0265, training accuracy : 99.27, test loss : 0.2397, test accuracy : 94.65\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0278, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0301, accuracy : 99.15\n",
            "iteration : 150, loss : 0.0307, accuracy : 99.16\n",
            "iteration : 200, loss : 0.0298, accuracy : 99.17\n",
            "iteration : 250, loss : 0.0304, accuracy : 99.16\n",
            "iteration : 300, loss : 0.0301, accuracy : 99.18\n",
            "iteration : 350, loss : 0.0301, accuracy : 99.16\n",
            "Epoch : 219, training loss : 0.0301, training accuracy : 99.16, test loss : 0.2496, test accuracy : 94.31\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0250, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0287, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0285, accuracy : 99.21\n",
            "iteration : 200, loss : 0.0278, accuracy : 99.22\n",
            "iteration : 250, loss : 0.0277, accuracy : 99.22\n",
            "iteration : 300, loss : 0.0281, accuracy : 99.21\n",
            "iteration : 350, loss : 0.0287, accuracy : 99.19\n",
            "Epoch : 220, training loss : 0.0288, training accuracy : 99.18, test loss : 0.2367, test accuracy : 94.57\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0313, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0301, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0291, accuracy : 99.14\n",
            "iteration : 200, loss : 0.0268, accuracy : 99.24\n",
            "iteration : 250, loss : 0.0273, accuracy : 99.21\n",
            "iteration : 300, loss : 0.0279, accuracy : 99.18\n",
            "iteration : 350, loss : 0.0287, accuracy : 99.17\n",
            "Epoch : 221, training loss : 0.0294, training accuracy : 99.15, test loss : 0.2553, test accuracy : 94.06\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0275, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0235, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0246, accuracy : 99.26\n",
            "iteration : 250, loss : 0.0247, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0257, accuracy : 99.22\n",
            "iteration : 350, loss : 0.0259, accuracy : 99.21\n",
            "Epoch : 222, training loss : 0.0256, training accuracy : 99.23, test loss : 0.2464, test accuracy : 94.47\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0220, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0246, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0241, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0251, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0258, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0261, accuracy : 99.26\n",
            "iteration : 350, loss : 0.0261, accuracy : 99.26\n",
            "Epoch : 223, training loss : 0.0267, training accuracy : 99.24, test loss : 0.2395, test accuracy : 94.48\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0248, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0273, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0268, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0278, accuracy : 99.16\n",
            "iteration : 250, loss : 0.0284, accuracy : 99.15\n",
            "iteration : 300, loss : 0.0290, accuracy : 99.13\n",
            "iteration : 350, loss : 0.0297, accuracy : 99.13\n",
            "Epoch : 224, training loss : 0.0298, training accuracy : 99.12, test loss : 0.2460, test accuracy : 94.41\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0288, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0293, accuracy : 99.22\n",
            "iteration : 150, loss : 0.0275, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0278, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0290, accuracy : 99.19\n",
            "iteration : 300, loss : 0.0282, accuracy : 99.20\n",
            "iteration : 350, loss : 0.0284, accuracy : 99.19\n",
            "Epoch : 225, training loss : 0.0284, training accuracy : 99.18, test loss : 0.2405, test accuracy : 94.52\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0253, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.30\n",
            "iteration : 150, loss : 0.0231, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0227, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0230, accuracy : 99.37\n",
            "iteration : 300, loss : 0.0227, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0232, accuracy : 99.36\n",
            "Epoch : 226, training loss : 0.0233, training accuracy : 99.35, test loss : 0.2463, test accuracy : 94.46\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0216, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0201, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0211, accuracy : 99.45\n",
            "iteration : 200, loss : 0.0229, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0243, accuracy : 99.30\n",
            "iteration : 300, loss : 0.0245, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0246, accuracy : 99.29\n",
            "Epoch : 227, training loss : 0.0247, training accuracy : 99.28, test loss : 0.2289, test accuracy : 94.76\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0221, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0227, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0209, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0212, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0223, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0224, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0231, accuracy : 99.34\n",
            "Epoch : 228, training loss : 0.0226, training accuracy : 99.36, test loss : 0.2437, test accuracy : 94.61\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0265, accuracy : 99.19\n",
            "iteration : 100, loss : 0.0236, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0230, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0225, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0232, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0228, accuracy : 99.35\n",
            "iteration : 350, loss : 0.0234, accuracy : 99.33\n",
            "Epoch : 229, training loss : 0.0238, training accuracy : 99.31, test loss : 0.2495, test accuracy : 94.35\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0238, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0245, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0249, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0248, accuracy : 99.29\n",
            "iteration : 250, loss : 0.0252, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0251, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0249, accuracy : 99.28\n",
            "Epoch : 230, training loss : 0.0252, training accuracy : 99.28, test loss : 0.2383, test accuracy : 94.69\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0216, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0234, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0241, accuracy : 99.29\n",
            "iteration : 200, loss : 0.0233, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0235, accuracy : 99.29\n",
            "iteration : 300, loss : 0.0237, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0238, accuracy : 99.27\n",
            "Epoch : 231, training loss : 0.0238, training accuracy : 99.28, test loss : 0.2410, test accuracy : 94.42\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0199, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0207, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0219, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0227, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0240, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0246, accuracy : 99.30\n",
            "iteration : 350, loss : 0.0241, accuracy : 99.33\n",
            "Epoch : 232, training loss : 0.0242, training accuracy : 99.32, test loss : 0.2316, test accuracy : 94.81\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0202, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0208, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0206, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0210, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0210, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0208, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0216, accuracy : 99.34\n",
            "Epoch : 233, training loss : 0.0217, training accuracy : 99.34, test loss : 0.2418, test accuracy : 94.49\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0224, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0205, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0191, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0184, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0193, accuracy : 99.46\n",
            "iteration : 300, loss : 0.0204, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0213, accuracy : 99.39\n",
            "Epoch : 234, training loss : 0.0212, training accuracy : 99.39, test loss : 0.2421, test accuracy : 94.61\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0206, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0224, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0220, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0212, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0212, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0207, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0208, accuracy : 99.47\n",
            "Epoch : 235, training loss : 0.0209, training accuracy : 99.46, test loss : 0.2444, test accuracy : 94.53\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0216, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0216, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0215, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0220, accuracy : 99.40\n",
            "iteration : 250, loss : 0.0214, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0218, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0215, accuracy : 99.41\n",
            "Epoch : 236, training loss : 0.0216, training accuracy : 99.41, test loss : 0.2398, test accuracy : 94.69\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0224, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0219, accuracy : 99.27\n",
            "iteration : 150, loss : 0.0218, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0230, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0233, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0237, accuracy : 99.30\n",
            "iteration : 350, loss : 0.0232, accuracy : 99.33\n",
            "Epoch : 237, training loss : 0.0231, training accuracy : 99.33, test loss : 0.2343, test accuracy : 94.69\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0189, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0196, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0183, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0182, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0180, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0187, accuracy : 99.50\n",
            "Epoch : 238, training loss : 0.0189, training accuracy : 99.49, test loss : 0.2445, test accuracy : 94.53\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0145, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0182, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0209, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0206, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0200, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0202, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0209, accuracy : 99.41\n",
            "Epoch : 239, training loss : 0.0210, training accuracy : 99.41, test loss : 0.2494, test accuracy : 94.43\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0187, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0186, accuracy : 99.39\n",
            "iteration : 150, loss : 0.0183, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0185, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0194, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0194, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0198, accuracy : 99.43\n",
            "Epoch : 240, training loss : 0.0195, training accuracy : 99.44, test loss : 0.2373, test accuracy : 94.69\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0167, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0174, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0174, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0187, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0188, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0190, accuracy : 99.46\n",
            "Epoch : 241, training loss : 0.0189, training accuracy : 99.46, test loss : 0.2414, test accuracy : 94.68\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0205, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0196, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0193, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0193, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0196, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0197, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0203, accuracy : 99.45\n",
            "Epoch : 242, training loss : 0.0202, training accuracy : 99.46, test loss : 0.2378, test accuracy : 94.68\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0226, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0233, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0212, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0208, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0201, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0208, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0214, accuracy : 99.42\n",
            "Epoch : 243, training loss : 0.0213, training accuracy : 99.42, test loss : 0.2460, test accuracy : 94.73\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0191, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0184, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0180, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0172, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0171, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0176, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0176, accuracy : 99.50\n",
            "Epoch : 244, training loss : 0.0179, training accuracy : 99.49, test loss : 0.2422, test accuracy : 94.71\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0189, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0185, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0187, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0186, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0186, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0185, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0181, accuracy : 99.48\n",
            "Epoch : 245, training loss : 0.0179, training accuracy : 99.49, test loss : 0.2347, test accuracy : 94.81\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0187, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0183, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0177, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0167, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0171, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0177, accuracy : 99.48\n",
            "Epoch : 246, training loss : 0.0176, training accuracy : 99.49, test loss : 0.2379, test accuracy : 94.76\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0186, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0171, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0172, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0174, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0174, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0174, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0174, accuracy : 99.51\n",
            "Epoch : 247, training loss : 0.0176, training accuracy : 99.50, test loss : 0.2411, test accuracy : 94.71\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0158, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0152, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0158, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0165, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0163, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0163, accuracy : 99.53\n",
            "Epoch : 248, training loss : 0.0162, training accuracy : 99.52, test loss : 0.2400, test accuracy : 94.69\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0188, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0181, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0180, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0181, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0173, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0177, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0183, accuracy : 99.47\n",
            "Epoch : 249, training loss : 0.0181, training accuracy : 99.48, test loss : 0.2407, test accuracy : 94.71\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0189, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0178, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0174, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0187, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0195, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0204, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0195, accuracy : 99.44\n",
            "Epoch : 250, training loss : 0.0192, training accuracy : 99.45, test loss : 0.2402, test accuracy : 94.72\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0155, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0156, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0158, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0164, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0158, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0151, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0151, accuracy : 99.61\n",
            "Epoch : 251, training loss : 0.0151, training accuracy : 99.60, test loss : 0.2400, test accuracy : 94.76\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0170, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0180, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0183, accuracy : 99.50\n",
            "iteration : 200, loss : 0.0174, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0167, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0167, accuracy : 99.52\n",
            "Epoch : 252, training loss : 0.0166, training accuracy : 99.52, test loss : 0.2359, test accuracy : 94.74\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0166, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0174, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0193, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0194, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0182, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0173, accuracy : 99.54\n",
            "Epoch : 253, training loss : 0.0170, training accuracy : 99.55, test loss : 0.2381, test accuracy : 94.63\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0149, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0152, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0143, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0153, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0149, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0149, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0153, accuracy : 99.62\n",
            "Epoch : 254, training loss : 0.0156, training accuracy : 99.61, test loss : 0.2380, test accuracy : 94.81\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0145, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0140, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0145, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0151, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0148, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0153, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0153, accuracy : 99.56\n",
            "Epoch : 255, training loss : 0.0158, training accuracy : 99.55, test loss : 0.2381, test accuracy : 94.78\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0157, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0161, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0148, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0152, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0157, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0156, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0154, accuracy : 99.60\n",
            "Epoch : 256, training loss : 0.0154, training accuracy : 99.60, test loss : 0.2374, test accuracy : 94.78\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0130, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0134, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0138, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0143, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0137, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0139, accuracy : 99.64\n",
            "Epoch : 257, training loss : 0.0141, training accuracy : 99.62, test loss : 0.2437, test accuracy : 94.63\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0147, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0136, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0137, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0139, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0138, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0137, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0140, accuracy : 99.62\n",
            "Epoch : 258, training loss : 0.0139, training accuracy : 99.62, test loss : 0.2342, test accuracy : 94.73\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0156, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0158, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0149, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0153, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0153, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0148, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0152, accuracy : 99.61\n",
            "Epoch : 259, training loss : 0.0153, training accuracy : 99.60, test loss : 0.2404, test accuracy : 94.76\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0155, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0163, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0151, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0150, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0145, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0149, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0155, accuracy : 99.59\n",
            "Epoch : 260, training loss : 0.0155, training accuracy : 99.59, test loss : 0.2377, test accuracy : 94.75\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0142, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0150, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0150, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0148, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0136, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0140, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0141, accuracy : 99.61\n",
            "Epoch : 261, training loss : 0.0140, training accuracy : 99.61, test loss : 0.2333, test accuracy : 94.87\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0126, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0124, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0128, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0124, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0124, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0125, accuracy : 99.68\n",
            "Epoch : 262, training loss : 0.0124, training accuracy : 99.67, test loss : 0.2339, test accuracy : 94.91\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0129, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0138, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0135, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0137, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0150, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0142, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0144, accuracy : 99.61\n",
            "Epoch : 263, training loss : 0.0144, training accuracy : 99.61, test loss : 0.2370, test accuracy : 94.89\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0098, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0113, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0115, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0116, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0119, accuracy : 99.66\n",
            "Epoch : 264, training loss : 0.0120, training accuracy : 99.66, test loss : 0.2394, test accuracy : 94.72\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0125, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0121, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0128, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0133, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0134, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0129, accuracy : 99.65\n",
            "Epoch : 265, training loss : 0.0129, training accuracy : 99.65, test loss : 0.2372, test accuracy : 94.83\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0133, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0121, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0131, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0130, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0134, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0130, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.65\n",
            "Epoch : 266, training loss : 0.0126, training accuracy : 99.65, test loss : 0.2388, test accuracy : 94.88\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0139, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0136, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0138, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0140, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0134, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0135, accuracy : 99.64\n",
            "Epoch : 267, training loss : 0.0137, training accuracy : 99.63, test loss : 0.2266, test accuracy : 95.14\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0136, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0130, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0135, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0127, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0127, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0126, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0125, accuracy : 99.68\n",
            "Epoch : 268, training loss : 0.0125, training accuracy : 99.68, test loss : 0.2351, test accuracy : 94.82\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0126, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0125, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0121, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0129, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0126, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0123, accuracy : 99.69\n",
            "Epoch : 269, training loss : 0.0122, training accuracy : 99.69, test loss : 0.2304, test accuracy : 94.97\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0159, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0137, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0139, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0144, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0143, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0144, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0138, accuracy : 99.65\n",
            "Epoch : 270, training loss : 0.0138, training accuracy : 99.64, test loss : 0.2326, test accuracy : 94.90\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0119, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0119, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0111, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0110, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0108, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.69\n",
            "Epoch : 271, training loss : 0.0110, training accuracy : 99.70, test loss : 0.2337, test accuracy : 94.78\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0112, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0120, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0112, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0115, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0121, accuracy : 99.68\n",
            "Epoch : 272, training loss : 0.0122, training accuracy : 99.68, test loss : 0.2399, test accuracy : 94.79\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0137, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0137, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0125, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0119, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0115, accuracy : 99.70\n",
            "Epoch : 273, training loss : 0.0115, training accuracy : 99.70, test loss : 0.2277, test accuracy : 95.06\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0116, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0131, accuracy : 99.61\n",
            "iteration : 150, loss : 0.0130, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0131, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0126, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0122, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0126, accuracy : 99.65\n",
            "Epoch : 274, training loss : 0.0126, training accuracy : 99.65, test loss : 0.2358, test accuracy : 94.89\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0097, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0100, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0107, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0110, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0110, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0109, accuracy : 99.71\n",
            "Epoch : 275, training loss : 0.0108, training accuracy : 99.71, test loss : 0.2377, test accuracy : 94.88\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0104, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0121, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0117, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0115, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.71\n",
            "Epoch : 276, training loss : 0.0109, training accuracy : 99.72, test loss : 0.2408, test accuracy : 94.65\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0104, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0112, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0121, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0126, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0122, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0125, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0122, accuracy : 99.69\n",
            "Epoch : 277, training loss : 0.0121, training accuracy : 99.70, test loss : 0.2367, test accuracy : 94.88\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0115, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0126, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0124, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0122, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0123, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0120, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0118, accuracy : 99.67\n",
            "Epoch : 278, training loss : 0.0115, training accuracy : 99.68, test loss : 0.2331, test accuracy : 94.96\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0142, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0118, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0123, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0115, accuracy : 99.71\n",
            "Epoch : 279, training loss : 0.0113, training accuracy : 99.72, test loss : 0.2342, test accuracy : 95.06\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0112, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0105, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0104, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0105, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.76\n",
            "Epoch : 280, training loss : 0.0104, training accuracy : 99.76, test loss : 0.2378, test accuracy : 94.86\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0094, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0118, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0108, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0111, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0107, accuracy : 99.75\n",
            "Epoch : 281, training loss : 0.0106, training accuracy : 99.75, test loss : 0.2336, test accuracy : 95.03\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0093, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0103, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0103, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0105, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0110, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0113, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.70\n",
            "Epoch : 282, training loss : 0.0110, training accuracy : 99.71, test loss : 0.2329, test accuracy : 94.94\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0120, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0117, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0126, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0120, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0114, accuracy : 99.69\n",
            "Epoch : 283, training loss : 0.0113, training accuracy : 99.69, test loss : 0.2354, test accuracy : 94.94\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0108, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0108, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0103, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0109, accuracy : 99.72\n",
            "Epoch : 284, training loss : 0.0108, training accuracy : 99.73, test loss : 0.2341, test accuracy : 94.89\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0141, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0127, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0121, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0112, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0107, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.73\n",
            "Epoch : 285, training loss : 0.0104, training accuracy : 99.73, test loss : 0.2359, test accuracy : 94.99\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0099, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0104, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.75\n",
            "Epoch : 286, training loss : 0.0107, training accuracy : 99.75, test loss : 0.2356, test accuracy : 94.91\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0100, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0096, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0098, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0099, accuracy : 99.75\n",
            "Epoch : 287, training loss : 0.0097, training accuracy : 99.75, test loss : 0.2330, test accuracy : 95.08\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0142, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0123, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0115, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0110, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0105, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.75\n",
            "Epoch : 288, training loss : 0.0101, training accuracy : 99.76, test loss : 0.2357, test accuracy : 94.86\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0097, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0095, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0098, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0095, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0099, accuracy : 99.75\n",
            "Epoch : 289, training loss : 0.0102, training accuracy : 99.73, test loss : 0.2329, test accuracy : 95.01\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0121, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0097, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0097, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0101, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0099, accuracy : 99.74\n",
            "Epoch : 290, training loss : 0.0100, training accuracy : 99.73, test loss : 0.2346, test accuracy : 94.86\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0129, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0127, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0123, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0115, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0106, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0105, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.72\n",
            "Epoch : 291, training loss : 0.0107, training accuracy : 99.72, test loss : 0.2339, test accuracy : 94.93\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0084, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0098, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0107, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0110, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0109, accuracy : 99.72\n",
            "Epoch : 292, training loss : 0.0108, training accuracy : 99.72, test loss : 0.2341, test accuracy : 94.92\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0100, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0093, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0094, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0101, accuracy : 99.76\n",
            "Epoch : 293, training loss : 0.0100, training accuracy : 99.76, test loss : 0.2410, test accuracy : 94.89\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0103, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0099, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0108, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0108, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.72\n",
            "Epoch : 294, training loss : 0.0107, training accuracy : 99.73, test loss : 0.2331, test accuracy : 95.03\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0082, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0104, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0107, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0110, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0115, accuracy : 99.71\n",
            "Epoch : 295, training loss : 0.0113, training accuracy : 99.71, test loss : 0.2333, test accuracy : 95.01\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0103, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0114, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0116, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0114, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0116, accuracy : 99.72\n",
            "Epoch : 296, training loss : 0.0115, training accuracy : 99.72, test loss : 0.2367, test accuracy : 94.86\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0137, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0123, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0116, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0112, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0107, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0105, accuracy : 99.73\n",
            "Epoch : 297, training loss : 0.0103, training accuracy : 99.73, test loss : 0.2295, test accuracy : 95.06\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0076, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0120, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0115, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0119, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.70\n",
            "Epoch : 298, training loss : 0.0114, training accuracy : 99.69, test loss : 0.2374, test accuracy : 94.91\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0130, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0114, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0110, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.72\n",
            "Epoch : 299, training loss : 0.0109, training accuracy : 99.72, test loss : 0.2310, test accuracy : 95.03\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0100, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0084, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.79\n",
            "Epoch : 300, training loss : 0.0089, training accuracy : 99.79, test loss : 0.2348, test accuracy : 94.93\n"
          ]
        }
      ],
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUQVIKR-X3v6",
        "outputId": "07cb10a2-bc8a-4d89-e6f9-cab26df69dd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.16281128386218174, 96.30454824830977)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CNz1iabSB21",
        "outputId": "f997f846-c3eb-4cea-b68a-7337f9295f22"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : RandAugment\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "outputId": "0c680fdb-6eea-4d42-a72f-f375e6e3142e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2S0lEQVR4nO3deZwU1bn/8c8zi2wiyCKyyiKiqIiCC3FNjAqaRI03xj0uCTFxT8xPcbsmJjdqrsYY4xZD1Bh3EzVKFPWqaHABERURFBFlQNkUkB1mnt8fT7Xd07MwLE3PUN/369Wv7q71VFX3eeqcU3XK3B0REUmvkmInQEREikuBQEQk5RQIRERSToFARCTlFAhERFJOgUBEJOUUCESaCDObYWbfLHY6ZPOjQCDVKLNpODM7yMyqzGyJmX1pZlPN7LRNnIZTzczN7NhNud6NJdmHFcVOR9opEMhmxczKNvEqZ7v7lsBWwAXAn82s3yZc/w+Az5N3kfWiQCANYmbNzOwGM5udvG4ws2bJuA5m9oSZLTSzz83sJTMrScZdZGazcs6YD65j+S3M7Doz+9jMFpnZy8mwGmeMuaUWM7vSzB42s3vMbDFwiZktN7N2OdPvbmbzzaw8+X66mb1nZl+Y2dNmtt2G7h8Po4hMeUCynq2T/TIvWdcTZtYtJ10vmNlVZvafZP+MNrMOOeNPTvbHAjO7tJZ9th1wIDAcOMzMOuWMO9XMXs6b3s1s++RzezP7l5ktNrNxZvbr3OmTaX9qZh8kabvKzPqY2SvJPA+a2RY503/LzCYmv4GxZjYgZ9wMM7vQzN5Oju0DZtbczFoB/wa6JKWqJWbWZQMOg6wnBQJpqEuBfYCBwG7AXsBlybifAxVAR6ATcAngyZnx2cCe7t4aOAyYUcfy/xcYBHwNaAf8P6CqgWk7EngYaAv8DngFOCZn/AnAw+6+2syOStL33SS9LwH31bXgJPM6YW0JMLMSM/sO0AGYlgwuAf4KbAf0AJYDN+XNegJwGrANsAVwYbK8/sAtwMlAF6A90C1v3lOA8e7+CPAecOLa0pnjT8BSYFuiNFFbiWIocUz2IY7H7ck6ugO7AMcnad0DGAn8OEnnbcDjmROFxLHJ8noRgfJUd18KDCMpVSWv2euwDbKRKBBIQ50I/Mrd57r7POCXRCYFsBroDGzn7qvd/SWPTqwqgWZAfzMrd/cZ7v5h/oKT0sPpwHnuPsvdK919rLuvbGDaXnH3R929yt2XA/eSzaQMOC4ZBpFZ/dbd33P3NcD/AAPrKhW4+wB3v7e2cYkuZraQyOT/CfzM3d9M5l3g7o+4+zJ3/xL4DXEGn+uv7v5+ku4HiUAL8F/AE+4+JtkPl1MzMJ6Ss1330sDqITMrJQLlfydpmwzcVcuk17j7Ynd/F5gEjHb36e6+iDiT3z2Z7kfAbe7+WnLs7gJWEgEk40Z3n+3unwP/ytlOaQQUCKShugAf53z/OBkGcRY+DRhtZtPN7GIAd58GnA9cCcw1s/vrKPp3AJoDNYJEA83M+/4wMCRZ1wGAE2f+EGfnf0iqMBYSVTkGdF3Pdc9297ZEG8GNwDcyI8yspZndllTvLAbGAG2TjDjjs5zPy4Atk89dcrcrOXtekLPsfYmz6/uTQfcCu5rZwAakuSNQRvX9lr8PAebkfF5ey/dMWrcDfp7Zp8l+7U729wF1b6c0AgoE0lCziT98Ro9kGO7+pbv/3N17A98GfpZpC3D3e919v2ReB66pZdnzgRVAn1rGLQVaZr4kmWjHvGmqdaHr7guB0UR1xAnAfZ7tZncm8GN3b5vzauHuY9e2A+qTnLVfRGTGRyWDfw70A/Z2962IoAQReNbmUyIzjRnMWhLVLhk/SJYz0cw+A15Lhp+SvOfvt21z5p0HrKF6VVN31t9M4Dd5+7Slu9dZ5ZZD3R83AgoEUpvypDEv8yoj6tEvM7OOSYPmFcA98FVD4fZJNcxiokqo0sz6mdk3krriFcRZZGX+yty9iqhjvt7MuphZqZkNSeZ7H2huZkckjb2XEdVNa3MvkSkeQ7b6BOBWYISZ7ZykvY2ZfW/dd1FN7r4KuI7YNwCtiW1eaNF4/d/rsLiHgW+Z2X5Jo+yvSP6vZtacCHLDiSqWzOsc4MTkeL0F7GxmA5Ppr8xJZyXwD+DKpNSyI9kAsj7+DJxpZntbaJUcr9YNmHcO0N7M2mzA+mUDKRBIbUYRGVjmdSXwa2A88DbwDjAhGQbQF3gWWEI01N7s7i8QGfbVxBn/Z0SD6CV1rPPCZLnjiOqaa4CSpD76p8AdwCziTLch150/nqRrjru/lRno7v9Mln1/Ul0ziWiwrJWZvWtm69IIOxLoYWbfBm4AWhDb/yrwVEMXktTLn0UEsU+BL8hu91HEcbnb3T/LvIC/AKXAUHd/nwgezwIfAC9XXwNnA22I4/I3ItA3tE0mP63jiXaCm5J0TgNObeC8U5J1T0+qlXTVUBGYHkwjImZ2DbCtu+t+hBRSiUAkhcxsRzMbkFTl7AWcQVz1JCm0qe/CFJHGoTVRJdMFmEu0bTxW1BRJ0ahqSEQk5VQ1JCKSck2uaqhDhw7es2fPYidDRKRJeeONN+a7e/49OEATDAQ9e/Zk/PjxxU6GiEiTYmYf1zVOVUMiIimnQCAiknIKBCIiKdfk2ghERNbH6tWrqaioYMWKFcVOSkE1b96cbt26UV5e3uB5FAhEJBUqKipo3bo1PXv2JPpH3Py4OwsWLKCiooJevXo1eD5VDYlIKqxYsYL27dtvtkEAwMxo3779Opd6FAhEJDU25yCQsT7bmJpAMGkSXHEFzJ1b7JSIiDQuqQkE770HV10F8+YVOyUikkYLFy7k5ptvXuf5Dj/8cBYuXLjxE5QjNYGgJNnSqvzHf4uIbAJ1BYLKyhoP7atm1KhRtG3btkCpCqm5akiBQESK6eKLL+bDDz9k4MCBlJeXs+WWW9K5c2cmTpzI5MmTOeqoo5g5cyYrVqzgvPPOY/jw4UC2W50lS5YwbNgw9ttvP8aOHUvXrl157LHHaNGixQanTYFARFLn/PNh4sSNu8yBA+GGG+oef/XVVzNp0iQmTpzICy+8wBFHHMGkSZO+usxz5MiRtGvXjuXLl7PnnntyzDHH0L59+2rL+OCDD7jvvvv485//zLHHHssjjzzCSSedtMFpVyAQESmCvfbaq9q1/jfeeCP//Gc8JG7mzJl88MEHNQJBr169GDhwIACDBg1ixowZGyUtCgQikjr1nblvKq1atfrq8wsvvMCzzz7LK6+8QsuWLTnooINqvRegWbNmX30uLS1l+fLlGyUtaiwWEdkEWrduzZdfflnruEWLFrH11lvTsmVLpkyZwquvvrpJ06YSgYjIJtC+fXv23XdfdtllF1q0aEGnTp2+Gjd06FBuvfVWBgwYQL9+/dhnn302adoUCERENpF777231uHNmjXj3//+d63jMu0AHTp0YNKkSV8Nv/DCCzdaulQ1JCKScqkJBJnuNxQIRESqS00gUIlARKR2qQkE7d58jrEModnsj4qdFBGRRiU1gaB8yecM4VVs2dJiJ0VEpFEpWCAws5FmNtfMJtUx/kQzezt5jTWz3QqVFgArLQXAK1U3JCKSq5AlgjuBofWM/wg40N0HAFcBtxcwLVhpbKoCgYgUw/p2Qw1www03sGzZso2coqyCBQJ3HwN8Xs/4se7+RfL1VaBbodICCgQiUlyNORA0lhvKzgBqv5sCMLPhwHCAHj16rNcKvgoEa+rv+1tEpBByu6E+5JBD2GabbXjwwQdZuXIlRx99NL/85S9ZunQpxx57LBUVFVRWVnL55ZczZ84cZs+ezde//nU6dOjA888/v9HTVvRAYGZfJwLBfnVN4+63k1QdDR482NdrPWVqIxCRRBH6oc7thnr06NE8/PDDvP7667g73/nOdxgzZgzz5s2jS5cuPPnkk0D0QdSmTRuuv/56nn/+eTp06LBx05wo6lVDZjYAuAM40t0XFHRdqhoSkUZi9OjRjB49mt1335099tiDKVOm8MEHH7Drrrvy7LPPctFFF/HSSy/Rpk2bTZKeopUIzKwH8A/gZHd/v+DrSwIBa3ksnIikQJH7oXZ3RowYwY9//OMa49544w1GjRrFiBEjOPTQQ7niiisKnp5CXj56H/AK0M/MKszsDDM708zOTCa5AmgP3GxmE81sfKHSAqoaEpHiyu2G+rDDDmPkyJEsWbIEgFmzZjF37lxmz55Ny5YtOemkk7jwwguZMGFCjXkLoWAlAnc/fi3jfwj8sFDrr6FEVUMiUjy53VAPGzaME044gSFDhgCw5ZZbcs899zBt2jR+8YtfUFJSQnl5ObfccgsAw4cPZ9iwYXTu3HnzbCzeVKxMVUMiUlz53VCfd9551b736dOHww47rMZ855xzDuecc07B0pWaLiZKkqoh9TonIlJdagKBrhoSEamdAoGIpIb7et2G1KSszzamLhCojUAknZo3b86CBQs262Dg7ixYsIDmzZuv03ypaSwuKdfloyJp1q1bNyoqKpg3b16xk1JQzZs3p1u3deu6LTWB4KsSgRqLRVKpvLycXr16FTsZjVLqqobU6ZyISHWpCQSZqiGVCEREqktNINBVQyIitUtdINBVQyIi1aUvEKhqSESkmtQEArURiIjULj2BoEwlAhGR2qQmEKiNQESkdqkJBF/dWawSgYhINakJBNkSgQKBiEiu1ASCEj2YRkSkVukJBLpqSESkVukJBLpqSESkVgoEIiIpV7BAYGYjzWyumU2qY7yZ2Y1mNs3M3jazPQqVFtDloyIidSlkieBOYGg944cBfZPXcOCWAqYF08PrRURqVbBA4O5jgM/rmeRI4G4PrwJtzaxzodJDiaqGRERqU8w2gq7AzJzvFcmwGsxsuJmNN7Px6/2YuSQQWJWqhkREchUzEFgtw2p9qrS73+7ug919cMeOHddvbaWqGhIRqU0xA0EF0D3nezdgdsHWpqohEZFaFTMQPA6cklw9tA+wyN0/LdjaLAogqhoSEamurFALNrP7gIOADmZWAfw3UA7g7rcCo4DDgWnAMuC0QqUlSRCVlKhEICKSp2CBwN2PX8t4B84q1PprU6VAICJSQ2ruLAYFAhGR2qQuEKiNQESkupQFglKVCERE8qQrEFgJuAKBiEiudAUCVQ2JiNSQqkBQSSmmqiERkWpSFQhcVUMiIjWkKhCoakhEpKZ0BQJT1ZCISL50BQJUNSQiki9VgcCtRCUCEZE8qQoEaiMQEakpXYHASjFVDYmIVJOqQOBqIxARqSFVgaDKSihR1ZCISDUpCwSlKhGIiORJVSBwdNWQiEi+VAWCKiuhxFU1JCKSK1WBQH0NiYjUlKpAoMtHRURqSlUgcCtRIBARyVPQQGBmQ81sqplNM7OLaxnfxsz+ZWZvmdm7ZnZaIdPj6PJREZF8BQsEZlYK/AkYBvQHjjez/nmTnQVMdvfdgIOA68xsi0KlqapEVUMiIvkKWSLYC5jm7tPdfRVwP3Bk3jQOtDYzA7YEPgfWFCpBaiwWEampkIGgKzAz53tFMizXTcBOwGzgHeA898Ll1I4uHxURyVfIQGC1DPO874cBE4EuwEDgJjPbqsaCzIab2XgzGz9v3rz1TpCqhkREaipkIKgAuud870ac+ec6DfiHh2nAR8CO+Qty99vdfbC7D+7YseN6J8jRVUMiIvkKGQjGAX3NrFfSAHwc8HjeNJ8ABwOYWSegHzC9UAnyEgUCEZF8ZYVasLuvMbOzgaeBUmCku79rZmcm428FrgLuNLN3iKqki9x9fsHSpC4mRERqKFggAHD3UcCovGG35nyeDRxayDTk0p3FIiI1perOYqyEEgUCEZFqUhUIVDUkIlJTqgKBLh8VEakpVYEAK8FQIBARyZWqQKCqIRGRmtIVCFQ1JCJSQ7oCgZVQoqohEZFq0hUIdGexiEgNDQoEZtbKzEqSzzuY2XfMrLywSdv41EYgIlJTQ0sEY4DmZtYVeI7oLO7OQiWqULykVFVDIiJ5GhoIzN2XAd8F/ujuRxNPHWtadGexiEgNDQ4EZjYEOBF4MhlW0H6KCiEai1U1JCKSq6GB4HxgBPDPpAfR3sDzBUtVgejyURGRmhp0Vu/uLwIvAiSNxvPd/dxCJqwgSnT5qIhIvoZeNXSvmW1lZq2AycBUM/tFYZO28XlJCaW6akhEpJqGVg31d/fFwFHE8wV6ACcXKlEFo76GRERqaGggKE/uGzgKeMzdV1PzQfSNni4fFRGpqaGB4DZgBtAKGGNm2wGLC5WoglEbgYhIDQ1tLL4RuDFn0Mdm9vXCJKlwvER3FouI5GtoY3EbM7vezMYnr+uI0kGT4qaqIRGRfA2tGhoJfAkcm7wWA38tVKIKRlVDIiI1NPTu4D7ufkzO91+a2cQCpKewSkoo1Z3FIiLVNLREsNzM9st8MbN9geVrm8nMhprZVDObZmYX1zHNQWY20czeNbMXG5ie9eIlpZSqRCAiUk1DSwRnAnebWZvk+xfAD+qbwcxKgT8BhwAVwDgze9zdJ+dM0xa4GRjq7p+Y2TbrmP51U5LEPXcwK+iqRESaigaVCNz9LXffDRgADHD33YFvrGW2vYBp7j7d3VcB9wNH5k1zAvAPd/8kWc/cdUr9usoEgiqVCkREMtbpCWXuvji5wxjgZ2uZvCswM+d7RTIs1w7A1mb2gpm9YWan1LYgMxueuWJp3rx565Lk6jKBoFLtBCIiGRvyqMq11a3UNj7/buQyYBBwBHAYcLmZ7VBjJvfb3X2wuw/u2LHjeiUWgNLSeFeJQETkKxvyTIG1dTFRAXTP+d4NmF3LNPPdfSmw1MzGALsB729AuuqmqiERkRrqLRGY2ZdmtriW15dAl7UsexzQ18x6mdkWwHHA43nTPAbsb2ZlZtYS2Bt4bz23Ze2SQOBrVDUkIpJRb4nA3Vuv74LdfY2ZnQ08DZQCI5OH2pyZjL/V3d8zs6eAt4Eq4A53n7S+61xrmkqiasgrq9ZaryUikhYFfdyku48iuq3OHXZr3vffAb8rZDq+Uholgqo1VRvUOCIisjlJV35oSSBYraohEZGMdAWC5KqhqjVqLBYRyUhXIMg0FlcqEIiIZCgQiIikXKoCgZWqjUBEJF+qAoGrjUBEpIZUBQJT1ZCISA2pCgSZNgJVDYmIZKUrEJRm7ywWEZGQqkBgOXcWi4hISFUgyFQNlb3xWpETIiLSeKQyELQ9/1QYP764aRERaSRSFQisJKfP0YkTi5YOEZHGJFWBYPaAoZzOX/CyMpg8udjJERFpFFIVCKpatOKvnM6qHQfAu+8WOzkiIo1CqgJB5kmVq7bfWSUCEZFEKgPByu37Q0UFLF5c3ASJiDQC6QwEvfvHhylTipcYEZFGIpWBYFWXnvHhk0+KlhYRkcYilYFgZcdu8aGioniJERFpJFIZCNa03hpatFAgEBGhwIHAzIaa2VQzm2ZmF9cz3Z5mVmlm/1XI9GQCQZUbdO8OM2cWcnUiIk1CwQKBmZUCfwKGAf2B482sfx3TXQM8Xai0ZJSXx/uKFUC3bioRiIhQ2BLBXsA0d5/u7quA+4Eja5nuHOARYG4B0wJAjx7xPmMGCgQiIolCBoKuQG7dS0Uy7Ctm1hU4Gri1vgWZ2XAzG29m4+fNm7feCerdO94//JCoGpo1Cyr1kBoRSbdCBgKrZZjnfb8BuMjd682N3f12dx/s7oM7duy43glq3Ro6dUoCQbduEQRat4Zp09Z7mSIiTV0hA0EF0D3nezdgdt40g4H7zWwG8F/AzWZ2VAHTRJ8+Sb7fPUna8uUwblwhVyki0qgVMhCMA/qaWS8z2wI4Dng8dwJ37+XuPd29J/Aw8FN3f7SAaaJPn6REcPDB8JvfxEBdPSQiKVawQODua4CziauB3gMedPd3zexMMzuzUOtdmz59omlgBc3hkkugTRsFAhFJtbJCLtzdRwGj8obV2jDs7qcWMi0ZffqAO0yfDv37E5cSqasJEUmxVN1ZDLD77vH++uvJAN1YJiIpl7pAsNNO0K4djBmTDFCJQERSLnWBoKQE9t8fXnopGdCjByxYAMuWFTVdIiLFkrpAABEIpk2D2bPJXkY6YUJR0yQiUiypDATDhsX7X/5CtB5DRIdnny1amkREiiWVgaB/f/jWt+CGG2DJLvvAE09Aq1bwyCPFTpqIyCaXykAAcNll8Pnn8L/XGRxxBBx6aAQEz+8FI8eSJVBVtekSKSKyCaQ2EOy9Nxx7LFx7bdxgxre/Hb2RvvVW7TMsWxbtCXfeuSmTKSJScKkNBABXXx39zl16KXDYYTHwhRfivbISrroqaVEm7kBbuBBee60IKRURKZxUB4JeveD88+Guu2Di3C7Qsye8/DJMnBjvV1wBv/99TDx9erxPnVr3As84A+6+u8CpFhHZuFIdCABGjIjuhn79a2DffeFf/4rbj087LSZ46KFoN/joo/heVyCorIwg8PjjtY8XEWmkUh8I2raFs8+Gf/wDZvfeD1atihEffQSlpfDxx/Dqq9lA8NlnsHhxzQV9+imsWZM0OIiINB2pDwQQ1UOtW8OvXv5GZP6HHBIjzjwT2reHk0+u3jZQW6ng44/jXY+/FJEmRoEA6NAheqS+7fkd+L97P4NHH436/gsuiEtKZ86MUkHfvjHD++9nZ37qqRif6a8oUzIQyXfXXTBwIKxYUeyUiFSjQJA477zokO6Un3Xg8xUt4Y474q7jffaB00+PiXbaKW48e+aZ+F5REfcgjBiRLRFUVsKcOTB2bPWAAVGl9OWX8XnWrGwD9KYwdy4MGQIffLDp1pnxi19k21yKZfXqKPpNnly8NNx0U1ye/OCDxUtDfebPh0WLip0KKQZ3b1KvQYMGeaGMH+++xRbu++zj/uWXOSM+/NAd3G+5xf2ss2Kihx92P/vsGN6unfuPfhSfwf2559xbt3Y/4IDqKzjoIPcjjojP++zj3r9/wxJWVeVeWem+YIH7s8+u38Y98ECk7brr1m/+tamsjFe+0aOz+6W28Rti2rTYjx9/vPZp77gj0nDBBRs3DflWr8778SQ++CC7H/baK/bFI4+4X3aZ+8qVGzcNq1a533+/+/z5tY+vrIzfVL5Bg9z326/2cRuqstL9rbc2/nKlwYDxXke+WvSMfV1fhQwE7u7/+Id7SYn7scfm/R8WLYoBU6dm/9DgvtVW8d66tXvz5vF52LB4LylxnzMn5p87193Mfeut3d9/Pzv/zJmReeRbvdp93rz4fM017t26uf/sZzHP5MnZ6ZYsadiGjRgR8x5/fP3TVVW5X3ut+yuvVB92xBGRmdblsMPcO3Rw/8tf4vuECe6vvur+zW9mt/WDD+pfb10Z4pw57qec4v61r0UwzLj88ljuZZe5r1iRPWBr1rj/7W/ZjHDZMveePWPawYOrL3vx4ux8Eya4H320+513Vp/moYdiu1asqDv9mW0YOtS9fftYVq7LLov1X3ppvA8alN0vZ5xRd5Cs7bexZIn7eee5//a31feHe+zzAQNiueefX3Pe2bPd+/Vzv+SS6sM/+yybnqefdn/ySfe993Y/6qj4U1x+eQTe+fPrP4656Z4xw/3zz92feML9f/83lv23v9U+/bJlsZ/XFhQrK90ffdR94cK1p0GqUSBYR1dfHXvm7rvrmOC55+JM94or3MeMcS8vjxn23jv7Z+rQId5vvjnmueuu7Ljc0kO3bu477RR/mIw1ayJDad7c/bbb3Nu2jWm33DLezz03prv77gg2Z54ZZ4H1ZVSHHx7z7rBDdh0PPxwZycKF2cwwcwbfrFlkBu7u48bFsD33jO9z58YfN2PWrOz2lJVF0Wq77eLVunWcAWcywbvvjszqj3+svoyrr45tOfjgbADMOP1099LSWMaf/hTDPv3Ufeeds/ulrMy9T58463zwwWymv3ix+8knx/cDDojlPPCA+7vvur/zTgTyE0/M7p/M8m680f3ii93vuy9KgOC+xx7Zbc8966+sjOmOOio7f3m5+y9+ESW4n/40hh1zTOz3Aw7I7o9MgD7wQPfp0yOjPffcCCZ77hknD7vuGiWf1q3d//u/o1STSWufPu6TJsWJyllnxfRdu0Zpc/vt3Y88Mn5vPXtGwNltt+zx/fjj7HG/994YntnW3JMcs3hv2dK9c+f4PnRo7KPM/OPGuZ96aqRh2bIIqCUl2fW1aRPvrVq5X3VVDB8xIn77EyfG9oH7lVdmA/ibb7rfc4/7Oee4jx0bv+/DDovpTjopfieffuq+//7uw4dn55s/P/5PCxbE57qCyxNPuI8aVXdpdl3UdyKzLhYv3vBl1EGBYB1VVcXvdMcdG/j7uO++qB7KnPVAZEY77ZQNEDvumP1DlZbGH3/rrbPTH364+/LlkRn88IcxbPvts+Mz85aXxx909uzIUDJ/1gEDInBkzuQ/+ywyouHD48ywrCy7jKeeymZ8W28dw3/wgzhL7drVfdttI9Nr1SqC3cEHZ9Nx5pmxrO9+NzK1H/4wm4YxY9w7dcoGrszrttuy684EC4j98+WX7kuXRsa3886xDbvuGmkeMMD9ooti+nPOcd9ll8gwHn00u6yvfS3eDz440r3zzjGsffvIiHbfPZvBPPVU9XSVlmaDeGmp+29+k60fzN/nN9/s3qJFZM677x5p6t49tjUTpDp1cv/JT9wrKiK45Abv0lL3KVPi2Myb5/7YY/FDq6pyHzkyMvmSkli3WZTAdt89tnvo0Mjshg7Npv0HP4jMcdttI11t28Z8554bmckf/lD9t9OlSzY9N98c64JshjxkSCzjpZei+vCxxyJD328/97593d9+O/Z9jx5RGunXL+Y/9tgItM2bZ497167+VZVpJuhAlCr69KkZcDp3zgbuzD7//vez40tK4rX//tmAntmu8vJ4lZXF/jrjjOzvK/d1wAHxX/jOd9yPOy7SnUnHoEHuHTu6/+pXse/+9a/Y/3vsEYH01FPjv/nRR9kM4pJL4vf5ySfu118f05aWxvGaOzdKVT//eRzzpUvdX3wxTlLuv9/9n/+M/+czz0TAmjYt/s8PPBDb8vOfRzpuvz1KVb/7XfwuJ02KwLeeFAjWw/33Z/PnRx5ZhxOGgw+OTMA9Du5vfxs/tH79IqPJ/DDvuCOqO3bbLc6scs+aIH5oq1fHWfuf/+y+774x/Kab4swsU7Vw1VWROUNkCNtt537IIXEGWFYWGUVmmYcckv3crFlUVxxxRLb6pl27+CHecEOc5ffvn52+e/fs58zwY47JDisvjz/I2LGR2bRvnx2XXz++//5RMjCLjGbPPWPcSy/FH2ibbeJ75oy/W7fIXK+5JhtA2rd3P/TQ+EM991yUiEaNymZwV1+dPXMePDj25aJFkVmfcELsxwsuiLPOCy90f/zx7DEcOzbOcL/4wv3b344Mwj1b0oAopZx6avzxL7kk/sS5P5KVKyP4Z04KXn21/t/NjBmRUV54Yfzha1NV5f7CCxGcM2e/FRXup50WGeBrr2WnzezzAw6I7Vi9OjKgTDC6664IAL17ZwNZbdWGK1dGJphZ/6pV2c8/+Un85nr0iBLA3LlRrdi7d5T4ZsyIdrUHHojfz/z5kXH+9reRoT30ULYa9dBD4zf33e9Gppo5bhMnRmZ53HEx7JRT4jgOGRIZ+ze/GSdio0bFdjRrFqWSa6+NgHbjjfE7b98+gu1uu0WpuFu32N5OnWKe3P8GxP9m332zQa1nz/htHXJItrQPsczMidhJJ2V/f7mvzMlGQ16Zk8PMMWnZMvvfLimJ0uV6UiBYD2vWxO8nk4/++tcNnHFtDW09e8ZZyBdfxEpWrox5zjnHfeDAOFurrUH4+uvjx7x0aUSp5s3jxzJtWvzJJ0xw//e/40e6xx7xg3/55Zj3vvsi0x07Nqoj7r03/kwZlZVxVvrJJ/Gnz92GhQvjTzVxonuvXpH5L1yYPds766wIaqNHZ+eZNi1e228f6amqiswBIv0Zl14apYm9944dnFnv4sXZOvZly2I/ucc+y5w9Xnpp7fv3/fejjn/58qhLv+CCbObnXnud+7q47LIIvg0xdWqcRRTLTTfF/qhPplQyb97a20AK4ckn43g+91x22KxZcTL13nvV0/nMM9V/P/nefrvuM+ZVq2qvunn//agidI/f8FVXRZtIJuCtWRMlVHD/xjfid3zaaXEic8opMfyvf80u7513IlDffXdUvf3P/0TJ/NFH3f/+9zjxePHFqOJ76KE4ibjttqjyHDkyfuO//338vu+6K04OTz89At5559V9AUAD1BcILMY3HYMHD/bx48dvsvWtWQNHHx2Pthw1Kq7ANNuABf7yl9GT6TXXrNt8VVUx35ZbxvfFi+MS1J12atj8a9ZAWdm6rTPf8uXQrFk87/PDD2NH9O5d9/QPPRQd9f3oR3Gp7cKFsMsuG5aGJ5+EH/4wLs/t1WvDliWNw+zZ0KVLsVNRt6lTYfx4OPHE6sOXLYvLgYcMKdy6V62CLbbYKIsyszfcfXCt4woZCMxsKPAHoBS4w92vzht/InBR8nUJ8BN3r6Mf6LCpAwFEH3R77BFltwsugOuv36SrFxHZYPUFgoLdUGZmpcCfgGFAf+B4M+ufN9lHwIHuPgC4Cri9UOnZEAMHwuuvwwknwI03wt//Hs+7FxHZHBTyzuK9gGnuPt3dVwH3A0fmTuDuY939i+Trq0C3AqZngwweDH/4A3TtCiedBB07wi23FDtVIiIbrpCBoCswM+d7RTKsLmcA/65thJkNN7PxZjZ+3rx5GzGJ66ZDh+ih4eWX41n3F18MZ50VJYQm1tQiIvKVQgaC2ppUa80uzezrRCC4qLbx7n67uw9298EdO3bciElcd1tsEY8tuOWWeITxzTdHCeGAA+CVV4qaNBGR9VLIQFABdM/53g2YnT+RmQ0A7gCOdPcmU/Pevz/85z/Rb9wNN0Sfc4ccEm0In31W7NSJiDRcwa4aMrMy4H3gYGAWMA44wd3fzZmmB/B/wCnuPrYhyy3GVUMN8emn8djjd96JK+EOPxy22SaqjhrzlXEikg5FuWrI3dcAZwNPA+8BD7r7u2Z2ppmdmUx2BdAeuNnMJppZ48vhG6hz57ik+PXX4/L6Bx6Aq6+ORxj8+tcxbs6cYqdSRKQm3VBWAFVV8ViCTz6Biy6CRx6J4WVl0S3/iBHw9NNQXh6POtigG9RERBqgaDeUFUJTCAT5Xn012hDGjo1nk1RVZcd16RJPwzzyyHhK5hFHxM268+dDv37FS7OIbF4UCBqRN9+EceOiZ4iXXoIJE2DSpHiYmVkEiZYt42mGl14aVyNlbmb77LN4YJqIyLpSIGjk3KNLkRUrovuKGTNi2N/+VnPab34z7mreb78oPRx6aAyvqooShYhIbRQImqhHHonSw9e+Fu/z58Njj8EOO0QpYfly2HVXaN0aXnsNhg+PO56rqmDPPWO+Fi2ibaJZs2JvjYgUkwLBZmjlSrjvPrjjjggQ/fvDo49GSaKkpHo7RHl5dNj5f/8X8518MnznO9Ehaf/+cMUV0Wg9YEDRNkdECkyBICVWrYqz/1WrosQwfnxcvTRmDDzxRHSe16VLdKedsc02MHcu9OwZpYvVqyMgDB0K554Lxx8fd1IvWgSDBkVfSyLS9CgQpFxVVVyxtPfeUTqYMQPeeAOmTIHLLoNvfxueegq23Taqlt55JwJCbXbZJYLEMcdEN+2tWsF228G8eVES2WUX6NZouw4USS8FAqnTjBmRkc+dG5exlpXF1UnXXhsB4sEH4zLWIUPiKqennor3VatqX16zZtFTa/PmcTPdoEHQpg38+9/QqVNUUb39dlwi27dvXCElIoWnQCAb1eLFESC6do3SxvLl0KNHBIe77opLYVeujBLDwoUxz1ZbxXy5WrSI+yf22w9mzoxlHXBAdObXq1dUZ82fD5dfHsGkVatNvqkimw0FAikK96hmWro0rmK69daokvrRj+Ipmy+8EF1xLFgQvbq6V6+SKinJtnmYRcP24sXw4x/D7rvDvffCgQfC978fgWb69FjWjjvGlVQikqVAII3WqlXRvrDttpGJT5oUJYVp06Ljvk8/hXvuiUAwYUKUGp59NuZt0SJKIy1bRoP2M8/E8JKSCBrbbhtVUIcfHoFiwQL48su4/2LLLaP6as2auP9C92DI5k6BQDYrkybFVVHf+x5Mngx//GNUVZ17bgSEiROj5DF/fjSIL1pUff4OHeCLL6KNYubMqMb67nejlNG3bzSoL1wYJYuePSOwtG+vPqGkaVMgkM3emjVRjZRv1ap4opxZZOaLF8MFF8DOO0ejdb9+cQntHXfAsmV1L3/77eHoo+NS3LZtozPBzD0b778fVVHHHadgIY2XAoHIWixdGtVOb74ZAaVjx7hbe+HCaLd46KG4L6NXr7iqKr/hG6LRu3//aETv1g1efDEebXr22REwvve9qIJasSIa0nfaKdpGRDYFBQKRjaCqKs74586N50uUl8cNe+3aRbvF/fdHI/jcuTH9FltEW8Tnn8f3nj2jBDJ7djSib7NN3McxdWoEiSFDopqqsjJKK1/7WgSQkkI+R1BSQ4FAZBNauTIauVu0iEx88uT4/sAD8N57UcoYMSIecTpjRs0uQTLfO3aMBu6ePSMA7b9/LLNHj2j8/v73Yeuto9TRs2e2P6mlSyMIlZdv8k2XRkyBQKQRcY+MffZsGD06+n0aPToatlu0iGqn3r3hySfjPdOgPWZMzJcpYbRpE9VYCxbE5333jSAxcmSs48ADoxpql10i8HTpki1huEenhr17RyM5qH1jc6dAILIZ+fLLKAX8/vfZO7nfeCO6EZkyJW7S69Ur+pTaaquoZmrVKto61qyJYNOxY0zbpk20acyaFc+6OOaYuHS3VatYTqYK65BDFCiaOgUCkZSo7eqpqVPjwUa9ekH37nHvxZQpEUDuuSfupxg2LEol06dHg3ZlZfVldO8ez+Vu2zbu25g6FSoqov+qffeNkgVEuwbEY1p33DFKIZWVuk+jMVAgEJFaLVoUpYrmzbN3gm+1VVwZ9dFH0SPt2LFxBdSCBdG+sXx5XIq7ww7xGNa3345585WWRl9T48dHECkvh1NOgXffjT6rdt4Zrrkmgkv//lEqybR3lJVF4FIpZONRIBCRglm8OALHihXZrs97944uQF54Ab71rQg406dHh4Xt2sFRR8WVVp98EsuorRTSs2fcv7HllnElVqbaq6wsOjDcbruY/733olRywAExrrw8SjCffx4PdNp557haa/nyCHppLZ0ULRCY2VDgD0ApcIe7X5033pLxhwPLgFPdfUJ9y1QgEGma3CNzbtcu2+j94IOR0b/9dmT6ffvCxx/HzX1PPx0BYMmSmKdTp2jgbt062kmqqqLhu2vXaFCvS1lZlDrmz4+2j+7dIxhsu20Em9mzo5qrXbsIKGZx5dXAgRHkli+P9fXpE8MXLowruL74IgLLwoUx7YoV2eV06hTv7lFdt2ZNfG7RIj536BDrWb060rJkSayjfftYzoIFsfzOnWP5JSXZu97XV1ECgZmVAu8DhwAVwDjgeHefnDPN4cA5RCDYG/iDu+9d33IVCETSa9GiCByrVsGcOZHhtmgRjeWzZkXGumpVNHi3bRtVUw89FPP17RuX8c6ZE5nxxx9HSaZduyjBzJ4dNxS6R/BYuTK73vxLfDdUWVmUgNYl+23TJp4m+LOfrd866wsEtdyUv9HsBUxz9+lJIu4HjgQm50xzJHC3RzR61czamllnd/+0gOkSkSaqTZt4b9EizuYzBg2KV20yDdgNsXRpBAmIoJF5RkfLlnG2X14eZ/JvvRVBaMWKGDd5crSz9OgRZ/Jz58ZZfWlpzJ9pwF+2LILKZ59lnyVeWRnBrVWrKLW0bBnrbds21rn11jHN888X7qFPhQwEXYHcAlsFcda/tmm6AtUCgZkNB4YD9OjRY6MnVEQEqj/zIhN0Mvr0yX7u3r36uLqC0MZ00kmFW3Yhb16vrb0/vyDUkGlw99vdfbC7D+7YseNGSZyIiIRCBoIKIDdudgNmr8c0IiJSQIUMBOOAvmbWy8y2AI4DHs+b5nHgFAv7AIvUPiAismkVrI3A3deY2dnA08TloyPd/V0zOzMZfyswirhiaBpx+ehphUqPiIjUrpCNxbj7KCKzzx12a85nB84qZBpERKR+6ulcRCTlFAhERFJOgUBEJOWaXKdzZjYP+Hg9Z+8AzN+IySkmbUvjpG1pnLQtsJ2713ojVpMLBBvCzMbX1ddGU6NtaZy0LY2TtqV+qhoSEUk5BQIRkZRLWyC4vdgJ2Ii0LY2TtqVx0rbUI1VtBCIiUlPaSgQiIpJHgUBEJOVSEwjMbKiZTTWzaWZ2cbHTs67MbIaZvWNmE81sfDKsnZk9Y2YfJO9bFzudtTGzkWY218wm5QyrM+1mNiI5TlPN7LDipLp2dWzLlWY2Kzk2E5NHsGbGNcptMbPuZva8mb1nZu+a2XnJ8CZ3XOrZlqZ4XJqb2etm9layLb9Mhhf2uLj7Zv8iej/9EOgNbAG8BfQvdrrWcRtmAB3yhl0LXJx8vhi4ptjprCPtBwB7AJPWlnagf3J8mgG9kuNWWuxtWMu2XAlcWMu0jXZbgM7AHsnn1sTzxfs3xeNSz7Y0xeNiwJbJ53LgNWCfQh+XtJQIvnp+sruvAjLPT27qjgTuSj7fBRxVvKTUzd3HAJ/nDa4r7UcC97v7Snf/iOiifK9Nkc6GqGNb6tJot8XdP3X3CcnnL4H3iMfENrnjUs+21KUxb4u7+5Lka3nycgp8XNISCOp6NnJT4sBoM3sjeYYzQCdPHuSTvG9TtNStu7rS3lSP1dlm9nZSdZQptjeJbTGznsDuxNlnkz4uedsCTfC4mFmpmU0E5gLPuHvBj0taAkGDno3cyO3r7nsAw4CzzOyAYieoQJrisboF6AMMBD4FrkuGN/ptMbMtgUeA8919cX2T1jKssW9Lkzwu7l7p7gOJR/fuZWa71DP5RtmWtASCJv9sZHefnbzPBf5JFP/mmFlngOR9bvFSuM7qSnuTO1buPif581YBfyZbNG/U22Jm5UTG+Xd3/0cyuEkel9q2pakelwx3Xwi8AAylwMclLYGgIc9PbrTMrJWZtc58Bg4FJhHb8INksh8AjxUnheulrrQ/DhxnZs3MrBfQF3i9COlrsMwfNHE0cWygEW+LmRnwF+A9d78+Z1STOy51bUsTPS4dzaxt8rkF8E1gCoU+LsVuJd+ErfGHE1cTfAhcWuz0rGPaexNXBrwFvJtJP9AeeA74IHlvV+y01pH++4ii+WriDOaM+tIOXJocp6nAsGKnvwHb8jfgHeDt5I/ZubFvC7AfUYXwNjAxeR3eFI9LPdvSFI/LAODNJM2TgCuS4QU9LupiQkQk5dJSNSQiInVQIBARSTkFAhGRlFMgEBFJOQUCEZGUUyCQRsvM3Myuy/l+oZlduZGWfaeZ/dfGWNZa1vO9pFfM5wu9rrz1nmpmN23KdUrTpUAgjdlK4Ltm1qHYCcllZqXrMPkZwE/d/euFSo/IhlIgkMZsDfF81gvyR+Sf0ZvZkuT9IDN70cweNLP3zexqMzsx6eP9HTPrk7OYb5rZS8l030rmLzWz35nZuKSzsh/nLPd5M7uXuEkpPz3HJ8ufZGbXJMOuIG52utXMflfLPL/IWU+m3/meZjbFzO5Khj9sZi2TcQeb2ZvJekaaWbNk+J5mNjbpw/71zF3oQBczeyrpw/7anO27M0nnO2ZWY99K+pQVOwEia/En4O1MRtZAuwE7Ed1FTwfucPe9LB5Ycg5wfjJdT+BAomOy581se+AUYJG775lktP8xs9HJ9HsBu3h09/sVM+sCXAMMAr4geok9yt1/ZWbfIPrEH583z6FEdwB7ER2HPZ50JPgJ0A84w93/Y2YjgZ8m1Tx3Age7+/tmdjfwEzO7GXgA+L67jzOzrYDlyWoGEj1xrgSmmtkfiV4ru7r7Lkk62q7DfpXNlEoE0qh59CJ5N3DuOsw2zqOP+pXErfeZjPwdIvPPeNDdq9z9AyJg7Ej043SKRTfArxG39vdNpn89Pwgk9gRecPd57r4G+DvxAJv6HJq83gQmJOvOrGemu/8n+XwPUaroB3zk7u8nw+9K1tEP+NTdx0HsryQNAM+5+yJ3XwFMBrZLtrO3mf3RzIYC9fU4KimhEoE0BTcQmeVfc4atITmRSTod2yJn3Mqcz1U536uo/pvP71/FibPzc9z96dwRZnYQsLSO9NXWFfDaGPBbd78tbz0960lXXcupq5+Y3P1QCZS5+xdmthtwGHAWcCxw+rolXTY3KhFIo+funwMPEg2vGTOIqhiIpzSVr8eiv2dmJUm7QW+i066niSqXcgAz2yHp8bU+rwEHmlmHpCH5eODFtczzNHC6RR/6mFlXM8s8bKSHmQ1JPh8PvEz0QNkzqb4CODlZxxSiLWDPZDmtzazOE7yk4b3E3R8BLiceuykppxKBNBXXAWfnfP8z8JiZvU70xljX2Xp9phKZaSfgTHdfYWZ3ENVHE5KSxjzW8ghQd//UzEYAzxNn6KPcvd4uwd19tJntBLwSq2EJcBJx5v4e8AMzu43obfKWJG2nAQ8lGf044FZ3X2Vm3wf+aNFt8XKi6+K6dAX+amaZk8AR9aVT0kG9j4o0IknV0BOZxlyRTUFVQyIiKacSgYhIyqlEICKScgoEIiIpp0AgIpJyCgQiIimnQCAiknL/H+XsPiJs5/+wAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : RandAugment\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eiY3bTlWipW",
        "outputId": "af16c04e-e96d-4543-97df-9fc85c1c2618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_loss_list_rand = [1.3079642264985134, 0.41276279398742405, 0.3641065050996739, 0.33621111706020385, 0.32227574123276603, 0.3105190498880578, 0.2985378698645245, 0.2845963137703859, 0.27256943354115576, 0.2628671395867498, 0.25574022410361746, 0.24721774489172105, 0.23312961429276763, 0.2307181321791194, 0.22826595868925414, 0.21507334950453222, 0.21533063833468005, 0.20384218360027323, 0.20217743917774703, 0.2004433953503606, 0.18824751913385987, 0.1920065844285133, 0.187250267825314, 0.18458147579173084, 0.17769597254713698, 0.17646904064589722, 0.17025284326779164, 0.17242334513523713, 0.16955551344568168, 0.1653165416766796, 0.16184331506975297, 0.16270417981556437, 0.15991212547754208, 0.15523525618198442, 0.15527952850107254, 0.158528862697726, 0.15280010509822103, 0.15449475903861568, 0.15078019666760595, 0.15182626324132853, 0.1501983254340283, 0.1461215344810389, 0.14986899500172635, 0.14550365379892874, 0.1403558284175509, 0.14542140672361947, 0.1405470239094441, 0.14365218938669053, 0.13810036563110062, 0.14059043776617464, 0.13908706698819065, 0.13695130306409625, 0.1333991897300007, 0.1321409744359452, 0.13560862030561377, 0.1312812770133823, 0.13144924573923353, 0.13186921439257257, 0.1303779829020907, 0.12855140429727108, 0.12881272610751432, 0.12862061339160452, 0.12293643534668093, 0.12759860502530726, 0.12760409868264264, 0.12412538664127754, 0.12261246541025354, 0.12362489930598879, 0.11684692019046483, 0.12635664669115368, 0.11725350842889408, 0.1202228733800291, 0.11977898440043616, 0.11819690775863201, 0.11406317815124019, 0.12160748152975952, 0.1135327710257717, 0.11757346191178493, 0.11692589301827962, 0.11527446912838837, 0.11390696881909357, 0.11128266612547362, 0.1084120699525041, 0.11216231442604285, 0.10896211025383333, 0.1105140705963945, 0.11013614156139576, 0.11042292131146361, 0.1066427088511709, 0.10727387718190022, 0.10637671745768408, 0.10528922357614125, 0.10317711308497562, 0.10493126684209196, 0.10703022915839665, 0.10020091456328305, 0.10443359014747265, 0.10327813833911567, 0.10253999184887745, 0.10005321747151413, 0.09775119930818717, 0.09895929236590943, 0.09956198874457177, 0.09663621041176684, 0.09820591854966348, 0.0958199133102083, 0.09834191584227693, 0.10007545721734847, 0.09484737071645293, 0.09604210347482343, 0.09365542412440224, 0.09515454136037084, 0.0915899477939457, 0.08897347942620597, 0.09372411305867237, 0.09134252944855185, 0.08858357435016613, 0.08518513028073278, 0.08997638049083674, 0.09038671507906461, 0.08876956635101782, 0.08737949878330718, 0.08710510870769902, 0.08606145591512929, 0.0835256307994124, 0.08409009710317705, 0.08514642417632791, 0.08500815152889749, 0.07965256468592298, 0.08329004821365038, 0.07942846951738687, 0.08278998299463977, 0.08198488589507455, 0.07826127050075993, 0.07908188051930289, 0.076796955444161, 0.07810755274674999, 0.07818539347499609, 0.07601612444643971, 0.07647686129875057, 0.07603131658915016, 0.07302434632913045, 0.07223893472354426, 0.07550516255391726, 0.07080602620634929, 0.07053892541585899, 0.07113152286315433, 0.07030726358708408, 0.07296305139193206, 0.0674845933848563, 0.0666364010973634, 0.06634756198864643, 0.06508453956080808, 0.06460393779358364, 0.06859736059486947, 0.06357380861875811, 0.06576774500532526, 0.06143479931816903, 0.06113883249491938, 0.05898804291527526, 0.06436446549870618, 0.062575965680606, 0.05618904328710705, 0.06056681182614066, 0.057390773533856076, 0.055834507522978354, 0.05655236725839294, 0.05552395822626708, 0.058229990793154815, 0.05438301761743176, 0.05628294004304057, 0.05478681649300989, 0.05533521129878435, 0.052947258238546895, 0.0534550957836029, 0.052291425183506275, 0.052318296121398045, 0.04810188965604396, 0.04881041513397522, 0.05085457057558344, 0.04860204716054041, 0.04748570773344194, 0.04570310991030277, 0.050219918913548676, 0.04907680233577039, 0.046715650073982026, 0.0483139068832202, 0.0456187082542899, 0.039992414365666425, 0.04334353352707576, 0.04216374684476008, 0.042458749894900776, 0.04058421820687347, 0.04625414172692229, 0.03840240793173936, 0.04297558541174948, 0.0419943235463748, 0.039768285462557056, 0.037575757089335984, 0.04107870026378006, 0.03676055792433924, 0.03773308889296784, 0.03924703417941524, 0.03560067948615204, 0.035394394668046295, 0.034579224325464106, 0.034057870963436924, 0.034051362065431794, 0.036438416197471984, 0.034808342434657705, 0.03285826306012749, 0.03179724503189911, 0.03259620334458384, 0.03070403641180658, 0.030086495885802155, 0.0316626976205031, 0.03077701206708978, 0.026507464029288632, 0.030121172760641024, 0.028750687549244057, 0.029356615014656934, 0.025606725985805195, 0.026744502662809762, 0.029752478933878, 0.028448736762342757, 0.02331312912987005, 0.024699779668429154, 0.022614457626854823, 0.023837428991455675, 0.02521528576773296, 0.023793862451771453, 0.024236088944220444, 0.021694000673778153, 0.021232543444717783, 0.02092921472063697, 0.021585240682491118, 0.023119772801132806, 0.01890946720092025, 0.0209501470829711, 0.019511741401153165, 0.018914120605588967, 0.02016598089796246, 0.021338878327231036, 0.01794326564027022, 0.017943615567060263, 0.01755146857011599, 0.01759808525054395, 0.016239320796215276, 0.018075850542677633, 0.019246497568182225, 0.015080467639124994, 0.016554365190424955, 0.017023207680991064, 0.01558076386462372, 0.0157793698015908, 0.015399959696164017, 0.014076877912559464, 0.013944049486650664, 0.015304512502676063, 0.015483319247015286, 0.014038200535607047, 0.012375226085008625, 0.014426614435485546, 0.011954178459132166, 0.012934604996618458, 0.012563755948078846, 0.013720255432181, 0.012539297958449802, 0.012156369318544542, 0.0137886715602372, 0.010996153222921943, 0.012231470978513442, 0.011482611671153223, 0.012633665982850885, 0.0108387998392336, 0.01094505579263189, 0.012128652638798644, 0.011514820721313398, 0.011337356862415203, 0.01035400173881755, 0.010573380134891223, 0.01104129131234738, 0.011334962008059085, 0.010769810515574525, 0.010359876090660691, 0.010690325047684515, 0.009734501771636428, 0.010143442243321225, 0.010232979197024815, 0.010022813219231298, 0.010693188950729867, 0.010847516659505887, 0.009985776471894053, 0.010729328628886885, 0.011304906725903676, 0.011516948485163957, 0.010286911307117298, 0.011423589299851902, 0.010949008481963196, 0.008929194090320024]\n",
            "train_acc_list_rand = [54.77395447326628, 87.3943885653785, 88.9655902593965, 89.9142403388036, 90.33986236103759, 90.63419798835362, 91.13393329804128, 91.6209634727369, 91.94706193753309, 92.18634197988354, 92.53149814716781, 92.69878242456326, 93.09052408681842, 93.29380624669137, 93.34886183165696, 93.76813128639492, 93.82742191635786, 94.04552673372154, 94.03705664372684, 94.13658020116463, 94.56855479089465, 94.44997353096876, 94.61725780836422, 94.51773425092641, 94.82265749073584, 94.97723663313923, 95.03652726310217, 95.01111699311805, 95.08099523557438, 95.28215987294865, 95.36897829539438, 95.29062996294336, 95.37321334039174, 95.57861302276336, 95.57649550026468, 95.43038644785601, 95.65272631021705, 95.46426680783483, 95.62308099523557, 95.6019057702488, 95.74166225516146, 95.8009528851244, 95.74166225516146, 95.8369507676019, 95.97035468501853, 95.8094229751191, 95.88777130757015, 95.784012705135, 95.9915299100053, 95.86024351508735, 95.95341450502912, 96.09952355743779, 96.12493382742191, 96.19481206987824, 95.99576495500264, 96.2710428798306, 96.1990471148756, 96.22022233986236, 96.10799364743251, 96.27527792482795, 96.24563260984648, 96.28163049232398, 96.47432503970354, 96.18845950238222, 96.31974589730017, 96.4065643197459, 96.3980942297512, 96.42773954473266, 96.70301746956062, 96.21386977236634, 96.51879301217575, 96.52302805717311, 96.52302805717311, 96.54843832715723, 96.70513499205929, 96.4700899947062, 96.67548967707782, 96.54843832715723, 96.63949179460032, 96.66913710958178, 96.67125463208046, 96.68395976707252, 96.8131286394918, 96.74748544203283, 96.79407093700371, 96.76230809952355, 96.85971413446268, 96.74536791953415, 96.80465854949709, 96.81736368448915, 96.89782953943886, 96.86818422445738, 96.99735309687665, 96.94229751191106, 96.84489147697194, 97.12228692429856, 96.83853890947591, 96.91688724192694, 97.01852832186341, 97.09687665431446, 97.10322922181048, 97.1011116993118, 97.13499205929062, 97.07358390682901, 97.17734250926416, 97.20275277924829, 97.03335097935415, 97.0206458443621, 97.13922710428798, 97.18581259925887, 97.27898358920064, 97.2006352567496, 97.28533615669666, 97.35733192165166, 97.23875066172577, 97.30227633668608, 97.41238750661726, 97.5796717840127, 97.3276866066702, 97.28110111169931, 97.41238750661726, 97.3996823716252, 97.3446267866596, 97.4356802541027, 97.56484912652196, 97.5076760190577, 97.41662255161461, 97.47379565907887, 97.59025939650608, 97.57543673901536, 97.66649020645845, 97.499205929063, 97.53943885653786, 97.61778718898888, 97.63472736897829, 97.73213340391742, 97.66013763896241, 97.70248808893595, 97.77448385389094, 97.76601376389624, 97.68554790894653, 97.8147167813658, 97.87188988883007, 97.7427210164108, 97.86341979883537, 97.89306511381683, 97.8507146638433, 97.95235574377978, 97.78930651138168, 97.9777660137639, 97.94600317628375, 98.03282159872948, 98.10058231868713, 98.04552673372154, 97.90577024880889, 98.07728957120169, 97.9502382212811, 98.21492853361568, 98.12175754367391, 98.23610375860244, 98.08999470619375, 98.06670195870831, 98.34833245103229, 98.18951826363156, 98.2657490735839, 98.31021704605611, 98.25304393859184, 98.29115934356803, 98.26151402858656, 98.35256749602965, 98.27845420857597, 98.39491794600318, 98.38009528851244, 98.47961884595024, 98.3928004235045, 98.40762308099524, 98.48173636844892, 98.50291159343568, 98.53890947591319, 98.44362096347274, 98.55584965590259, 98.53679195341451, 98.56643726839597, 98.48385389094759, 98.54949708840657, 98.54737956590789, 98.5283218634198, 98.65113816834304, 98.79724722075171, 98.68501852832186, 98.74642668078349, 98.72525145579672, 98.79089465325569, 98.62572789835892, 98.85653785071466, 98.74007411328745, 98.74219163578613, 98.8798305982001, 98.9348861831657, 98.75277924827951, 98.93700370566437, 98.9073583906829, 98.85865537321334, 98.94970884065643, 98.93700370566437, 98.9518263631551, 98.96876654314453, 99.05770248808894, 98.89465325569084, 99.02170460561143, 99.05558496559026, 99.0619375330863, 99.05770248808894, 99.12546320804658, 99.12969825304394, 99.07464266807835, 99.08311275807306, 99.26521969295923, 99.16146109052409, 99.17628374801482, 99.1508734780307, 99.22710428798305, 99.23980942297511, 99.1233456855479, 99.18051879301217, 99.35415563790366, 99.28427739544733, 99.36474325039704, 99.31180518793012, 99.2779248279513, 99.28215987294865, 99.32451032292218, 99.33721545791424, 99.39015352038115, 99.46426680783483, 99.40709370037057, 99.33086289041822, 99.49179460031763, 99.41132874536792, 99.43885653785071, 99.45579671784013, 99.45579671784013, 99.42403388035999, 99.48755955532027, 99.48755955532027, 99.4854420328216, 99.50238221281101, 99.52355743779778, 99.48332451032292, 99.44944415034409, 99.6019057702488, 99.5214399152991, 99.5489677077819, 99.61461090524087, 99.55320275277924, 99.6019057702488, 99.62308099523557, 99.6209634727369, 99.60402329274748, 99.5934356802541, 99.61037586024352, 99.67178401270513, 99.60825833774484, 99.65907887771307, 99.65272631021705, 99.65484383271573, 99.62731604023293, 99.67601905770249, 99.69084171519323, 99.64213869772367, 99.69931180518793, 99.68237162519851, 99.70354685018528, 99.65060878771837, 99.71201694017999, 99.72260455267337, 99.69507676019057, 99.68025410269983, 99.71836950767602, 99.76283748014822, 99.7480148226575, 99.70566437268396, 99.69084171519323, 99.7289571201694, 99.7289571201694, 99.74589730015882, 99.75013234515616, 99.75860243515088, 99.73319216516676, 99.73319216516676, 99.71625198517734, 99.72260455267337, 99.76071995764956, 99.73319216516676, 99.70989941768131, 99.7204870301747, 99.73107464266808, 99.69084171519323, 99.72472207517205, 99.79036527263102]\n",
            "test_loss_list_rand = [1.2242924027583177, 0.4766860316489257, 0.4852036163210869, 0.4449080118507731, 0.3844677253681071, 0.3908769353201576, 0.33308059773316573, 0.3329603055528566, 0.3105304033002433, 0.2816346858208086, 0.2945175337806052, 0.277948475059341, 0.3032138998455861, 0.2831221893429756, 0.27575827159864064, 0.26203144776324433, 0.31333797563816984, 0.26917517349562226, 0.253851704628152, 0.2555090201032512, 0.2524060876492192, 0.2693595473161515, 0.2468755625042261, 0.24303570393399865, 0.29847537346330344, 0.23665455542504787, 0.24601385775296128, 0.26269593287040205, 0.241777080385124, 0.2399024426498834, 0.2372869025258457, 0.238308228114072, 0.2611099873468572, 0.2243046425100343, 0.24302090302694077, 0.2619777427730607, 0.23600055122127137, 0.2472808970247998, 0.23857459320011093, 0.23040143489910692, 0.23448149746685637, 0.23396167033078039, 0.23945477546430102, 0.24786251283013352, 0.24238866938314602, 0.23962249183187298, 0.25362784189044263, 0.22922139680560896, 0.2559051115211903, 0.23962331296620415, 0.23080364543506326, 0.22837937785787324, 0.247460591625057, 0.24376308035982006, 0.2347778041353997, 0.24077762334662325, 0.23032311426804347, 0.23188659777024798, 0.23087097901631803, 0.2372709251575026, 0.23744144363730563, 0.2374997875634946, 0.2297166377744254, 0.26560663293097536, 0.23270482321580252, 0.23472226854852019, 0.23707742974454282, 0.24279768688275533, 0.23788784751120737, 0.2408738124239094, 0.23345124054992317, 0.23940513425451868, 0.23605135612774128, 0.25073060765862465, 0.24483406474339028, 0.2369142651704012, 0.23664433430588128, 0.23283696320711397, 0.2526469752009885, 0.2293763486303243, 0.2379093081051228, 0.24137731407787286, 0.22815602656234713, 0.23056939514536484, 0.23905373445036365, 0.23256402484634342, 0.2537045240511789, 0.25104906607200117, 0.24098055192506782, 0.23426727585348428, 0.24968813399912096, 0.2255474401491822, 0.25570899116642337, 0.2658700168351917, 0.22768254453937212, 0.24381475685639123, 0.2371177078023845, 0.24363475305703924, 0.23996025838834398, 0.2301647606141427, 0.2312738028373204, 0.23250958069647645, 0.2470913831436751, 0.24546197549823454, 0.23853133864445136, 0.23482449931622135, 0.23961933846494146, 0.2363752949453306, 0.2366328848452837, 0.25023742631881263, 0.2523412738433656, 0.23386995731761642, 0.2235797121207796, 0.24124343740735568, 0.23789687029213882, 0.25894979935358553, 0.2407712943705858, 0.2473073466041801, 0.24556619305090577, 0.23585436084106856, 0.23382639826512805, 0.24988360268374285, 0.24265338982656307, 0.2321539395762717, 0.24257982144241824, 0.2509613745827593, 0.264335074900266, 0.2360795667102816, 0.24269597353778927, 0.2401655425920206, 0.2393073042964234, 0.24763254971042567, 0.23858816633183583, 0.2453654737704817, 0.24818577219312096, 0.2519976277379136, 0.24066407218867658, 0.24328664805301847, 0.2436335614754581, 0.23726862157676734, 0.2504358967291374, 0.24724281604821777, 0.24097077188757704, 0.23132461413521976, 0.24096911921001532, 0.23906312499414473, 0.23964464410628175, 0.2407097773553402, 0.24008852922741106, 0.24373903726318888, 0.24455829663202167, 0.25426618283724084, 0.24766666304283574, 0.24456659613140658, 0.23325322789377442, 0.2446374527050876, 0.24081618506826608, 0.2533364076824749, 0.2727439739083981, 0.2566343823301734, 0.24697071872651577, 0.25414097462506857, 0.25726331458153096, 0.24916001133547694, 0.24456613417714834, 0.24002879870799826, 0.23932889017148637, 0.24253216307318093, 0.24666448398584537, 0.24409404376923455, 0.2470602400820045, 0.24511258984806344, 0.24469052970993752, 0.24681322200808162, 0.2418613201737696, 0.2395190042717492, 0.25079511499543694, 0.24951477871075564, 0.24402216294159493, 0.23870123306051919, 0.23937643794160263, 0.2437281411360292, 0.2480512512665169, 0.25320648938855705, 0.2562090756517707, 0.23295449858129608, 0.24561562255828404, 0.24209332596693262, 0.2466022344853948, 0.24022627408232758, 0.24576352584157504, 0.25353608178157433, 0.2395598959068165, 0.24042998858745776, 0.23662985444945447, 0.23703962266810386, 0.235718719916893, 0.2613097871562429, 0.2468722050863446, 0.24700676372238234, 0.2492630069929303, 0.2531062271811214, 0.2422399223946473, 0.2507193155732809, 0.24836585325572422, 0.2427369980506745, 0.24839953073829996, 0.24479242154926645, 0.24397121939588995, 0.23672433973600468, 0.23719421193441925, 0.2379821332938531, 0.24366306029625384, 0.24493778684158243, 0.2369944912600605, 0.24019140792170576, 0.2482641932152796, 0.23967477823516317, 0.24958386357791504, 0.2367109964492128, 0.25533891494805905, 0.24639802287314452, 0.23953964434308456, 0.24597930709155752, 0.24053751825190642, 0.24633424845980664, 0.22887894597050606, 0.24372562420937946, 0.2494921569949856, 0.2383151944525832, 0.24101026823707655, 0.23164858017116785, 0.24176706885005914, 0.24208627506981, 0.24435219391449994, 0.23975776935763218, 0.23425774537392108, 0.24447299043337503, 0.24938677952570074, 0.23728075723949016, 0.24136556608273702, 0.2378267114982009, 0.24595940985954276, 0.24221335073896483, 0.23467198031607503, 0.23794158814730598, 0.2411233695023054, 0.23995255867895834, 0.2407341397221328, 0.24021167078000658, 0.2400192185305059, 0.23594458068848825, 0.23809927128547548, 0.2379592138470388, 0.2380814891052889, 0.23737910618165545, 0.24371199371001007, 0.23417508491661912, 0.24041478861780727, 0.2377036967333041, 0.2333276258671985, 0.2338991296824579, 0.23701730861748552, 0.23940507240374298, 0.2372228802893968, 0.23878907943235747, 0.22658240720720998, 0.23505923559195271, 0.23041910179616773, 0.23255765283315935, 0.23369695666227855, 0.23989456293501837, 0.2277499797626161, 0.23583768852347253, 0.2377351895339933, 0.24077455684378304, 0.2367346158853787, 0.23311336334867805, 0.2341520840064714, 0.23782339239693887, 0.233605924838533, 0.23291249989586718, 0.23535046233868628, 0.2341424756473405, 0.23586027203219048, 0.2356172766098205, 0.2329787541655641, 0.23569418476693624, 0.23289725104091213, 0.23461141331376983, 0.2339027374434997, 0.23413826437557445, 0.24099776943615986, 0.23312585029349314, 0.23334580722867565, 0.23665643463312996, 0.22950803500819295, 0.23740327323549518, 0.23103500169464478, 0.23477486165368236]\n",
            "test_acc_list_rand = [63.2759680393362, 85.48325138291334, 85.77519975414874, 86.6740934234788, 88.42962507682851, 88.21834665027659, 89.89320835894284, 90.08143822987093, 90.69606637984019, 91.61032575291948, 91.36447449293178, 91.82544560540873, 91.16856177012907, 91.79471419791027, 91.80623847572218, 92.490012292563, 91.03027043638599, 92.30946527350953, 92.62062077443147, 92.64751075599263, 92.9240934234788, 92.35556238475722, 93.00092194222495, 93.06622618315919, 91.64105716041794, 93.3082360172096, 93.09311616472034, 92.47080516287646, 93.21988322065151, 93.03549477566072, 93.44652735095268, 93.35049170251997, 92.45159803318992, 93.64244007375538, 93.43884449907806, 92.70129071911494, 93.63475722188076, 93.02397049784881, 93.28902888752305, 93.3620159803319, 93.28518746158574, 93.45036877688999, 93.42732022126613, 93.21220036877689, 93.40043023970497, 93.38506453595575, 92.9740319606638, 93.5118315918869, 92.77043638598647, 93.39274738783037, 93.73079287031346, 93.69237861094038, 93.28134603564843, 93.27366318377382, 93.5118315918869, 93.5118315918869, 93.75, 93.76152427781193, 93.63475722188076, 93.39658881376766, 93.48878303626306, 93.58097725875845, 93.56945298094652, 92.59757221880763, 93.69622003687769, 93.58866011063307, 93.65012292563, 93.31207744314689, 93.60018438844499, 93.53103872157345, 93.71926859250154, 93.6539643515673, 93.700061462815, 93.39658881376766, 93.33512599877075, 93.69622003687769, 93.67701290719116, 93.799938537185, 93.22756607252612, 93.78841425937308, 93.72695144437616, 93.52719729563614, 93.9420712968654, 93.81146281499693, 93.66933005531654, 93.82298709280884, 93.21988322065151, 93.3159188690842, 93.63859864781807, 93.75768285187462, 93.33512599877075, 93.93054701905348, 93.02397049784881, 92.98555623847572, 93.98432698217579, 93.37738168408113, 93.91902274124155, 93.76152427781193, 93.75384142593731, 93.91518131530424, 94.01505838967425, 93.8959741856177, 93.57329440688383, 93.56945298094652, 93.87292562999386, 93.89981561155501, 93.58866011063307, 93.76536570374923, 93.66548862937923, 93.4618930547019, 93.4618930547019, 93.86140135218193, 94.14950829748003, 93.92286416717886, 93.79609711124769, 93.34665027658266, 93.74231714812538, 93.71542716656423, 93.64244007375538, 93.83451137062077, 93.98048555623848, 93.44268592501537, 93.80762138905962, 94.0419483712354, 93.68085433312845, 93.4580516287646, 93.49646588813768, 94.05731407498463, 93.65012292563, 93.66548862937923, 93.92670559311617, 93.9958512599877, 93.92286416717886, 93.85371850030731, 93.79609711124769, 93.799938537185, 93.93054701905348, 93.57329440688383, 93.84987707437, 94.07652120467118, 93.68469575906576, 93.82682851874615, 94.20328826060233, 94.23786109403811, 93.88444990780577, 93.99969268592501, 94.03426551936079, 94.0918869084204, 93.93438844499079, 93.88060848186846, 93.8421942224954, 94.11109403810694, 94.03042409342348, 94.19560540872772, 94.16103257529196, 94.05731407498463, 94.11877688998156, 93.8921327596804, 93.18146896127843, 93.98048555623848, 94.03426551936079, 93.76920712968654, 93.75384142593731, 94.13414259373079, 93.8959741856177, 94.26859250153657, 94.16487400122925, 94.00737553779963, 94.11493546404425, 94.1418254456054, 94.01121696373694, 94.25706822372464, 94.1840811309158, 94.13030116779349, 94.25322679778733, 94.21865396435157, 94.07652120467118, 94.10725261216963, 94.31468961278426, 94.26859250153657, 94.34157959434542, 94.38383527965581, 94.1418254456054, 93.93054701905348, 93.98432698217579, 94.42609096496619, 94.0918869084204, 94.37231100184388, 94.15334972341734, 94.1917639827904, 94.07267977873387, 94.1379840196681, 94.41456668715428, 94.21097111247695, 94.42609096496619, 94.23786109403811, 94.44913952059004, 93.75384142593731, 94.24554394591273, 94.34926244622004, 94.17639827904118, 94.04578979717272, 94.41456668715428, 94.31853103872157, 94.29164105716042, 94.42224953902888, 94.16871542716656, 94.4798709280885, 94.37231100184388, 94.55285802089736, 94.57974800245852, 94.4721880762139, 94.40688383527966, 94.3761524277812, 94.57974800245852, 94.49523663183774, 94.39151813153042, 94.64505224339274, 94.31084818684695, 94.5720651505839, 94.06499692685925, 94.4721880762139, 94.48371235402581, 94.41456668715428, 94.51828518746159, 94.46450522433928, 94.75645359557468, 94.60663798401967, 94.35310387215735, 94.68730792870313, 94.42224953902888, 94.80639213275968, 94.49139520590043, 94.61047940995698, 94.52980946527352, 94.69114935464044, 94.69114935464044, 94.52980946527352, 94.4299323909035, 94.68730792870313, 94.68346650276582, 94.68346650276582, 94.72572218807622, 94.71419791026429, 94.8140749846343, 94.75645359557468, 94.71035648432698, 94.68730792870313, 94.71419791026429, 94.7180393362016, 94.76029502151198, 94.74492931776275, 94.63352796558083, 94.8140749846343, 94.77950215119853, 94.77950215119853, 94.63352796558083, 94.72572218807622, 94.76413644744929, 94.74877074370006, 94.86785494775661, 94.9139520590043, 94.89090350338046, 94.7180393362016, 94.82944068838353, 94.87553779963122, 95.14059618930547, 94.8179164105716, 94.97157344806392, 94.90242778119237, 94.77950215119853, 94.79486785494775, 95.059926244622, 94.89474492931777, 94.88322065150584, 94.65273509526736, 94.87553779963122, 94.9562077443147, 95.0560848186847, 94.85633066994468, 95.02535341118623, 94.94468346650277, 94.94468346650277, 94.89090350338046, 94.99078057775046, 94.9139520590043, 95.08297480024585, 94.8640135218193, 95.009987707437, 94.85633066994468, 94.92931776275353, 94.91779348494161, 94.89474492931777, 95.03303626306085, 95.01382913337432, 94.8640135218193, 95.059926244622, 94.91011063306699, 95.02535341118623, 94.93315918869084]\n"
          ]
        }
      ],
      "source": [
        "print(f\"train_loss_list_rand = {train_loss_list}\") \n",
        "print(f\"train_acc_list_rand = {train_acc_list}\")\n",
        "print(f\"test_loss_list_rand = {test_loss_list}\")\n",
        "print(f\"test_acc_list_rand = {test_acc_list}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "2d35a857fdeedecb30594b1c7eb95a8c0480700735195f416faf3d51f501baa5"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "012c4fccfa124eb294393069e02f09d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02696ee9f9ce4417ae638d5844c2ec73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1878c24981c347adb95e0e89c72fe9f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d96196040e641699e8a78db79cf7b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3c542fe55a24623ba09e0f6cbcf6857",
              "IPY_MODEL_dcc6414fe83041d8b5ebd3934cb1ca90",
              "IPY_MODEL_92dff67c5c9049298e504f042450a0fc"
            ],
            "layout": "IPY_MODEL_e13f87e33fad4a1e95fb438dd982ee13"
          }
        },
        "5ae2aba4cc914176a5ccfc663eb6310a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcc36657b3d44ea8a74b5ee30f4677d9",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d563a2f339ec4d0e896caf5b765d2be5",
            "value": 64275384
          }
        },
        "5c3384e0e4044a3d82e3e66f3682220a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "722efa37812c450c8997c8a0b916a30c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02696ee9f9ce4417ae638d5844c2ec73",
            "placeholder": "",
            "style": "IPY_MODEL_5c3384e0e4044a3d82e3e66f3682220a",
            "value": " 64275384/64275384 [00:06&lt;00:00, 20759009.68it/s]"
          }
        },
        "78da9a8381fe41d6b4341e3603daf12e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84d2ff23e4634816b1c3291e34c4661b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92dff67c5c9049298e504f042450a0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d2ff23e4634816b1c3291e34c4661b",
            "placeholder": "",
            "style": "IPY_MODEL_1878c24981c347adb95e0e89c72fe9f7",
            "value": " 182040794/182040794 [00:11&lt;00:00, 21134256.13it/s]"
          }
        },
        "969dedb7d7e74432919aaee552cc4cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a57bfb0e83374f4493cc9062e20a718b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc3ee902af6e47b7b518a1aabaf7ba85",
            "placeholder": "",
            "style": "IPY_MODEL_b19fb9dad8774b1c8161b72430accf56",
            "value": "100%"
          }
        },
        "a7a1a99f011e4610ad5ac35d32b324fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a57bfb0e83374f4493cc9062e20a718b",
              "IPY_MODEL_5ae2aba4cc914176a5ccfc663eb6310a",
              "IPY_MODEL_722efa37812c450c8997c8a0b916a30c"
            ],
            "layout": "IPY_MODEL_f51e75cc2ff347969ebd105e903a8952"
          }
        },
        "b19fb9dad8774b1c8161b72430accf56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc3ee902af6e47b7b518a1aabaf7ba85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcc36657b3d44ea8a74b5ee30f4677d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3c542fe55a24623ba09e0f6cbcf6857": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa88d9d680484321a6889b511696efff",
            "placeholder": "",
            "style": "IPY_MODEL_78da9a8381fe41d6b4341e3603daf12e",
            "value": "100%"
          }
        },
        "d563a2f339ec4d0e896caf5b765d2be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dcc6414fe83041d8b5ebd3934cb1ca90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_012c4fccfa124eb294393069e02f09d2",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_969dedb7d7e74432919aaee552cc4cde",
            "value": 182040794
          }
        },
        "e13f87e33fad4a1e95fb438dd982ee13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f51e75cc2ff347969ebd105e903a8952": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa88d9d680484321a6889b511696efff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}