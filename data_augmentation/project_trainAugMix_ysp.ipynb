{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3478b1e088f442cabae7a6b4c6821527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_790dc1b2808e4771981d53629e2d7f15",
              "IPY_MODEL_ba371195279c49c0a1d9867f61e96398",
              "IPY_MODEL_7e1fc76296324f2e8f5774432ce80f81"
            ],
            "layout": "IPY_MODEL_8ba648f26ed245e091c678223e4e004b"
          }
        },
        "790dc1b2808e4771981d53629e2d7f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4698fbe4e7a34d1f835e9970e083abf2",
            "placeholder": "​",
            "style": "IPY_MODEL_931da73ca0ce462397a689ba5a94e8f9",
            "value": "100%"
          }
        },
        "ba371195279c49c0a1d9867f61e96398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6fe69e220bc4bc58799b15de72ee6d5",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19e2f3b37ce941ccba53f5c0a87a69fa",
            "value": 182040794
          }
        },
        "7e1fc76296324f2e8f5774432ce80f81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2566522176a7428497730d0eed97b549",
            "placeholder": "​",
            "style": "IPY_MODEL_3aa6d2217abe46d9b611fee7bc0508fd",
            "value": " 182040794/182040794 [02:04&lt;00:00, 4371759.74it/s]"
          }
        },
        "8ba648f26ed245e091c678223e4e004b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4698fbe4e7a34d1f835e9970e083abf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "931da73ca0ce462397a689ba5a94e8f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6fe69e220bc4bc58799b15de72ee6d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e2f3b37ce941ccba53f5c0a87a69fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2566522176a7428497730d0eed97b549": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aa6d2217abe46d9b611fee7bc0508fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "652c6298a112438eb7f24360306aeebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09ed0d0e88c34fd2a1ef562b2af4373f",
              "IPY_MODEL_3040997ba93f429d96b366bca05d6a2e",
              "IPY_MODEL_fa1c89356bae414e9e2786735afee0b2"
            ],
            "layout": "IPY_MODEL_a2b314a6c8114bd493aab9d0f4df29f3"
          }
        },
        "09ed0d0e88c34fd2a1ef562b2af4373f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58281269f59644cdaa550a1cc512418a",
            "placeholder": "​",
            "style": "IPY_MODEL_8cddc08c72e540c8953dc6e3035a324c",
            "value": "100%"
          }
        },
        "3040997ba93f429d96b366bca05d6a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a64f8470aee4f1dba402acf0534fe2c",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f40292a413ac48cbbd245d6023ae3b31",
            "value": 64275384
          }
        },
        "fa1c89356bae414e9e2786735afee0b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_514bfe0020984972913996f9c8e2a272",
            "placeholder": "​",
            "style": "IPY_MODEL_996e690e08c64c1fa87c5b5932b9e8e6",
            "value": " 64275384/64275384 [01:14&lt;00:00, 2426388.45it/s]"
          }
        },
        "a2b314a6c8114bd493aab9d0f4df29f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58281269f59644cdaa550a1cc512418a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cddc08c72e540c8953dc6e3035a324c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a64f8470aee4f1dba402acf0534fe2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f40292a413ac48cbbd245d6023ae3b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "514bfe0020984972913996f9c8e2a272": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "996e690e08c64c1fa87c5b5932b9e8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "3478b1e088f442cabae7a6b4c6821527",
            "790dc1b2808e4771981d53629e2d7f15",
            "ba371195279c49c0a1d9867f61e96398",
            "7e1fc76296324f2e8f5774432ce80f81",
            "8ba648f26ed245e091c678223e4e004b",
            "4698fbe4e7a34d1f835e9970e083abf2",
            "931da73ca0ce462397a689ba5a94e8f9",
            "b6fe69e220bc4bc58799b15de72ee6d5",
            "19e2f3b37ce941ccba53f5c0a87a69fa",
            "2566522176a7428497730d0eed97b549",
            "3aa6d2217abe46d9b611fee7bc0508fd",
            "652c6298a112438eb7f24360306aeebb",
            "09ed0d0e88c34fd2a1ef562b2af4373f",
            "3040997ba93f429d96b366bca05d6a2e",
            "fa1c89356bae414e9e2786735afee0b2",
            "a2b314a6c8114bd493aab9d0f4df29f3",
            "58281269f59644cdaa550a1cc512418a",
            "8cddc08c72e540c8953dc6e3035a324c",
            "6a64f8470aee4f1dba402acf0534fe2c",
            "f40292a413ac48cbbd245d6023ae3b31",
            "514bfe0020984972913996f9c8e2a272",
            "996e690e08c64c1fa87c5b5932b9e8e6"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "6bef932c-130b-40d3-baa7-ba78be078c6d"
      },
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.AugMix(),\n",
        "    transforms.ToTensor(),\n",
        "    \n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3478b1e088f442cabae7a6b4c6821527"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "652c6298a112438eb7f24360306aeebb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))\n",
        "print(trainset[4][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "32e2a0d5-b840-405c-cb67-128fd972991e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "2571b39f-91a9-4070-e36f-1c260e97258d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "7bc5a758-f85d-4c61-f035-8c652bba121b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArgupDVRwB8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdef14e3-e2ff-47e2-d686-0cab5630f1b0"
      },
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3528, accuracy : 17.70\n",
            "iteration : 100, loss : 2.1579, accuracy : 24.35\n",
            "iteration : 150, loss : 1.7669, accuracy : 38.76\n",
            "iteration : 200, loss : 1.4652, accuracy : 49.69\n",
            "iteration : 250, loss : 1.2657, accuracy : 56.80\n",
            "iteration : 300, loss : 1.1269, accuracy : 61.77\n",
            "iteration : 350, loss : 1.0257, accuracy : 65.40\n",
            "Epoch :   1, training loss : 0.9938, training accuracy : 66.52, test loss : 0.4969, test accuracy : 84.63\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.3615, accuracy : 88.84\n",
            "iteration : 100, loss : 0.3618, accuracy : 88.91\n",
            "iteration : 150, loss : 0.3662, accuracy : 88.69\n",
            "iteration : 200, loss : 0.3602, accuracy : 88.92\n",
            "iteration : 250, loss : 0.3591, accuracy : 88.95\n",
            "iteration : 300, loss : 0.3594, accuracy : 88.99\n",
            "iteration : 350, loss : 0.3552, accuracy : 89.16\n",
            "Epoch :   2, training loss : 0.3539, training accuracy : 89.19, test loss : 0.4496, test accuracy : 85.74\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.2911, accuracy : 90.98\n",
            "iteration : 100, loss : 0.3047, accuracy : 90.48\n",
            "iteration : 150, loss : 0.3227, accuracy : 90.09\n",
            "iteration : 200, loss : 0.3181, accuracy : 90.30\n",
            "iteration : 250, loss : 0.3137, accuracy : 90.46\n",
            "iteration : 300, loss : 0.3150, accuracy : 90.42\n",
            "iteration : 350, loss : 0.3145, accuracy : 90.50\n",
            "Epoch :   3, training loss : 0.3142, training accuracy : 90.53, test loss : 0.3900, test accuracy : 87.95\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.2961, accuracy : 90.98\n",
            "iteration : 100, loss : 0.2900, accuracy : 91.26\n",
            "iteration : 150, loss : 0.2917, accuracy : 91.30\n",
            "iteration : 200, loss : 0.2905, accuracy : 91.27\n",
            "iteration : 250, loss : 0.2926, accuracy : 91.21\n",
            "iteration : 300, loss : 0.2913, accuracy : 91.24\n",
            "iteration : 350, loss : 0.2935, accuracy : 91.20\n",
            "Epoch :   4, training loss : 0.2923, training accuracy : 91.25, test loss : 0.3806, test accuracy : 88.44\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.2687, accuracy : 91.81\n",
            "iteration : 100, loss : 0.2639, accuracy : 92.04\n",
            "iteration : 150, loss : 0.2678, accuracy : 91.90\n",
            "iteration : 200, loss : 0.2720, accuracy : 91.81\n",
            "iteration : 250, loss : 0.2739, accuracy : 91.76\n",
            "iteration : 300, loss : 0.2751, accuracy : 91.83\n",
            "iteration : 350, loss : 0.2744, accuracy : 91.84\n",
            "Epoch :   5, training loss : 0.2742, training accuracy : 91.84, test loss : 0.3205, test accuracy : 90.47\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.2474, accuracy : 92.55\n",
            "iteration : 100, loss : 0.2444, accuracy : 92.56\n",
            "iteration : 150, loss : 0.2578, accuracy : 92.19\n",
            "iteration : 200, loss : 0.2602, accuracy : 92.14\n",
            "iteration : 250, loss : 0.2624, accuracy : 92.12\n",
            "iteration : 300, loss : 0.2627, accuracy : 92.12\n",
            "iteration : 350, loss : 0.2609, accuracy : 92.23\n",
            "Epoch :   6, training loss : 0.2612, training accuracy : 92.22, test loss : 0.3916, test accuracy : 88.46\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.2372, accuracy : 93.08\n",
            "iteration : 100, loss : 0.2392, accuracy : 93.09\n",
            "iteration : 150, loss : 0.2462, accuracy : 92.82\n",
            "iteration : 200, loss : 0.2479, accuracy : 92.81\n",
            "iteration : 250, loss : 0.2531, accuracy : 92.64\n",
            "iteration : 300, loss : 0.2535, accuracy : 92.65\n",
            "iteration : 350, loss : 0.2522, accuracy : 92.65\n",
            "Epoch :   7, training loss : 0.2534, training accuracy : 92.61, test loss : 0.3204, test accuracy : 90.52\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.2248, accuracy : 93.28\n",
            "iteration : 100, loss : 0.2241, accuracy : 93.31\n",
            "iteration : 150, loss : 0.2254, accuracy : 93.28\n",
            "iteration : 200, loss : 0.2328, accuracy : 93.04\n",
            "iteration : 250, loss : 0.2329, accuracy : 93.13\n",
            "iteration : 300, loss : 0.2385, accuracy : 92.96\n",
            "iteration : 350, loss : 0.2390, accuracy : 92.95\n",
            "Epoch :   8, training loss : 0.2381, training accuracy : 92.98, test loss : 0.2757, test accuracy : 91.91\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.2007, accuracy : 94.44\n",
            "iteration : 100, loss : 0.2057, accuracy : 94.24\n",
            "iteration : 150, loss : 0.2117, accuracy : 93.91\n",
            "iteration : 200, loss : 0.2139, accuracy : 93.84\n",
            "iteration : 250, loss : 0.2187, accuracy : 93.67\n",
            "iteration : 300, loss : 0.2198, accuracy : 93.58\n",
            "iteration : 350, loss : 0.2239, accuracy : 93.49\n",
            "Epoch :   9, training loss : 0.2244, training accuracy : 93.48, test loss : 0.2974, test accuracy : 91.23\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.1994, accuracy : 94.19\n",
            "iteration : 100, loss : 0.1990, accuracy : 94.09\n",
            "iteration : 150, loss : 0.2127, accuracy : 93.84\n",
            "iteration : 200, loss : 0.2118, accuracy : 93.84\n",
            "iteration : 250, loss : 0.2096, accuracy : 93.87\n",
            "iteration : 300, loss : 0.2117, accuracy : 93.77\n",
            "iteration : 350, loss : 0.2126, accuracy : 93.76\n",
            "Epoch :  10, training loss : 0.2127, training accuracy : 93.78, test loss : 0.2830, test accuracy : 91.68\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.1714, accuracy : 94.97\n",
            "iteration : 100, loss : 0.1902, accuracy : 94.31\n",
            "iteration : 150, loss : 0.1948, accuracy : 94.24\n",
            "iteration : 200, loss : 0.1942, accuracy : 94.25\n",
            "iteration : 250, loss : 0.1975, accuracy : 94.19\n",
            "iteration : 300, loss : 0.2004, accuracy : 94.19\n",
            "iteration : 350, loss : 0.2028, accuracy : 94.12\n",
            "Epoch :  11, training loss : 0.2022, training accuracy : 94.11, test loss : 0.2807, test accuracy : 92.01\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.1598, accuracy : 95.53\n",
            "iteration : 100, loss : 0.1714, accuracy : 95.05\n",
            "iteration : 150, loss : 0.1825, accuracy : 94.71\n",
            "iteration : 200, loss : 0.1847, accuracy : 94.63\n",
            "iteration : 250, loss : 0.1852, accuracy : 94.55\n",
            "iteration : 300, loss : 0.1901, accuracy : 94.41\n",
            "iteration : 350, loss : 0.1905, accuracy : 94.42\n",
            "Epoch :  12, training loss : 0.1921, training accuracy : 94.38, test loss : 0.2834, test accuracy : 91.65\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.1525, accuracy : 95.67\n",
            "iteration : 100, loss : 0.1548, accuracy : 95.58\n",
            "iteration : 150, loss : 0.1572, accuracy : 95.44\n",
            "iteration : 200, loss : 0.1653, accuracy : 95.23\n",
            "iteration : 250, loss : 0.1743, accuracy : 94.95\n",
            "iteration : 300, loss : 0.1762, accuracy : 94.95\n",
            "iteration : 350, loss : 0.1785, accuracy : 94.88\n",
            "Epoch :  13, training loss : 0.1792, training accuracy : 94.85, test loss : 0.2693, test accuracy : 91.92\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.1570, accuracy : 95.58\n",
            "iteration : 100, loss : 0.1620, accuracy : 95.46\n",
            "iteration : 150, loss : 0.1599, accuracy : 95.44\n",
            "iteration : 200, loss : 0.1627, accuracy : 95.33\n",
            "iteration : 250, loss : 0.1680, accuracy : 95.18\n",
            "iteration : 300, loss : 0.1686, accuracy : 95.14\n",
            "iteration : 350, loss : 0.1687, accuracy : 95.14\n",
            "Epoch :  14, training loss : 0.1696, training accuracy : 95.10, test loss : 0.2667, test accuracy : 92.13\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.1508, accuracy : 95.53\n",
            "iteration : 100, loss : 0.1456, accuracy : 95.61\n",
            "iteration : 150, loss : 0.1468, accuracy : 95.62\n",
            "iteration : 200, loss : 0.1483, accuracy : 95.61\n",
            "iteration : 250, loss : 0.1494, accuracy : 95.62\n",
            "iteration : 300, loss : 0.1542, accuracy : 95.55\n",
            "iteration : 350, loss : 0.1571, accuracy : 95.50\n",
            "Epoch :  15, training loss : 0.1598, training accuracy : 95.42, test loss : 0.2500, test accuracy : 92.95\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.1311, accuracy : 96.20\n",
            "iteration : 100, loss : 0.1376, accuracy : 96.09\n",
            "iteration : 150, loss : 0.1426, accuracy : 95.99\n",
            "iteration : 200, loss : 0.1446, accuracy : 95.94\n",
            "iteration : 250, loss : 0.1452, accuracy : 95.93\n",
            "iteration : 300, loss : 0.1472, accuracy : 95.86\n",
            "iteration : 350, loss : 0.1515, accuracy : 95.73\n",
            "Epoch :  16, training loss : 0.1511, training accuracy : 95.74, test loss : 0.2611, test accuracy : 92.58\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.1272, accuracy : 96.34\n",
            "iteration : 100, loss : 0.1277, accuracy : 96.36\n",
            "iteration : 150, loss : 0.1286, accuracy : 96.36\n",
            "iteration : 200, loss : 0.1346, accuracy : 96.23\n",
            "iteration : 250, loss : 0.1407, accuracy : 96.02\n",
            "iteration : 300, loss : 0.1417, accuracy : 95.99\n",
            "iteration : 350, loss : 0.1446, accuracy : 95.91\n",
            "Epoch :  17, training loss : 0.1449, training accuracy : 95.88, test loss : 0.2560, test accuracy : 92.93\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.1166, accuracy : 96.91\n",
            "iteration : 100, loss : 0.1168, accuracy : 96.61\n",
            "iteration : 150, loss : 0.1222, accuracy : 96.47\n",
            "iteration : 200, loss : 0.1281, accuracy : 96.29\n",
            "iteration : 250, loss : 0.1357, accuracy : 96.11\n",
            "iteration : 300, loss : 0.1370, accuracy : 96.09\n",
            "iteration : 350, loss : 0.1385, accuracy : 96.07\n",
            "Epoch :  18, training loss : 0.1384, training accuracy : 96.07, test loss : 0.2404, test accuracy : 93.30\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.1117, accuracy : 96.78\n",
            "iteration : 100, loss : 0.1176, accuracy : 96.58\n",
            "iteration : 150, loss : 0.1230, accuracy : 96.43\n",
            "iteration : 200, loss : 0.1252, accuracy : 96.36\n",
            "iteration : 250, loss : 0.1263, accuracy : 96.31\n",
            "iteration : 300, loss : 0.1304, accuracy : 96.21\n",
            "iteration : 350, loss : 0.1313, accuracy : 96.21\n",
            "Epoch :  19, training loss : 0.1310, training accuracy : 96.22, test loss : 0.2461, test accuracy : 93.14\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.1094, accuracy : 97.00\n",
            "iteration : 100, loss : 0.1184, accuracy : 96.55\n",
            "iteration : 150, loss : 0.1165, accuracy : 96.60\n",
            "iteration : 200, loss : 0.1205, accuracy : 96.54\n",
            "iteration : 250, loss : 0.1248, accuracy : 96.41\n",
            "iteration : 300, loss : 0.1274, accuracy : 96.36\n",
            "iteration : 350, loss : 0.1286, accuracy : 96.29\n",
            "Epoch :  20, training loss : 0.1287, training accuracy : 96.27, test loss : 0.2512, test accuracy : 93.27\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.1126, accuracy : 96.64\n",
            "iteration : 100, loss : 0.1178, accuracy : 96.51\n",
            "iteration : 150, loss : 0.1154, accuracy : 96.65\n",
            "iteration : 200, loss : 0.1144, accuracy : 96.67\n",
            "iteration : 250, loss : 0.1137, accuracy : 96.66\n",
            "iteration : 300, loss : 0.1160, accuracy : 96.59\n",
            "iteration : 350, loss : 0.1196, accuracy : 96.50\n",
            "Epoch :  21, training loss : 0.1211, training accuracy : 96.45, test loss : 0.2554, test accuracy : 92.96\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.0940, accuracy : 97.41\n",
            "iteration : 100, loss : 0.0950, accuracy : 97.34\n",
            "iteration : 150, loss : 0.1022, accuracy : 97.08\n",
            "iteration : 200, loss : 0.1088, accuracy : 96.89\n",
            "iteration : 250, loss : 0.1091, accuracy : 96.90\n",
            "iteration : 300, loss : 0.1127, accuracy : 96.82\n",
            "iteration : 350, loss : 0.1172, accuracy : 96.64\n",
            "Epoch :  22, training loss : 0.1175, training accuracy : 96.62, test loss : 0.2498, test accuracy : 93.08\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.0899, accuracy : 97.44\n",
            "iteration : 100, loss : 0.1039, accuracy : 97.10\n",
            "iteration : 150, loss : 0.1021, accuracy : 97.08\n",
            "iteration : 200, loss : 0.1055, accuracy : 97.06\n",
            "iteration : 250, loss : 0.1101, accuracy : 96.84\n",
            "iteration : 300, loss : 0.1122, accuracy : 96.79\n",
            "iteration : 350, loss : 0.1132, accuracy : 96.76\n",
            "Epoch :  23, training loss : 0.1139, training accuracy : 96.74, test loss : 0.2624, test accuracy : 92.95\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.0955, accuracy : 97.39\n",
            "iteration : 100, loss : 0.1008, accuracy : 97.26\n",
            "iteration : 150, loss : 0.1066, accuracy : 97.02\n",
            "iteration : 200, loss : 0.1064, accuracy : 96.97\n",
            "iteration : 250, loss : 0.1064, accuracy : 96.93\n",
            "iteration : 300, loss : 0.1085, accuracy : 96.89\n",
            "iteration : 350, loss : 0.1129, accuracy : 96.75\n",
            "Epoch :  24, training loss : 0.1134, training accuracy : 96.74, test loss : 0.2501, test accuracy : 93.04\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.0796, accuracy : 97.62\n",
            "iteration : 100, loss : 0.0908, accuracy : 97.38\n",
            "iteration : 150, loss : 0.0973, accuracy : 97.07\n",
            "iteration : 200, loss : 0.1033, accuracy : 96.95\n",
            "iteration : 250, loss : 0.1044, accuracy : 96.92\n",
            "iteration : 300, loss : 0.1069, accuracy : 96.86\n",
            "iteration : 350, loss : 0.1095, accuracy : 96.76\n",
            "Epoch :  25, training loss : 0.1091, training accuracy : 96.77, test loss : 0.2706, test accuracy : 92.75\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.0876, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0892, accuracy : 97.55\n",
            "iteration : 150, loss : 0.0924, accuracy : 97.45\n",
            "iteration : 200, loss : 0.0977, accuracy : 97.23\n",
            "iteration : 250, loss : 0.0991, accuracy : 97.18\n",
            "iteration : 300, loss : 0.0999, accuracy : 97.09\n",
            "iteration : 350, loss : 0.1013, accuracy : 97.08\n",
            "Epoch :  26, training loss : 0.1016, training accuracy : 97.06, test loss : 0.2594, test accuracy : 93.20\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.0890, accuracy : 97.33\n",
            "iteration : 100, loss : 0.0904, accuracy : 97.30\n",
            "iteration : 150, loss : 0.0946, accuracy : 97.18\n",
            "iteration : 200, loss : 0.0988, accuracy : 97.06\n",
            "iteration : 250, loss : 0.1024, accuracy : 96.91\n",
            "iteration : 300, loss : 0.1028, accuracy : 96.87\n",
            "iteration : 350, loss : 0.1061, accuracy : 96.81\n",
            "Epoch :  27, training loss : 0.1061, training accuracy : 96.83, test loss : 0.2524, test accuracy : 93.11\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.0864, accuracy : 97.34\n",
            "iteration : 100, loss : 0.0890, accuracy : 97.41\n",
            "iteration : 150, loss : 0.0913, accuracy : 97.40\n",
            "iteration : 200, loss : 0.0927, accuracy : 97.29\n",
            "iteration : 250, loss : 0.0967, accuracy : 97.15\n",
            "iteration : 300, loss : 0.0982, accuracy : 97.10\n",
            "iteration : 350, loss : 0.0984, accuracy : 97.11\n",
            "Epoch :  28, training loss : 0.0986, training accuracy : 97.10, test loss : 0.2602, test accuracy : 93.04\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.0837, accuracy : 97.50\n",
            "iteration : 100, loss : 0.0840, accuracy : 97.52\n",
            "iteration : 150, loss : 0.0876, accuracy : 97.44\n",
            "iteration : 200, loss : 0.0901, accuracy : 97.32\n",
            "iteration : 250, loss : 0.0933, accuracy : 97.24\n",
            "iteration : 300, loss : 0.0939, accuracy : 97.24\n",
            "iteration : 350, loss : 0.0976, accuracy : 97.12\n",
            "Epoch :  29, training loss : 0.0985, training accuracy : 97.08, test loss : 0.2430, test accuracy : 93.66\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.0804, accuracy : 97.91\n",
            "iteration : 100, loss : 0.0766, accuracy : 97.97\n",
            "iteration : 150, loss : 0.0820, accuracy : 97.74\n",
            "iteration : 200, loss : 0.0828, accuracy : 97.75\n",
            "iteration : 250, loss : 0.0847, accuracy : 97.65\n",
            "iteration : 300, loss : 0.0894, accuracy : 97.47\n",
            "iteration : 350, loss : 0.0927, accuracy : 97.36\n",
            "Epoch :  30, training loss : 0.0925, training accuracy : 97.35, test loss : 0.2627, test accuracy : 93.29\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.0752, accuracy : 97.70\n",
            "iteration : 100, loss : 0.0809, accuracy : 97.59\n",
            "iteration : 150, loss : 0.0821, accuracy : 97.58\n",
            "iteration : 200, loss : 0.0837, accuracy : 97.56\n",
            "iteration : 250, loss : 0.0871, accuracy : 97.45\n",
            "iteration : 300, loss : 0.0890, accuracy : 97.37\n",
            "iteration : 350, loss : 0.0943, accuracy : 97.21\n",
            "Epoch :  31, training loss : 0.0957, training accuracy : 97.19, test loss : 0.2799, test accuracy : 92.57\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.0737, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0736, accuracy : 97.77\n",
            "iteration : 150, loss : 0.0778, accuracy : 97.65\n",
            "iteration : 200, loss : 0.0841, accuracy : 97.48\n",
            "iteration : 250, loss : 0.0852, accuracy : 97.46\n",
            "iteration : 300, loss : 0.0853, accuracy : 97.45\n",
            "iteration : 350, loss : 0.0885, accuracy : 97.35\n",
            "Epoch :  32, training loss : 0.0894, training accuracy : 97.31, test loss : 0.2754, test accuracy : 92.80\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.0852, accuracy : 97.56\n",
            "iteration : 100, loss : 0.0875, accuracy : 97.53\n",
            "iteration : 150, loss : 0.0828, accuracy : 97.65\n",
            "iteration : 200, loss : 0.0822, accuracy : 97.62\n",
            "iteration : 250, loss : 0.0840, accuracy : 97.56\n",
            "iteration : 300, loss : 0.0869, accuracy : 97.49\n",
            "iteration : 350, loss : 0.0897, accuracy : 97.40\n",
            "Epoch :  33, training loss : 0.0904, training accuracy : 97.37, test loss : 0.2633, test accuracy : 93.15\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.0641, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0687, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0706, accuracy : 97.97\n",
            "iteration : 200, loss : 0.0742, accuracy : 97.86\n",
            "iteration : 250, loss : 0.0791, accuracy : 97.72\n",
            "iteration : 300, loss : 0.0827, accuracy : 97.65\n",
            "iteration : 350, loss : 0.0844, accuracy : 97.59\n",
            "Epoch :  34, training loss : 0.0844, training accuracy : 97.58, test loss : 0.2683, test accuracy : 92.91\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.0643, accuracy : 98.02\n",
            "iteration : 100, loss : 0.0796, accuracy : 97.58\n",
            "iteration : 150, loss : 0.0802, accuracy : 97.57\n",
            "iteration : 200, loss : 0.0834, accuracy : 97.50\n",
            "iteration : 250, loss : 0.0855, accuracy : 97.46\n",
            "iteration : 300, loss : 0.0871, accuracy : 97.40\n",
            "iteration : 350, loss : 0.0874, accuracy : 97.38\n",
            "Epoch :  35, training loss : 0.0873, training accuracy : 97.37, test loss : 0.2847, test accuracy : 92.63\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.0619, accuracy : 98.14\n",
            "iteration : 100, loss : 0.0659, accuracy : 97.93\n",
            "iteration : 150, loss : 0.0733, accuracy : 97.74\n",
            "iteration : 200, loss : 0.0794, accuracy : 97.63\n",
            "iteration : 250, loss : 0.0811, accuracy : 97.60\n",
            "iteration : 300, loss : 0.0825, accuracy : 97.53\n",
            "iteration : 350, loss : 0.0845, accuracy : 97.45\n",
            "Epoch :  36, training loss : 0.0846, training accuracy : 97.45, test loss : 0.2823, test accuracy : 92.60\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.0725, accuracy : 97.84\n",
            "iteration : 100, loss : 0.0720, accuracy : 97.89\n",
            "iteration : 150, loss : 0.0756, accuracy : 97.74\n",
            "iteration : 200, loss : 0.0776, accuracy : 97.68\n",
            "iteration : 250, loss : 0.0771, accuracy : 97.70\n",
            "iteration : 300, loss : 0.0803, accuracy : 97.59\n",
            "iteration : 350, loss : 0.0849, accuracy : 97.42\n",
            "Epoch :  37, training loss : 0.0860, training accuracy : 97.39, test loss : 0.2649, test accuracy : 92.95\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.0846, accuracy : 97.45\n",
            "iteration : 100, loss : 0.0790, accuracy : 97.59\n",
            "iteration : 150, loss : 0.0760, accuracy : 97.71\n",
            "iteration : 200, loss : 0.0757, accuracy : 97.75\n",
            "iteration : 250, loss : 0.0775, accuracy : 97.70\n",
            "iteration : 300, loss : 0.0816, accuracy : 97.59\n",
            "iteration : 350, loss : 0.0846, accuracy : 97.48\n",
            "Epoch :  38, training loss : 0.0841, training accuracy : 97.49, test loss : 0.2512, test accuracy : 93.35\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.0711, accuracy : 97.88\n",
            "iteration : 100, loss : 0.0719, accuracy : 97.98\n",
            "iteration : 150, loss : 0.0755, accuracy : 97.85\n",
            "iteration : 200, loss : 0.0764, accuracy : 97.78\n",
            "iteration : 250, loss : 0.0767, accuracy : 97.76\n",
            "iteration : 300, loss : 0.0810, accuracy : 97.57\n",
            "iteration : 350, loss : 0.0829, accuracy : 97.54\n",
            "Epoch :  39, training loss : 0.0843, training accuracy : 97.50, test loss : 0.2872, test accuracy : 92.80\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.0807, accuracy : 97.48\n",
            "iteration : 100, loss : 0.0755, accuracy : 97.66\n",
            "iteration : 150, loss : 0.0762, accuracy : 97.61\n",
            "iteration : 200, loss : 0.0762, accuracy : 97.64\n",
            "iteration : 250, loss : 0.0765, accuracy : 97.65\n",
            "iteration : 300, loss : 0.0795, accuracy : 97.55\n",
            "iteration : 350, loss : 0.0824, accuracy : 97.46\n",
            "Epoch :  40, training loss : 0.0831, training accuracy : 97.44, test loss : 0.2688, test accuracy : 93.06\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.0516, accuracy : 98.62\n",
            "iteration : 100, loss : 0.0582, accuracy : 98.34\n",
            "iteration : 150, loss : 0.0647, accuracy : 98.15\n",
            "iteration : 200, loss : 0.0719, accuracy : 97.91\n",
            "iteration : 250, loss : 0.0742, accuracy : 97.83\n",
            "iteration : 300, loss : 0.0776, accuracy : 97.71\n",
            "iteration : 350, loss : 0.0780, accuracy : 97.68\n",
            "Epoch :  41, training loss : 0.0792, training accuracy : 97.64, test loss : 0.2730, test accuracy : 92.93\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.0705, accuracy : 97.95\n",
            "iteration : 100, loss : 0.0709, accuracy : 97.95\n",
            "iteration : 150, loss : 0.0746, accuracy : 97.74\n",
            "iteration : 200, loss : 0.0765, accuracy : 97.70\n",
            "iteration : 250, loss : 0.0776, accuracy : 97.67\n",
            "iteration : 300, loss : 0.0800, accuracy : 97.62\n",
            "iteration : 350, loss : 0.0802, accuracy : 97.62\n",
            "Epoch :  42, training loss : 0.0802, training accuracy : 97.62, test loss : 0.2820, test accuracy : 92.84\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.0648, accuracy : 98.23\n",
            "iteration : 100, loss : 0.0676, accuracy : 98.17\n",
            "iteration : 150, loss : 0.0682, accuracy : 98.09\n",
            "iteration : 200, loss : 0.0716, accuracy : 98.00\n",
            "iteration : 250, loss : 0.0759, accuracy : 97.82\n",
            "iteration : 300, loss : 0.0776, accuracy : 97.75\n",
            "iteration : 350, loss : 0.0788, accuracy : 97.73\n",
            "Epoch :  43, training loss : 0.0793, training accuracy : 97.72, test loss : 0.2728, test accuracy : 92.95\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.0533, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0563, accuracy : 98.36\n",
            "iteration : 150, loss : 0.0562, accuracy : 98.29\n",
            "iteration : 200, loss : 0.0590, accuracy : 98.24\n",
            "iteration : 250, loss : 0.0620, accuracy : 98.12\n",
            "iteration : 300, loss : 0.0670, accuracy : 97.99\n",
            "iteration : 350, loss : 0.0719, accuracy : 97.84\n",
            "Epoch :  44, training loss : 0.0735, training accuracy : 97.80, test loss : 0.2717, test accuracy : 92.96\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.0600, accuracy : 98.28\n",
            "iteration : 100, loss : 0.0605, accuracy : 98.25\n",
            "iteration : 150, loss : 0.0641, accuracy : 98.07\n",
            "iteration : 200, loss : 0.0667, accuracy : 97.92\n",
            "iteration : 250, loss : 0.0697, accuracy : 97.81\n",
            "iteration : 300, loss : 0.0708, accuracy : 97.80\n",
            "iteration : 350, loss : 0.0736, accuracy : 97.75\n",
            "Epoch :  45, training loss : 0.0751, training accuracy : 97.70, test loss : 0.2891, test accuracy : 92.72\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.0757, accuracy : 97.80\n",
            "iteration : 100, loss : 0.0668, accuracy : 98.00\n",
            "iteration : 150, loss : 0.0693, accuracy : 97.85\n",
            "iteration : 200, loss : 0.0690, accuracy : 97.86\n",
            "iteration : 250, loss : 0.0715, accuracy : 97.77\n",
            "iteration : 300, loss : 0.0730, accuracy : 97.73\n",
            "iteration : 350, loss : 0.0740, accuracy : 97.75\n",
            "Epoch :  46, training loss : 0.0744, training accuracy : 97.76, test loss : 0.2703, test accuracy : 93.20\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.0486, accuracy : 98.66\n",
            "iteration : 100, loss : 0.0572, accuracy : 98.30\n",
            "iteration : 150, loss : 0.0627, accuracy : 98.11\n",
            "iteration : 200, loss : 0.0640, accuracy : 98.13\n",
            "iteration : 250, loss : 0.0671, accuracy : 98.05\n",
            "iteration : 300, loss : 0.0692, accuracy : 97.98\n",
            "iteration : 350, loss : 0.0711, accuracy : 97.91\n",
            "Epoch :  47, training loss : 0.0716, training accuracy : 97.89, test loss : 0.2840, test accuracy : 92.94\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.0681, accuracy : 97.92\n",
            "iteration : 100, loss : 0.0704, accuracy : 97.76\n",
            "iteration : 150, loss : 0.0706, accuracy : 97.81\n",
            "iteration : 200, loss : 0.0730, accuracy : 97.77\n",
            "iteration : 250, loss : 0.0757, accuracy : 97.68\n",
            "iteration : 300, loss : 0.0773, accuracy : 97.65\n",
            "iteration : 350, loss : 0.0781, accuracy : 97.66\n",
            "Epoch :  48, training loss : 0.0784, training accuracy : 97.66, test loss : 0.2797, test accuracy : 92.92\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.0531, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0536, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0602, accuracy : 98.26\n",
            "iteration : 200, loss : 0.0625, accuracy : 98.17\n",
            "iteration : 250, loss : 0.0679, accuracy : 98.01\n",
            "iteration : 300, loss : 0.0703, accuracy : 97.92\n",
            "iteration : 350, loss : 0.0725, accuracy : 97.84\n",
            "Epoch :  49, training loss : 0.0732, training accuracy : 97.81, test loss : 0.2801, test accuracy : 93.12\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.0631, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0627, accuracy : 98.27\n",
            "iteration : 150, loss : 0.0616, accuracy : 98.24\n",
            "iteration : 200, loss : 0.0649, accuracy : 98.11\n",
            "iteration : 250, loss : 0.0678, accuracy : 97.98\n",
            "iteration : 300, loss : 0.0705, accuracy : 97.93\n",
            "iteration : 350, loss : 0.0711, accuracy : 97.88\n",
            "Epoch :  50, training loss : 0.0718, training accuracy : 97.86, test loss : 0.2853, test accuracy : 92.66\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.0684, accuracy : 97.95\n",
            "iteration : 100, loss : 0.0655, accuracy : 98.08\n",
            "iteration : 150, loss : 0.0617, accuracy : 98.18\n",
            "iteration : 200, loss : 0.0625, accuracy : 98.17\n",
            "iteration : 250, loss : 0.0658, accuracy : 98.04\n",
            "iteration : 300, loss : 0.0694, accuracy : 97.95\n",
            "iteration : 350, loss : 0.0713, accuracy : 97.89\n",
            "Epoch :  51, training loss : 0.0713, training accuracy : 97.88, test loss : 0.2735, test accuracy : 93.11\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.0648, accuracy : 98.00\n",
            "iteration : 100, loss : 0.0629, accuracy : 98.12\n",
            "iteration : 150, loss : 0.0612, accuracy : 98.18\n",
            "iteration : 200, loss : 0.0644, accuracy : 98.07\n",
            "iteration : 250, loss : 0.0672, accuracy : 98.03\n",
            "iteration : 300, loss : 0.0695, accuracy : 97.94\n",
            "iteration : 350, loss : 0.0714, accuracy : 97.88\n",
            "Epoch :  52, training loss : 0.0732, training accuracy : 97.84, test loss : 0.2917, test accuracy : 92.80\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.0596, accuracy : 98.20\n",
            "iteration : 100, loss : 0.0627, accuracy : 98.09\n",
            "iteration : 150, loss : 0.0628, accuracy : 98.10\n",
            "iteration : 200, loss : 0.0683, accuracy : 97.96\n",
            "iteration : 250, loss : 0.0670, accuracy : 98.03\n",
            "iteration : 300, loss : 0.0672, accuracy : 98.03\n",
            "iteration : 350, loss : 0.0688, accuracy : 97.96\n",
            "Epoch :  53, training loss : 0.0691, training accuracy : 97.96, test loss : 0.2772, test accuracy : 93.01\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.0681, accuracy : 98.14\n",
            "iteration : 100, loss : 0.0624, accuracy : 98.20\n",
            "iteration : 150, loss : 0.0619, accuracy : 98.23\n",
            "iteration : 200, loss : 0.0647, accuracy : 98.12\n",
            "iteration : 250, loss : 0.0649, accuracy : 98.13\n",
            "iteration : 300, loss : 0.0669, accuracy : 98.08\n",
            "iteration : 350, loss : 0.0666, accuracy : 98.08\n",
            "Epoch :  54, training loss : 0.0667, training accuracy : 98.07, test loss : 0.2940, test accuracy : 92.75\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.0722, accuracy : 97.75\n",
            "iteration : 100, loss : 0.0692, accuracy : 97.93\n",
            "iteration : 150, loss : 0.0713, accuracy : 97.91\n",
            "iteration : 200, loss : 0.0701, accuracy : 97.92\n",
            "iteration : 250, loss : 0.0697, accuracy : 97.93\n",
            "iteration : 300, loss : 0.0696, accuracy : 97.91\n",
            "iteration : 350, loss : 0.0707, accuracy : 97.90\n",
            "Epoch :  55, training loss : 0.0708, training accuracy : 97.91, test loss : 0.2814, test accuracy : 93.00\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.0536, accuracy : 98.47\n",
            "iteration : 100, loss : 0.0585, accuracy : 98.29\n",
            "iteration : 150, loss : 0.0608, accuracy : 98.21\n",
            "iteration : 200, loss : 0.0631, accuracy : 98.09\n",
            "iteration : 250, loss : 0.0664, accuracy : 98.02\n",
            "iteration : 300, loss : 0.0681, accuracy : 97.94\n",
            "iteration : 350, loss : 0.0703, accuracy : 97.86\n",
            "Epoch :  56, training loss : 0.0706, training accuracy : 97.85, test loss : 0.2896, test accuracy : 92.76\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.0639, accuracy : 98.25\n",
            "iteration : 100, loss : 0.0630, accuracy : 98.13\n",
            "iteration : 150, loss : 0.0679, accuracy : 98.02\n",
            "iteration : 200, loss : 0.0693, accuracy : 97.94\n",
            "iteration : 250, loss : 0.0717, accuracy : 97.85\n",
            "iteration : 300, loss : 0.0713, accuracy : 97.88\n",
            "iteration : 350, loss : 0.0731, accuracy : 97.83\n",
            "Epoch :  57, training loss : 0.0726, training accuracy : 97.84, test loss : 0.3044, test accuracy : 92.37\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.0621, accuracy : 98.25\n",
            "iteration : 100, loss : 0.0579, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0583, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0618, accuracy : 98.20\n",
            "iteration : 250, loss : 0.0606, accuracy : 98.21\n",
            "iteration : 300, loss : 0.0632, accuracy : 98.10\n",
            "iteration : 350, loss : 0.0642, accuracy : 98.06\n",
            "Epoch :  58, training loss : 0.0642, training accuracy : 98.06, test loss : 0.2730, test accuracy : 93.20\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.0575, accuracy : 98.30\n",
            "iteration : 100, loss : 0.0562, accuracy : 98.33\n",
            "iteration : 150, loss : 0.0586, accuracy : 98.25\n",
            "iteration : 200, loss : 0.0618, accuracy : 98.13\n",
            "iteration : 250, loss : 0.0638, accuracy : 98.06\n",
            "iteration : 300, loss : 0.0665, accuracy : 97.97\n",
            "iteration : 350, loss : 0.0675, accuracy : 97.94\n",
            "Epoch :  59, training loss : 0.0682, training accuracy : 97.92, test loss : 0.2753, test accuracy : 93.12\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.0537, accuracy : 98.34\n",
            "iteration : 100, loss : 0.0610, accuracy : 98.08\n",
            "iteration : 150, loss : 0.0605, accuracy : 98.18\n",
            "iteration : 200, loss : 0.0641, accuracy : 98.07\n",
            "iteration : 250, loss : 0.0657, accuracy : 98.01\n",
            "iteration : 300, loss : 0.0668, accuracy : 97.96\n",
            "iteration : 350, loss : 0.0693, accuracy : 97.87\n",
            "Epoch :  60, training loss : 0.0701, training accuracy : 97.84, test loss : 0.2903, test accuracy : 92.70\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.0570, accuracy : 98.42\n",
            "iteration : 100, loss : 0.0591, accuracy : 98.32\n",
            "iteration : 150, loss : 0.0597, accuracy : 98.29\n",
            "iteration : 200, loss : 0.0628, accuracy : 98.16\n",
            "iteration : 250, loss : 0.0632, accuracy : 98.11\n",
            "iteration : 300, loss : 0.0650, accuracy : 98.05\n",
            "iteration : 350, loss : 0.0646, accuracy : 98.07\n",
            "Epoch :  61, training loss : 0.0641, training accuracy : 98.09, test loss : 0.2642, test accuracy : 93.27\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.0427, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0516, accuracy : 98.46\n",
            "iteration : 150, loss : 0.0553, accuracy : 98.34\n",
            "iteration : 200, loss : 0.0573, accuracy : 98.25\n",
            "iteration : 250, loss : 0.0632, accuracy : 98.08\n",
            "iteration : 300, loss : 0.0652, accuracy : 98.02\n",
            "iteration : 350, loss : 0.0658, accuracy : 97.99\n",
            "Epoch :  62, training loss : 0.0659, training accuracy : 97.98, test loss : 0.2954, test accuracy : 92.83\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.0510, accuracy : 98.48\n",
            "iteration : 100, loss : 0.0524, accuracy : 98.46\n",
            "iteration : 150, loss : 0.0564, accuracy : 98.34\n",
            "iteration : 200, loss : 0.0575, accuracy : 98.27\n",
            "iteration : 250, loss : 0.0618, accuracy : 98.12\n",
            "iteration : 300, loss : 0.0648, accuracy : 98.05\n",
            "iteration : 350, loss : 0.0664, accuracy : 97.98\n",
            "Epoch :  63, training loss : 0.0674, training accuracy : 97.96, test loss : 0.2935, test accuracy : 92.57\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.0625, accuracy : 98.23\n",
            "iteration : 100, loss : 0.0582, accuracy : 98.28\n",
            "iteration : 150, loss : 0.0600, accuracy : 98.26\n",
            "iteration : 200, loss : 0.0597, accuracy : 98.27\n",
            "iteration : 250, loss : 0.0622, accuracy : 98.17\n",
            "iteration : 300, loss : 0.0630, accuracy : 98.13\n",
            "iteration : 350, loss : 0.0633, accuracy : 98.09\n",
            "Epoch :  64, training loss : 0.0637, training accuracy : 98.08, test loss : 0.2973, test accuracy : 92.61\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.0629, accuracy : 98.17\n",
            "iteration : 100, loss : 0.0643, accuracy : 98.09\n",
            "iteration : 150, loss : 0.0654, accuracy : 98.06\n",
            "iteration : 200, loss : 0.0666, accuracy : 98.02\n",
            "iteration : 250, loss : 0.0693, accuracy : 97.96\n",
            "iteration : 300, loss : 0.0686, accuracy : 97.97\n",
            "iteration : 350, loss : 0.0688, accuracy : 97.94\n",
            "Epoch :  65, training loss : 0.0691, training accuracy : 97.93, test loss : 0.3145, test accuracy : 92.06\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.0537, accuracy : 98.34\n",
            "iteration : 100, loss : 0.0535, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0560, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0568, accuracy : 98.30\n",
            "iteration : 250, loss : 0.0578, accuracy : 98.28\n",
            "iteration : 300, loss : 0.0579, accuracy : 98.27\n",
            "iteration : 350, loss : 0.0600, accuracy : 98.19\n",
            "Epoch :  66, training loss : 0.0611, training accuracy : 98.16, test loss : 0.2843, test accuracy : 92.87\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.0547, accuracy : 98.36\n",
            "iteration : 100, loss : 0.0545, accuracy : 98.30\n",
            "iteration : 150, loss : 0.0571, accuracy : 98.26\n",
            "iteration : 200, loss : 0.0563, accuracy : 98.27\n",
            "iteration : 250, loss : 0.0592, accuracy : 98.22\n",
            "iteration : 300, loss : 0.0620, accuracy : 98.14\n",
            "iteration : 350, loss : 0.0634, accuracy : 98.08\n",
            "Epoch :  67, training loss : 0.0655, training accuracy : 98.03, test loss : 0.2946, test accuracy : 92.66\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.0559, accuracy : 98.28\n",
            "iteration : 100, loss : 0.0538, accuracy : 98.37\n",
            "iteration : 150, loss : 0.0571, accuracy : 98.28\n",
            "iteration : 200, loss : 0.0599, accuracy : 98.18\n",
            "iteration : 250, loss : 0.0614, accuracy : 98.15\n",
            "iteration : 300, loss : 0.0625, accuracy : 98.13\n",
            "iteration : 350, loss : 0.0621, accuracy : 98.17\n",
            "Epoch :  68, training loss : 0.0628, training accuracy : 98.15, test loss : 0.2840, test accuracy : 93.00\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.0522, accuracy : 98.56\n",
            "iteration : 100, loss : 0.0591, accuracy : 98.34\n",
            "iteration : 150, loss : 0.0606, accuracy : 98.28\n",
            "iteration : 200, loss : 0.0637, accuracy : 98.17\n",
            "iteration : 250, loss : 0.0645, accuracy : 98.15\n",
            "iteration : 300, loss : 0.0667, accuracy : 98.06\n",
            "iteration : 350, loss : 0.0655, accuracy : 98.09\n",
            "Epoch :  69, training loss : 0.0654, training accuracy : 98.08, test loss : 0.2869, test accuracy : 92.81\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.0483, accuracy : 98.48\n",
            "iteration : 100, loss : 0.0503, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0539, accuracy : 98.36\n",
            "iteration : 200, loss : 0.0552, accuracy : 98.33\n",
            "iteration : 250, loss : 0.0575, accuracy : 98.23\n",
            "iteration : 300, loss : 0.0602, accuracy : 98.15\n",
            "iteration : 350, loss : 0.0645, accuracy : 98.02\n",
            "Epoch :  70, training loss : 0.0655, training accuracy : 97.99, test loss : 0.2724, test accuracy : 92.93\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.0489, accuracy : 98.78\n",
            "iteration : 100, loss : 0.0515, accuracy : 98.66\n",
            "iteration : 150, loss : 0.0547, accuracy : 98.49\n",
            "iteration : 200, loss : 0.0561, accuracy : 98.40\n",
            "iteration : 250, loss : 0.0559, accuracy : 98.37\n",
            "iteration : 300, loss : 0.0574, accuracy : 98.32\n",
            "iteration : 350, loss : 0.0591, accuracy : 98.24\n",
            "Epoch :  71, training loss : 0.0598, training accuracy : 98.21, test loss : 0.2910, test accuracy : 92.92\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.0565, accuracy : 98.23\n",
            "iteration : 100, loss : 0.0600, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0574, accuracy : 98.18\n",
            "iteration : 200, loss : 0.0594, accuracy : 98.10\n",
            "iteration : 250, loss : 0.0596, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0608, accuracy : 98.06\n",
            "iteration : 350, loss : 0.0615, accuracy : 98.05\n",
            "Epoch :  72, training loss : 0.0617, training accuracy : 98.04, test loss : 0.2947, test accuracy : 92.75\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.0559, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0545, accuracy : 98.47\n",
            "iteration : 150, loss : 0.0554, accuracy : 98.38\n",
            "iteration : 200, loss : 0.0576, accuracy : 98.29\n",
            "iteration : 250, loss : 0.0575, accuracy : 98.28\n",
            "iteration : 300, loss : 0.0601, accuracy : 98.20\n",
            "iteration : 350, loss : 0.0612, accuracy : 98.16\n",
            "Epoch :  73, training loss : 0.0618, training accuracy : 98.13, test loss : 0.2751, test accuracy : 93.25\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.0518, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0547, accuracy : 98.40\n",
            "iteration : 150, loss : 0.0575, accuracy : 98.27\n",
            "iteration : 200, loss : 0.0582, accuracy : 98.27\n",
            "iteration : 250, loss : 0.0606, accuracy : 98.21\n",
            "iteration : 300, loss : 0.0619, accuracy : 98.18\n",
            "iteration : 350, loss : 0.0629, accuracy : 98.16\n",
            "Epoch :  74, training loss : 0.0624, training accuracy : 98.18, test loss : 0.2934, test accuracy : 92.91\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.0530, accuracy : 98.41\n",
            "iteration : 100, loss : 0.0548, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0553, accuracy : 98.32\n",
            "iteration : 200, loss : 0.0580, accuracy : 98.21\n",
            "iteration : 250, loss : 0.0592, accuracy : 98.16\n",
            "iteration : 300, loss : 0.0592, accuracy : 98.16\n",
            "iteration : 350, loss : 0.0605, accuracy : 98.12\n",
            "Epoch :  75, training loss : 0.0624, training accuracy : 98.06, test loss : 0.3037, test accuracy : 92.27\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.0597, accuracy : 98.25\n",
            "iteration : 100, loss : 0.0558, accuracy : 98.36\n",
            "iteration : 150, loss : 0.0575, accuracy : 98.29\n",
            "iteration : 200, loss : 0.0592, accuracy : 98.21\n",
            "iteration : 250, loss : 0.0610, accuracy : 98.17\n",
            "iteration : 300, loss : 0.0626, accuracy : 98.11\n",
            "iteration : 350, loss : 0.0640, accuracy : 98.08\n",
            "Epoch :  76, training loss : 0.0641, training accuracy : 98.09, test loss : 0.2630, test accuracy : 93.39\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.0587, accuracy : 98.27\n",
            "iteration : 100, loss : 0.0531, accuracy : 98.39\n",
            "iteration : 150, loss : 0.0518, accuracy : 98.47\n",
            "iteration : 200, loss : 0.0541, accuracy : 98.41\n",
            "iteration : 250, loss : 0.0545, accuracy : 98.38\n",
            "iteration : 300, loss : 0.0569, accuracy : 98.27\n",
            "iteration : 350, loss : 0.0581, accuracy : 98.23\n",
            "Epoch :  77, training loss : 0.0583, training accuracy : 98.24, test loss : 0.2775, test accuracy : 93.17\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.0547, accuracy : 98.42\n",
            "iteration : 100, loss : 0.0511, accuracy : 98.52\n",
            "iteration : 150, loss : 0.0470, accuracy : 98.63\n",
            "iteration : 200, loss : 0.0497, accuracy : 98.54\n",
            "iteration : 250, loss : 0.0522, accuracy : 98.47\n",
            "iteration : 300, loss : 0.0535, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0560, accuracy : 98.36\n",
            "Epoch :  78, training loss : 0.0558, training accuracy : 98.37, test loss : 0.2888, test accuracy : 92.97\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.0510, accuracy : 98.36\n",
            "iteration : 100, loss : 0.0538, accuracy : 98.29\n",
            "iteration : 150, loss : 0.0574, accuracy : 98.21\n",
            "iteration : 200, loss : 0.0606, accuracy : 98.16\n",
            "iteration : 250, loss : 0.0607, accuracy : 98.18\n",
            "iteration : 300, loss : 0.0617, accuracy : 98.13\n",
            "iteration : 350, loss : 0.0617, accuracy : 98.13\n",
            "Epoch :  79, training loss : 0.0622, training accuracy : 98.10, test loss : 0.2784, test accuracy : 93.00\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.0416, accuracy : 98.77\n",
            "iteration : 100, loss : 0.0459, accuracy : 98.68\n",
            "iteration : 150, loss : 0.0453, accuracy : 98.65\n",
            "iteration : 200, loss : 0.0478, accuracy : 98.57\n",
            "iteration : 250, loss : 0.0500, accuracy : 98.49\n",
            "iteration : 300, loss : 0.0540, accuracy : 98.35\n",
            "iteration : 350, loss : 0.0585, accuracy : 98.23\n",
            "Epoch :  80, training loss : 0.0592, training accuracy : 98.23, test loss : 0.2702, test accuracy : 93.18\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.0491, accuracy : 98.47\n",
            "iteration : 100, loss : 0.0494, accuracy : 98.53\n",
            "iteration : 150, loss : 0.0539, accuracy : 98.42\n",
            "iteration : 200, loss : 0.0554, accuracy : 98.34\n",
            "iteration : 250, loss : 0.0552, accuracy : 98.33\n",
            "iteration : 300, loss : 0.0573, accuracy : 98.24\n",
            "iteration : 350, loss : 0.0584, accuracy : 98.20\n",
            "Epoch :  81, training loss : 0.0592, training accuracy : 98.17, test loss : 0.2921, test accuracy : 92.81\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.0534, accuracy : 98.36\n",
            "iteration : 100, loss : 0.0548, accuracy : 98.34\n",
            "iteration : 150, loss : 0.0554, accuracy : 98.32\n",
            "iteration : 200, loss : 0.0550, accuracy : 98.34\n",
            "iteration : 250, loss : 0.0554, accuracy : 98.36\n",
            "iteration : 300, loss : 0.0550, accuracy : 98.36\n",
            "iteration : 350, loss : 0.0564, accuracy : 98.29\n",
            "Epoch :  82, training loss : 0.0569, training accuracy : 98.27, test loss : 0.2950, test accuracy : 92.96\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.0496, accuracy : 98.36\n",
            "iteration : 100, loss : 0.0540, accuracy : 98.29\n",
            "iteration : 150, loss : 0.0548, accuracy : 98.29\n",
            "iteration : 200, loss : 0.0550, accuracy : 98.34\n",
            "iteration : 250, loss : 0.0569, accuracy : 98.24\n",
            "iteration : 300, loss : 0.0565, accuracy : 98.27\n",
            "iteration : 350, loss : 0.0578, accuracy : 98.23\n",
            "Epoch :  83, training loss : 0.0584, training accuracy : 98.21, test loss : 0.2836, test accuracy : 92.99\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.0470, accuracy : 98.58\n",
            "iteration : 100, loss : 0.0479, accuracy : 98.51\n",
            "iteration : 150, loss : 0.0517, accuracy : 98.45\n",
            "iteration : 200, loss : 0.0524, accuracy : 98.39\n",
            "iteration : 250, loss : 0.0543, accuracy : 98.38\n",
            "iteration : 300, loss : 0.0550, accuracy : 98.34\n",
            "iteration : 350, loss : 0.0569, accuracy : 98.27\n",
            "Epoch :  84, training loss : 0.0572, training accuracy : 98.25, test loss : 0.2781, test accuracy : 93.25\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.0411, accuracy : 98.75\n",
            "iteration : 100, loss : 0.0452, accuracy : 98.67\n",
            "iteration : 150, loss : 0.0449, accuracy : 98.68\n",
            "iteration : 200, loss : 0.0475, accuracy : 98.65\n",
            "iteration : 250, loss : 0.0500, accuracy : 98.58\n",
            "iteration : 300, loss : 0.0515, accuracy : 98.51\n",
            "iteration : 350, loss : 0.0536, accuracy : 98.44\n",
            "Epoch :  85, training loss : 0.0541, training accuracy : 98.42, test loss : 0.2923, test accuracy : 92.75\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.0493, accuracy : 98.42\n",
            "iteration : 100, loss : 0.0509, accuracy : 98.45\n",
            "iteration : 150, loss : 0.0535, accuracy : 98.40\n",
            "iteration : 200, loss : 0.0545, accuracy : 98.30\n",
            "iteration : 250, loss : 0.0559, accuracy : 98.29\n",
            "iteration : 300, loss : 0.0564, accuracy : 98.25\n",
            "iteration : 350, loss : 0.0582, accuracy : 98.19\n",
            "Epoch :  86, training loss : 0.0585, training accuracy : 98.19, test loss : 0.2880, test accuracy : 92.77\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.0462, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0478, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0484, accuracy : 98.57\n",
            "iteration : 200, loss : 0.0489, accuracy : 98.55\n",
            "iteration : 250, loss : 0.0498, accuracy : 98.53\n",
            "iteration : 300, loss : 0.0520, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0538, accuracy : 98.38\n",
            "Epoch :  87, training loss : 0.0543, training accuracy : 98.37, test loss : 0.2803, test accuracy : 93.16\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.0459, accuracy : 98.70\n",
            "iteration : 100, loss : 0.0535, accuracy : 98.40\n",
            "iteration : 150, loss : 0.0538, accuracy : 98.38\n",
            "iteration : 200, loss : 0.0506, accuracy : 98.48\n",
            "iteration : 250, loss : 0.0507, accuracy : 98.47\n",
            "iteration : 300, loss : 0.0518, accuracy : 98.47\n",
            "iteration : 350, loss : 0.0534, accuracy : 98.42\n",
            "Epoch :  88, training loss : 0.0544, training accuracy : 98.39, test loss : 0.2898, test accuracy : 92.94\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.0530, accuracy : 98.36\n",
            "iteration : 100, loss : 0.0524, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0522, accuracy : 98.44\n",
            "iteration : 200, loss : 0.0530, accuracy : 98.42\n",
            "iteration : 250, loss : 0.0531, accuracy : 98.44\n",
            "iteration : 300, loss : 0.0529, accuracy : 98.45\n",
            "iteration : 350, loss : 0.0544, accuracy : 98.40\n",
            "Epoch :  89, training loss : 0.0550, training accuracy : 98.37, test loss : 0.3029, test accuracy : 92.88\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.0469, accuracy : 98.52\n",
            "iteration : 100, loss : 0.0489, accuracy : 98.58\n",
            "iteration : 150, loss : 0.0507, accuracy : 98.51\n",
            "iteration : 200, loss : 0.0517, accuracy : 98.46\n",
            "iteration : 250, loss : 0.0548, accuracy : 98.39\n",
            "iteration : 300, loss : 0.0559, accuracy : 98.35\n",
            "iteration : 350, loss : 0.0567, accuracy : 98.33\n",
            "Epoch :  90, training loss : 0.0573, training accuracy : 98.32, test loss : 0.2815, test accuracy : 93.07\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.0478, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0515, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0517, accuracy : 98.51\n",
            "iteration : 200, loss : 0.0503, accuracy : 98.52\n",
            "iteration : 250, loss : 0.0518, accuracy : 98.47\n",
            "iteration : 300, loss : 0.0521, accuracy : 98.47\n",
            "iteration : 350, loss : 0.0534, accuracy : 98.43\n",
            "Epoch :  91, training loss : 0.0537, training accuracy : 98.42, test loss : 0.2841, test accuracy : 93.20\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.0484, accuracy : 98.34\n",
            "iteration : 100, loss : 0.0549, accuracy : 98.23\n",
            "iteration : 150, loss : 0.0517, accuracy : 98.42\n",
            "iteration : 200, loss : 0.0506, accuracy : 98.49\n",
            "iteration : 250, loss : 0.0510, accuracy : 98.47\n",
            "iteration : 300, loss : 0.0516, accuracy : 98.45\n",
            "iteration : 350, loss : 0.0527, accuracy : 98.42\n",
            "Epoch :  92, training loss : 0.0531, training accuracy : 98.40, test loss : 0.2814, test accuracy : 93.34\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.0422, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0446, accuracy : 98.66\n",
            "iteration : 150, loss : 0.0486, accuracy : 98.56\n",
            "iteration : 200, loss : 0.0486, accuracy : 98.54\n",
            "iteration : 250, loss : 0.0508, accuracy : 98.47\n",
            "iteration : 300, loss : 0.0519, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0531, accuracy : 98.40\n",
            "Epoch :  93, training loss : 0.0527, training accuracy : 98.42, test loss : 0.2793, test accuracy : 93.26\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.0408, accuracy : 98.92\n",
            "iteration : 100, loss : 0.0448, accuracy : 98.78\n",
            "iteration : 150, loss : 0.0460, accuracy : 98.74\n",
            "iteration : 200, loss : 0.0468, accuracy : 98.64\n",
            "iteration : 250, loss : 0.0503, accuracy : 98.52\n",
            "iteration : 300, loss : 0.0510, accuracy : 98.45\n",
            "iteration : 350, loss : 0.0513, accuracy : 98.46\n",
            "Epoch :  94, training loss : 0.0515, training accuracy : 98.45, test loss : 0.2858, test accuracy : 93.17\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.0467, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0464, accuracy : 98.70\n",
            "iteration : 150, loss : 0.0474, accuracy : 98.66\n",
            "iteration : 200, loss : 0.0484, accuracy : 98.64\n",
            "iteration : 250, loss : 0.0486, accuracy : 98.61\n",
            "iteration : 300, loss : 0.0520, accuracy : 98.51\n",
            "iteration : 350, loss : 0.0530, accuracy : 98.46\n",
            "Epoch :  95, training loss : 0.0537, training accuracy : 98.43, test loss : 0.2939, test accuracy : 92.68\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.0445, accuracy : 98.77\n",
            "iteration : 100, loss : 0.0491, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0525, accuracy : 98.40\n",
            "iteration : 200, loss : 0.0532, accuracy : 98.36\n",
            "iteration : 250, loss : 0.0526, accuracy : 98.37\n",
            "iteration : 300, loss : 0.0542, accuracy : 98.34\n",
            "iteration : 350, loss : 0.0562, accuracy : 98.27\n",
            "Epoch :  96, training loss : 0.0568, training accuracy : 98.25, test loss : 0.3006, test accuracy : 92.67\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.0434, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0419, accuracy : 98.73\n",
            "iteration : 150, loss : 0.0479, accuracy : 98.60\n",
            "iteration : 200, loss : 0.0466, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0472, accuracy : 98.65\n",
            "iteration : 300, loss : 0.0485, accuracy : 98.59\n",
            "iteration : 350, loss : 0.0502, accuracy : 98.53\n",
            "Epoch :  97, training loss : 0.0510, training accuracy : 98.52, test loss : 0.2826, test accuracy : 93.15\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.0461, accuracy : 98.73\n",
            "iteration : 100, loss : 0.0460, accuracy : 98.66\n",
            "iteration : 150, loss : 0.0478, accuracy : 98.58\n",
            "iteration : 200, loss : 0.0460, accuracy : 98.62\n",
            "iteration : 250, loss : 0.0463, accuracy : 98.58\n",
            "iteration : 300, loss : 0.0476, accuracy : 98.53\n",
            "iteration : 350, loss : 0.0483, accuracy : 98.53\n",
            "Epoch :  98, training loss : 0.0493, training accuracy : 98.51, test loss : 0.2836, test accuracy : 93.04\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.0493, accuracy : 98.45\n",
            "iteration : 100, loss : 0.0559, accuracy : 98.27\n",
            "iteration : 150, loss : 0.0585, accuracy : 98.18\n",
            "iteration : 200, loss : 0.0588, accuracy : 98.18\n",
            "iteration : 250, loss : 0.0570, accuracy : 98.27\n",
            "iteration : 300, loss : 0.0577, accuracy : 98.25\n",
            "iteration : 350, loss : 0.0591, accuracy : 98.21\n",
            "Epoch :  99, training loss : 0.0599, training accuracy : 98.19, test loss : 0.2878, test accuracy : 92.87\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.0449, accuracy : 98.70\n",
            "iteration : 100, loss : 0.0498, accuracy : 98.61\n",
            "iteration : 150, loss : 0.0494, accuracy : 98.62\n",
            "iteration : 200, loss : 0.0490, accuracy : 98.63\n",
            "iteration : 250, loss : 0.0497, accuracy : 98.63\n",
            "iteration : 300, loss : 0.0496, accuracy : 98.63\n",
            "iteration : 350, loss : 0.0496, accuracy : 98.61\n",
            "Epoch : 100, training loss : 0.0507, training accuracy : 98.57, test loss : 0.2912, test accuracy : 92.89\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.0355, accuracy : 98.89\n",
            "iteration : 100, loss : 0.0405, accuracy : 98.82\n",
            "iteration : 150, loss : 0.0429, accuracy : 98.74\n",
            "iteration : 200, loss : 0.0456, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0469, accuracy : 98.62\n",
            "iteration : 300, loss : 0.0481, accuracy : 98.61\n",
            "iteration : 350, loss : 0.0500, accuracy : 98.55\n",
            "Epoch : 101, training loss : 0.0510, training accuracy : 98.53, test loss : 0.2883, test accuracy : 93.17\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.0429, accuracy : 98.81\n",
            "iteration : 100, loss : 0.0438, accuracy : 98.75\n",
            "iteration : 150, loss : 0.0456, accuracy : 98.67\n",
            "iteration : 200, loss : 0.0460, accuracy : 98.69\n",
            "iteration : 250, loss : 0.0460, accuracy : 98.68\n",
            "iteration : 300, loss : 0.0473, accuracy : 98.65\n",
            "iteration : 350, loss : 0.0484, accuracy : 98.61\n",
            "Epoch : 102, training loss : 0.0492, training accuracy : 98.58, test loss : 0.2854, test accuracy : 92.91\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.0489, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0477, accuracy : 98.57\n",
            "iteration : 150, loss : 0.0483, accuracy : 98.52\n",
            "iteration : 200, loss : 0.0479, accuracy : 98.52\n",
            "iteration : 250, loss : 0.0479, accuracy : 98.54\n",
            "iteration : 300, loss : 0.0466, accuracy : 98.59\n",
            "iteration : 350, loss : 0.0481, accuracy : 98.53\n",
            "Epoch : 103, training loss : 0.0482, training accuracy : 98.53, test loss : 0.2843, test accuracy : 93.18\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.0372, accuracy : 98.98\n",
            "iteration : 100, loss : 0.0418, accuracy : 98.86\n",
            "iteration : 150, loss : 0.0431, accuracy : 98.86\n",
            "iteration : 200, loss : 0.0455, accuracy : 98.77\n",
            "iteration : 250, loss : 0.0456, accuracy : 98.77\n",
            "iteration : 300, loss : 0.0462, accuracy : 98.74\n",
            "iteration : 350, loss : 0.0465, accuracy : 98.71\n",
            "Epoch : 104, training loss : 0.0476, training accuracy : 98.67, test loss : 0.2773, test accuracy : 93.29\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.0385, accuracy : 99.06\n",
            "iteration : 100, loss : 0.0418, accuracy : 98.93\n",
            "iteration : 150, loss : 0.0415, accuracy : 98.87\n",
            "iteration : 200, loss : 0.0437, accuracy : 98.79\n",
            "iteration : 250, loss : 0.0469, accuracy : 98.66\n",
            "iteration : 300, loss : 0.0474, accuracy : 98.65\n",
            "iteration : 350, loss : 0.0487, accuracy : 98.59\n",
            "Epoch : 105, training loss : 0.0496, training accuracy : 98.55, test loss : 0.2741, test accuracy : 93.39\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.0463, accuracy : 98.58\n",
            "iteration : 100, loss : 0.0495, accuracy : 98.47\n",
            "iteration : 150, loss : 0.0502, accuracy : 98.44\n",
            "iteration : 200, loss : 0.0512, accuracy : 98.43\n",
            "iteration : 250, loss : 0.0513, accuracy : 98.42\n",
            "iteration : 300, loss : 0.0513, accuracy : 98.44\n",
            "iteration : 350, loss : 0.0506, accuracy : 98.47\n",
            "Epoch : 106, training loss : 0.0506, training accuracy : 98.45, test loss : 0.2899, test accuracy : 93.02\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.0500, accuracy : 98.53\n",
            "iteration : 100, loss : 0.0486, accuracy : 98.58\n",
            "iteration : 150, loss : 0.0505, accuracy : 98.52\n",
            "iteration : 200, loss : 0.0503, accuracy : 98.57\n",
            "iteration : 250, loss : 0.0521, accuracy : 98.52\n",
            "iteration : 300, loss : 0.0517, accuracy : 98.52\n",
            "iteration : 350, loss : 0.0524, accuracy : 98.49\n",
            "Epoch : 107, training loss : 0.0522, training accuracy : 98.48, test loss : 0.3128, test accuracy : 92.69\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.0409, accuracy : 98.80\n",
            "iteration : 100, loss : 0.0412, accuracy : 98.80\n",
            "iteration : 150, loss : 0.0397, accuracy : 98.82\n",
            "iteration : 200, loss : 0.0413, accuracy : 98.73\n",
            "iteration : 250, loss : 0.0452, accuracy : 98.64\n",
            "iteration : 300, loss : 0.0473, accuracy : 98.60\n",
            "iteration : 350, loss : 0.0476, accuracy : 98.58\n",
            "Epoch : 108, training loss : 0.0485, training accuracy : 98.55, test loss : 0.2973, test accuracy : 92.71\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.0415, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0458, accuracy : 98.71\n",
            "iteration : 150, loss : 0.0463, accuracy : 98.68\n",
            "iteration : 200, loss : 0.0466, accuracy : 98.69\n",
            "iteration : 250, loss : 0.0490, accuracy : 98.60\n",
            "iteration : 300, loss : 0.0486, accuracy : 98.57\n",
            "iteration : 350, loss : 0.0493, accuracy : 98.53\n",
            "Epoch : 109, training loss : 0.0494, training accuracy : 98.53, test loss : 0.2826, test accuracy : 93.05\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.0390, accuracy : 98.78\n",
            "iteration : 100, loss : 0.0412, accuracy : 98.80\n",
            "iteration : 150, loss : 0.0438, accuracy : 98.70\n",
            "iteration : 200, loss : 0.0435, accuracy : 98.67\n",
            "iteration : 250, loss : 0.0439, accuracy : 98.69\n",
            "iteration : 300, loss : 0.0461, accuracy : 98.63\n",
            "iteration : 350, loss : 0.0476, accuracy : 98.58\n",
            "Epoch : 110, training loss : 0.0476, training accuracy : 98.60, test loss : 0.2764, test accuracy : 93.21\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.0375, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0370, accuracy : 98.96\n",
            "iteration : 150, loss : 0.0403, accuracy : 98.77\n",
            "iteration : 200, loss : 0.0411, accuracy : 98.75\n",
            "iteration : 250, loss : 0.0424, accuracy : 98.72\n",
            "iteration : 300, loss : 0.0436, accuracy : 98.69\n",
            "iteration : 350, loss : 0.0454, accuracy : 98.64\n",
            "Epoch : 111, training loss : 0.0454, training accuracy : 98.64, test loss : 0.2776, test accuracy : 93.40\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.0346, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0352, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0372, accuracy : 98.98\n",
            "iteration : 200, loss : 0.0410, accuracy : 98.84\n",
            "iteration : 250, loss : 0.0429, accuracy : 98.78\n",
            "iteration : 300, loss : 0.0426, accuracy : 98.76\n",
            "iteration : 350, loss : 0.0438, accuracy : 98.72\n",
            "Epoch : 112, training loss : 0.0449, training accuracy : 98.66, test loss : 0.3064, test accuracy : 92.83\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.0436, accuracy : 98.80\n",
            "iteration : 100, loss : 0.0439, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0441, accuracy : 98.73\n",
            "iteration : 200, loss : 0.0440, accuracy : 98.72\n",
            "iteration : 250, loss : 0.0451, accuracy : 98.69\n",
            "iteration : 300, loss : 0.0457, accuracy : 98.68\n",
            "iteration : 350, loss : 0.0461, accuracy : 98.65\n",
            "Epoch : 113, training loss : 0.0467, training accuracy : 98.63, test loss : 0.2808, test accuracy : 93.22\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.0447, accuracy : 98.83\n",
            "iteration : 100, loss : 0.0432, accuracy : 98.84\n",
            "iteration : 150, loss : 0.0430, accuracy : 98.84\n",
            "iteration : 200, loss : 0.0435, accuracy : 98.84\n",
            "iteration : 250, loss : 0.0448, accuracy : 98.78\n",
            "iteration : 300, loss : 0.0463, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0476, accuracy : 98.70\n",
            "Epoch : 114, training loss : 0.0474, training accuracy : 98.70, test loss : 0.2821, test accuracy : 93.33\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0376, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0386, accuracy : 98.98\n",
            "iteration : 150, loss : 0.0418, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0434, accuracy : 98.77\n",
            "iteration : 250, loss : 0.0441, accuracy : 98.74\n",
            "iteration : 300, loss : 0.0441, accuracy : 98.75\n",
            "iteration : 350, loss : 0.0453, accuracy : 98.73\n",
            "Epoch : 115, training loss : 0.0459, training accuracy : 98.73, test loss : 0.3036, test accuracy : 92.80\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0444, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0441, accuracy : 98.66\n",
            "iteration : 150, loss : 0.0472, accuracy : 98.58\n",
            "iteration : 200, loss : 0.0467, accuracy : 98.63\n",
            "iteration : 250, loss : 0.0470, accuracy : 98.59\n",
            "iteration : 300, loss : 0.0481, accuracy : 98.56\n",
            "iteration : 350, loss : 0.0499, accuracy : 98.51\n",
            "Epoch : 116, training loss : 0.0505, training accuracy : 98.48, test loss : 0.3035, test accuracy : 92.88\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.0357, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0380, accuracy : 98.91\n",
            "iteration : 150, loss : 0.0374, accuracy : 98.91\n",
            "iteration : 200, loss : 0.0393, accuracy : 98.85\n",
            "iteration : 250, loss : 0.0422, accuracy : 98.75\n",
            "iteration : 300, loss : 0.0441, accuracy : 98.72\n",
            "iteration : 350, loss : 0.0451, accuracy : 98.69\n",
            "Epoch : 117, training loss : 0.0448, training accuracy : 98.70, test loss : 0.2829, test accuracy : 93.31\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.0363, accuracy : 98.95\n",
            "iteration : 100, loss : 0.0403, accuracy : 98.80\n",
            "iteration : 150, loss : 0.0408, accuracy : 98.80\n",
            "iteration : 200, loss : 0.0412, accuracy : 98.77\n",
            "iteration : 250, loss : 0.0417, accuracy : 98.74\n",
            "iteration : 300, loss : 0.0429, accuracy : 98.69\n",
            "iteration : 350, loss : 0.0457, accuracy : 98.64\n",
            "Epoch : 118, training loss : 0.0464, training accuracy : 98.63, test loss : 0.2906, test accuracy : 93.12\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.0492, accuracy : 98.62\n",
            "iteration : 100, loss : 0.0436, accuracy : 98.75\n",
            "iteration : 150, loss : 0.0418, accuracy : 98.75\n",
            "iteration : 200, loss : 0.0412, accuracy : 98.77\n",
            "iteration : 250, loss : 0.0430, accuracy : 98.69\n",
            "iteration : 300, loss : 0.0436, accuracy : 98.69\n",
            "iteration : 350, loss : 0.0457, accuracy : 98.63\n",
            "Epoch : 119, training loss : 0.0453, training accuracy : 98.66, test loss : 0.2816, test accuracy : 93.33\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.0457, accuracy : 98.66\n",
            "iteration : 100, loss : 0.0454, accuracy : 98.70\n",
            "iteration : 150, loss : 0.0459, accuracy : 98.69\n",
            "iteration : 200, loss : 0.0459, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0446, accuracy : 98.75\n",
            "iteration : 300, loss : 0.0456, accuracy : 98.71\n",
            "iteration : 350, loss : 0.0454, accuracy : 98.71\n",
            "Epoch : 120, training loss : 0.0454, training accuracy : 98.72, test loss : 0.2773, test accuracy : 93.33\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.0418, accuracy : 98.89\n",
            "iteration : 100, loss : 0.0397, accuracy : 98.95\n",
            "iteration : 150, loss : 0.0399, accuracy : 98.89\n",
            "iteration : 200, loss : 0.0412, accuracy : 98.83\n",
            "iteration : 250, loss : 0.0418, accuracy : 98.81\n",
            "iteration : 300, loss : 0.0421, accuracy : 98.79\n",
            "iteration : 350, loss : 0.0424, accuracy : 98.78\n",
            "Epoch : 121, training loss : 0.0425, training accuracy : 98.78, test loss : 0.2837, test accuracy : 93.30\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.0397, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0384, accuracy : 98.90\n",
            "iteration : 150, loss : 0.0364, accuracy : 98.98\n",
            "iteration : 200, loss : 0.0373, accuracy : 98.97\n",
            "iteration : 250, loss : 0.0388, accuracy : 98.90\n",
            "iteration : 300, loss : 0.0398, accuracy : 98.87\n",
            "iteration : 350, loss : 0.0420, accuracy : 98.77\n",
            "Epoch : 122, training loss : 0.0437, training accuracy : 98.73, test loss : 0.3049, test accuracy : 92.82\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.0572, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0488, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0456, accuracy : 98.73\n",
            "iteration : 200, loss : 0.0469, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0464, accuracy : 98.67\n",
            "iteration : 300, loss : 0.0460, accuracy : 98.68\n",
            "iteration : 350, loss : 0.0453, accuracy : 98.71\n",
            "Epoch : 123, training loss : 0.0455, training accuracy : 98.71, test loss : 0.2853, test accuracy : 93.24\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.0380, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0378, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0405, accuracy : 98.81\n",
            "iteration : 200, loss : 0.0420, accuracy : 98.76\n",
            "iteration : 250, loss : 0.0415, accuracy : 98.79\n",
            "iteration : 300, loss : 0.0422, accuracy : 98.76\n",
            "iteration : 350, loss : 0.0432, accuracy : 98.71\n",
            "Epoch : 124, training loss : 0.0440, training accuracy : 98.69, test loss : 0.3026, test accuracy : 92.91\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.0450, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0445, accuracy : 98.62\n",
            "iteration : 150, loss : 0.0425, accuracy : 98.73\n",
            "iteration : 200, loss : 0.0436, accuracy : 98.70\n",
            "iteration : 250, loss : 0.0449, accuracy : 98.70\n",
            "iteration : 300, loss : 0.0457, accuracy : 98.66\n",
            "iteration : 350, loss : 0.0455, accuracy : 98.69\n",
            "Epoch : 125, training loss : 0.0457, training accuracy : 98.67, test loss : 0.2882, test accuracy : 92.99\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.0343, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0385, accuracy : 98.90\n",
            "iteration : 150, loss : 0.0396, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0395, accuracy : 98.86\n",
            "iteration : 250, loss : 0.0384, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0403, accuracy : 98.80\n",
            "iteration : 350, loss : 0.0410, accuracy : 98.78\n",
            "Epoch : 126, training loss : 0.0405, training accuracy : 98.81, test loss : 0.2715, test accuracy : 93.59\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0367, accuracy : 99.06\n",
            "iteration : 100, loss : 0.0334, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0375, accuracy : 99.02\n",
            "iteration : 200, loss : 0.0401, accuracy : 98.91\n",
            "iteration : 250, loss : 0.0406, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0420, accuracy : 98.83\n",
            "iteration : 350, loss : 0.0425, accuracy : 98.83\n",
            "Epoch : 127, training loss : 0.0425, training accuracy : 98.82, test loss : 0.2924, test accuracy : 92.89\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0432, accuracy : 98.80\n",
            "iteration : 100, loss : 0.0403, accuracy : 98.90\n",
            "iteration : 150, loss : 0.0437, accuracy : 98.74\n",
            "iteration : 200, loss : 0.0452, accuracy : 98.68\n",
            "iteration : 250, loss : 0.0435, accuracy : 98.72\n",
            "iteration : 300, loss : 0.0453, accuracy : 98.67\n",
            "iteration : 350, loss : 0.0447, accuracy : 98.69\n",
            "Epoch : 128, training loss : 0.0447, training accuracy : 98.69, test loss : 0.2786, test accuracy : 93.50\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.0327, accuracy : 99.19\n",
            "iteration : 100, loss : 0.0320, accuracy : 99.16\n",
            "iteration : 150, loss : 0.0316, accuracy : 99.14\n",
            "iteration : 200, loss : 0.0336, accuracy : 99.07\n",
            "iteration : 250, loss : 0.0373, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0369, accuracy : 98.99\n",
            "iteration : 350, loss : 0.0372, accuracy : 98.98\n",
            "Epoch : 129, training loss : 0.0378, training accuracy : 98.96, test loss : 0.2807, test accuracy : 93.38\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.0295, accuracy : 99.20\n",
            "iteration : 100, loss : 0.0348, accuracy : 99.03\n",
            "iteration : 150, loss : 0.0360, accuracy : 98.99\n",
            "iteration : 200, loss : 0.0362, accuracy : 98.99\n",
            "iteration : 250, loss : 0.0374, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0377, accuracy : 98.92\n",
            "iteration : 350, loss : 0.0393, accuracy : 98.88\n",
            "Epoch : 130, training loss : 0.0394, training accuracy : 98.87, test loss : 0.2959, test accuracy : 92.94\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.0321, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0341, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0402, accuracy : 98.86\n",
            "iteration : 200, loss : 0.0389, accuracy : 98.91\n",
            "iteration : 250, loss : 0.0387, accuracy : 98.91\n",
            "iteration : 300, loss : 0.0402, accuracy : 98.84\n",
            "iteration : 350, loss : 0.0414, accuracy : 98.79\n",
            "Epoch : 131, training loss : 0.0417, training accuracy : 98.79, test loss : 0.2875, test accuracy : 93.12\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.0384, accuracy : 98.88\n",
            "iteration : 100, loss : 0.0371, accuracy : 98.92\n",
            "iteration : 150, loss : 0.0389, accuracy : 98.89\n",
            "iteration : 200, loss : 0.0380, accuracy : 98.92\n",
            "iteration : 250, loss : 0.0392, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0403, accuracy : 98.82\n",
            "iteration : 350, loss : 0.0425, accuracy : 98.76\n",
            "Epoch : 132, training loss : 0.0424, training accuracy : 98.76, test loss : 0.2974, test accuracy : 93.03\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.0445, accuracy : 98.62\n",
            "iteration : 100, loss : 0.0424, accuracy : 98.81\n",
            "iteration : 150, loss : 0.0428, accuracy : 98.81\n",
            "iteration : 200, loss : 0.0424, accuracy : 98.82\n",
            "iteration : 250, loss : 0.0441, accuracy : 98.77\n",
            "iteration : 300, loss : 0.0439, accuracy : 98.77\n",
            "iteration : 350, loss : 0.0447, accuracy : 98.73\n",
            "Epoch : 133, training loss : 0.0446, training accuracy : 98.74, test loss : 0.2821, test accuracy : 93.27\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0288, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0331, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0323, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0346, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0354, accuracy : 99.00\n",
            "iteration : 300, loss : 0.0380, accuracy : 98.90\n",
            "iteration : 350, loss : 0.0385, accuracy : 98.88\n",
            "Epoch : 134, training loss : 0.0389, training accuracy : 98.87, test loss : 0.2912, test accuracy : 93.13\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.0464, accuracy : 98.75\n",
            "iteration : 100, loss : 0.0395, accuracy : 98.93\n",
            "iteration : 150, loss : 0.0402, accuracy : 98.87\n",
            "iteration : 200, loss : 0.0399, accuracy : 98.89\n",
            "iteration : 250, loss : 0.0389, accuracy : 98.92\n",
            "iteration : 300, loss : 0.0395, accuracy : 98.91\n",
            "iteration : 350, loss : 0.0398, accuracy : 98.87\n",
            "Epoch : 135, training loss : 0.0406, training accuracy : 98.84, test loss : 0.2796, test accuracy : 93.37\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0362, accuracy : 98.95\n",
            "iteration : 100, loss : 0.0398, accuracy : 98.84\n",
            "iteration : 150, loss : 0.0397, accuracy : 98.81\n",
            "iteration : 200, loss : 0.0377, accuracy : 98.88\n",
            "iteration : 250, loss : 0.0379, accuracy : 98.88\n",
            "iteration : 300, loss : 0.0379, accuracy : 98.89\n",
            "iteration : 350, loss : 0.0391, accuracy : 98.86\n",
            "Epoch : 136, training loss : 0.0393, training accuracy : 98.85, test loss : 0.3114, test accuracy : 92.69\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0366, accuracy : 98.92\n",
            "iteration : 100, loss : 0.0375, accuracy : 98.91\n",
            "iteration : 150, loss : 0.0362, accuracy : 98.95\n",
            "iteration : 200, loss : 0.0377, accuracy : 98.91\n",
            "iteration : 250, loss : 0.0374, accuracy : 98.94\n",
            "iteration : 300, loss : 0.0379, accuracy : 98.93\n",
            "iteration : 350, loss : 0.0384, accuracy : 98.91\n",
            "Epoch : 137, training loss : 0.0393, training accuracy : 98.88, test loss : 0.2880, test accuracy : 93.18\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.0308, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0322, accuracy : 99.15\n",
            "iteration : 150, loss : 0.0311, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0347, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0353, accuracy : 99.02\n",
            "iteration : 300, loss : 0.0357, accuracy : 98.99\n",
            "iteration : 350, loss : 0.0367, accuracy : 98.94\n",
            "Epoch : 138, training loss : 0.0370, training accuracy : 98.93, test loss : 0.2911, test accuracy : 93.22\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0382, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0340, accuracy : 99.04\n",
            "iteration : 150, loss : 0.0349, accuracy : 99.01\n",
            "iteration : 200, loss : 0.0350, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0354, accuracy : 98.99\n",
            "iteration : 300, loss : 0.0370, accuracy : 98.91\n",
            "iteration : 350, loss : 0.0384, accuracy : 98.88\n",
            "Epoch : 139, training loss : 0.0387, training accuracy : 98.86, test loss : 0.3016, test accuracy : 92.85\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.0306, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0307, accuracy : 99.07\n",
            "iteration : 150, loss : 0.0332, accuracy : 99.02\n",
            "iteration : 200, loss : 0.0322, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0342, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0351, accuracy : 99.00\n",
            "iteration : 350, loss : 0.0361, accuracy : 98.97\n",
            "Epoch : 140, training loss : 0.0366, training accuracy : 98.96, test loss : 0.2835, test accuracy : 93.28\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0362, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0356, accuracy : 98.99\n",
            "iteration : 150, loss : 0.0352, accuracy : 99.00\n",
            "iteration : 200, loss : 0.0371, accuracy : 98.95\n",
            "iteration : 250, loss : 0.0377, accuracy : 98.92\n",
            "iteration : 300, loss : 0.0379, accuracy : 98.91\n",
            "iteration : 350, loss : 0.0379, accuracy : 98.90\n",
            "Epoch : 141, training loss : 0.0378, training accuracy : 98.91, test loss : 0.2790, test accuracy : 93.52\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0305, accuracy : 99.16\n",
            "iteration : 100, loss : 0.0331, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0339, accuracy : 99.10\n",
            "iteration : 200, loss : 0.0349, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0353, accuracy : 99.01\n",
            "iteration : 300, loss : 0.0357, accuracy : 99.02\n",
            "iteration : 350, loss : 0.0363, accuracy : 99.02\n",
            "Epoch : 142, training loss : 0.0363, training accuracy : 99.01, test loss : 0.2809, test accuracy : 93.38\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0313, accuracy : 99.08\n",
            "iteration : 100, loss : 0.0297, accuracy : 99.16\n",
            "iteration : 150, loss : 0.0293, accuracy : 99.14\n",
            "iteration : 200, loss : 0.0311, accuracy : 99.11\n",
            "iteration : 250, loss : 0.0334, accuracy : 99.07\n",
            "iteration : 300, loss : 0.0349, accuracy : 99.01\n",
            "iteration : 350, loss : 0.0368, accuracy : 98.96\n",
            "Epoch : 143, training loss : 0.0371, training accuracy : 98.96, test loss : 0.2733, test accuracy : 93.43\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0317, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0357, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0351, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0356, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0353, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0344, accuracy : 99.05\n",
            "iteration : 350, loss : 0.0348, accuracy : 99.03\n",
            "Epoch : 144, training loss : 0.0348, training accuracy : 99.03, test loss : 0.2879, test accuracy : 93.29\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0375, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0381, accuracy : 98.93\n",
            "iteration : 150, loss : 0.0364, accuracy : 98.97\n",
            "iteration : 200, loss : 0.0351, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0353, accuracy : 99.00\n",
            "iteration : 300, loss : 0.0364, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0366, accuracy : 98.93\n",
            "Epoch : 145, training loss : 0.0369, training accuracy : 98.93, test loss : 0.2900, test accuracy : 93.40\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0360, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0358, accuracy : 99.10\n",
            "iteration : 150, loss : 0.0337, accuracy : 99.15\n",
            "iteration : 200, loss : 0.0344, accuracy : 99.08\n",
            "iteration : 250, loss : 0.0341, accuracy : 99.09\n",
            "iteration : 300, loss : 0.0354, accuracy : 99.03\n",
            "iteration : 350, loss : 0.0362, accuracy : 98.99\n",
            "Epoch : 146, training loss : 0.0363, training accuracy : 98.99, test loss : 0.2831, test accuracy : 93.34\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0360, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0343, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0319, accuracy : 99.21\n",
            "iteration : 200, loss : 0.0309, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0325, accuracy : 99.17\n",
            "iteration : 300, loss : 0.0342, accuracy : 99.07\n",
            "iteration : 350, loss : 0.0343, accuracy : 99.06\n",
            "Epoch : 147, training loss : 0.0349, training accuracy : 99.04, test loss : 0.2902, test accuracy : 93.34\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0300, accuracy : 99.19\n",
            "iteration : 100, loss : 0.0313, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0347, accuracy : 98.99\n",
            "iteration : 200, loss : 0.0356, accuracy : 98.96\n",
            "iteration : 250, loss : 0.0373, accuracy : 98.92\n",
            "iteration : 300, loss : 0.0371, accuracy : 98.92\n",
            "iteration : 350, loss : 0.0375, accuracy : 98.91\n",
            "Epoch : 148, training loss : 0.0380, training accuracy : 98.90, test loss : 0.2862, test accuracy : 93.22\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0264, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0316, accuracy : 99.15\n",
            "iteration : 150, loss : 0.0344, accuracy : 99.12\n",
            "iteration : 200, loss : 0.0348, accuracy : 99.10\n",
            "iteration : 250, loss : 0.0334, accuracy : 99.11\n",
            "iteration : 300, loss : 0.0342, accuracy : 99.07\n",
            "iteration : 350, loss : 0.0337, accuracy : 99.09\n",
            "Epoch : 149, training loss : 0.0337, training accuracy : 99.09, test loss : 0.2836, test accuracy : 93.49\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0287, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0318, accuracy : 99.10\n",
            "iteration : 150, loss : 0.0319, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0333, accuracy : 99.03\n",
            "iteration : 250, loss : 0.0337, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0347, accuracy : 98.98\n",
            "iteration : 350, loss : 0.0370, accuracy : 98.92\n",
            "Epoch : 150, training loss : 0.0370, training accuracy : 98.92, test loss : 0.2816, test accuracy : 93.44\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0330, accuracy : 98.98\n",
            "iteration : 100, loss : 0.0366, accuracy : 98.91\n",
            "iteration : 150, loss : 0.0363, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0349, accuracy : 98.96\n",
            "iteration : 250, loss : 0.0352, accuracy : 98.97\n",
            "iteration : 300, loss : 0.0351, accuracy : 98.98\n",
            "iteration : 350, loss : 0.0365, accuracy : 98.93\n",
            "Epoch : 151, training loss : 0.0370, training accuracy : 98.91, test loss : 0.2880, test accuracy : 93.36\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0313, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0297, accuracy : 99.08\n",
            "iteration : 150, loss : 0.0320, accuracy : 99.07\n",
            "iteration : 200, loss : 0.0327, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0327, accuracy : 99.04\n",
            "iteration : 300, loss : 0.0328, accuracy : 99.05\n",
            "iteration : 350, loss : 0.0346, accuracy : 98.99\n",
            "Epoch : 152, training loss : 0.0346, training accuracy : 99.00, test loss : 0.2845, test accuracy : 93.17\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0327, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0317, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0333, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0331, accuracy : 99.08\n",
            "iteration : 250, loss : 0.0334, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0329, accuracy : 99.07\n",
            "iteration : 350, loss : 0.0348, accuracy : 99.02\n",
            "Epoch : 153, training loss : 0.0347, training accuracy : 99.03, test loss : 0.2826, test accuracy : 93.57\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0281, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0305, accuracy : 99.17\n",
            "iteration : 150, loss : 0.0300, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0315, accuracy : 99.15\n",
            "iteration : 250, loss : 0.0326, accuracy : 99.13\n",
            "iteration : 300, loss : 0.0341, accuracy : 99.08\n",
            "iteration : 350, loss : 0.0345, accuracy : 99.06\n",
            "Epoch : 154, training loss : 0.0343, training accuracy : 99.07, test loss : 0.2700, test accuracy : 93.75\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0251, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0277, accuracy : 99.30\n",
            "iteration : 200, loss : 0.0302, accuracy : 99.17\n",
            "iteration : 250, loss : 0.0305, accuracy : 99.16\n",
            "iteration : 300, loss : 0.0303, accuracy : 99.16\n",
            "iteration : 350, loss : 0.0304, accuracy : 99.16\n",
            "Epoch : 155, training loss : 0.0311, training accuracy : 99.14, test loss : 0.2868, test accuracy : 93.21\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0294, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0275, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0281, accuracy : 99.26\n",
            "iteration : 200, loss : 0.0299, accuracy : 99.20\n",
            "iteration : 250, loss : 0.0293, accuracy : 99.21\n",
            "iteration : 300, loss : 0.0306, accuracy : 99.16\n",
            "iteration : 350, loss : 0.0316, accuracy : 99.13\n",
            "Epoch : 156, training loss : 0.0320, training accuracy : 99.12, test loss : 0.2988, test accuracy : 93.18\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0270, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0284, accuracy : 99.26\n",
            "iteration : 150, loss : 0.0295, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0306, accuracy : 99.11\n",
            "iteration : 250, loss : 0.0311, accuracy : 99.10\n",
            "iteration : 300, loss : 0.0317, accuracy : 99.08\n",
            "iteration : 350, loss : 0.0327, accuracy : 99.03\n",
            "Epoch : 157, training loss : 0.0328, training accuracy : 99.04, test loss : 0.2880, test accuracy : 93.19\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0347, accuracy : 99.08\n",
            "iteration : 100, loss : 0.0329, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0325, accuracy : 99.12\n",
            "iteration : 200, loss : 0.0319, accuracy : 99.11\n",
            "iteration : 250, loss : 0.0322, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0316, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0333, accuracy : 99.08\n",
            "Epoch : 158, training loss : 0.0336, training accuracy : 99.06, test loss : 0.2757, test accuracy : 93.40\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0312, accuracy : 99.08\n",
            "iteration : 100, loss : 0.0307, accuracy : 99.13\n",
            "iteration : 150, loss : 0.0319, accuracy : 99.11\n",
            "iteration : 200, loss : 0.0324, accuracy : 99.10\n",
            "iteration : 250, loss : 0.0314, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0319, accuracy : 99.11\n",
            "iteration : 350, loss : 0.0317, accuracy : 99.12\n",
            "Epoch : 159, training loss : 0.0317, training accuracy : 99.12, test loss : 0.2802, test accuracy : 93.48\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0238, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0283, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0293, accuracy : 99.21\n",
            "iteration : 200, loss : 0.0295, accuracy : 99.24\n",
            "iteration : 250, loss : 0.0300, accuracy : 99.22\n",
            "iteration : 300, loss : 0.0300, accuracy : 99.23\n",
            "iteration : 350, loss : 0.0305, accuracy : 99.20\n",
            "Epoch : 160, training loss : 0.0308, training accuracy : 99.20, test loss : 0.2835, test accuracy : 93.41\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0232, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0294, accuracy : 99.19\n",
            "iteration : 150, loss : 0.0299, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0301, accuracy : 99.18\n",
            "iteration : 250, loss : 0.0300, accuracy : 99.19\n",
            "iteration : 300, loss : 0.0316, accuracy : 99.15\n",
            "iteration : 350, loss : 0.0331, accuracy : 99.11\n",
            "Epoch : 161, training loss : 0.0328, training accuracy : 99.12, test loss : 0.2835, test accuracy : 93.33\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0222, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0296, accuracy : 99.24\n",
            "iteration : 150, loss : 0.0286, accuracy : 99.28\n",
            "iteration : 200, loss : 0.0278, accuracy : 99.29\n",
            "iteration : 250, loss : 0.0279, accuracy : 99.26\n",
            "iteration : 300, loss : 0.0286, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0287, accuracy : 99.22\n",
            "Epoch : 162, training loss : 0.0288, training accuracy : 99.22, test loss : 0.2861, test accuracy : 93.48\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0262, accuracy : 99.28\n",
            "iteration : 100, loss : 0.0295, accuracy : 99.16\n",
            "iteration : 150, loss : 0.0307, accuracy : 99.15\n",
            "iteration : 200, loss : 0.0297, accuracy : 99.16\n",
            "iteration : 250, loss : 0.0303, accuracy : 99.16\n",
            "iteration : 300, loss : 0.0309, accuracy : 99.13\n",
            "iteration : 350, loss : 0.0311, accuracy : 99.13\n",
            "Epoch : 163, training loss : 0.0314, training accuracy : 99.13, test loss : 0.2878, test accuracy : 93.25\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0236, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0261, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0278, accuracy : 99.22\n",
            "iteration : 200, loss : 0.0298, accuracy : 99.16\n",
            "iteration : 250, loss : 0.0306, accuracy : 99.13\n",
            "iteration : 300, loss : 0.0311, accuracy : 99.11\n",
            "iteration : 350, loss : 0.0315, accuracy : 99.12\n",
            "Epoch : 164, training loss : 0.0314, training accuracy : 99.12, test loss : 0.2777, test accuracy : 93.57\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0213, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0260, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0268, accuracy : 99.28\n",
            "iteration : 200, loss : 0.0275, accuracy : 99.21\n",
            "iteration : 250, loss : 0.0279, accuracy : 99.22\n",
            "iteration : 300, loss : 0.0280, accuracy : 99.23\n",
            "iteration : 350, loss : 0.0286, accuracy : 99.19\n",
            "Epoch : 165, training loss : 0.0283, training accuracy : 99.21, test loss : 0.2972, test accuracy : 93.25\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0222, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0283, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0292, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0304, accuracy : 99.17\n",
            "iteration : 250, loss : 0.0303, accuracy : 99.17\n",
            "iteration : 300, loss : 0.0314, accuracy : 99.15\n",
            "iteration : 350, loss : 0.0322, accuracy : 99.11\n",
            "Epoch : 166, training loss : 0.0320, training accuracy : 99.12, test loss : 0.2829, test accuracy : 93.47\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0291, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0304, accuracy : 99.22\n",
            "iteration : 150, loss : 0.0330, accuracy : 99.12\n",
            "iteration : 200, loss : 0.0331, accuracy : 99.12\n",
            "iteration : 250, loss : 0.0322, accuracy : 99.15\n",
            "iteration : 300, loss : 0.0320, accuracy : 99.15\n",
            "iteration : 350, loss : 0.0311, accuracy : 99.18\n",
            "Epoch : 167, training loss : 0.0313, training accuracy : 99.17, test loss : 0.2720, test accuracy : 93.59\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0267, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0275, accuracy : 99.27\n",
            "iteration : 150, loss : 0.0286, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0288, accuracy : 99.21\n",
            "iteration : 250, loss : 0.0281, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0285, accuracy : 99.21\n",
            "iteration : 350, loss : 0.0278, accuracy : 99.22\n",
            "Epoch : 168, training loss : 0.0278, training accuracy : 99.22, test loss : 0.2752, test accuracy : 93.67\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0246, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0259, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0261, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0256, accuracy : 99.29\n",
            "iteration : 250, loss : 0.0259, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0267, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0269, accuracy : 99.25\n",
            "Epoch : 169, training loss : 0.0271, training accuracy : 99.25, test loss : 0.2828, test accuracy : 93.60\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0289, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0304, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0293, accuracy : 99.22\n",
            "iteration : 200, loss : 0.0294, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0298, accuracy : 99.19\n",
            "iteration : 300, loss : 0.0305, accuracy : 99.17\n",
            "iteration : 350, loss : 0.0311, accuracy : 99.14\n",
            "Epoch : 170, training loss : 0.0312, training accuracy : 99.13, test loss : 0.2836, test accuracy : 93.32\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0305, accuracy : 99.03\n",
            "iteration : 100, loss : 0.0301, accuracy : 99.08\n",
            "iteration : 150, loss : 0.0296, accuracy : 99.14\n",
            "iteration : 200, loss : 0.0290, accuracy : 99.17\n",
            "iteration : 250, loss : 0.0290, accuracy : 99.17\n",
            "iteration : 300, loss : 0.0294, accuracy : 99.16\n",
            "iteration : 350, loss : 0.0297, accuracy : 99.15\n",
            "Epoch : 171, training loss : 0.0296, training accuracy : 99.16, test loss : 0.2779, test accuracy : 93.63\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0193, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0211, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0233, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0257, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0264, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0274, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0274, accuracy : 99.29\n",
            "Epoch : 172, training loss : 0.0273, training accuracy : 99.30, test loss : 0.2780, test accuracy : 93.68\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0302, accuracy : 99.16\n",
            "iteration : 100, loss : 0.0278, accuracy : 99.21\n",
            "iteration : 150, loss : 0.0269, accuracy : 99.26\n",
            "iteration : 200, loss : 0.0277, accuracy : 99.20\n",
            "iteration : 250, loss : 0.0279, accuracy : 99.19\n",
            "iteration : 300, loss : 0.0280, accuracy : 99.19\n",
            "iteration : 350, loss : 0.0292, accuracy : 99.17\n",
            "Epoch : 173, training loss : 0.0295, training accuracy : 99.16, test loss : 0.2806, test accuracy : 93.63\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0251, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0232, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0237, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0240, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0242, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0248, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0251, accuracy : 99.33\n",
            "Epoch : 174, training loss : 0.0259, training accuracy : 99.30, test loss : 0.2961, test accuracy : 93.38\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0203, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0254, accuracy : 99.39\n",
            "iteration : 150, loss : 0.0251, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0238, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0248, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0244, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0248, accuracy : 99.32\n",
            "Epoch : 175, training loss : 0.0252, training accuracy : 99.31, test loss : 0.2848, test accuracy : 93.60\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0282, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0262, accuracy : 99.30\n",
            "iteration : 150, loss : 0.0283, accuracy : 99.23\n",
            "iteration : 200, loss : 0.0303, accuracy : 99.17\n",
            "iteration : 250, loss : 0.0302, accuracy : 99.15\n",
            "iteration : 300, loss : 0.0310, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0302, accuracy : 99.17\n",
            "Epoch : 176, training loss : 0.0301, training accuracy : 99.18, test loss : 0.2824, test accuracy : 93.73\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0191, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0236, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0254, accuracy : 99.34\n",
            "iteration : 200, loss : 0.0250, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0260, accuracy : 99.31\n",
            "iteration : 300, loss : 0.0262, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0262, accuracy : 99.29\n",
            "Epoch : 177, training loss : 0.0265, training accuracy : 99.28, test loss : 0.2913, test accuracy : 93.08\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0257, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0256, accuracy : 99.24\n",
            "iteration : 150, loss : 0.0257, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0257, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0269, accuracy : 99.22\n",
            "iteration : 300, loss : 0.0264, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0267, accuracy : 99.24\n",
            "Epoch : 178, training loss : 0.0268, training accuracy : 99.24, test loss : 0.2999, test accuracy : 93.39\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0238, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0232, accuracy : 99.39\n",
            "iteration : 150, loss : 0.0268, accuracy : 99.28\n",
            "iteration : 200, loss : 0.0272, accuracy : 99.25\n",
            "iteration : 250, loss : 0.0277, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0280, accuracy : 99.19\n",
            "iteration : 350, loss : 0.0282, accuracy : 99.20\n",
            "Epoch : 179, training loss : 0.0281, training accuracy : 99.21, test loss : 0.2822, test accuracy : 93.56\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0221, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0239, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0235, accuracy : 99.34\n",
            "iteration : 200, loss : 0.0237, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0237, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0242, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0250, accuracy : 99.33\n",
            "Epoch : 180, training loss : 0.0251, training accuracy : 99.32, test loss : 0.2884, test accuracy : 93.42\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0346, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0291, accuracy : 99.17\n",
            "iteration : 150, loss : 0.0272, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0286, accuracy : 99.24\n",
            "iteration : 250, loss : 0.0275, accuracy : 99.26\n",
            "iteration : 300, loss : 0.0276, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0282, accuracy : 99.23\n",
            "Epoch : 181, training loss : 0.0279, training accuracy : 99.24, test loss : 0.2816, test accuracy : 93.46\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0237, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0241, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0239, accuracy : 99.45\n",
            "iteration : 200, loss : 0.0242, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0232, accuracy : 99.46\n",
            "iteration : 300, loss : 0.0236, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0240, accuracy : 99.42\n",
            "Epoch : 182, training loss : 0.0244, training accuracy : 99.41, test loss : 0.2808, test accuracy : 93.53\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0225, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0246, accuracy : 99.35\n",
            "iteration : 150, loss : 0.0246, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0251, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0268, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0270, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0269, accuracy : 99.27\n",
            "Epoch : 183, training loss : 0.0265, training accuracy : 99.28, test loss : 0.2826, test accuracy : 93.59\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0250, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0235, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0237, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0233, accuracy : 99.33\n",
            "iteration : 250, loss : 0.0246, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0237, accuracy : 99.35\n",
            "iteration : 350, loss : 0.0232, accuracy : 99.38\n",
            "Epoch : 184, training loss : 0.0233, training accuracy : 99.37, test loss : 0.2740, test accuracy : 93.70\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0176, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0184, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0205, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0211, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0220, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0231, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0236, accuracy : 99.40\n",
            "Epoch : 185, training loss : 0.0239, training accuracy : 99.38, test loss : 0.2722, test accuracy : 93.90\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0226, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0233, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0228, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0242, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0246, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0263, accuracy : 99.30\n",
            "iteration : 350, loss : 0.0268, accuracy : 99.29\n",
            "Epoch : 186, training loss : 0.0265, training accuracy : 99.29, test loss : 0.2805, test accuracy : 93.52\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0177, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0209, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0222, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0230, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0236, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0238, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0237, accuracy : 99.37\n",
            "Epoch : 187, training loss : 0.0239, training accuracy : 99.35, test loss : 0.2805, test accuracy : 93.68\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0216, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0224, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0238, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0242, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0245, accuracy : 99.37\n",
            "iteration : 300, loss : 0.0245, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0243, accuracy : 99.36\n",
            "Epoch : 188, training loss : 0.0246, training accuracy : 99.35, test loss : 0.2804, test accuracy : 93.59\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0208, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0258, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0231, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0226, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0225, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0229, accuracy : 99.40\n",
            "iteration : 350, loss : 0.0227, accuracy : 99.41\n",
            "Epoch : 189, training loss : 0.0227, training accuracy : 99.41, test loss : 0.2837, test accuracy : 93.52\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0240, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0228, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0233, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0247, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0241, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0245, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0241, accuracy : 99.39\n",
            "Epoch : 190, training loss : 0.0243, training accuracy : 99.38, test loss : 0.2737, test accuracy : 93.72\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0223, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0222, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0223, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0229, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0220, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0226, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0229, accuracy : 99.46\n",
            "Epoch : 191, training loss : 0.0230, training accuracy : 99.45, test loss : 0.2791, test accuracy : 93.67\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0181, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0199, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0202, accuracy : 99.45\n",
            "iteration : 200, loss : 0.0204, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0210, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0215, accuracy : 99.44\n",
            "iteration : 350, loss : 0.0220, accuracy : 99.43\n",
            "Epoch : 192, training loss : 0.0221, training accuracy : 99.43, test loss : 0.2741, test accuracy : 93.63\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0205, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0196, accuracy : 99.50\n",
            "iteration : 150, loss : 0.0204, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0215, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0220, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0219, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0223, accuracy : 99.39\n",
            "Epoch : 193, training loss : 0.0221, training accuracy : 99.40, test loss : 0.2753, test accuracy : 93.74\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0257, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0260, accuracy : 99.24\n",
            "iteration : 150, loss : 0.0246, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0242, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0253, accuracy : 99.29\n",
            "iteration : 300, loss : 0.0256, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0257, accuracy : 99.30\n",
            "Epoch : 194, training loss : 0.0259, training accuracy : 99.29, test loss : 0.2904, test accuracy : 93.15\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0237, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0236, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0234, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0224, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0234, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0236, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0232, accuracy : 99.43\n",
            "Epoch : 195, training loss : 0.0228, training accuracy : 99.43, test loss : 0.2744, test accuracy : 93.65\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0123, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0162, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0168, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0167, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0175, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0171, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0182, accuracy : 99.56\n",
            "Epoch : 196, training loss : 0.0183, training accuracy : 99.56, test loss : 0.2717, test accuracy : 93.78\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0259, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0248, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0228, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0235, accuracy : 99.38\n",
            "iteration : 250, loss : 0.0234, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0241, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0245, accuracy : 99.36\n",
            "Epoch : 197, training loss : 0.0243, training accuracy : 99.37, test loss : 0.2713, test accuracy : 93.70\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0225, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0189, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0182, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0181, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0182, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0186, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0196, accuracy : 99.50\n",
            "Epoch : 198, training loss : 0.0197, training accuracy : 99.50, test loss : 0.2693, test accuracy : 93.69\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0187, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0178, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0188, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0196, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0210, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0210, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0217, accuracy : 99.43\n",
            "Epoch : 199, training loss : 0.0216, training accuracy : 99.43, test loss : 0.2744, test accuracy : 93.65\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0229, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0206, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0195, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0201, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0212, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0214, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0206, accuracy : 99.48\n",
            "Epoch : 200, training loss : 0.0208, training accuracy : 99.47, test loss : 0.2700, test accuracy : 93.97\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0164, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0175, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0190, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0189, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0199, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0203, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0207, accuracy : 99.48\n",
            "Epoch : 201, training loss : 0.0208, training accuracy : 99.48, test loss : 0.2682, test accuracy : 93.74\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0159, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0167, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0165, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0182, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0178, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0198, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0199, accuracy : 99.49\n",
            "Epoch : 202, training loss : 0.0197, training accuracy : 99.50, test loss : 0.2668, test accuracy : 93.99\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0225, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0224, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0207, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0196, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0196, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0200, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0205, accuracy : 99.44\n",
            "Epoch : 203, training loss : 0.0203, training accuracy : 99.45, test loss : 0.2740, test accuracy : 93.80\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0183, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0185, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0176, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0183, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0191, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0195, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0190, accuracy : 99.51\n",
            "Epoch : 204, training loss : 0.0188, training accuracy : 99.52, test loss : 0.2709, test accuracy : 93.90\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0179, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0181, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0192, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0201, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0199, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0201, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0203, accuracy : 99.50\n",
            "Epoch : 205, training loss : 0.0203, training accuracy : 99.51, test loss : 0.2788, test accuracy : 93.73\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0168, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0176, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0185, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0188, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0191, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0193, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0215, accuracy : 99.47\n",
            "Epoch : 206, training loss : 0.0214, training accuracy : 99.46, test loss : 0.2767, test accuracy : 93.49\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0231, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0206, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0189, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0197, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0198, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0200, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0197, accuracy : 99.52\n",
            "Epoch : 207, training loss : 0.0197, training accuracy : 99.52, test loss : 0.2713, test accuracy : 93.88\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0216, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0194, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0185, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0189, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0187, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0187, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0188, accuracy : 99.52\n",
            "Epoch : 208, training loss : 0.0187, training accuracy : 99.53, test loss : 0.2770, test accuracy : 93.67\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0176, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0170, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0172, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0181, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0179, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0177, accuracy : 99.58\n",
            "Epoch : 209, training loss : 0.0181, training accuracy : 99.58, test loss : 0.2641, test accuracy : 94.01\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0161, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0164, accuracy : 99.62\n",
            "iteration : 200, loss : 0.0163, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0178, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0187, accuracy : 99.52\n",
            "Epoch : 210, training loss : 0.0190, training accuracy : 99.51, test loss : 0.2679, test accuracy : 93.88\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0152, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0170, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0183, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0175, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0179, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0187, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0195, accuracy : 99.45\n",
            "Epoch : 211, training loss : 0.0193, training accuracy : 99.46, test loss : 0.2757, test accuracy : 93.59\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0157, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0177, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0162, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0164, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0171, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0170, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0179, accuracy : 99.55\n",
            "Epoch : 212, training loss : 0.0178, training accuracy : 99.55, test loss : 0.2688, test accuracy : 93.70\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0200, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0201, accuracy : 99.50\n",
            "iteration : 150, loss : 0.0185, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0180, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0183, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0177, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0175, accuracy : 99.56\n",
            "Epoch : 213, training loss : 0.0176, training accuracy : 99.56, test loss : 0.2735, test accuracy : 93.88\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0134, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0145, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0158, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0162, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0157, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0152, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0152, accuracy : 99.59\n",
            "Epoch : 214, training loss : 0.0151, training accuracy : 99.59, test loss : 0.2668, test accuracy : 94.04\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0146, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0162, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0162, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0157, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0159, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0157, accuracy : 99.59\n",
            "iteration : 350, loss : 0.0165, accuracy : 99.58\n",
            "Epoch : 215, training loss : 0.0164, training accuracy : 99.58, test loss : 0.2715, test accuracy : 93.90\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0150, accuracy : 99.65\n",
            "iteration : 150, loss : 0.0147, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0147, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0156, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0168, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0166, accuracy : 99.58\n",
            "Epoch : 216, training loss : 0.0167, training accuracy : 99.59, test loss : 0.2681, test accuracy : 94.00\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0129, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0151, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0168, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0172, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0167, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0180, accuracy : 99.58\n",
            "Epoch : 217, training loss : 0.0177, training accuracy : 99.58, test loss : 0.2695, test accuracy : 93.97\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0214, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0184, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0174, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0166, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0173, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0168, accuracy : 99.58\n",
            "iteration : 350, loss : 0.0163, accuracy : 99.59\n",
            "Epoch : 218, training loss : 0.0160, training accuracy : 99.60, test loss : 0.2726, test accuracy : 93.99\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0164, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0175, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0177, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0180, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0170, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0163, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0165, accuracy : 99.59\n",
            "Epoch : 219, training loss : 0.0164, training accuracy : 99.59, test loss : 0.2686, test accuracy : 93.98\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0144, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0161, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0166, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0175, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0168, accuracy : 99.56\n",
            "iteration : 300, loss : 0.0168, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0167, accuracy : 99.56\n",
            "Epoch : 220, training loss : 0.0169, training accuracy : 99.57, test loss : 0.2753, test accuracy : 93.88\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0130, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0147, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0153, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0145, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0151, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0147, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0156, accuracy : 99.60\n",
            "Epoch : 221, training loss : 0.0158, training accuracy : 99.61, test loss : 0.2655, test accuracy : 93.87\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0218, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0174, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0160, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0155, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0160, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0157, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0159, accuracy : 99.61\n",
            "Epoch : 222, training loss : 0.0162, training accuracy : 99.60, test loss : 0.2733, test accuracy : 93.73\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0164, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0188, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0182, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0170, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0183, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0173, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0168, accuracy : 99.57\n",
            "Epoch : 223, training loss : 0.0167, training accuracy : 99.58, test loss : 0.2634, test accuracy : 93.99\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0149, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0146, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0142, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0148, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0153, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0148, accuracy : 99.66\n",
            "Epoch : 224, training loss : 0.0151, training accuracy : 99.65, test loss : 0.2690, test accuracy : 93.94\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0166, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0182, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0180, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0169, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0170, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0166, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0161, accuracy : 99.63\n",
            "Epoch : 225, training loss : 0.0162, training accuracy : 99.62, test loss : 0.2632, test accuracy : 94.05\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0147, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0133, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0147, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0154, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0151, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0153, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0150, accuracy : 99.65\n",
            "Epoch : 226, training loss : 0.0151, training accuracy : 99.64, test loss : 0.2679, test accuracy : 93.85\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0146, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0138, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0139, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0143, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0151, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0150, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0152, accuracy : 99.64\n",
            "Epoch : 227, training loss : 0.0152, training accuracy : 99.64, test loss : 0.2714, test accuracy : 93.91\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0143, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0136, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0148, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0141, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0141, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0139, accuracy : 99.64\n",
            "Epoch : 228, training loss : 0.0140, training accuracy : 99.64, test loss : 0.2661, test accuracy : 93.95\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0122, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0127, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0131, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0131, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0138, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0140, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0150, accuracy : 99.66\n",
            "Epoch : 229, training loss : 0.0151, training accuracy : 99.65, test loss : 0.2568, test accuracy : 94.07\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0127, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0129, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0140, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0145, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0144, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0145, accuracy : 99.68\n",
            "Epoch : 230, training loss : 0.0147, training accuracy : 99.67, test loss : 0.2642, test accuracy : 94.05\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0137, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0131, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0124, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0123, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.69\n",
            "Epoch : 231, training loss : 0.0126, training accuracy : 99.69, test loss : 0.2605, test accuracy : 94.13\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0142, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0131, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0137, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0135, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0139, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0144, accuracy : 99.67\n",
            "Epoch : 232, training loss : 0.0144, training accuracy : 99.67, test loss : 0.2682, test accuracy : 93.91\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0116, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0144, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0141, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0152, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0158, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0150, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0148, accuracy : 99.65\n",
            "Epoch : 233, training loss : 0.0147, training accuracy : 99.65, test loss : 0.2683, test accuracy : 93.89\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0117, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0133, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0128, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0140, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0137, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0135, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0134, accuracy : 99.69\n",
            "Epoch : 234, training loss : 0.0135, training accuracy : 99.68, test loss : 0.2628, test accuracy : 94.02\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0101, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0126, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0151, accuracy : 99.61\n",
            "iteration : 200, loss : 0.0141, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0145, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0140, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0138, accuracy : 99.67\n",
            "Epoch : 235, training loss : 0.0140, training accuracy : 99.66, test loss : 0.2565, test accuracy : 94.31\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0174, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0140, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0141, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0132, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0129, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0136, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0138, accuracy : 99.68\n",
            "Epoch : 236, training loss : 0.0138, training accuracy : 99.68, test loss : 0.2598, test accuracy : 94.00\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0144, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0123, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0118, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0122, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0125, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0125, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0126, accuracy : 99.71\n",
            "Epoch : 237, training loss : 0.0125, training accuracy : 99.71, test loss : 0.2684, test accuracy : 94.10\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0098, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0123, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0120, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0122, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0120, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0125, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.69\n",
            "Epoch : 238, training loss : 0.0125, training accuracy : 99.70, test loss : 0.2646, test accuracy : 94.09\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0108, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0088, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0092, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0089, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0092, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0097, accuracy : 99.78\n",
            "Epoch : 239, training loss : 0.0099, training accuracy : 99.78, test loss : 0.2736, test accuracy : 93.97\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0116, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0132, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0129, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0129, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0130, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0135, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0136, accuracy : 99.69\n",
            "Epoch : 240, training loss : 0.0137, training accuracy : 99.68, test loss : 0.2578, test accuracy : 94.09\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0130, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0123, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0122, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0116, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0117, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0115, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0114, accuracy : 99.74\n",
            "Epoch : 241, training loss : 0.0115, training accuracy : 99.73, test loss : 0.2623, test accuracy : 94.06\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0117, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0100, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0099, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0099, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0117, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.73\n",
            "Epoch : 242, training loss : 0.0119, training accuracy : 99.72, test loss : 0.2577, test accuracy : 94.17\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0092, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0102, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0112, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0108, accuracy : 99.75\n",
            "Epoch : 243, training loss : 0.0106, training accuracy : 99.75, test loss : 0.2600, test accuracy : 94.10\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0137, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0117, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0113, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0109, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0115, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0116, accuracy : 99.75\n",
            "Epoch : 244, training loss : 0.0115, training accuracy : 99.75, test loss : 0.2671, test accuracy : 94.03\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0081, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0105, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0113, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0122, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0117, accuracy : 99.71\n",
            "Epoch : 245, training loss : 0.0115, training accuracy : 99.71, test loss : 0.2595, test accuracy : 94.14\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0130, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0134, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0140, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0124, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0121, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0114, accuracy : 99.73\n",
            "Epoch : 246, training loss : 0.0113, training accuracy : 99.73, test loss : 0.2614, test accuracy : 94.18\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0119, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0125, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0129, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0127, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0120, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0124, accuracy : 99.73\n",
            "Epoch : 247, training loss : 0.0123, training accuracy : 99.73, test loss : 0.2560, test accuracy : 94.29\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0083, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0085, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0099, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0099, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0100, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0100, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.78\n",
            "Epoch : 248, training loss : 0.0105, training accuracy : 99.78, test loss : 0.2575, test accuracy : 94.19\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0128, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0134, accuracy : 99.67\n",
            "iteration : 150, loss : 0.0113, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0118, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0119, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0124, accuracy : 99.73\n",
            "Epoch : 249, training loss : 0.0123, training accuracy : 99.74, test loss : 0.2590, test accuracy : 94.13\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0082, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0088, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0087, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0095, accuracy : 99.79\n",
            "Epoch : 250, training loss : 0.0096, training accuracy : 99.78, test loss : 0.2549, test accuracy : 94.27\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0122, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0105, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0094, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0102, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.75\n",
            "Epoch : 251, training loss : 0.0106, training accuracy : 99.74, test loss : 0.2578, test accuracy : 94.10\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0105, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0104, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0107, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0111, accuracy : 99.76\n",
            "Epoch : 252, training loss : 0.0110, training accuracy : 99.76, test loss : 0.2544, test accuracy : 94.29\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0075, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0091, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0107, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0101, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0097, accuracy : 99.77\n",
            "Epoch : 253, training loss : 0.0096, training accuracy : 99.78, test loss : 0.2582, test accuracy : 94.31\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0117, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0125, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0123, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0120, accuracy : 99.69\n",
            "iteration : 250, loss : 0.0118, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0124, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0122, accuracy : 99.71\n",
            "Epoch : 254, training loss : 0.0121, training accuracy : 99.72, test loss : 0.2548, test accuracy : 94.20\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0076, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0098, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0091, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0107, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.77\n",
            "Epoch : 255, training loss : 0.0103, training accuracy : 99.77, test loss : 0.2545, test accuracy : 94.21\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0100, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0109, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0107, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0110, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.76\n",
            "Epoch : 256, training loss : 0.0111, training accuracy : 99.76, test loss : 0.2562, test accuracy : 94.07\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0118, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0113, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0118, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0117, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0113, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0111, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.77\n",
            "Epoch : 257, training loss : 0.0103, training accuracy : 99.77, test loss : 0.2541, test accuracy : 94.21\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0084, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0086, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0091, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0089, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0098, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0098, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0100, accuracy : 99.77\n",
            "Epoch : 258, training loss : 0.0098, training accuracy : 99.77, test loss : 0.2555, test accuracy : 94.31\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0109, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0104, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0105, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0100, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0103, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0100, accuracy : 99.75\n",
            "Epoch : 259, training loss : 0.0102, training accuracy : 99.74, test loss : 0.2482, test accuracy : 94.41\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0075, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0084, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0100, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0101, accuracy : 99.78\n",
            "iteration : 250, loss : 0.0103, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0102, accuracy : 99.77\n",
            "Epoch : 260, training loss : 0.0104, training accuracy : 99.76, test loss : 0.2461, test accuracy : 94.49\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0100, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0097, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0089, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0096, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0092, accuracy : 99.77\n",
            "Epoch : 261, training loss : 0.0090, training accuracy : 99.78, test loss : 0.2470, test accuracy : 94.35\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0093, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0098, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0099, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0093, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0093, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0094, accuracy : 99.79\n",
            "Epoch : 262, training loss : 0.0095, training accuracy : 99.79, test loss : 0.2485, test accuracy : 94.45\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0079, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0083, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0088, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.79\n",
            "Epoch : 263, training loss : 0.0088, training accuracy : 99.79, test loss : 0.2507, test accuracy : 94.36\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0106, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0086, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0093, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0089, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0094, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0091, accuracy : 99.80\n",
            "Epoch : 264, training loss : 0.0091, training accuracy : 99.80, test loss : 0.2489, test accuracy : 94.27\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0081, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0089, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0084, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0081, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0086, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0091, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.81\n",
            "Epoch : 265, training loss : 0.0092, training accuracy : 99.81, test loss : 0.2493, test accuracy : 94.42\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0104, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0106, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0096, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0094, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.80\n",
            "Epoch : 266, training loss : 0.0088, training accuracy : 99.81, test loss : 0.2516, test accuracy : 94.36\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0051, accuracy : 99.92\n",
            "iteration : 100, loss : 0.0068, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0069, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0072, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0071, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0080, accuracy : 99.84\n",
            "Epoch : 267, training loss : 0.0083, training accuracy : 99.83, test loss : 0.2481, test accuracy : 94.42\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0110, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0102, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0100, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0098, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0096, accuracy : 99.79\n",
            "Epoch : 268, training loss : 0.0096, training accuracy : 99.79, test loss : 0.2527, test accuracy : 94.23\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0061, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0079, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0074, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0079, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0082, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0083, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0083, accuracy : 99.82\n",
            "Epoch : 269, training loss : 0.0082, training accuracy : 99.82, test loss : 0.2471, test accuracy : 94.39\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0119, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0104, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0096, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0090, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0091, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0090, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0087, accuracy : 99.80\n",
            "Epoch : 270, training loss : 0.0088, training accuracy : 99.80, test loss : 0.2464, test accuracy : 94.30\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0091, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0090, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0081, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0080, accuracy : 99.84\n",
            "Epoch : 271, training loss : 0.0078, training accuracy : 99.85, test loss : 0.2436, test accuracy : 94.51\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0082, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.81\n",
            "iteration : 150, loss : 0.0101, accuracy : 99.77\n",
            "iteration : 200, loss : 0.0100, accuracy : 99.77\n",
            "iteration : 250, loss : 0.0096, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0097, accuracy : 99.77\n",
            "iteration : 350, loss : 0.0099, accuracy : 99.76\n",
            "Epoch : 272, training loss : 0.0096, training accuracy : 99.76, test loss : 0.2463, test accuracy : 94.40\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0073, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0071, accuracy : 99.85\n",
            "iteration : 200, loss : 0.0077, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0080, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0081, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0080, accuracy : 99.83\n",
            "Epoch : 273, training loss : 0.0080, training accuracy : 99.82, test loss : 0.2461, test accuracy : 94.48\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0094, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0099, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0101, accuracy : 99.79\n",
            "iteration : 200, loss : 0.0092, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0090, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0091, accuracy : 99.81\n",
            "iteration : 350, loss : 0.0087, accuracy : 99.82\n",
            "Epoch : 274, training loss : 0.0087, training accuracy : 99.82, test loss : 0.2429, test accuracy : 94.43\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0077, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0071, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0076, accuracy : 99.89\n",
            "iteration : 200, loss : 0.0081, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0088, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0085, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0084, accuracy : 99.84\n",
            "Epoch : 275, training loss : 0.0085, training accuracy : 99.84, test loss : 0.2444, test accuracy : 94.52\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0114, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0104, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0092, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0095, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0095, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0092, accuracy : 99.82\n",
            "Epoch : 276, training loss : 0.0092, training accuracy : 99.82, test loss : 0.2459, test accuracy : 94.43\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0089, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0066, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0071, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0082, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0089, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0085, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.81\n",
            "Epoch : 277, training loss : 0.0089, training accuracy : 99.81, test loss : 0.2470, test accuracy : 94.53\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0092, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0092, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0092, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0093, accuracy : 99.79\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.79\n",
            "Epoch : 278, training loss : 0.0088, training accuracy : 99.80, test loss : 0.2429, test accuracy : 94.46\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0079, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.86\n",
            "iteration : 150, loss : 0.0070, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0073, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0071, accuracy : 99.87\n",
            "iteration : 300, loss : 0.0069, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0071, accuracy : 99.86\n",
            "Epoch : 279, training loss : 0.0071, training accuracy : 99.86, test loss : 0.2447, test accuracy : 94.52\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0096, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0087, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0090, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.78\n",
            "iteration : 300, loss : 0.0099, accuracy : 99.78\n",
            "iteration : 350, loss : 0.0092, accuracy : 99.80\n",
            "Epoch : 280, training loss : 0.0094, training accuracy : 99.80, test loss : 0.2452, test accuracy : 94.41\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0078, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0072, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0079, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0083, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0080, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0078, accuracy : 99.84\n",
            "Epoch : 281, training loss : 0.0077, training accuracy : 99.85, test loss : 0.2443, test accuracy : 94.53\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0095, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0091, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0088, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0088, accuracy : 99.82\n",
            "Epoch : 282, training loss : 0.0087, training accuracy : 99.82, test loss : 0.2454, test accuracy : 94.53\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0091, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0087, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0091, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0089, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0085, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0082, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0085, accuracy : 99.82\n",
            "Epoch : 283, training loss : 0.0085, training accuracy : 99.82, test loss : 0.2445, test accuracy : 94.45\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0105, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0099, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0096, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0087, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0084, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0087, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0084, accuracy : 99.83\n",
            "Epoch : 284, training loss : 0.0085, training accuracy : 99.83, test loss : 0.2404, test accuracy : 94.58\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0084, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0082, accuracy : 99.81\n",
            "iteration : 200, loss : 0.0077, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0076, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.83\n",
            "Epoch : 285, training loss : 0.0077, training accuracy : 99.82, test loss : 0.2458, test accuracy : 94.48\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0059, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0079, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0073, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0071, accuracy : 99.85\n",
            "Epoch : 286, training loss : 0.0072, training accuracy : 99.85, test loss : 0.2451, test accuracy : 94.37\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0101, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0080, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0078, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0077, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0073, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0075, accuracy : 99.85\n",
            "Epoch : 287, training loss : 0.0076, training accuracy : 99.85, test loss : 0.2416, test accuracy : 94.47\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0078, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0063, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0064, accuracy : 99.86\n",
            "iteration : 200, loss : 0.0060, accuracy : 99.88\n",
            "iteration : 250, loss : 0.0067, accuracy : 99.86\n",
            "iteration : 300, loss : 0.0066, accuracy : 99.87\n",
            "iteration : 350, loss : 0.0068, accuracy : 99.85\n",
            "Epoch : 288, training loss : 0.0068, training accuracy : 99.85, test loss : 0.2434, test accuracy : 94.53\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0089, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0090, accuracy : 99.78\n",
            "iteration : 150, loss : 0.0079, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0080, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0077, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0076, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0076, accuracy : 99.83\n",
            "Epoch : 289, training loss : 0.0075, training accuracy : 99.83, test loss : 0.2438, test accuracy : 94.55\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0088, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0067, accuracy : 99.83\n",
            "iteration : 150, loss : 0.0067, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0070, accuracy : 99.82\n",
            "iteration : 250, loss : 0.0071, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0077, accuracy : 99.80\n",
            "iteration : 350, loss : 0.0080, accuracy : 99.81\n",
            "Epoch : 290, training loss : 0.0077, training accuracy : 99.82, test loss : 0.2404, test accuracy : 94.58\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0073, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0073, accuracy : 99.82\n",
            "iteration : 150, loss : 0.0071, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0066, accuracy : 99.86\n",
            "iteration : 250, loss : 0.0076, accuracy : 99.83\n",
            "iteration : 300, loss : 0.0074, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0074, accuracy : 99.83\n",
            "Epoch : 291, training loss : 0.0074, training accuracy : 99.83, test loss : 0.2459, test accuracy : 94.53\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0052, accuracy : 99.89\n",
            "iteration : 100, loss : 0.0073, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0073, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0070, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0070, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0070, accuracy : 99.85\n",
            "Epoch : 292, training loss : 0.0072, training accuracy : 99.85, test loss : 0.2404, test accuracy : 94.59\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.86\n",
            "iteration : 100, loss : 0.0074, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0081, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0080, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0079, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0073, accuracy : 99.83\n",
            "Epoch : 293, training loss : 0.0074, training accuracy : 99.83, test loss : 0.2425, test accuracy : 94.53\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0103, accuracy : 99.75\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.76\n",
            "iteration : 150, loss : 0.0085, accuracy : 99.78\n",
            "iteration : 200, loss : 0.0079, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0080, accuracy : 99.81\n",
            "iteration : 300, loss : 0.0078, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0081, accuracy : 99.81\n",
            "Epoch : 294, training loss : 0.0083, training accuracy : 99.81, test loss : 0.2408, test accuracy : 94.46\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0067, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0068, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0078, accuracy : 99.83\n",
            "iteration : 200, loss : 0.0071, accuracy : 99.84\n",
            "iteration : 250, loss : 0.0074, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0076, accuracy : 99.83\n",
            "Epoch : 295, training loss : 0.0075, training accuracy : 99.84, test loss : 0.2431, test accuracy : 94.45\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0065, accuracy : 99.84\n",
            "iteration : 100, loss : 0.0062, accuracy : 99.88\n",
            "iteration : 150, loss : 0.0075, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0077, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0078, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0081, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0077, accuracy : 99.84\n",
            "Epoch : 296, training loss : 0.0077, training accuracy : 99.85, test loss : 0.2443, test accuracy : 94.40\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0062, accuracy : 99.91\n",
            "iteration : 100, loss : 0.0071, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0083, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0083, accuracy : 99.83\n",
            "iteration : 250, loss : 0.0077, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0075, accuracy : 99.85\n",
            "iteration : 350, loss : 0.0073, accuracy : 99.85\n",
            "Epoch : 297, training loss : 0.0074, training accuracy : 99.84, test loss : 0.2405, test accuracy : 94.58\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0055, accuracy : 99.88\n",
            "iteration : 100, loss : 0.0071, accuracy : 99.85\n",
            "iteration : 150, loss : 0.0080, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0075, accuracy : 99.84\n",
            "iteration : 300, loss : 0.0079, accuracy : 99.84\n",
            "iteration : 350, loss : 0.0079, accuracy : 99.84\n",
            "Epoch : 298, training loss : 0.0078, training accuracy : 99.84, test loss : 0.2372, test accuracy : 94.58\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0086, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0075, accuracy : 99.84\n",
            "iteration : 150, loss : 0.0077, accuracy : 99.84\n",
            "iteration : 200, loss : 0.0074, accuracy : 99.85\n",
            "iteration : 250, loss : 0.0072, accuracy : 99.85\n",
            "iteration : 300, loss : 0.0076, accuracy : 99.83\n",
            "iteration : 350, loss : 0.0076, accuracy : 99.83\n",
            "Epoch : 299, training loss : 0.0075, training accuracy : 99.84, test loss : 0.2407, test accuracy : 94.59\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0079, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0086, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0080, accuracy : 99.80\n",
            "iteration : 200, loss : 0.0075, accuracy : 99.81\n",
            "iteration : 250, loss : 0.0076, accuracy : 99.82\n",
            "iteration : 300, loss : 0.0078, accuracy : 99.82\n",
            "iteration : 350, loss : 0.0076, accuracy : 99.82\n",
            "Epoch : 300, training loss : 0.0074, training accuracy : 99.83, test loss : 0.2391, test accuracy : 94.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "id": "iUQVIKR-X3v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2304c941-e3af-4d91-fbcd-a2d03e8f2a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.19231817637588464, 95.40181315304241)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : AugMix\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7CNz1iabSB21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "7810dfa9-5a4e-419f-dac5-df9eecc341a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : AugMix\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "54939fa6-31ac-4769-e08a-1a265ee865a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_mix = {train_loss_list}\") \n",
        "print(f\"train_acc_list_mix = {train_acc_list}\")\n",
        "print(f\"test_loss_list_mix = {test_loss_list}\")\n",
        "print(f\"test_acc_list_mix = {test_acc_list}\")"
      ],
      "metadata": {
        "id": "3eiY3bTlWipW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d5d0354-1712-4410-b529-f31e2d68b70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_mix = [0.9938080568462206, 0.35387038586908565, 0.31415230790935555, 0.2923403612683782, 0.27419339736221926, 0.26117862579299184, 0.2533990740776062, 0.23811954647544922, 0.22444075424739016, 0.2126544232815908, 0.20220531072962253, 0.1921394082508113, 0.17919883605466302, 0.16964546475953202, 0.15979611703010432, 0.1510940795426123, 0.14491855573706672, 0.13836499743749134, 0.1309642790435614, 0.12873398524716617, 0.12111276983306175, 0.11748368124449803, 0.11389688689130431, 0.11343211955756838, 0.10908843426987892, 0.10161420127482919, 0.10613548600172931, 0.09860493359345246, 0.09852099015824194, 0.09253312145337099, 0.09571743579606619, 0.0894239721163542, 0.09043425027795923, 0.08437888851000606, 0.08734228457185392, 0.08459739824681263, 0.08603016985201739, 0.08408435344630423, 0.0842538814143679, 0.08313639493290648, 0.07915610890976862, 0.08019161985828706, 0.07934672697409381, 0.07348008640615275, 0.07512537754698578, 0.07437080344523438, 0.07162883216489944, 0.0783673867578227, 0.07318035851871256, 0.0718035651370883, 0.07133971100425736, 0.07315107509948132, 0.06907403099585355, 0.06669923856723195, 0.07077471686522085, 0.07059726227317156, 0.07259669639220648, 0.06424928312682039, 0.06816990300129624, 0.07005442472220194, 0.06414420206761012, 0.06591054424138734, 0.067448822017961, 0.06368785791684618, 0.06911977814121782, 0.061097499508117915, 0.0654788069770265, 0.06277953536529851, 0.06540644472227672, 0.06547933941338524, 0.05982598949344904, 0.06166121948555839, 0.06178239522865269, 0.0624033973292324, 0.06242782084730663, 0.06412811665152146, 0.058263260732356936, 0.05578944062546582, 0.06216381148998007, 0.059200966200692866, 0.0591901072205062, 0.056872100685407635, 0.05843282742350082, 0.0572318464333348, 0.05414615397853903, 0.058493246657516415, 0.054253520179025044, 0.05435604806408932, 0.05502198836046869, 0.057328540733371805, 0.05369477488509402, 0.05310384723229137, 0.052687724674913054, 0.05150350224009657, 0.053693892988237506, 0.05677452959943068, 0.05102892336624909, 0.049259034192396536, 0.0598858642962264, 0.05066231658510601, 0.050952362551588595, 0.04916074803997548, 0.048183402951382764, 0.04758161058154735, 0.049602617113409325, 0.05060552086726558, 0.05222517778582084, 0.048518680028496235, 0.04943236661191555, 0.047581606836696745, 0.045368864389040524, 0.04489896042451646, 0.04670504703587754, 0.04741490361101542, 0.045931538310770174, 0.05050023533479591, 0.04482567616988246, 0.04641831352256824, 0.04533794787548341, 0.04535460998973024, 0.04252487677591159, 0.04374113354357439, 0.04545966434976475, 0.0439752453312197, 0.04567845454906444, 0.04052308556777715, 0.042460269868868555, 0.04470255330323587, 0.03780500727092347, 0.039394413955196696, 0.041715514285838216, 0.04243160037924523, 0.04460260131949089, 0.03893550564497104, 0.04060774496163375, 0.0392627027302193, 0.03932004146527771, 0.03701888793021905, 0.038655105051975745, 0.03664332534991669, 0.03777366958124447, 0.03629977536655276, 0.0370603178433448, 0.03480971295343646, 0.036863508949757914, 0.036259976162658476, 0.0348965569795551, 0.037977313778683706, 0.03366919478457068, 0.037030147160794634, 0.037005113133009734, 0.03459730316369938, 0.03465932765962975, 0.03426345206196432, 0.031120636634816363, 0.031996667062040914, 0.03282833266700429, 0.033598321229396114, 0.03170881769935371, 0.030779237351014323, 0.03284029248120077, 0.028771491670163303, 0.03136438176636572, 0.03139654590008745, 0.02833839970310206, 0.031977539331220634, 0.0312720158529849, 0.027770824104466737, 0.02705349339371633, 0.031242900163308794, 0.02964044412090708, 0.027261546995935, 0.029478413548174143, 0.025900761455835062, 0.02524898826084062, 0.030148965941974262, 0.026517891811540094, 0.0267597234878578, 0.02813903715645738, 0.025121289003150776, 0.027856107311560532, 0.024366022234020356, 0.026482225155336707, 0.023338035278421067, 0.023866847919383486, 0.026543004754400714, 0.02394126550872434, 0.024592813269707164, 0.022726859586303272, 0.024343733735089784, 0.023029831621094246, 0.02211659858356606, 0.022111268837741883, 0.02589301477933459, 0.022846899337329595, 0.01829227261266704, 0.02427225847296962, 0.01970740448529542, 0.021607845043637323, 0.02081029445341811, 0.02079579379374217, 0.019704281869861815, 0.020279064681444085, 0.018792231148235035, 0.020250071839588445, 0.02139727115065748, 0.0197049227466196, 0.018660546160242483, 0.018084673524574255, 0.018957647942334697, 0.01931706416076125, 0.01777278999880027, 0.01756590040183568, 0.015129794698551823, 0.016428295707489115, 0.016727321038873196, 0.017704555430376353, 0.016021391709693543, 0.01640474740958452, 0.016935019902153025, 0.015831608947386666, 0.016178442318583043, 0.016684757473974472, 0.01505513205085717, 0.016159819172150116, 0.015077039387410248, 0.015197812229695257, 0.014000309425845318, 0.015123876789395462, 0.014679165285391113, 0.012634015024695192, 0.014364219435016173, 0.014739077076691942, 0.013465223549663273, 0.014016012728358127, 0.013800337897086148, 0.012497749787232861, 0.012516363376882786, 0.009907300361598714, 0.013667509257591514, 0.011486864030401557, 0.011876725475619318, 0.010641783018101784, 0.011468470570136639, 0.011481490557531253, 0.011336377388065767, 0.01226908068247089, 0.010469431545002371, 0.012306197240713954, 0.009624016467421278, 0.010600388426141848, 0.010997156687976848, 0.009556004850428582, 0.012064924670274806, 0.010308084436528243, 0.011091969076286607, 0.010341640694955299, 0.009817854730982768, 0.010156398601224267, 0.010387308722700127, 0.009045329112130492, 0.009465475142668418, 0.00877862685482424, 0.009081994435752472, 0.009157319344658415, 0.0088296683156878, 0.008334625229052695, 0.00961835922415906, 0.008183363720340538, 0.008775008815217027, 0.007791329284077435, 0.009641242586454846, 0.008015534814654125, 0.00867622388087637, 0.008515111755940628, 0.00922561221286594, 0.008882333553550123, 0.008829943086134225, 0.007059801428270877, 0.009350860031412505, 0.007657677646938379, 0.008748184417268427, 0.008469099999811793, 0.008482573831549827, 0.007719665183054743, 0.007240170458657288, 0.007590437193746934, 0.0068299245801665956, 0.007549433960127556, 0.007730878702791402, 0.007419600550887909, 0.007170365307464953, 0.007350631618208496, 0.008344900159564041, 0.007527452433888162, 0.007712224023265204, 0.007403429063178377, 0.00781597496738511, 0.00748860295596949, 0.007374107671098981]\n",
            "train_acc_list_mix = [66.52408681842245, 89.18581259925887, 90.52620434092113, 91.24827951296983, 91.84330333509793, 92.22022233986236, 92.6056114346215, 92.97617787188989, 93.47803070407623, 93.7787188988883, 94.10905240868185, 94.37797776601376, 94.85018528321864, 95.10005293806246, 95.41556379036527, 95.74377977766014, 95.87930121757543, 96.071995764955, 96.215987294865, 96.27316040232928, 96.44679724722076, 96.6225516146109, 96.74325039703547, 96.73689782953944, 96.77077818951827, 97.05876124933827, 96.83218634197988, 97.09687665431446, 97.07570142932768, 97.35309687665432, 97.18581259925887, 97.31286394917946, 97.36791953414505, 97.5796717840127, 97.37003705664372, 97.44626786659609, 97.39332980412917, 97.4907358390683, 97.50132345156167, 97.4356802541027, 97.643197458973, 97.6156696664902, 97.71942826892536, 97.80201164637374, 97.70460561143462, 97.75754367390154, 97.88671254632081, 97.66437268395977, 97.81259925886712, 97.85706723133933, 97.88035997882477, 97.8422445738486, 97.95659078877713, 98.07093700370567, 97.91212281630493, 97.84647961884595, 97.84436209634727, 98.05611434621493, 97.92271042879831, 97.84012705134992, 98.0857596611964, 97.9777660137639, 97.96294335627316, 98.07517204870302, 97.93118051879301, 98.16410799364743, 98.0307040762308, 98.1492853361567, 98.07940709370037, 97.9862361037586, 98.21492853361568, 98.04340921122287, 98.13234515616729, 98.18104817363684, 98.0582318687136, 98.0857596611964, 98.24033880359978, 98.36527263102171, 98.09846479618847, 98.22763366860772, 98.17257808364214, 98.27210164107994, 98.212811011117, 98.25304393859184, 98.42456326098464, 98.19163578613023, 98.36739015352038, 98.38644785600847, 98.36950767601905, 98.32080465854949, 98.4203282159873, 98.39915299100053, 98.41609317098994, 98.45209105346744, 98.42668078348332, 98.25092641609317, 98.51561672842774, 98.5092641609317, 98.1852832186342, 98.57278983589201, 98.5283218634198, 98.58337744838539, 98.53255690841715, 98.67019587083112, 98.55373213340391, 98.45420857596612, 98.47961884595024, 98.55373213340391, 98.53043938591847, 98.59608258337745, 98.64266807834834, 98.66172578083642, 98.63208046585495, 98.69772366331392, 98.72525145579672, 98.48385389094759, 98.69560614081524, 98.62572789835892, 98.6553732133404, 98.71889888830069, 98.78242456326099, 98.73372154579143, 98.71254632080466, 98.6913710958179, 98.67443091582848, 98.80783483324511, 98.81842244573849, 98.68713605082054, 98.96029645314981, 98.87347803070408, 98.78665960825833, 98.76336686077289, 98.73795659078878, 98.86500794070938, 98.8353626257279, 98.85230280571731, 98.87559555320276, 98.9348861831657, 98.86077289571202, 98.95817893065114, 98.90947591318158, 99.01323451561673, 98.95606140815246, 99.03229221810481, 98.93065113816834, 98.98570672313393, 99.03864478560085, 98.90312334568554, 99.08946532556908, 98.9158284806776, 98.91371095817892, 99.00052938062467, 99.0344097406035, 99.06829010058232, 99.14452091053468, 99.1233456855479, 99.03864478560085, 99.06405505558496, 99.12122816304924, 99.19745897300159, 99.11699311805188, 99.22075172048703, 99.12546320804658, 99.11911064055056, 99.20592906299629, 99.11699311805188, 99.17204870301747, 99.22075172048703, 99.2503970354685, 99.13393329804128, 99.15722604552673, 99.29698253043938, 99.16357861302276, 99.30121757543674, 99.31180518793012, 99.18051879301217, 99.2779248279513, 99.23980942297511, 99.20804658549497, 99.3223928004235, 99.23980942297511, 99.41344626786659, 99.28215987294865, 99.37109581789306, 99.38168343038645, 99.29486500794071, 99.34780307040762, 99.35415563790366, 99.41344626786659, 99.37533086289042, 99.45156167284277, 99.43250397035469, 99.40285865537321, 99.29062996294336, 99.43038644785601, 99.5574377977766, 99.37321334039174, 99.49814716781366, 99.43038644785601, 99.47273689782953, 99.47908946532557, 99.49602964531498, 99.45367919534145, 99.51508734780307, 99.50873478030704, 99.46426680783483, 99.51932239280042, 99.5299100052938, 99.57649550026468, 99.50661725780836, 99.45791424033881, 99.55320275277924, 99.55955532027528, 99.5934356802541, 99.57861302276336, 99.59131815775542, 99.5849655902594, 99.60402329274748, 99.58708311275808, 99.5659078877713, 99.60614081524616, 99.59978824775013, 99.57861302276336, 99.64849126521969, 99.62308099523557, 99.63790365272631, 99.63578613022763, 99.64002117522499, 99.64849126521969, 99.66966649020645, 99.69084171519323, 99.66543144520911, 99.64849126521969, 99.68025410269983, 99.66331392271043, 99.67601905770249, 99.71413446267867, 99.69719428268925, 99.77766013763896, 99.68237162519851, 99.73319216516676, 99.72260455267337, 99.7480148226575, 99.7480148226575, 99.71201694017999, 99.73107464266808, 99.73107464266808, 99.784012705135, 99.73530968766543, 99.784012705135, 99.74377977766014, 99.76283748014822, 99.77554261514028, 99.71836950767602, 99.7734250926416, 99.76283748014822, 99.76919004764426, 99.7734250926416, 99.74377977766014, 99.75860243515088, 99.77554261514028, 99.79460031762838, 99.78613022763366, 99.80307040762308, 99.8094229751191, 99.80518793012176, 99.83059820010588, 99.78613022763366, 99.82001058761249, 99.80307040762308, 99.84542085759661, 99.76283748014822, 99.82424563260984, 99.82212811011117, 99.8369507676019, 99.81577554261514, 99.8094229751191, 99.79671784012704, 99.86236103758603, 99.79671784012704, 99.84542085759661, 99.81577554261514, 99.82001058761249, 99.83059820010588, 99.82424563260984, 99.85389094759132, 99.84542085759661, 99.84753838009529, 99.83059820010588, 99.81789306511382, 99.82636315510852, 99.84542085759661, 99.82636315510852, 99.8094229751191, 99.8369507676019, 99.84542085759661, 99.84330333509793, 99.84118581259926, 99.8369507676019, 99.82636315510852]\n",
            "test_loss_list_mix = [0.4968765592750381, 0.4496412401398023, 0.39002737402915955, 0.3806089258515367, 0.3204718660198006, 0.3915786291016083, 0.3204018429798238, 0.27567775876206513, 0.297381679728335, 0.28301442688440576, 0.28070780485138, 0.2833815569459808, 0.26925351503579054, 0.26673262694156635, 0.2499614215510733, 0.26110943147510873, 0.2559659517571038, 0.24042939486018583, 0.24612251061069615, 0.25121348559418144, 0.2554320693381277, 0.24982432885935493, 0.26238466039592145, 0.25014860489789176, 0.27063990981482405, 0.25935193300977644, 0.25238811593575805, 0.26022648216024336, 0.24298064929305338, 0.2627089251855425, 0.27986022293129387, 0.27539388061154124, 0.26327539124873045, 0.2683038656398946, 0.28468784642424066, 0.28231597783080503, 0.2648708233061959, 0.2511889289553259, 0.2871778339658882, 0.26883744331551535, 0.2730213494277468, 0.28196089287452836, 0.2727588803379559, 0.2716984305275129, 0.28908429240040917, 0.27025664857059134, 0.28404617488530337, 0.2796709204056099, 0.28010478157404006, 0.2852920090925752, 0.2734685718428855, 0.2916511903498687, 0.27724011083517003, 0.2940135949528685, 0.2814183736986974, 0.2895678594051039, 0.3044459021778083, 0.2729633482830489, 0.2753490212683876, 0.2902740286845787, 0.2641575384665938, 0.29541431860450434, 0.2934904679765596, 0.2973368406003597, 0.3145442524851829, 0.2842784762820777, 0.2945947372811098, 0.2840387991574757, 0.2869349299181326, 0.27243131910469015, 0.29096475291047613, 0.29465110983480425, 0.2751436970908852, 0.29335411254535704, 0.30367974758002103, 0.2629663415399252, 0.2774942562933646, 0.2887668024529429, 0.2783941021371706, 0.2701931654603458, 0.2921381458347919, 0.2950442577255707, 0.28360421898975674, 0.27811501302993763, 0.29225470819601823, 0.2879698169326373, 0.2802756140191181, 0.2898149842250289, 0.3028535102950592, 0.2814833966686445, 0.28412492144122425, 0.2813930624328992, 0.27925870720954504, 0.2858440491060416, 0.29389001138727455, 0.30059922844463705, 0.2825733188028429, 0.2835676666787442, 0.2877841207618807, 0.29116936455316406, 0.28830072700100784, 0.285358795932695, 0.28429602919255986, 0.2773189860049124, 0.27414730530889597, 0.2898979450703836, 0.31280472980556534, 0.29734473288351415, 0.2825610628899406, 0.2763801336507587, 0.277620185458777, 0.30638950756367517, 0.2807564251855308, 0.282097570561603, 0.3035717385948873, 0.30351242861327005, 0.2829094833749182, 0.2905926219753775, 0.2816025845560373, 0.2773329648280553, 0.2836565510826368, 0.3049090450227845, 0.2853395010837737, 0.3026421479150361, 0.2882088396661714, 0.2714636234164822, 0.2923690947137919, 0.2785814857183426, 0.28070347221093433, 0.2958538033725584, 0.2875262723103458, 0.29740006582555817, 0.2820720241642466, 0.2911566118253212, 0.27959447470950144, 0.3113881464609328, 0.2879708492580582, 0.2911304790234449, 0.30164744040253116, 0.28354411742047353, 0.27903590285602736, 0.28088901288734347, 0.27327312507173596, 0.2879488106963097, 0.28996947509985344, 0.2830904172733426, 0.29018380200746013, 0.28617805264451923, 0.28362530970252026, 0.2815610739566824, 0.2880097535968411, 0.28445453794819175, 0.28255120502310055, 0.2700368627431054, 0.2868249623433632, 0.2987582326191021, 0.28800590190232966, 0.27571498953244267, 0.2802151000017629, 0.2834977245816559, 0.2834719321233969, 0.28607808749246244, 0.2878175952161352, 0.2777362868701126, 0.29719853200310586, 0.2828675587022421, 0.2720199037416309, 0.27524494520370285, 0.2827897258742037, 0.28359566126749214, 0.2779345959698891, 0.27804525879522163, 0.28059227344598253, 0.29605638752600144, 0.2847536772160846, 0.28240000672054055, 0.2913353420501831, 0.29991633287977937, 0.2821591763355422, 0.28840978052832333, 0.2816306263683181, 0.280769894867405, 0.28257956050847677, 0.2740063602104783, 0.2721924037471706, 0.280458538354758, 0.2804504399206124, 0.2803858033582276, 0.28368200007460864, 0.27371273335872914, 0.2791220264819761, 0.27405345537608455, 0.2753484202366249, 0.2904026607307149, 0.2743520236154105, 0.2717200533906911, 0.27128687258079354, 0.2693210001487066, 0.2743913666345179, 0.2700155303977868, 0.26815189790966754, 0.26675478429259625, 0.2739511370914532, 0.2709468726214825, 0.27877162732914385, 0.2767229562113975, 0.27129210420318095, 0.27698782736472055, 0.26407752993206185, 0.2678539391844442, 0.27572355346352445, 0.2688033563066639, 0.273533514582132, 0.26681174989789724, 0.27152517490892436, 0.2680956854612804, 0.26951589610646753, 0.2725548438508721, 0.2685760261819643, 0.2753346236970495, 0.2654521403122036, 0.27330513671040535, 0.26338427238093287, 0.26901105538412345, 0.26318498032496257, 0.2679282346849932, 0.27139169043477845, 0.2661011636841531, 0.2567788078175748, 0.26415850057759704, 0.26049654120031523, 0.26823244156206355, 0.2683363047008421, 0.2627703683523863, 0.2564784280645351, 0.25979823937785684, 0.26836607398867024, 0.2646418652661583, 0.27362839380900067, 0.25775680436259685, 0.2622781395546946, 0.2577361859308154, 0.259969442242793, 0.2670592286934455, 0.25948775546880914, 0.2614140360177878, 0.2559678828453316, 0.25749495658366117, 0.2589990839398667, 0.25487637667752366, 0.2578467573678377, 0.25440817833969404, 0.2581749869254874, 0.2548055907565297, 0.25454593778533097, 0.256248529982187, 0.2540844309454163, 0.25549203602998866, 0.24818837269227587, 0.24607416837220536, 0.2469723666013748, 0.24849579098415286, 0.2506902605231267, 0.248916748399828, 0.24932225204675512, 0.2515677338210391, 0.248076570895957, 0.25269879287510527, 0.2470587625661317, 0.24636757936255604, 0.24355621403083205, 0.24634781338311001, 0.24613529066646508, 0.24289194699011596, 0.2443599361268913, 0.24590234871150232, 0.2470326214553971, 0.24292154000688562, 0.24474981166057141, 0.24522259354810505, 0.2442855668692466, 0.24541391723551878, 0.2444736195600354, 0.24036579289674467, 0.24579018818251058, 0.24510042405888147, 0.2416404786647535, 0.24342773950147426, 0.24383827958109916, 0.2404104194640383, 0.24592714732987622, 0.2404489804278402, 0.24246013735183605, 0.2407533844805085, 0.24311559112267753, 0.24433566783280933, 0.24052496796802564, 0.23717293172946893, 0.2407319616416798, 0.239128411491858]\n",
            "test_acc_list_mix = [84.63045482483098, 85.73678549477566, 87.94944683466503, 88.44499078057775, 90.46558082360173, 88.45651505838967, 90.52320221266135, 91.9099569760295, 91.23386601106331, 91.67947141979103, 92.0098340503995, 91.64874001229256, 91.91763982790411, 92.12891825445605, 92.94714197910264, 92.58220651505839, 92.93177627535341, 93.30439459127228, 93.14305470190534, 93.27366318377382, 92.96250768285188, 93.08159188690843, 92.94714197910264, 93.03933620159803, 92.74738783036263, 93.20067609096496, 93.10848186846958, 93.04317762753534, 93.6578057775046, 93.28902888752305, 92.57068223724647, 92.79732636754764, 93.15073755377996, 92.90872771972957, 92.62830362630608, 92.60141364474492, 92.95098340503995, 93.35049170251997, 92.79732636754764, 93.05854333128457, 92.9279348494161, 92.84342347879533, 92.94714197910264, 92.96250768285188, 92.72433927473878, 93.20451751690227, 92.93561770129072, 92.92025199754148, 93.1200061462815, 92.65903503380454, 93.11232329440688, 92.80116779348494, 93.01244622003688, 92.75122925629994, 92.99708051628765, 92.76275353411187, 92.37092808850646, 93.19683466502765, 93.1200061462815, 92.70129071911494, 93.2659803318992, 92.8318992009834, 92.57452366318377, 92.60909649661954, 92.06361401352181, 92.86647203441917, 92.65903503380454, 93.00476336816226, 92.81269207129687, 92.93177627535341, 92.91641057160417, 92.74738783036263, 93.25061462814998, 92.91256914566686, 92.27105101413645, 93.39274738783037, 93.1737861094038, 92.9740319606638, 93.00476336816226, 93.18146896127843, 92.81269207129687, 92.96250768285188, 92.98939766441303, 93.24677320221267, 92.75122925629994, 92.76659496004918, 93.15842040565458, 92.93945912722803, 92.8779963122311, 93.06622618315919, 93.20451751690227, 93.34280885064535, 93.25829748002458, 93.1661032575292, 92.6820835894284, 92.67055931161647, 93.15073755377996, 93.03549477566072, 92.86647203441917, 92.89336201598033, 93.1699446834665, 92.91256914566686, 93.17762753534112, 93.28902888752305, 93.38890596189306, 93.01628764597419, 92.69360786724032, 92.70513214505225, 93.05086047940996, 93.20835894283958, 93.39658881376766, 92.8318992009834, 93.21988322065151, 93.33128457283344, 92.79732636754764, 92.88183773816841, 93.3082360172096, 93.1238475722188, 93.32744314689613, 93.32744314689613, 93.30439459127228, 92.82037492317149, 93.23909035033805, 92.91256914566686, 92.99323909035034, 93.59250153657038, 92.88952059004302, 93.50030731407499, 93.37738168408113, 92.93945912722803, 93.1238475722188, 93.0278119237861, 93.26982175783651, 93.13153042409343, 93.36969883220651, 92.69360786724032, 93.18146896127843, 93.21988322065151, 92.85110633066995, 93.28134603564843, 93.51567301782421, 93.38122311001844, 93.43116164720344, 93.28518746158574, 93.39658881376766, 93.33512599877075, 93.34280885064535, 93.2160417947142, 93.49262446220037, 93.43884449907806, 93.3581745543946, 93.1661032575292, 93.57329440688383, 93.74615857406269, 93.21220036877689, 93.18146896127843, 93.18531038721574, 93.40043023970497, 93.48110018438844, 93.4119545175169, 93.32744314689613, 93.48110018438844, 93.25445605408727, 93.56561155500921, 93.25061462814998, 93.46957590657652, 93.58866011063307, 93.66548862937923, 93.59634296250768, 93.3159188690842, 93.63475722188076, 93.68085433312845, 93.62707437000614, 93.37738168408113, 93.59634296250768, 93.73463429625077, 93.08159188690843, 93.39274738783037, 93.5579287031346, 93.41963736939152, 93.4618930547019, 93.53103872157345, 93.58866011063307, 93.69622003687769, 93.8959741856177, 93.51567301782421, 93.68085433312845, 93.59250153657038, 93.51951444376152, 93.71542716656423, 93.66548862937923, 93.63091579594345, 93.73847572218807, 93.15457897971727, 93.65012292563, 93.78457283343577, 93.70390288875231, 93.69237861094038, 93.64628149969269, 93.96896127842655, 93.73847572218807, 93.99200983405039, 93.79609711124769, 93.90365703749232, 93.73463429625077, 93.49262446220037, 93.88060848186846, 93.66933005531654, 94.00737553779963, 93.88444990780577, 93.59250153657038, 93.70390288875231, 93.88444990780577, 94.0419483712354, 93.8959741856177, 93.99969268592501, 93.96511985248924, 93.99200983405039, 93.98048555623848, 93.88060848186846, 93.86524277811924, 93.73463429625077, 93.99200983405039, 93.93822987092808, 94.04578979717272, 93.8460356484327, 93.90749846342962, 93.95359557467732, 94.06883835279656, 94.04963122311001, 94.12645974185618, 93.90749846342962, 93.88829133374308, 94.01889981561156, 94.30700676090964, 94.00353411186232, 94.10341118623234, 94.0880454824831, 93.96511985248924, 94.0918869084204, 94.05731407498463, 94.16871542716656, 94.09956976029503, 94.02658266748617, 94.1379840196681, 94.17639827904118, 94.29164105716042, 94.18792255685311, 94.12645974185618, 94.26859250153657, 94.10341118623234, 94.29164105716042, 94.31084818684695, 94.20328826060233, 94.21481253841426, 94.06883835279656, 94.21481253841426, 94.31468961278426, 94.41456668715428, 94.49139520590043, 94.34926244622004, 94.45298094652735, 94.36462814996926, 94.27243392747388, 94.41840811309157, 94.36462814996926, 94.42224953902888, 94.22633681622618, 94.39151813153042, 94.29548248309773, 94.50676090964966, 94.39920098340504, 94.4798709280885, 94.43377381684081, 94.5221266133989, 94.43377381684081, 94.52980946527352, 94.46450522433928, 94.5221266133989, 94.41072526121697, 94.52980946527352, 94.5259680393362, 94.45298094652735, 94.57974800245852, 94.4798709280885, 94.37231100184388, 94.4721880762139, 94.53365089121081, 94.54517516902274, 94.5759065765212, 94.53365089121081, 94.59127228027043, 94.5259680393362, 94.46066379840197, 94.44913952059004, 94.39920098340504, 94.57974800245852, 94.58358942839583, 94.58743085433314, 94.61047940995698]\n"
          ]
        }
      ]
    }
  ]
}