{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8ntKy6oKQGJv"
      },
      "outputs": [],
      "source": [
        "# ## Source code for unpickle function: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "# def unpickle(file):\n",
        "#     import pickle\n",
        "#     with open(file, 'rb') as fo:\n",
        "#         dict = pickle.load(fo, encoding='bytes')\n",
        "#     return dict\n",
        "\n",
        "# import numpy as np\n",
        "# def get_mean_color():\n",
        "#     d=unpickle('./data/cifar-10-batches-py/data_batch_1')\n",
        "#     channels = d[b'data']\n",
        "#     for i in range(2,6):\n",
        "#         d=unpickle('./data/cifar-10-batches-py/data_batch_'+str(i))\n",
        "#         channels=np.concatenate((channels, d[b'data']), axis=0)\n",
        "#     r=np.mean(channels[:,:1024])/255  \n",
        "#     g=np.mean(channels[:,1024:2048])/255\n",
        "#     b=np.mean(channels[:,2048:])/255\n",
        "#     return(r,g,b)\n",
        "# get_mean_color()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "4d96196040e641699e8a78db79cf7b4e",
            "c3c542fe55a24623ba09e0f6cbcf6857",
            "dcc6414fe83041d8b5ebd3934cb1ca90",
            "92dff67c5c9049298e504f042450a0fc",
            "e13f87e33fad4a1e95fb438dd982ee13",
            "fa88d9d680484321a6889b511696efff",
            "78da9a8381fe41d6b4341e3603daf12e",
            "012c4fccfa124eb294393069e02f09d2",
            "969dedb7d7e74432919aaee552cc4cde",
            "84d2ff23e4634816b1c3291e34c4661b",
            "1878c24981c347adb95e0e89c72fe9f7",
            "a7a1a99f011e4610ad5ac35d32b324fd",
            "a57bfb0e83374f4493cc9062e20a718b",
            "5ae2aba4cc914176a5ccfc663eb6310a",
            "722efa37812c450c8997c8a0b916a30c",
            "f51e75cc2ff347969ebd105e903a8952",
            "bc3ee902af6e47b7b518a1aabaf7ba85",
            "b19fb9dad8774b1c8161b72430accf56",
            "bcc36657b3d44ea8a74b5ee30f4677d9",
            "d563a2f339ec4d0e896caf5b765d2be5",
            "02696ee9f9ce4417ae638d5844c2ec73",
            "5c3384e0e4044a3d82e3e66f3682220a"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "6c2947f4-6216-4d3e-c8ce-740013f7732e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./data\\train_32x32.mat\n",
            "Using downloaded and verified file: ./data\\test_32x32.mat\n"
          ]
        }
      ],
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandAugment(),\n",
        "    transforms.ToTensor(),\n",
        "    \n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "dccde39f-d470-4cdd-b75e-19839fdf9548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "73257 26032\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))\n",
        "print(trainset[4][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te71lQ17B1L_"
      },
      "source": [
        "#Data Augmentation\n",
        "Data augmentation performs random modifications of the image as a preprocessing step. It serves the following purposes:\n",
        "1. It increases the amount of data for training.\n",
        "2. By deleting features, it prevents the network from relying on a narrow set of features, which may not generalize.\n",
        "3. By changing features while maintaining the same output, it helps the network become tolerant of changes that do not change the image lab. \n",
        "\n",
        "In short, data augmentation desensitivizes the network, so it extracts features that are invariant to changes that should not affect the prediction. \n",
        "\n",
        "We showcase a few random data augmentation provided by PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RyG26xoJC0Pa"
      },
      "outputs": [],
      "source": [
        "# import torch.nn as nn\n",
        "# transforms = torch.nn.Sequential(\n",
        "#     T.Resize(256), # resize the short edge to 256.\n",
        "#     T.RandomCrop(224), #randomly crop a 224x224 region from the image\n",
        "#     T.RandomErasing(p=1, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
        "#     #T.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0.1)\n",
        "#     #T.AutoAugment()\n",
        "# )\n",
        "\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# dog1 = dog1.to(device)\n",
        "# # dog2 = dog2.to(device)\n",
        "\n",
        "# # transformed_dog1 = transforms(dog1)\n",
        "# transformed_dog1 = transforms(dog1)\n",
        "# show([transformed_dog1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "outputs": [],
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "outputs": [],
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # This is the \"stem\"\n",
        "        # For CIFAR (32x32 images), it does not perform downsampling\n",
        "        # It should downsample for ImageNet\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "01b301a0-525f-4c79-c065-9926ced8467e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "9f6d7183-e99e-4f93-e660-af58e8de94b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(class_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ArgupDVRwB8i"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3751, accuracy : 16.53\n",
            "iteration : 100, loss : 2.3005, accuracy : 17.37\n",
            "iteration : 150, loss : 2.1694, accuracy : 22.66\n",
            "iteration : 200, loss : 1.9115, accuracy : 32.52\n",
            "iteration : 250, loss : 1.6752, accuracy : 41.33\n",
            "iteration : 300, loss : 1.4923, accuracy : 48.11\n",
            "iteration : 350, loss : 1.3504, accuracy : 53.24\n",
            "Epoch :   1, training loss : 1.3080, training accuracy : 54.77, test loss : 1.2243, test accuracy : 63.28\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.4546, accuracy : 86.06\n",
            "iteration : 100, loss : 0.4424, accuracy : 86.60\n",
            "iteration : 150, loss : 0.4364, accuracy : 86.78\n",
            "iteration : 200, loss : 0.4349, accuracy : 86.75\n",
            "iteration : 250, loss : 0.4229, accuracy : 87.09\n",
            "iteration : 300, loss : 0.4164, accuracy : 87.30\n",
            "iteration : 350, loss : 0.4143, accuracy : 87.34\n",
            "Epoch :   2, training loss : 0.4128, training accuracy : 87.39, test loss : 0.4767, test accuracy : 85.48\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.3723, accuracy : 89.20\n",
            "iteration : 100, loss : 0.3607, accuracy : 89.29\n",
            "iteration : 150, loss : 0.3561, accuracy : 89.35\n",
            "iteration : 200, loss : 0.3618, accuracy : 89.12\n",
            "iteration : 250, loss : 0.3650, accuracy : 88.91\n",
            "iteration : 300, loss : 0.3653, accuracy : 88.91\n",
            "iteration : 350, loss : 0.3632, accuracy : 88.98\n",
            "Epoch :   3, training loss : 0.3641, training accuracy : 88.97, test loss : 0.4852, test accuracy : 85.78\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3227, accuracy : 90.17\n",
            "iteration : 100, loss : 0.3312, accuracy : 90.18\n",
            "iteration : 150, loss : 0.3295, accuracy : 90.21\n",
            "iteration : 200, loss : 0.3316, accuracy : 90.11\n",
            "iteration : 250, loss : 0.3324, accuracy : 90.07\n",
            "iteration : 300, loss : 0.3377, accuracy : 89.89\n",
            "iteration : 350, loss : 0.3366, accuracy : 89.93\n",
            "Epoch :   4, training loss : 0.3362, training accuracy : 89.91, test loss : 0.4449, test accuracy : 86.67\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.3160, accuracy : 90.36\n",
            "iteration : 100, loss : 0.3220, accuracy : 90.27\n",
            "iteration : 150, loss : 0.3220, accuracy : 90.24\n",
            "iteration : 200, loss : 0.3253, accuracy : 90.25\n",
            "iteration : 250, loss : 0.3239, accuracy : 90.20\n",
            "iteration : 300, loss : 0.3219, accuracy : 90.32\n",
            "iteration : 350, loss : 0.3214, accuracy : 90.33\n",
            "Epoch :   5, training loss : 0.3223, training accuracy : 90.34, test loss : 0.3845, test accuracy : 88.43\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.3220, accuracy : 90.05\n",
            "iteration : 100, loss : 0.3138, accuracy : 90.45\n",
            "iteration : 150, loss : 0.3092, accuracy : 90.66\n",
            "iteration : 200, loss : 0.3115, accuracy : 90.55\n",
            "iteration : 250, loss : 0.3101, accuracy : 90.62\n",
            "iteration : 300, loss : 0.3095, accuracy : 90.67\n",
            "iteration : 350, loss : 0.3112, accuracy : 90.61\n",
            "Epoch :   6, training loss : 0.3105, training accuracy : 90.63, test loss : 0.3909, test accuracy : 88.22\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.2839, accuracy : 91.55\n",
            "iteration : 100, loss : 0.2922, accuracy : 91.23\n",
            "iteration : 150, loss : 0.3018, accuracy : 90.97\n",
            "iteration : 200, loss : 0.3002, accuracy : 90.98\n",
            "iteration : 250, loss : 0.3009, accuracy : 90.94\n",
            "iteration : 300, loss : 0.2974, accuracy : 91.06\n",
            "iteration : 350, loss : 0.2995, accuracy : 91.07\n",
            "Epoch :   7, training loss : 0.2985, training accuracy : 91.13, test loss : 0.3331, test accuracy : 89.89\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.2792, accuracy : 91.69\n",
            "iteration : 100, loss : 0.2805, accuracy : 91.62\n",
            "iteration : 150, loss : 0.2829, accuracy : 91.64\n",
            "iteration : 200, loss : 0.2843, accuracy : 91.58\n",
            "iteration : 250, loss : 0.2825, accuracy : 91.64\n",
            "iteration : 300, loss : 0.2832, accuracy : 91.66\n",
            "iteration : 350, loss : 0.2855, accuracy : 91.58\n",
            "Epoch :   8, training loss : 0.2846, training accuracy : 91.62, test loss : 0.3330, test accuracy : 90.08\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.2685, accuracy : 92.34\n",
            "iteration : 100, loss : 0.2723, accuracy : 92.12\n",
            "iteration : 150, loss : 0.2736, accuracy : 91.96\n",
            "iteration : 200, loss : 0.2694, accuracy : 92.03\n",
            "iteration : 250, loss : 0.2710, accuracy : 91.97\n",
            "iteration : 300, loss : 0.2720, accuracy : 91.93\n",
            "iteration : 350, loss : 0.2719, accuracy : 91.96\n",
            "Epoch :   9, training loss : 0.2726, training accuracy : 91.95, test loss : 0.3105, test accuracy : 90.70\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2523, accuracy : 92.78\n",
            "iteration : 100, loss : 0.2483, accuracy : 92.70\n",
            "iteration : 150, loss : 0.2557, accuracy : 92.49\n",
            "iteration : 200, loss : 0.2588, accuracy : 92.36\n",
            "iteration : 250, loss : 0.2621, accuracy : 92.27\n",
            "iteration : 300, loss : 0.2605, accuracy : 92.29\n",
            "iteration : 350, loss : 0.2626, accuracy : 92.19\n",
            "Epoch :  10, training loss : 0.2629, training accuracy : 92.19, test loss : 0.2816, test accuracy : 91.61\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.2713, accuracy : 92.06\n",
            "iteration : 100, loss : 0.2568, accuracy : 92.55\n",
            "iteration : 150, loss : 0.2555, accuracy : 92.59\n",
            "iteration : 200, loss : 0.2552, accuracy : 92.57\n",
            "iteration : 250, loss : 0.2575, accuracy : 92.48\n",
            "iteration : 300, loss : 0.2563, accuracy : 92.50\n",
            "iteration : 350, loss : 0.2550, accuracy : 92.55\n",
            "Epoch :  11, training loss : 0.2557, training accuracy : 92.53, test loss : 0.2945, test accuracy : 91.36\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.2294, accuracy : 93.08\n",
            "iteration : 100, loss : 0.2397, accuracy : 92.89\n",
            "iteration : 150, loss : 0.2466, accuracy : 92.71\n",
            "iteration : 200, loss : 0.2476, accuracy : 92.68\n",
            "iteration : 250, loss : 0.2454, accuracy : 92.71\n",
            "iteration : 300, loss : 0.2464, accuracy : 92.70\n",
            "iteration : 350, loss : 0.2471, accuracy : 92.70\n",
            "Epoch :  12, training loss : 0.2472, training accuracy : 92.70, test loss : 0.2779, test accuracy : 91.83\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.2250, accuracy : 93.14\n",
            "iteration : 100, loss : 0.2176, accuracy : 93.54\n",
            "iteration : 150, loss : 0.2294, accuracy : 93.26\n",
            "iteration : 200, loss : 0.2283, accuracy : 93.28\n",
            "iteration : 250, loss : 0.2297, accuracy : 93.22\n",
            "iteration : 300, loss : 0.2324, accuracy : 93.14\n",
            "iteration : 350, loss : 0.2328, accuracy : 93.11\n",
            "Epoch :  13, training loss : 0.2331, training accuracy : 93.09, test loss : 0.3032, test accuracy : 91.17\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.2125, accuracy : 93.62\n",
            "iteration : 100, loss : 0.2082, accuracy : 93.71\n",
            "iteration : 150, loss : 0.2158, accuracy : 93.53\n",
            "iteration : 200, loss : 0.2215, accuracy : 93.47\n",
            "iteration : 250, loss : 0.2291, accuracy : 93.36\n",
            "iteration : 300, loss : 0.2286, accuracy : 93.36\n",
            "iteration : 350, loss : 0.2315, accuracy : 93.27\n",
            "Epoch :  14, training loss : 0.2307, training accuracy : 93.29, test loss : 0.2831, test accuracy : 91.79\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.2051, accuracy : 94.08\n",
            "iteration : 100, loss : 0.2192, accuracy : 93.62\n",
            "iteration : 150, loss : 0.2208, accuracy : 93.49\n",
            "iteration : 200, loss : 0.2233, accuracy : 93.46\n",
            "iteration : 250, loss : 0.2241, accuracy : 93.42\n",
            "iteration : 300, loss : 0.2279, accuracy : 93.34\n",
            "iteration : 350, loss : 0.2283, accuracy : 93.34\n",
            "Epoch :  15, training loss : 0.2283, training accuracy : 93.35, test loss : 0.2758, test accuracy : 91.81\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.2141, accuracy : 93.97\n",
            "iteration : 100, loss : 0.2158, accuracy : 93.77\n",
            "iteration : 150, loss : 0.2191, accuracy : 93.81\n",
            "iteration : 200, loss : 0.2167, accuracy : 93.82\n",
            "iteration : 250, loss : 0.2162, accuracy : 93.87\n",
            "iteration : 300, loss : 0.2156, accuracy : 93.82\n",
            "iteration : 350, loss : 0.2140, accuracy : 93.84\n",
            "Epoch :  16, training loss : 0.2151, training accuracy : 93.77, test loss : 0.2620, test accuracy : 92.49\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.2065, accuracy : 94.20\n",
            "iteration : 100, loss : 0.2049, accuracy : 94.11\n",
            "iteration : 150, loss : 0.2158, accuracy : 93.76\n",
            "iteration : 200, loss : 0.2138, accuracy : 93.81\n",
            "iteration : 250, loss : 0.2158, accuracy : 93.77\n",
            "iteration : 300, loss : 0.2151, accuracy : 93.80\n",
            "iteration : 350, loss : 0.2144, accuracy : 93.85\n",
            "Epoch :  17, training loss : 0.2153, training accuracy : 93.83, test loss : 0.3133, test accuracy : 91.03\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.2063, accuracy : 93.95\n",
            "iteration : 100, loss : 0.2024, accuracy : 94.17\n",
            "iteration : 150, loss : 0.1980, accuracy : 94.18\n",
            "iteration : 200, loss : 0.1992, accuracy : 94.16\n",
            "iteration : 250, loss : 0.1997, accuracy : 94.16\n",
            "iteration : 300, loss : 0.2025, accuracy : 94.11\n",
            "iteration : 350, loss : 0.2045, accuracy : 94.03\n",
            "Epoch :  18, training loss : 0.2038, training accuracy : 94.05, test loss : 0.2692, test accuracy : 92.31\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.1876, accuracy : 94.44\n",
            "iteration : 100, loss : 0.1918, accuracy : 94.30\n",
            "iteration : 150, loss : 0.1942, accuracy : 94.22\n",
            "iteration : 200, loss : 0.2001, accuracy : 94.07\n",
            "iteration : 250, loss : 0.2045, accuracy : 93.89\n",
            "iteration : 300, loss : 0.2032, accuracy : 93.94\n",
            "iteration : 350, loss : 0.2017, accuracy : 94.00\n",
            "Epoch :  19, training loss : 0.2022, training accuracy : 94.04, test loss : 0.2539, test accuracy : 92.62\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.1891, accuracy : 94.50\n",
            "iteration : 100, loss : 0.2009, accuracy : 94.27\n",
            "iteration : 150, loss : 0.1957, accuracy : 94.36\n",
            "iteration : 200, loss : 0.1935, accuracy : 94.37\n",
            "iteration : 250, loss : 0.1935, accuracy : 94.37\n",
            "iteration : 300, loss : 0.1957, accuracy : 94.25\n",
            "iteration : 350, loss : 0.1996, accuracy : 94.17\n",
            "Epoch :  20, training loss : 0.2004, training accuracy : 94.14, test loss : 0.2555, test accuracy : 92.65\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.1858, accuracy : 94.89\n",
            "iteration : 100, loss : 0.1838, accuracy : 94.94\n",
            "iteration : 150, loss : 0.1861, accuracy : 94.69\n",
            "iteration : 200, loss : 0.1848, accuracy : 94.68\n",
            "iteration : 250, loss : 0.1859, accuracy : 94.69\n",
            "iteration : 300, loss : 0.1841, accuracy : 94.72\n",
            "iteration : 350, loss : 0.1867, accuracy : 94.63\n",
            "Epoch :  21, training loss : 0.1882, training accuracy : 94.57, test loss : 0.2524, test accuracy : 92.92\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.1716, accuracy : 94.70\n",
            "iteration : 100, loss : 0.1771, accuracy : 94.79\n",
            "iteration : 150, loss : 0.1807, accuracy : 94.67\n",
            "iteration : 200, loss : 0.1839, accuracy : 94.65\n",
            "iteration : 250, loss : 0.1891, accuracy : 94.49\n",
            "iteration : 300, loss : 0.1917, accuracy : 94.43\n",
            "iteration : 350, loss : 0.1911, accuracy : 94.47\n",
            "Epoch :  22, training loss : 0.1920, training accuracy : 94.45, test loss : 0.2694, test accuracy : 92.36\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.1706, accuracy : 95.11\n",
            "iteration : 100, loss : 0.1787, accuracy : 95.05\n",
            "iteration : 150, loss : 0.1825, accuracy : 94.86\n",
            "iteration : 200, loss : 0.1839, accuracy : 94.73\n",
            "iteration : 250, loss : 0.1846, accuracy : 94.76\n",
            "iteration : 300, loss : 0.1864, accuracy : 94.66\n",
            "iteration : 350, loss : 0.1877, accuracy : 94.61\n",
            "Epoch :  23, training loss : 0.1873, training accuracy : 94.62, test loss : 0.2469, test accuracy : 93.00\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.1661, accuracy : 95.20\n",
            "iteration : 100, loss : 0.1774, accuracy : 94.70\n",
            "iteration : 150, loss : 0.1777, accuracy : 94.71\n",
            "iteration : 200, loss : 0.1795, accuracy : 94.67\n",
            "iteration : 250, loss : 0.1828, accuracy : 94.57\n",
            "iteration : 300, loss : 0.1850, accuracy : 94.49\n",
            "iteration : 350, loss : 0.1849, accuracy : 94.53\n",
            "Epoch :  24, training loss : 0.1846, training accuracy : 94.52, test loss : 0.2430, test accuracy : 93.07\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.1646, accuracy : 95.17\n",
            "iteration : 100, loss : 0.1658, accuracy : 95.24\n",
            "iteration : 150, loss : 0.1712, accuracy : 95.07\n",
            "iteration : 200, loss : 0.1712, accuracy : 95.10\n",
            "iteration : 250, loss : 0.1721, accuracy : 95.00\n",
            "iteration : 300, loss : 0.1710, accuracy : 95.06\n",
            "iteration : 350, loss : 0.1740, accuracy : 94.94\n",
            "Epoch :  25, training loss : 0.1777, training accuracy : 94.82, test loss : 0.2985, test accuracy : 91.64\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.1702, accuracy : 95.45\n",
            "iteration : 100, loss : 0.1702, accuracy : 95.32\n",
            "iteration : 150, loss : 0.1650, accuracy : 95.36\n",
            "iteration : 200, loss : 0.1691, accuracy : 95.22\n",
            "iteration : 250, loss : 0.1724, accuracy : 95.17\n",
            "iteration : 300, loss : 0.1740, accuracy : 95.11\n",
            "iteration : 350, loss : 0.1766, accuracy : 95.00\n",
            "Epoch :  26, training loss : 0.1765, training accuracy : 94.98, test loss : 0.2367, test accuracy : 93.31\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.1508, accuracy : 95.53\n",
            "iteration : 100, loss : 0.1605, accuracy : 95.23\n",
            "iteration : 150, loss : 0.1611, accuracy : 95.32\n",
            "iteration : 200, loss : 0.1635, accuracy : 95.22\n",
            "iteration : 250, loss : 0.1683, accuracy : 95.12\n",
            "iteration : 300, loss : 0.1700, accuracy : 95.03\n",
            "iteration : 350, loss : 0.1700, accuracy : 95.05\n",
            "Epoch :  27, training loss : 0.1703, training accuracy : 95.04, test loss : 0.2460, test accuracy : 93.09\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.1780, accuracy : 94.81\n",
            "iteration : 100, loss : 0.1746, accuracy : 94.91\n",
            "iteration : 150, loss : 0.1725, accuracy : 94.96\n",
            "iteration : 200, loss : 0.1761, accuracy : 94.91\n",
            "iteration : 250, loss : 0.1720, accuracy : 95.04\n",
            "iteration : 300, loss : 0.1733, accuracy : 95.00\n",
            "iteration : 350, loss : 0.1717, accuracy : 95.04\n",
            "Epoch :  28, training loss : 0.1724, training accuracy : 95.01, test loss : 0.2627, test accuracy : 92.47\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.1606, accuracy : 95.59\n",
            "iteration : 100, loss : 0.1568, accuracy : 95.63\n",
            "iteration : 150, loss : 0.1597, accuracy : 95.45\n",
            "iteration : 200, loss : 0.1623, accuracy : 95.38\n",
            "iteration : 250, loss : 0.1634, accuracy : 95.30\n",
            "iteration : 300, loss : 0.1655, accuracy : 95.22\n",
            "iteration : 350, loss : 0.1676, accuracy : 95.12\n",
            "Epoch :  29, training loss : 0.1696, training accuracy : 95.08, test loss : 0.2418, test accuracy : 93.22\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.1527, accuracy : 95.61\n",
            "iteration : 100, loss : 0.1527, accuracy : 95.73\n",
            "iteration : 150, loss : 0.1570, accuracy : 95.55\n",
            "iteration : 200, loss : 0.1556, accuracy : 95.62\n",
            "iteration : 250, loss : 0.1600, accuracy : 95.46\n",
            "iteration : 300, loss : 0.1632, accuracy : 95.33\n",
            "iteration : 350, loss : 0.1656, accuracy : 95.26\n",
            "Epoch :  30, training loss : 0.1653, training accuracy : 95.28, test loss : 0.2399, test accuracy : 93.04\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.1382, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1473, accuracy : 95.81\n",
            "iteration : 150, loss : 0.1522, accuracy : 95.67\n",
            "iteration : 200, loss : 0.1546, accuracy : 95.55\n",
            "iteration : 250, loss : 0.1601, accuracy : 95.41\n",
            "iteration : 300, loss : 0.1599, accuracy : 95.41\n",
            "iteration : 350, loss : 0.1617, accuracy : 95.36\n",
            "Epoch :  31, training loss : 0.1618, training accuracy : 95.37, test loss : 0.2373, test accuracy : 93.45\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.1503, accuracy : 95.77\n",
            "iteration : 100, loss : 0.1476, accuracy : 95.69\n",
            "iteration : 150, loss : 0.1526, accuracy : 95.49\n",
            "iteration : 200, loss : 0.1569, accuracy : 95.49\n",
            "iteration : 250, loss : 0.1597, accuracy : 95.39\n",
            "iteration : 300, loss : 0.1628, accuracy : 95.29\n",
            "iteration : 350, loss : 0.1630, accuracy : 95.27\n",
            "Epoch :  32, training loss : 0.1627, training accuracy : 95.29, test loss : 0.2383, test accuracy : 93.35\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.1406, accuracy : 95.83\n",
            "iteration : 100, loss : 0.1441, accuracy : 95.80\n",
            "iteration : 150, loss : 0.1485, accuracy : 95.66\n",
            "iteration : 200, loss : 0.1534, accuracy : 95.58\n",
            "iteration : 250, loss : 0.1587, accuracy : 95.41\n",
            "iteration : 300, loss : 0.1571, accuracy : 95.44\n",
            "iteration : 350, loss : 0.1572, accuracy : 95.45\n",
            "Epoch :  33, training loss : 0.1599, training accuracy : 95.37, test loss : 0.2611, test accuracy : 92.45\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.1485, accuracy : 95.72\n",
            "iteration : 100, loss : 0.1462, accuracy : 95.93\n",
            "iteration : 150, loss : 0.1492, accuracy : 95.84\n",
            "iteration : 200, loss : 0.1519, accuracy : 95.66\n",
            "iteration : 250, loss : 0.1505, accuracy : 95.67\n",
            "iteration : 300, loss : 0.1521, accuracy : 95.67\n",
            "iteration : 350, loss : 0.1543, accuracy : 95.60\n",
            "Epoch :  34, training loss : 0.1552, training accuracy : 95.58, test loss : 0.2243, test accuracy : 93.64\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.1348, accuracy : 96.09\n",
            "iteration : 100, loss : 0.1505, accuracy : 95.63\n",
            "iteration : 150, loss : 0.1526, accuracy : 95.58\n",
            "iteration : 200, loss : 0.1480, accuracy : 95.78\n",
            "iteration : 250, loss : 0.1516, accuracy : 95.62\n",
            "iteration : 300, loss : 0.1509, accuracy : 95.65\n",
            "iteration : 350, loss : 0.1539, accuracy : 95.61\n",
            "Epoch :  35, training loss : 0.1553, training accuracy : 95.58, test loss : 0.2430, test accuracy : 93.44\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.1315, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1452, accuracy : 95.63\n",
            "iteration : 150, loss : 0.1525, accuracy : 95.52\n",
            "iteration : 200, loss : 0.1501, accuracy : 95.55\n",
            "iteration : 250, loss : 0.1510, accuracy : 95.56\n",
            "iteration : 300, loss : 0.1549, accuracy : 95.48\n",
            "iteration : 350, loss : 0.1571, accuracy : 95.47\n",
            "Epoch :  36, training loss : 0.1585, training accuracy : 95.43, test loss : 0.2620, test accuracy : 92.70\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.1421, accuracy : 96.08\n",
            "iteration : 100, loss : 0.1447, accuracy : 96.02\n",
            "iteration : 150, loss : 0.1450, accuracy : 95.91\n",
            "iteration : 200, loss : 0.1456, accuracy : 95.92\n",
            "iteration : 250, loss : 0.1478, accuracy : 95.81\n",
            "iteration : 300, loss : 0.1493, accuracy : 95.78\n",
            "iteration : 350, loss : 0.1512, accuracy : 95.70\n",
            "Epoch :  37, training loss : 0.1528, training accuracy : 95.65, test loss : 0.2360, test accuracy : 93.63\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.1389, accuracy : 96.05\n",
            "iteration : 100, loss : 0.1456, accuracy : 95.80\n",
            "iteration : 150, loss : 0.1459, accuracy : 95.66\n",
            "iteration : 200, loss : 0.1469, accuracy : 95.63\n",
            "iteration : 250, loss : 0.1500, accuracy : 95.59\n",
            "iteration : 300, loss : 0.1511, accuracy : 95.56\n",
            "iteration : 350, loss : 0.1528, accuracy : 95.50\n",
            "Epoch :  38, training loss : 0.1545, training accuracy : 95.46, test loss : 0.2473, test accuracy : 93.02\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.1367, accuracy : 95.92\n",
            "iteration : 100, loss : 0.1395, accuracy : 95.91\n",
            "iteration : 150, loss : 0.1444, accuracy : 95.76\n",
            "iteration : 200, loss : 0.1473, accuracy : 95.68\n",
            "iteration : 250, loss : 0.1487, accuracy : 95.70\n",
            "iteration : 300, loss : 0.1470, accuracy : 95.74\n",
            "iteration : 350, loss : 0.1496, accuracy : 95.65\n",
            "Epoch :  39, training loss : 0.1508, training accuracy : 95.62, test loss : 0.2386, test accuracy : 93.29\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.1441, accuracy : 95.95\n",
            "iteration : 100, loss : 0.1414, accuracy : 95.88\n",
            "iteration : 150, loss : 0.1401, accuracy : 95.92\n",
            "iteration : 200, loss : 0.1406, accuracy : 95.92\n",
            "iteration : 250, loss : 0.1433, accuracy : 95.91\n",
            "iteration : 300, loss : 0.1465, accuracy : 95.76\n",
            "iteration : 350, loss : 0.1519, accuracy : 95.62\n",
            "Epoch :  40, training loss : 0.1518, training accuracy : 95.60, test loss : 0.2304, test accuracy : 93.36\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.1353, accuracy : 96.36\n",
            "iteration : 100, loss : 0.1392, accuracy : 96.16\n",
            "iteration : 150, loss : 0.1482, accuracy : 95.92\n",
            "iteration : 200, loss : 0.1480, accuracy : 95.88\n",
            "iteration : 250, loss : 0.1497, accuracy : 95.83\n",
            "iteration : 300, loss : 0.1521, accuracy : 95.70\n",
            "iteration : 350, loss : 0.1500, accuracy : 95.72\n",
            "Epoch :  41, training loss : 0.1502, training accuracy : 95.74, test loss : 0.2345, test accuracy : 93.29\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.1316, accuracy : 96.25\n",
            "iteration : 100, loss : 0.1341, accuracy : 96.09\n",
            "iteration : 150, loss : 0.1368, accuracy : 95.99\n",
            "iteration : 200, loss : 0.1387, accuracy : 96.00\n",
            "iteration : 250, loss : 0.1396, accuracy : 95.97\n",
            "iteration : 300, loss : 0.1422, accuracy : 95.92\n",
            "iteration : 350, loss : 0.1454, accuracy : 95.80\n",
            "Epoch :  42, training loss : 0.1461, training accuracy : 95.80, test loss : 0.2340, test accuracy : 93.45\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.1327, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1378, accuracy : 96.10\n",
            "iteration : 150, loss : 0.1453, accuracy : 95.89\n",
            "iteration : 200, loss : 0.1452, accuracy : 95.92\n",
            "iteration : 250, loss : 0.1474, accuracy : 95.80\n",
            "iteration : 300, loss : 0.1478, accuracy : 95.76\n",
            "iteration : 350, loss : 0.1491, accuracy : 95.75\n",
            "Epoch :  43, training loss : 0.1499, training accuracy : 95.74, test loss : 0.2395, test accuracy : 93.43\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.1335, accuracy : 96.27\n",
            "iteration : 100, loss : 0.1346, accuracy : 96.16\n",
            "iteration : 150, loss : 0.1401, accuracy : 95.99\n",
            "iteration : 200, loss : 0.1424, accuracy : 95.90\n",
            "iteration : 250, loss : 0.1409, accuracy : 95.96\n",
            "iteration : 300, loss : 0.1441, accuracy : 95.88\n",
            "iteration : 350, loss : 0.1447, accuracy : 95.86\n",
            "Epoch :  44, training loss : 0.1455, training accuracy : 95.84, test loss : 0.2479, test accuracy : 93.21\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.1424, accuracy : 96.05\n",
            "iteration : 100, loss : 0.1464, accuracy : 95.67\n",
            "iteration : 150, loss : 0.1416, accuracy : 95.82\n",
            "iteration : 200, loss : 0.1425, accuracy : 95.87\n",
            "iteration : 250, loss : 0.1394, accuracy : 95.96\n",
            "iteration : 300, loss : 0.1389, accuracy : 96.02\n",
            "iteration : 350, loss : 0.1398, accuracy : 95.99\n",
            "Epoch :  45, training loss : 0.1404, training accuracy : 95.97, test loss : 0.2424, test accuracy : 93.40\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.1415, accuracy : 95.97\n",
            "iteration : 100, loss : 0.1377, accuracy : 95.99\n",
            "iteration : 150, loss : 0.1390, accuracy : 96.05\n",
            "iteration : 200, loss : 0.1430, accuracy : 95.80\n",
            "iteration : 250, loss : 0.1422, accuracy : 95.87\n",
            "iteration : 300, loss : 0.1447, accuracy : 95.84\n",
            "iteration : 350, loss : 0.1452, accuracy : 95.81\n",
            "Epoch :  46, training loss : 0.1454, training accuracy : 95.81, test loss : 0.2396, test accuracy : 93.39\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.1292, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1284, accuracy : 96.24\n",
            "iteration : 150, loss : 0.1326, accuracy : 96.14\n",
            "iteration : 200, loss : 0.1342, accuracy : 96.12\n",
            "iteration : 250, loss : 0.1375, accuracy : 96.03\n",
            "iteration : 300, loss : 0.1389, accuracy : 95.98\n",
            "iteration : 350, loss : 0.1396, accuracy : 95.93\n",
            "Epoch :  47, training loss : 0.1405, training accuracy : 95.89, test loss : 0.2536, test accuracy : 92.97\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.1218, accuracy : 96.31\n",
            "iteration : 100, loss : 0.1288, accuracy : 96.02\n",
            "iteration : 150, loss : 0.1328, accuracy : 95.95\n",
            "iteration : 200, loss : 0.1369, accuracy : 95.86\n",
            "iteration : 250, loss : 0.1394, accuracy : 95.82\n",
            "iteration : 300, loss : 0.1434, accuracy : 95.74\n",
            "iteration : 350, loss : 0.1436, accuracy : 95.77\n",
            "Epoch :  48, training loss : 0.1437, training accuracy : 95.78, test loss : 0.2292, test accuracy : 93.51\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.1042, accuracy : 96.91\n",
            "iteration : 100, loss : 0.1193, accuracy : 96.42\n",
            "iteration : 150, loss : 0.1305, accuracy : 96.15\n",
            "iteration : 200, loss : 0.1335, accuracy : 96.08\n",
            "iteration : 250, loss : 0.1348, accuracy : 96.08\n",
            "iteration : 300, loss : 0.1356, accuracy : 96.05\n",
            "iteration : 350, loss : 0.1381, accuracy : 95.99\n",
            "Epoch :  49, training loss : 0.1381, training accuracy : 95.99, test loss : 0.2559, test accuracy : 92.77\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.1277, accuracy : 96.11\n",
            "iteration : 100, loss : 0.1331, accuracy : 96.08\n",
            "iteration : 150, loss : 0.1326, accuracy : 96.10\n",
            "iteration : 200, loss : 0.1340, accuracy : 96.07\n",
            "iteration : 250, loss : 0.1348, accuracy : 96.05\n",
            "iteration : 300, loss : 0.1381, accuracy : 95.99\n",
            "iteration : 350, loss : 0.1409, accuracy : 95.86\n",
            "Epoch :  50, training loss : 0.1406, training accuracy : 95.86, test loss : 0.2396, test accuracy : 93.39\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.1355, accuracy : 96.19\n",
            "iteration : 100, loss : 0.1307, accuracy : 96.23\n",
            "iteration : 150, loss : 0.1340, accuracy : 96.06\n",
            "iteration : 200, loss : 0.1364, accuracy : 96.02\n",
            "iteration : 250, loss : 0.1361, accuracy : 96.03\n",
            "iteration : 300, loss : 0.1381, accuracy : 95.96\n",
            "iteration : 350, loss : 0.1389, accuracy : 95.94\n",
            "Epoch :  51, training loss : 0.1391, training accuracy : 95.95, test loss : 0.2308, test accuracy : 93.73\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.1158, accuracy : 96.77\n",
            "iteration : 100, loss : 0.1254, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1318, accuracy : 96.21\n",
            "iteration : 200, loss : 0.1314, accuracy : 96.22\n",
            "iteration : 250, loss : 0.1378, accuracy : 96.08\n",
            "iteration : 300, loss : 0.1363, accuracy : 96.14\n",
            "iteration : 350, loss : 0.1372, accuracy : 96.10\n",
            "Epoch :  52, training loss : 0.1370, training accuracy : 96.10, test loss : 0.2284, test accuracy : 93.69\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.1245, accuracy : 96.56\n",
            "iteration : 100, loss : 0.1242, accuracy : 96.46\n",
            "iteration : 150, loss : 0.1270, accuracy : 96.35\n",
            "iteration : 200, loss : 0.1314, accuracy : 96.22\n",
            "iteration : 250, loss : 0.1330, accuracy : 96.21\n",
            "iteration : 300, loss : 0.1335, accuracy : 96.14\n",
            "iteration : 350, loss : 0.1344, accuracy : 96.09\n",
            "Epoch :  53, training loss : 0.1334, training accuracy : 96.12, test loss : 0.2475, test accuracy : 93.28\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.1275, accuracy : 96.23\n",
            "iteration : 100, loss : 0.1231, accuracy : 96.35\n",
            "iteration : 150, loss : 0.1216, accuracy : 96.50\n",
            "iteration : 200, loss : 0.1257, accuracy : 96.39\n",
            "iteration : 250, loss : 0.1256, accuracy : 96.37\n",
            "iteration : 300, loss : 0.1304, accuracy : 96.22\n",
            "iteration : 350, loss : 0.1307, accuracy : 96.22\n",
            "Epoch :  54, training loss : 0.1321, training accuracy : 96.19, test loss : 0.2438, test accuracy : 93.27\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.1357, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1322, accuracy : 96.13\n",
            "iteration : 150, loss : 0.1310, accuracy : 96.18\n",
            "iteration : 200, loss : 0.1359, accuracy : 96.04\n",
            "iteration : 250, loss : 0.1348, accuracy : 96.09\n",
            "iteration : 300, loss : 0.1339, accuracy : 96.03\n",
            "iteration : 350, loss : 0.1354, accuracy : 96.01\n",
            "Epoch :  55, training loss : 0.1356, training accuracy : 96.00, test loss : 0.2348, test accuracy : 93.51\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.1224, accuracy : 96.52\n",
            "iteration : 100, loss : 0.1247, accuracy : 96.44\n",
            "iteration : 150, loss : 0.1180, accuracy : 96.71\n",
            "iteration : 200, loss : 0.1223, accuracy : 96.50\n",
            "iteration : 250, loss : 0.1242, accuracy : 96.48\n",
            "iteration : 300, loss : 0.1266, accuracy : 96.42\n",
            "iteration : 350, loss : 0.1300, accuracy : 96.29\n",
            "Epoch :  56, training loss : 0.1313, training accuracy : 96.27, test loss : 0.2408, test accuracy : 93.51\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.1181, accuracy : 96.69\n",
            "iteration : 100, loss : 0.1229, accuracy : 96.54\n",
            "iteration : 150, loss : 0.1221, accuracy : 96.51\n",
            "iteration : 200, loss : 0.1237, accuracy : 96.44\n",
            "iteration : 250, loss : 0.1238, accuracy : 96.39\n",
            "iteration : 300, loss : 0.1258, accuracy : 96.33\n",
            "iteration : 350, loss : 0.1297, accuracy : 96.25\n",
            "Epoch :  57, training loss : 0.1314, training accuracy : 96.20, test loss : 0.2303, test accuracy : 93.75\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.1232, accuracy : 96.45\n",
            "iteration : 100, loss : 0.1234, accuracy : 96.47\n",
            "iteration : 150, loss : 0.1233, accuracy : 96.48\n",
            "iteration : 200, loss : 0.1222, accuracy : 96.49\n",
            "iteration : 250, loss : 0.1268, accuracy : 96.36\n",
            "iteration : 300, loss : 0.1277, accuracy : 96.35\n",
            "iteration : 350, loss : 0.1304, accuracy : 96.25\n",
            "Epoch :  58, training loss : 0.1319, training accuracy : 96.22, test loss : 0.2319, test accuracy : 93.76\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.1231, accuracy : 96.39\n",
            "iteration : 100, loss : 0.1192, accuracy : 96.42\n",
            "iteration : 150, loss : 0.1228, accuracy : 96.33\n",
            "iteration : 200, loss : 0.1275, accuracy : 96.18\n",
            "iteration : 250, loss : 0.1309, accuracy : 96.10\n",
            "iteration : 300, loss : 0.1299, accuracy : 96.13\n",
            "iteration : 350, loss : 0.1304, accuracy : 96.11\n",
            "Epoch :  59, training loss : 0.1304, training accuracy : 96.11, test loss : 0.2309, test accuracy : 93.63\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.1217, accuracy : 96.33\n",
            "iteration : 100, loss : 0.1195, accuracy : 96.50\n",
            "iteration : 150, loss : 0.1219, accuracy : 96.45\n",
            "iteration : 200, loss : 0.1240, accuracy : 96.41\n",
            "iteration : 250, loss : 0.1237, accuracy : 96.40\n",
            "iteration : 300, loss : 0.1261, accuracy : 96.34\n",
            "iteration : 350, loss : 0.1277, accuracy : 96.29\n",
            "Epoch :  60, training loss : 0.1286, training accuracy : 96.28, test loss : 0.2373, test accuracy : 93.40\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.1033, accuracy : 96.92\n",
            "iteration : 100, loss : 0.1134, accuracy : 96.68\n",
            "iteration : 150, loss : 0.1205, accuracy : 96.52\n",
            "iteration : 200, loss : 0.1243, accuracy : 96.41\n",
            "iteration : 250, loss : 0.1273, accuracy : 96.30\n",
            "iteration : 300, loss : 0.1258, accuracy : 96.35\n",
            "iteration : 350, loss : 0.1275, accuracy : 96.27\n",
            "Epoch :  61, training loss : 0.1288, training accuracy : 96.25, test loss : 0.2374, test accuracy : 93.49\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.0998, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1132, accuracy : 96.80\n",
            "iteration : 150, loss : 0.1166, accuracy : 96.66\n",
            "iteration : 200, loss : 0.1192, accuracy : 96.51\n",
            "iteration : 250, loss : 0.1206, accuracy : 96.49\n",
            "iteration : 300, loss : 0.1246, accuracy : 96.39\n",
            "iteration : 350, loss : 0.1285, accuracy : 96.29\n",
            "Epoch :  62, training loss : 0.1286, training accuracy : 96.28, test loss : 0.2375, test accuracy : 93.58\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.1072, accuracy : 96.91\n",
            "iteration : 100, loss : 0.1099, accuracy : 96.87\n",
            "iteration : 150, loss : 0.1089, accuracy : 96.81\n",
            "iteration : 200, loss : 0.1149, accuracy : 96.67\n",
            "iteration : 250, loss : 0.1177, accuracy : 96.60\n",
            "iteration : 300, loss : 0.1204, accuracy : 96.53\n",
            "iteration : 350, loss : 0.1220, accuracy : 96.50\n",
            "Epoch :  63, training loss : 0.1229, training accuracy : 96.47, test loss : 0.2297, test accuracy : 93.57\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.1144, accuracy : 96.53\n",
            "iteration : 100, loss : 0.1187, accuracy : 96.44\n",
            "iteration : 150, loss : 0.1231, accuracy : 96.33\n",
            "iteration : 200, loss : 0.1228, accuracy : 96.30\n",
            "iteration : 250, loss : 0.1246, accuracy : 96.27\n",
            "iteration : 300, loss : 0.1237, accuracy : 96.26\n",
            "iteration : 350, loss : 0.1257, accuracy : 96.24\n",
            "Epoch :  64, training loss : 0.1276, training accuracy : 96.19, test loss : 0.2656, test accuracy : 92.60\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.1218, accuracy : 96.41\n",
            "iteration : 100, loss : 0.1211, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1187, accuracy : 96.47\n",
            "iteration : 200, loss : 0.1230, accuracy : 96.39\n",
            "iteration : 250, loss : 0.1259, accuracy : 96.36\n",
            "iteration : 300, loss : 0.1263, accuracy : 96.37\n",
            "iteration : 350, loss : 0.1273, accuracy : 96.32\n",
            "Epoch :  65, training loss : 0.1276, training accuracy : 96.32, test loss : 0.2327, test accuracy : 93.70\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.1204, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1198, accuracy : 96.55\n",
            "iteration : 150, loss : 0.1203, accuracy : 96.57\n",
            "iteration : 200, loss : 0.1196, accuracy : 96.59\n",
            "iteration : 250, loss : 0.1212, accuracy : 96.50\n",
            "iteration : 300, loss : 0.1244, accuracy : 96.38\n",
            "iteration : 350, loss : 0.1247, accuracy : 96.38\n",
            "Epoch :  66, training loss : 0.1241, training accuracy : 96.41, test loss : 0.2347, test accuracy : 93.59\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.1068, accuracy : 96.78\n",
            "iteration : 100, loss : 0.1091, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1094, accuracy : 96.64\n",
            "iteration : 200, loss : 0.1177, accuracy : 96.47\n",
            "iteration : 250, loss : 0.1206, accuracy : 96.42\n",
            "iteration : 300, loss : 0.1213, accuracy : 96.42\n",
            "iteration : 350, loss : 0.1212, accuracy : 96.44\n",
            "Epoch :  67, training loss : 0.1226, training accuracy : 96.40, test loss : 0.2371, test accuracy : 93.65\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.1083, accuracy : 96.72\n",
            "iteration : 100, loss : 0.1072, accuracy : 96.75\n",
            "iteration : 150, loss : 0.1091, accuracy : 96.80\n",
            "iteration : 200, loss : 0.1145, accuracy : 96.66\n",
            "iteration : 250, loss : 0.1191, accuracy : 96.55\n",
            "iteration : 300, loss : 0.1210, accuracy : 96.50\n",
            "iteration : 350, loss : 0.1234, accuracy : 96.42\n",
            "Epoch :  68, training loss : 0.1236, training accuracy : 96.43, test loss : 0.2428, test accuracy : 93.31\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.1173, accuracy : 96.67\n",
            "iteration : 100, loss : 0.1093, accuracy : 96.87\n",
            "iteration : 150, loss : 0.1111, accuracy : 96.83\n",
            "iteration : 200, loss : 0.1132, accuracy : 96.80\n",
            "iteration : 250, loss : 0.1159, accuracy : 96.75\n",
            "iteration : 300, loss : 0.1167, accuracy : 96.71\n",
            "iteration : 350, loss : 0.1168, accuracy : 96.70\n",
            "Epoch :  69, training loss : 0.1168, training accuracy : 96.70, test loss : 0.2379, test accuracy : 93.60\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.1239, accuracy : 96.27\n",
            "iteration : 100, loss : 0.1255, accuracy : 96.34\n",
            "iteration : 150, loss : 0.1261, accuracy : 96.40\n",
            "iteration : 200, loss : 0.1265, accuracy : 96.34\n",
            "iteration : 250, loss : 0.1256, accuracy : 96.34\n",
            "iteration : 300, loss : 0.1245, accuracy : 96.31\n",
            "iteration : 350, loss : 0.1248, accuracy : 96.27\n",
            "Epoch :  70, training loss : 0.1264, training accuracy : 96.21, test loss : 0.2409, test accuracy : 93.53\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.0996, accuracy : 97.05\n",
            "iteration : 100, loss : 0.1100, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1137, accuracy : 96.66\n",
            "iteration : 200, loss : 0.1149, accuracy : 96.61\n",
            "iteration : 250, loss : 0.1142, accuracy : 96.64\n",
            "iteration : 300, loss : 0.1146, accuracy : 96.60\n",
            "iteration : 350, loss : 0.1166, accuracy : 96.54\n",
            "Epoch :  71, training loss : 0.1173, training accuracy : 96.52, test loss : 0.2335, test accuracy : 93.72\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.1188, accuracy : 96.58\n",
            "iteration : 100, loss : 0.1183, accuracy : 96.64\n",
            "iteration : 150, loss : 0.1205, accuracy : 96.51\n",
            "iteration : 200, loss : 0.1173, accuracy : 96.60\n",
            "iteration : 250, loss : 0.1193, accuracy : 96.59\n",
            "iteration : 300, loss : 0.1202, accuracy : 96.54\n",
            "iteration : 350, loss : 0.1198, accuracy : 96.52\n",
            "Epoch :  72, training loss : 0.1202, training accuracy : 96.52, test loss : 0.2394, test accuracy : 93.65\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.1189, accuracy : 96.47\n",
            "iteration : 100, loss : 0.1084, accuracy : 96.86\n",
            "iteration : 150, loss : 0.1109, accuracy : 96.84\n",
            "iteration : 200, loss : 0.1153, accuracy : 96.71\n",
            "iteration : 250, loss : 0.1209, accuracy : 96.53\n",
            "iteration : 300, loss : 0.1203, accuracy : 96.51\n",
            "iteration : 350, loss : 0.1200, accuracy : 96.50\n",
            "Epoch :  73, training loss : 0.1198, training accuracy : 96.52, test loss : 0.2361, test accuracy : 93.70\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.1139, accuracy : 96.97\n",
            "iteration : 100, loss : 0.1165, accuracy : 96.66\n",
            "iteration : 150, loss : 0.1159, accuracy : 96.61\n",
            "iteration : 200, loss : 0.1129, accuracy : 96.73\n",
            "iteration : 250, loss : 0.1154, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1171, accuracy : 96.61\n",
            "iteration : 350, loss : 0.1178, accuracy : 96.59\n",
            "Epoch :  74, training loss : 0.1182, training accuracy : 96.55, test loss : 0.2507, test accuracy : 93.40\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.0973, accuracy : 97.30\n",
            "iteration : 100, loss : 0.1041, accuracy : 97.09\n",
            "iteration : 150, loss : 0.1045, accuracy : 97.05\n",
            "iteration : 200, loss : 0.1065, accuracy : 96.89\n",
            "iteration : 250, loss : 0.1100, accuracy : 96.80\n",
            "iteration : 300, loss : 0.1107, accuracy : 96.79\n",
            "iteration : 350, loss : 0.1124, accuracy : 96.74\n",
            "Epoch :  75, training loss : 0.1141, training accuracy : 96.71, test loss : 0.2448, test accuracy : 93.34\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.1194, accuracy : 96.59\n",
            "iteration : 100, loss : 0.1168, accuracy : 96.62\n",
            "iteration : 150, loss : 0.1182, accuracy : 96.57\n",
            "iteration : 200, loss : 0.1188, accuracy : 96.51\n",
            "iteration : 250, loss : 0.1210, accuracy : 96.48\n",
            "iteration : 300, loss : 0.1227, accuracy : 96.43\n",
            "iteration : 350, loss : 0.1223, accuracy : 96.44\n",
            "Epoch :  76, training loss : 0.1216, training accuracy : 96.47, test loss : 0.2369, test accuracy : 93.70\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.0957, accuracy : 97.23\n",
            "iteration : 100, loss : 0.0987, accuracy : 97.12\n",
            "iteration : 150, loss : 0.1050, accuracy : 97.04\n",
            "iteration : 200, loss : 0.1080, accuracy : 96.89\n",
            "iteration : 250, loss : 0.1087, accuracy : 96.87\n",
            "iteration : 300, loss : 0.1116, accuracy : 96.77\n",
            "iteration : 350, loss : 0.1123, accuracy : 96.71\n",
            "Epoch :  77, training loss : 0.1135, training accuracy : 96.68, test loss : 0.2366, test accuracy : 93.68\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.1051, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1036, accuracy : 96.84\n",
            "iteration : 150, loss : 0.1051, accuracy : 96.83\n",
            "iteration : 200, loss : 0.1090, accuracy : 96.77\n",
            "iteration : 250, loss : 0.1156, accuracy : 96.59\n",
            "iteration : 300, loss : 0.1179, accuracy : 96.53\n",
            "iteration : 350, loss : 0.1172, accuracy : 96.56\n",
            "Epoch :  78, training loss : 0.1176, training accuracy : 96.55, test loss : 0.2328, test accuracy : 93.80\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.1053, accuracy : 96.97\n",
            "iteration : 100, loss : 0.1110, accuracy : 96.80\n",
            "iteration : 150, loss : 0.1144, accuracy : 96.69\n",
            "iteration : 200, loss : 0.1141, accuracy : 96.73\n",
            "iteration : 250, loss : 0.1160, accuracy : 96.64\n",
            "iteration : 300, loss : 0.1159, accuracy : 96.65\n",
            "iteration : 350, loss : 0.1154, accuracy : 96.69\n",
            "Epoch :  79, training loss : 0.1169, training accuracy : 96.64, test loss : 0.2526, test accuracy : 93.23\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.0987, accuracy : 97.08\n",
            "iteration : 100, loss : 0.1038, accuracy : 97.00\n",
            "iteration : 150, loss : 0.1089, accuracy : 96.85\n",
            "iteration : 200, loss : 0.1131, accuracy : 96.77\n",
            "iteration : 250, loss : 0.1120, accuracy : 96.79\n",
            "iteration : 300, loss : 0.1116, accuracy : 96.81\n",
            "iteration : 350, loss : 0.1147, accuracy : 96.69\n",
            "Epoch :  80, training loss : 0.1153, training accuracy : 96.67, test loss : 0.2294, test accuracy : 93.79\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.1065, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1125, accuracy : 96.66\n",
            "iteration : 150, loss : 0.1123, accuracy : 96.68\n",
            "iteration : 200, loss : 0.1122, accuracy : 96.68\n",
            "iteration : 250, loss : 0.1125, accuracy : 96.71\n",
            "iteration : 300, loss : 0.1137, accuracy : 96.64\n",
            "iteration : 350, loss : 0.1143, accuracy : 96.64\n",
            "Epoch :  81, training loss : 0.1139, training accuracy : 96.67, test loss : 0.2379, test accuracy : 93.73\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.1087, accuracy : 96.67\n",
            "iteration : 100, loss : 0.1048, accuracy : 96.80\n",
            "iteration : 150, loss : 0.1092, accuracy : 96.70\n",
            "iteration : 200, loss : 0.1075, accuracy : 96.75\n",
            "iteration : 250, loss : 0.1100, accuracy : 96.74\n",
            "iteration : 300, loss : 0.1100, accuracy : 96.71\n",
            "iteration : 350, loss : 0.1102, accuracy : 96.70\n",
            "Epoch :  82, training loss : 0.1113, training accuracy : 96.68, test loss : 0.2414, test accuracy : 93.53\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.0929, accuracy : 97.45\n",
            "iteration : 100, loss : 0.1026, accuracy : 97.08\n",
            "iteration : 150, loss : 0.1004, accuracy : 97.07\n",
            "iteration : 200, loss : 0.1013, accuracy : 97.04\n",
            "iteration : 250, loss : 0.1027, accuracy : 97.02\n",
            "iteration : 300, loss : 0.1054, accuracy : 96.96\n",
            "iteration : 350, loss : 0.1084, accuracy : 96.81\n",
            "Epoch :  83, training loss : 0.1084, training accuracy : 96.81, test loss : 0.2282, test accuracy : 93.94\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.1111, accuracy : 96.77\n",
            "iteration : 100, loss : 0.1104, accuracy : 96.79\n",
            "iteration : 150, loss : 0.1054, accuracy : 96.98\n",
            "iteration : 200, loss : 0.1123, accuracy : 96.79\n",
            "iteration : 250, loss : 0.1122, accuracy : 96.76\n",
            "iteration : 300, loss : 0.1129, accuracy : 96.73\n",
            "iteration : 350, loss : 0.1132, accuracy : 96.73\n",
            "Epoch :  84, training loss : 0.1122, training accuracy : 96.75, test loss : 0.2306, test accuracy : 93.81\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.0903, accuracy : 97.30\n",
            "iteration : 100, loss : 0.1004, accuracy : 97.06\n",
            "iteration : 150, loss : 0.0975, accuracy : 97.07\n",
            "iteration : 200, loss : 0.0993, accuracy : 97.07\n",
            "iteration : 250, loss : 0.1040, accuracy : 96.95\n",
            "iteration : 300, loss : 0.1065, accuracy : 96.86\n",
            "iteration : 350, loss : 0.1079, accuracy : 96.81\n",
            "Epoch :  85, training loss : 0.1090, training accuracy : 96.79, test loss : 0.2391, test accuracy : 93.67\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.1058, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1092, accuracy : 96.77\n",
            "iteration : 150, loss : 0.1055, accuracy : 96.90\n",
            "iteration : 200, loss : 0.1064, accuracy : 96.86\n",
            "iteration : 250, loss : 0.1089, accuracy : 96.77\n",
            "iteration : 300, loss : 0.1098, accuracy : 96.77\n",
            "iteration : 350, loss : 0.1101, accuracy : 96.77\n",
            "Epoch :  86, training loss : 0.1105, training accuracy : 96.76, test loss : 0.2326, test accuracy : 93.82\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.1046, accuracy : 96.73\n",
            "iteration : 100, loss : 0.1030, accuracy : 96.95\n",
            "iteration : 150, loss : 0.1047, accuracy : 96.94\n",
            "iteration : 200, loss : 0.1084, accuracy : 96.88\n",
            "iteration : 250, loss : 0.1062, accuracy : 96.91\n",
            "iteration : 300, loss : 0.1088, accuracy : 96.87\n",
            "iteration : 350, loss : 0.1101, accuracy : 96.83\n",
            "Epoch :  87, training loss : 0.1101, training accuracy : 96.86, test loss : 0.2537, test accuracy : 93.22\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.1079, accuracy : 96.77\n",
            "iteration : 100, loss : 0.0977, accuracy : 97.04\n",
            "iteration : 150, loss : 0.0999, accuracy : 97.02\n",
            "iteration : 200, loss : 0.1052, accuracy : 96.83\n",
            "iteration : 250, loss : 0.1092, accuracy : 96.74\n",
            "iteration : 300, loss : 0.1098, accuracy : 96.75\n",
            "iteration : 350, loss : 0.1101, accuracy : 96.75\n",
            "Epoch :  88, training loss : 0.1104, training accuracy : 96.75, test loss : 0.2510, test accuracy : 93.32\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.0958, accuracy : 97.06\n",
            "iteration : 100, loss : 0.1036, accuracy : 96.83\n",
            "iteration : 150, loss : 0.1048, accuracy : 96.84\n",
            "iteration : 200, loss : 0.1057, accuracy : 96.86\n",
            "iteration : 250, loss : 0.1039, accuracy : 96.91\n",
            "iteration : 300, loss : 0.1063, accuracy : 96.83\n",
            "iteration : 350, loss : 0.1056, accuracy : 96.85\n",
            "Epoch :  89, training loss : 0.1066, training accuracy : 96.80, test loss : 0.2410, test accuracy : 93.64\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.1016, accuracy : 96.94\n",
            "iteration : 100, loss : 0.1008, accuracy : 97.06\n",
            "iteration : 150, loss : 0.1057, accuracy : 96.88\n",
            "iteration : 200, loss : 0.1040, accuracy : 96.91\n",
            "iteration : 250, loss : 0.1046, accuracy : 96.90\n",
            "iteration : 300, loss : 0.1078, accuracy : 96.79\n",
            "iteration : 350, loss : 0.1075, accuracy : 96.80\n",
            "Epoch :  90, training loss : 0.1073, training accuracy : 96.82, test loss : 0.2343, test accuracy : 93.76\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.0935, accuracy : 97.23\n",
            "iteration : 100, loss : 0.0998, accuracy : 97.06\n",
            "iteration : 150, loss : 0.0999, accuracy : 97.08\n",
            "iteration : 200, loss : 0.1006, accuracy : 97.05\n",
            "iteration : 250, loss : 0.1030, accuracy : 97.02\n",
            "iteration : 300, loss : 0.1042, accuracy : 96.97\n",
            "iteration : 350, loss : 0.1059, accuracy : 96.91\n",
            "Epoch :  91, training loss : 0.1064, training accuracy : 96.90, test loss : 0.2497, test accuracy : 93.34\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.1002, accuracy : 97.00\n",
            "iteration : 100, loss : 0.1018, accuracy : 96.91\n",
            "iteration : 150, loss : 0.0997, accuracy : 96.99\n",
            "iteration : 200, loss : 0.0983, accuracy : 97.08\n",
            "iteration : 250, loss : 0.0990, accuracy : 97.07\n",
            "iteration : 300, loss : 0.1020, accuracy : 96.97\n",
            "iteration : 350, loss : 0.1045, accuracy : 96.90\n",
            "Epoch :  92, training loss : 0.1053, training accuracy : 96.87, test loss : 0.2255, test accuracy : 93.93\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.0887, accuracy : 97.50\n",
            "iteration : 100, loss : 0.0882, accuracy : 97.50\n",
            "iteration : 150, loss : 0.0900, accuracy : 97.43\n",
            "iteration : 200, loss : 0.0945, accuracy : 97.25\n",
            "iteration : 250, loss : 0.0983, accuracy : 97.15\n",
            "iteration : 300, loss : 0.1007, accuracy : 97.07\n",
            "iteration : 350, loss : 0.1023, accuracy : 97.01\n",
            "Epoch :  93, training loss : 0.1032, training accuracy : 97.00, test loss : 0.2557, test accuracy : 93.02\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.0879, accuracy : 97.44\n",
            "iteration : 100, loss : 0.0963, accuracy : 97.20\n",
            "iteration : 150, loss : 0.0971, accuracy : 97.17\n",
            "iteration : 200, loss : 0.0964, accuracy : 97.16\n",
            "iteration : 250, loss : 0.1009, accuracy : 97.01\n",
            "iteration : 300, loss : 0.1045, accuracy : 96.94\n",
            "iteration : 350, loss : 0.1059, accuracy : 96.91\n",
            "Epoch :  94, training loss : 0.1049, training accuracy : 96.94, test loss : 0.2659, test accuracy : 92.99\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.1104, accuracy : 96.66\n",
            "iteration : 100, loss : 0.1025, accuracy : 97.06\n",
            "iteration : 150, loss : 0.1013, accuracy : 97.12\n",
            "iteration : 200, loss : 0.1048, accuracy : 97.02\n",
            "iteration : 250, loss : 0.1050, accuracy : 96.95\n",
            "iteration : 300, loss : 0.1047, accuracy : 96.91\n",
            "iteration : 350, loss : 0.1064, accuracy : 96.87\n",
            "Epoch :  95, training loss : 0.1070, training accuracy : 96.84, test loss : 0.2277, test accuracy : 93.98\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.0957, accuracy : 97.31\n",
            "iteration : 100, loss : 0.0916, accuracy : 97.41\n",
            "iteration : 150, loss : 0.0947, accuracy : 97.30\n",
            "iteration : 200, loss : 0.0953, accuracy : 97.28\n",
            "iteration : 250, loss : 0.0957, accuracy : 97.30\n",
            "iteration : 300, loss : 0.0951, accuracy : 97.30\n",
            "iteration : 350, loss : 0.0980, accuracy : 97.19\n",
            "Epoch :  96, training loss : 0.1002, training accuracy : 97.12, test loss : 0.2438, test accuracy : 93.38\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.0967, accuracy : 97.09\n",
            "iteration : 100, loss : 0.0939, accuracy : 97.21\n",
            "iteration : 150, loss : 0.0971, accuracy : 97.08\n",
            "iteration : 200, loss : 0.0996, accuracy : 96.97\n",
            "iteration : 250, loss : 0.1018, accuracy : 96.92\n",
            "iteration : 300, loss : 0.1042, accuracy : 96.85\n",
            "iteration : 350, loss : 0.1042, accuracy : 96.85\n",
            "Epoch :  97, training loss : 0.1044, training accuracy : 96.84, test loss : 0.2371, test accuracy : 93.92\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.0881, accuracy : 97.39\n",
            "iteration : 100, loss : 0.0894, accuracy : 97.26\n",
            "iteration : 150, loss : 0.0906, accuracy : 97.26\n",
            "iteration : 200, loss : 0.0973, accuracy : 97.10\n",
            "iteration : 250, loss : 0.1015, accuracy : 96.98\n",
            "iteration : 300, loss : 0.1024, accuracy : 96.99\n",
            "iteration : 350, loss : 0.1030, accuracy : 96.92\n",
            "Epoch :  98, training loss : 0.1033, training accuracy : 96.92, test loss : 0.2436, test accuracy : 93.76\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.0965, accuracy : 97.39\n",
            "iteration : 100, loss : 0.0916, accuracy : 97.43\n",
            "iteration : 150, loss : 0.0978, accuracy : 97.29\n",
            "iteration : 200, loss : 0.0985, accuracy : 97.23\n",
            "iteration : 250, loss : 0.0997, accuracy : 97.16\n",
            "iteration : 300, loss : 0.1005, accuracy : 97.11\n",
            "iteration : 350, loss : 0.1013, accuracy : 97.04\n",
            "Epoch :  99, training loss : 0.1025, training accuracy : 97.02, test loss : 0.2400, test accuracy : 93.75\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.0834, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0872, accuracy : 97.51\n",
            "iteration : 150, loss : 0.0937, accuracy : 97.30\n",
            "iteration : 200, loss : 0.0961, accuracy : 97.25\n",
            "iteration : 250, loss : 0.0960, accuracy : 97.26\n",
            "iteration : 300, loss : 0.0981, accuracy : 97.17\n",
            "iteration : 350, loss : 0.0998, accuracy : 97.10\n",
            "Epoch : 100, training loss : 0.1001, training accuracy : 97.10, test loss : 0.2302, test accuracy : 93.92\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.0868, accuracy : 97.42\n",
            "iteration : 100, loss : 0.0938, accuracy : 97.27\n",
            "iteration : 150, loss : 0.1003, accuracy : 97.05\n",
            "iteration : 200, loss : 0.0979, accuracy : 97.09\n",
            "iteration : 250, loss : 0.0971, accuracy : 97.16\n",
            "iteration : 300, loss : 0.0972, accuracy : 97.12\n",
            "iteration : 350, loss : 0.0972, accuracy : 97.11\n",
            "Epoch : 101, training loss : 0.0978, training accuracy : 97.10, test loss : 0.2313, test accuracy : 94.02\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.0994, accuracy : 97.11\n",
            "iteration : 100, loss : 0.0938, accuracy : 97.24\n",
            "iteration : 150, loss : 0.0968, accuracy : 97.14\n",
            "iteration : 200, loss : 0.0970, accuracy : 97.12\n",
            "iteration : 250, loss : 0.0976, accuracy : 97.13\n",
            "iteration : 300, loss : 0.0964, accuracy : 97.17\n",
            "iteration : 350, loss : 0.0983, accuracy : 97.11\n",
            "Epoch : 102, training loss : 0.0990, training accuracy : 97.10, test loss : 0.2325, test accuracy : 93.90\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.0881, accuracy : 97.58\n",
            "iteration : 100, loss : 0.0921, accuracy : 97.39\n",
            "iteration : 150, loss : 0.0899, accuracy : 97.43\n",
            "iteration : 200, loss : 0.0935, accuracy : 97.32\n",
            "iteration : 250, loss : 0.0964, accuracy : 97.26\n",
            "iteration : 300, loss : 0.0985, accuracy : 97.18\n",
            "iteration : 350, loss : 0.0996, accuracy : 97.15\n",
            "Epoch : 103, training loss : 0.0996, training accuracy : 97.13, test loss : 0.2471, test accuracy : 93.57\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.0920, accuracy : 97.17\n",
            "iteration : 100, loss : 0.0929, accuracy : 97.19\n",
            "iteration : 150, loss : 0.0933, accuracy : 97.18\n",
            "iteration : 200, loss : 0.0913, accuracy : 97.20\n",
            "iteration : 250, loss : 0.0924, accuracy : 97.18\n",
            "iteration : 300, loss : 0.0935, accuracy : 97.14\n",
            "iteration : 350, loss : 0.0957, accuracy : 97.08\n",
            "Epoch : 104, training loss : 0.0966, training accuracy : 97.07, test loss : 0.2455, test accuracy : 93.57\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.0862, accuracy : 97.28\n",
            "iteration : 100, loss : 0.0966, accuracy : 97.20\n",
            "iteration : 150, loss : 0.0982, accuracy : 97.15\n",
            "iteration : 200, loss : 0.0985, accuracy : 97.20\n",
            "iteration : 250, loss : 0.0968, accuracy : 97.21\n",
            "iteration : 300, loss : 0.0977, accuracy : 97.16\n",
            "iteration : 350, loss : 0.0980, accuracy : 97.17\n",
            "Epoch : 105, training loss : 0.0982, training accuracy : 97.18, test loss : 0.2385, test accuracy : 93.87\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.0827, accuracy : 97.61\n",
            "iteration : 100, loss : 0.0868, accuracy : 97.44\n",
            "iteration : 150, loss : 0.0891, accuracy : 97.35\n",
            "iteration : 200, loss : 0.0946, accuracy : 97.16\n",
            "iteration : 250, loss : 0.0965, accuracy : 97.11\n",
            "iteration : 300, loss : 0.0956, accuracy : 97.18\n",
            "iteration : 350, loss : 0.0958, accuracy : 97.21\n",
            "Epoch : 106, training loss : 0.0958, training accuracy : 97.20, test loss : 0.2348, test accuracy : 93.90\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.0815, accuracy : 97.56\n",
            "iteration : 100, loss : 0.0874, accuracy : 97.29\n",
            "iteration : 150, loss : 0.0883, accuracy : 97.34\n",
            "iteration : 200, loss : 0.0913, accuracy : 97.23\n",
            "iteration : 250, loss : 0.0945, accuracy : 97.11\n",
            "iteration : 300, loss : 0.0958, accuracy : 97.10\n",
            "iteration : 350, loss : 0.0978, accuracy : 97.05\n",
            "Epoch : 107, training loss : 0.0983, training accuracy : 97.03, test loss : 0.2396, test accuracy : 93.59\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.0906, accuracy : 97.38\n",
            "iteration : 100, loss : 0.0856, accuracy : 97.42\n",
            "iteration : 150, loss : 0.0891, accuracy : 97.35\n",
            "iteration : 200, loss : 0.0941, accuracy : 97.21\n",
            "iteration : 250, loss : 0.0970, accuracy : 97.09\n",
            "iteration : 300, loss : 0.1002, accuracy : 97.02\n",
            "iteration : 350, loss : 0.1003, accuracy : 97.03\n",
            "Epoch : 108, training loss : 0.1001, training accuracy : 97.02, test loss : 0.2364, test accuracy : 93.77\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.0867, accuracy : 97.50\n",
            "iteration : 100, loss : 0.0875, accuracy : 97.35\n",
            "iteration : 150, loss : 0.0855, accuracy : 97.39\n",
            "iteration : 200, loss : 0.0918, accuracy : 97.23\n",
            "iteration : 250, loss : 0.0930, accuracy : 97.20\n",
            "iteration : 300, loss : 0.0944, accuracy : 97.18\n",
            "iteration : 350, loss : 0.0945, accuracy : 97.15\n",
            "Epoch : 109, training loss : 0.0948, training accuracy : 97.14, test loss : 0.2366, test accuracy : 93.67\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.0774, accuracy : 97.70\n",
            "iteration : 100, loss : 0.0831, accuracy : 97.62\n",
            "iteration : 150, loss : 0.0850, accuracy : 97.53\n",
            "iteration : 200, loss : 0.0885, accuracy : 97.39\n",
            "iteration : 250, loss : 0.0931, accuracy : 97.28\n",
            "iteration : 300, loss : 0.0945, accuracy : 97.23\n",
            "iteration : 350, loss : 0.0952, accuracy : 97.20\n",
            "Epoch : 110, training loss : 0.0960, training accuracy : 97.19, test loss : 0.2502, test accuracy : 93.46\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.0947, accuracy : 97.38\n",
            "iteration : 100, loss : 0.0892, accuracy : 97.40\n",
            "iteration : 150, loss : 0.0873, accuracy : 97.49\n",
            "iteration : 200, loss : 0.0870, accuracy : 97.48\n",
            "iteration : 250, loss : 0.0890, accuracy : 97.42\n",
            "iteration : 300, loss : 0.0905, accuracy : 97.37\n",
            "iteration : 350, loss : 0.0917, accuracy : 97.36\n",
            "Epoch : 111, training loss : 0.0937, training accuracy : 97.28, test loss : 0.2523, test accuracy : 93.46\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.0838, accuracy : 97.64\n",
            "iteration : 100, loss : 0.0836, accuracy : 97.54\n",
            "iteration : 150, loss : 0.0848, accuracy : 97.53\n",
            "iteration : 200, loss : 0.0895, accuracy : 97.38\n",
            "iteration : 250, loss : 0.0917, accuracy : 97.32\n",
            "iteration : 300, loss : 0.0950, accuracy : 97.21\n",
            "iteration : 350, loss : 0.0950, accuracy : 97.20\n",
            "Epoch : 112, training loss : 0.0952, training accuracy : 97.20, test loss : 0.2339, test accuracy : 93.86\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.0829, accuracy : 97.53\n",
            "iteration : 100, loss : 0.0842, accuracy : 97.53\n",
            "iteration : 150, loss : 0.0854, accuracy : 97.50\n",
            "iteration : 200, loss : 0.0859, accuracy : 97.48\n",
            "iteration : 250, loss : 0.0886, accuracy : 97.39\n",
            "iteration : 300, loss : 0.0892, accuracy : 97.35\n",
            "iteration : 350, loss : 0.0909, accuracy : 97.31\n",
            "Epoch : 113, training loss : 0.0916, training accuracy : 97.29, test loss : 0.2236, test accuracy : 94.15\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.0828, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0854, accuracy : 97.51\n",
            "iteration : 150, loss : 0.0857, accuracy : 97.49\n",
            "iteration : 200, loss : 0.0858, accuracy : 97.45\n",
            "iteration : 250, loss : 0.0887, accuracy : 97.38\n",
            "iteration : 300, loss : 0.0889, accuracy : 97.36\n",
            "iteration : 350, loss : 0.0891, accuracy : 97.38\n",
            "Epoch : 114, training loss : 0.0890, training accuracy : 97.36, test loss : 0.2412, test accuracy : 93.92\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.0843, accuracy : 97.47\n",
            "iteration : 100, loss : 0.0827, accuracy : 97.54\n",
            "iteration : 150, loss : 0.0845, accuracy : 97.50\n",
            "iteration : 200, loss : 0.0867, accuracy : 97.44\n",
            "iteration : 250, loss : 0.0905, accuracy : 97.34\n",
            "iteration : 300, loss : 0.0908, accuracy : 97.31\n",
            "iteration : 350, loss : 0.0936, accuracy : 97.24\n",
            "Epoch : 115, training loss : 0.0937, training accuracy : 97.24, test loss : 0.2379, test accuracy : 93.80\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.0836, accuracy : 97.58\n",
            "iteration : 100, loss : 0.0943, accuracy : 97.41\n",
            "iteration : 150, loss : 0.0914, accuracy : 97.38\n",
            "iteration : 200, loss : 0.0893, accuracy : 97.44\n",
            "iteration : 250, loss : 0.0892, accuracy : 97.44\n",
            "iteration : 300, loss : 0.0896, accuracy : 97.40\n",
            "iteration : 350, loss : 0.0915, accuracy : 97.32\n",
            "Epoch : 116, training loss : 0.0913, training accuracy : 97.30, test loss : 0.2589, test accuracy : 93.35\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.0820, accuracy : 97.75\n",
            "iteration : 100, loss : 0.0763, accuracy : 97.80\n",
            "iteration : 150, loss : 0.0832, accuracy : 97.52\n",
            "iteration : 200, loss : 0.0853, accuracy : 97.47\n",
            "iteration : 250, loss : 0.0873, accuracy : 97.41\n",
            "iteration : 300, loss : 0.0889, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0887, accuracy : 97.40\n",
            "Epoch : 117, training loss : 0.0886, training accuracy : 97.41, test loss : 0.2408, test accuracy : 93.74\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.0814, accuracy : 97.61\n",
            "iteration : 100, loss : 0.0818, accuracy : 97.62\n",
            "iteration : 150, loss : 0.0832, accuracy : 97.66\n",
            "iteration : 200, loss : 0.0852, accuracy : 97.59\n",
            "iteration : 250, loss : 0.0863, accuracy : 97.55\n",
            "iteration : 300, loss : 0.0858, accuracy : 97.54\n",
            "iteration : 350, loss : 0.0848, accuracy : 97.59\n",
            "Epoch : 118, training loss : 0.0852, training accuracy : 97.58, test loss : 0.2473, test accuracy : 93.72\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.0703, accuracy : 97.95\n",
            "iteration : 100, loss : 0.0743, accuracy : 97.75\n",
            "iteration : 150, loss : 0.0768, accuracy : 97.71\n",
            "iteration : 200, loss : 0.0835, accuracy : 97.51\n",
            "iteration : 250, loss : 0.0844, accuracy : 97.47\n",
            "iteration : 300, loss : 0.0883, accuracy : 97.35\n",
            "iteration : 350, loss : 0.0883, accuracy : 97.35\n",
            "Epoch : 119, training loss : 0.0900, training accuracy : 97.33, test loss : 0.2456, test accuracy : 93.64\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.0763, accuracy : 97.73\n",
            "iteration : 100, loss : 0.0767, accuracy : 97.73\n",
            "iteration : 150, loss : 0.0803, accuracy : 97.65\n",
            "iteration : 200, loss : 0.0809, accuracy : 97.61\n",
            "iteration : 250, loss : 0.0839, accuracy : 97.51\n",
            "iteration : 300, loss : 0.0864, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0896, accuracy : 97.30\n",
            "Epoch : 120, training loss : 0.0904, training accuracy : 97.28, test loss : 0.2359, test accuracy : 93.83\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.0804, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0831, accuracy : 97.55\n",
            "iteration : 150, loss : 0.0851, accuracy : 97.48\n",
            "iteration : 200, loss : 0.0827, accuracy : 97.59\n",
            "iteration : 250, loss : 0.0843, accuracy : 97.51\n",
            "iteration : 300, loss : 0.0853, accuracy : 97.48\n",
            "iteration : 350, loss : 0.0878, accuracy : 97.43\n",
            "Epoch : 121, training loss : 0.0888, training accuracy : 97.41, test loss : 0.2338, test accuracy : 93.98\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.0786, accuracy : 97.62\n",
            "iteration : 100, loss : 0.0800, accuracy : 97.63\n",
            "iteration : 150, loss : 0.0810, accuracy : 97.60\n",
            "iteration : 200, loss : 0.0809, accuracy : 97.66\n",
            "iteration : 250, loss : 0.0832, accuracy : 97.58\n",
            "iteration : 300, loss : 0.0847, accuracy : 97.50\n",
            "iteration : 350, loss : 0.0864, accuracy : 97.42\n",
            "Epoch : 122, training loss : 0.0874, training accuracy : 97.40, test loss : 0.2499, test accuracy : 93.44\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.0802, accuracy : 97.61\n",
            "iteration : 100, loss : 0.0844, accuracy : 97.48\n",
            "iteration : 150, loss : 0.0885, accuracy : 97.33\n",
            "iteration : 200, loss : 0.0876, accuracy : 97.39\n",
            "iteration : 250, loss : 0.0865, accuracy : 97.38\n",
            "iteration : 300, loss : 0.0872, accuracy : 97.37\n",
            "iteration : 350, loss : 0.0870, accuracy : 97.35\n",
            "Epoch : 123, training loss : 0.0871, training accuracy : 97.34, test loss : 0.2427, test accuracy : 93.81\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.0757, accuracy : 97.77\n",
            "iteration : 100, loss : 0.0806, accuracy : 97.56\n",
            "iteration : 150, loss : 0.0844, accuracy : 97.42\n",
            "iteration : 200, loss : 0.0853, accuracy : 97.44\n",
            "iteration : 250, loss : 0.0857, accuracy : 97.47\n",
            "iteration : 300, loss : 0.0873, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0864, accuracy : 97.42\n",
            "Epoch : 124, training loss : 0.0861, training accuracy : 97.44, test loss : 0.2322, test accuracy : 94.04\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.0725, accuracy : 98.09\n",
            "iteration : 100, loss : 0.0749, accuracy : 97.88\n",
            "iteration : 150, loss : 0.0766, accuracy : 97.76\n",
            "iteration : 200, loss : 0.0779, accuracy : 97.71\n",
            "iteration : 250, loss : 0.0794, accuracy : 97.66\n",
            "iteration : 300, loss : 0.0814, accuracy : 97.61\n",
            "iteration : 350, loss : 0.0829, accuracy : 97.60\n",
            "Epoch : 125, training loss : 0.0835, training accuracy : 97.56, test loss : 0.2426, test accuracy : 93.68\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.0768, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0713, accuracy : 97.80\n",
            "iteration : 150, loss : 0.0762, accuracy : 97.64\n",
            "iteration : 200, loss : 0.0811, accuracy : 97.55\n",
            "iteration : 250, loss : 0.0828, accuracy : 97.54\n",
            "iteration : 300, loss : 0.0825, accuracy : 97.57\n",
            "iteration : 350, loss : 0.0841, accuracy : 97.52\n",
            "Epoch : 126, training loss : 0.0841, training accuracy : 97.51, test loss : 0.2510, test accuracy : 93.46\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.0814, accuracy : 97.47\n",
            "iteration : 100, loss : 0.0894, accuracy : 97.25\n",
            "iteration : 150, loss : 0.0874, accuracy : 97.30\n",
            "iteration : 200, loss : 0.0857, accuracy : 97.35\n",
            "iteration : 250, loss : 0.0833, accuracy : 97.46\n",
            "iteration : 300, loss : 0.0826, accuracy : 97.49\n",
            "iteration : 350, loss : 0.0842, accuracy : 97.45\n",
            "Epoch : 127, training loss : 0.0851, training accuracy : 97.42, test loss : 0.2643, test accuracy : 93.50\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0810, accuracy : 97.53\n",
            "iteration : 100, loss : 0.0838, accuracy : 97.48\n",
            "iteration : 150, loss : 0.0834, accuracy : 97.49\n",
            "iteration : 200, loss : 0.0844, accuracy : 97.51\n",
            "iteration : 250, loss : 0.0817, accuracy : 97.60\n",
            "iteration : 300, loss : 0.0831, accuracy : 97.54\n",
            "iteration : 350, loss : 0.0848, accuracy : 97.48\n",
            "Epoch : 128, training loss : 0.0850, training accuracy : 97.47, test loss : 0.2361, test accuracy : 94.06\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.0839, accuracy : 97.41\n",
            "iteration : 100, loss : 0.0781, accuracy : 97.67\n",
            "iteration : 150, loss : 0.0795, accuracy : 97.66\n",
            "iteration : 200, loss : 0.0769, accuracy : 97.72\n",
            "iteration : 250, loss : 0.0765, accuracy : 97.69\n",
            "iteration : 300, loss : 0.0762, accuracy : 97.68\n",
            "iteration : 350, loss : 0.0791, accuracy : 97.58\n",
            "Epoch : 129, training loss : 0.0797, training accuracy : 97.59, test loss : 0.2427, test accuracy : 93.65\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.0692, accuracy : 97.98\n",
            "iteration : 100, loss : 0.0732, accuracy : 97.86\n",
            "iteration : 150, loss : 0.0768, accuracy : 97.73\n",
            "iteration : 200, loss : 0.0798, accuracy : 97.69\n",
            "iteration : 250, loss : 0.0786, accuracy : 97.72\n",
            "iteration : 300, loss : 0.0802, accuracy : 97.67\n",
            "iteration : 350, loss : 0.0832, accuracy : 97.59\n",
            "Epoch : 130, training loss : 0.0833, training accuracy : 97.58, test loss : 0.2402, test accuracy : 93.67\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.0781, accuracy : 97.84\n",
            "iteration : 100, loss : 0.0781, accuracy : 97.82\n",
            "iteration : 150, loss : 0.0778, accuracy : 97.79\n",
            "iteration : 200, loss : 0.0771, accuracy : 97.80\n",
            "iteration : 250, loss : 0.0779, accuracy : 97.76\n",
            "iteration : 300, loss : 0.0785, accuracy : 97.73\n",
            "iteration : 350, loss : 0.0792, accuracy : 97.67\n",
            "Epoch : 131, training loss : 0.0794, training accuracy : 97.67, test loss : 0.2393, test accuracy : 93.93\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.0753, accuracy : 97.75\n",
            "iteration : 100, loss : 0.0756, accuracy : 97.65\n",
            "iteration : 150, loss : 0.0780, accuracy : 97.59\n",
            "iteration : 200, loss : 0.0807, accuracy : 97.55\n",
            "iteration : 250, loss : 0.0820, accuracy : 97.52\n",
            "iteration : 300, loss : 0.0822, accuracy : 97.54\n",
            "iteration : 350, loss : 0.0817, accuracy : 97.53\n",
            "Epoch : 132, training loss : 0.0828, training accuracy : 97.50, test loss : 0.2476, test accuracy : 94.00\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.0719, accuracy : 97.86\n",
            "iteration : 100, loss : 0.0759, accuracy : 97.77\n",
            "iteration : 150, loss : 0.0768, accuracy : 97.71\n",
            "iteration : 200, loss : 0.0778, accuracy : 97.67\n",
            "iteration : 250, loss : 0.0807, accuracy : 97.56\n",
            "iteration : 300, loss : 0.0812, accuracy : 97.57\n",
            "iteration : 350, loss : 0.0821, accuracy : 97.55\n",
            "Epoch : 133, training loss : 0.0820, training accuracy : 97.54, test loss : 0.2386, test accuracy : 93.92\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0795, accuracy : 97.42\n",
            "iteration : 100, loss : 0.0736, accuracy : 97.69\n",
            "iteration : 150, loss : 0.0750, accuracy : 97.62\n",
            "iteration : 200, loss : 0.0739, accuracy : 97.68\n",
            "iteration : 250, loss : 0.0743, accuracy : 97.68\n",
            "iteration : 300, loss : 0.0753, accuracy : 97.68\n",
            "iteration : 350, loss : 0.0775, accuracy : 97.65\n",
            "Epoch : 134, training loss : 0.0783, training accuracy : 97.62, test loss : 0.2454, test accuracy : 93.85\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.0622, accuracy : 98.12\n",
            "iteration : 100, loss : 0.0669, accuracy : 98.09\n",
            "iteration : 150, loss : 0.0728, accuracy : 97.82\n",
            "iteration : 200, loss : 0.0770, accuracy : 97.68\n",
            "iteration : 250, loss : 0.0777, accuracy : 97.66\n",
            "iteration : 300, loss : 0.0776, accuracy : 97.66\n",
            "iteration : 350, loss : 0.0786, accuracy : 97.64\n",
            "Epoch : 135, training loss : 0.0791, training accuracy : 97.63, test loss : 0.2482, test accuracy : 93.80\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0763, accuracy : 97.55\n",
            "iteration : 100, loss : 0.0740, accuracy : 97.66\n",
            "iteration : 150, loss : 0.0707, accuracy : 97.82\n",
            "iteration : 200, loss : 0.0701, accuracy : 97.93\n",
            "iteration : 250, loss : 0.0740, accuracy : 97.81\n",
            "iteration : 300, loss : 0.0757, accuracy : 97.75\n",
            "iteration : 350, loss : 0.0767, accuracy : 97.73\n",
            "Epoch : 136, training loss : 0.0768, training accuracy : 97.73, test loss : 0.2520, test accuracy : 93.80\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.0698, accuracy : 98.05\n",
            "iteration : 100, loss : 0.0669, accuracy : 98.04\n",
            "iteration : 150, loss : 0.0708, accuracy : 97.93\n",
            "iteration : 200, loss : 0.0722, accuracy : 97.87\n",
            "iteration : 250, loss : 0.0750, accuracy : 97.76\n",
            "iteration : 300, loss : 0.0780, accuracy : 97.65\n",
            "iteration : 350, loss : 0.0782, accuracy : 97.66\n",
            "Epoch : 137, training loss : 0.0781, training accuracy : 97.66, test loss : 0.2407, test accuracy : 93.93\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.0709, accuracy : 98.02\n",
            "iteration : 100, loss : 0.0748, accuracy : 97.84\n",
            "iteration : 150, loss : 0.0741, accuracy : 97.81\n",
            "iteration : 200, loss : 0.0740, accuracy : 97.83\n",
            "iteration : 250, loss : 0.0757, accuracy : 97.78\n",
            "iteration : 300, loss : 0.0771, accuracy : 97.74\n",
            "iteration : 350, loss : 0.0781, accuracy : 97.70\n",
            "Epoch : 138, training loss : 0.0782, training accuracy : 97.70, test loss : 0.2433, test accuracy : 93.57\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.0679, accuracy : 98.08\n",
            "iteration : 100, loss : 0.0705, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0750, accuracy : 97.84\n",
            "iteration : 200, loss : 0.0736, accuracy : 97.84\n",
            "iteration : 250, loss : 0.0746, accuracy : 97.83\n",
            "iteration : 300, loss : 0.0753, accuracy : 97.79\n",
            "iteration : 350, loss : 0.0762, accuracy : 97.75\n",
            "Epoch : 139, training loss : 0.0760, training accuracy : 97.77, test loss : 0.2436, test accuracy : 93.85\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.0852, accuracy : 97.41\n",
            "iteration : 100, loss : 0.0783, accuracy : 97.67\n",
            "iteration : 150, loss : 0.0762, accuracy : 97.79\n",
            "iteration : 200, loss : 0.0777, accuracy : 97.79\n",
            "iteration : 250, loss : 0.0782, accuracy : 97.78\n",
            "iteration : 300, loss : 0.0775, accuracy : 97.77\n",
            "iteration : 350, loss : 0.0772, accuracy : 97.76\n",
            "Epoch : 140, training loss : 0.0765, training accuracy : 97.77, test loss : 0.2373, test accuracy : 94.08\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0613, accuracy : 98.16\n",
            "iteration : 100, loss : 0.0667, accuracy : 97.94\n",
            "iteration : 150, loss : 0.0690, accuracy : 97.83\n",
            "iteration : 200, loss : 0.0732, accuracy : 97.75\n",
            "iteration : 250, loss : 0.0729, accuracy : 97.79\n",
            "iteration : 300, loss : 0.0744, accuracy : 97.71\n",
            "iteration : 350, loss : 0.0756, accuracy : 97.69\n",
            "Epoch : 141, training loss : 0.0760, training accuracy : 97.69, test loss : 0.2504, test accuracy : 93.68\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.0717, accuracy : 97.88\n",
            "iteration : 100, loss : 0.0706, accuracy : 97.91\n",
            "iteration : 150, loss : 0.0681, accuracy : 97.99\n",
            "iteration : 200, loss : 0.0706, accuracy : 97.91\n",
            "iteration : 250, loss : 0.0698, accuracy : 97.92\n",
            "iteration : 300, loss : 0.0714, accuracy : 97.85\n",
            "iteration : 350, loss : 0.0726, accuracy : 97.82\n",
            "Epoch : 142, training loss : 0.0730, training accuracy : 97.81, test loss : 0.2472, test accuracy : 93.83\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.0680, accuracy : 97.88\n",
            "iteration : 100, loss : 0.0688, accuracy : 97.95\n",
            "iteration : 150, loss : 0.0703, accuracy : 97.95\n",
            "iteration : 200, loss : 0.0696, accuracy : 97.95\n",
            "iteration : 250, loss : 0.0716, accuracy : 97.88\n",
            "iteration : 300, loss : 0.0727, accuracy : 97.83\n",
            "iteration : 350, loss : 0.0723, accuracy : 97.87\n",
            "Epoch : 143, training loss : 0.0722, training accuracy : 97.87, test loss : 0.2410, test accuracy : 94.20\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.0670, accuracy : 98.09\n",
            "iteration : 100, loss : 0.0677, accuracy : 98.02\n",
            "iteration : 150, loss : 0.0689, accuracy : 97.92\n",
            "iteration : 200, loss : 0.0711, accuracy : 97.84\n",
            "iteration : 250, loss : 0.0726, accuracy : 97.83\n",
            "iteration : 300, loss : 0.0757, accuracy : 97.72\n",
            "iteration : 350, loss : 0.0754, accuracy : 97.74\n",
            "Epoch : 144, training loss : 0.0755, training accuracy : 97.74, test loss : 0.2313, test accuracy : 94.24\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.0669, accuracy : 98.16\n",
            "iteration : 100, loss : 0.0688, accuracy : 97.98\n",
            "iteration : 150, loss : 0.0675, accuracy : 97.98\n",
            "iteration : 200, loss : 0.0710, accuracy : 97.87\n",
            "iteration : 250, loss : 0.0700, accuracy : 97.91\n",
            "iteration : 300, loss : 0.0712, accuracy : 97.85\n",
            "iteration : 350, loss : 0.0707, accuracy : 97.87\n",
            "Epoch : 145, training loss : 0.0708, training accuracy : 97.86, test loss : 0.2410, test accuracy : 93.88\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0587, accuracy : 98.12\n",
            "iteration : 100, loss : 0.0683, accuracy : 97.90\n",
            "iteration : 150, loss : 0.0680, accuracy : 97.90\n",
            "iteration : 200, loss : 0.0664, accuracy : 97.96\n",
            "iteration : 250, loss : 0.0698, accuracy : 97.89\n",
            "iteration : 300, loss : 0.0698, accuracy : 97.90\n",
            "iteration : 350, loss : 0.0702, accuracy : 97.90\n",
            "Epoch : 146, training loss : 0.0705, training accuracy : 97.89, test loss : 0.2391, test accuracy : 94.00\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0634, accuracy : 98.08\n",
            "iteration : 100, loss : 0.0663, accuracy : 98.04\n",
            "iteration : 150, loss : 0.0670, accuracy : 98.06\n",
            "iteration : 200, loss : 0.0685, accuracy : 97.97\n",
            "iteration : 250, loss : 0.0693, accuracy : 97.92\n",
            "iteration : 300, loss : 0.0703, accuracy : 97.89\n",
            "iteration : 350, loss : 0.0720, accuracy : 97.83\n",
            "Epoch : 147, training loss : 0.0711, training accuracy : 97.85, test loss : 0.2396, test accuracy : 94.03\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.0683, accuracy : 98.08\n",
            "iteration : 100, loss : 0.0674, accuracy : 98.08\n",
            "iteration : 150, loss : 0.0654, accuracy : 98.10\n",
            "iteration : 200, loss : 0.0683, accuracy : 98.04\n",
            "iteration : 250, loss : 0.0691, accuracy : 98.02\n",
            "iteration : 300, loss : 0.0689, accuracy : 98.01\n",
            "iteration : 350, loss : 0.0699, accuracy : 97.98\n",
            "Epoch : 148, training loss : 0.0703, training accuracy : 97.95, test loss : 0.2407, test accuracy : 94.09\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0691, accuracy : 97.98\n",
            "iteration : 100, loss : 0.0696, accuracy : 97.94\n",
            "iteration : 150, loss : 0.0723, accuracy : 97.87\n",
            "iteration : 200, loss : 0.0749, accuracy : 97.77\n",
            "iteration : 250, loss : 0.0751, accuracy : 97.73\n",
            "iteration : 300, loss : 0.0740, accuracy : 97.75\n",
            "iteration : 350, loss : 0.0728, accuracy : 97.79\n",
            "Epoch : 149, training loss : 0.0730, training accuracy : 97.79, test loss : 0.2401, test accuracy : 93.93\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0696, accuracy : 97.92\n",
            "iteration : 100, loss : 0.0679, accuracy : 97.97\n",
            "iteration : 150, loss : 0.0676, accuracy : 98.02\n",
            "iteration : 200, loss : 0.0689, accuracy : 97.93\n",
            "iteration : 250, loss : 0.0672, accuracy : 97.97\n",
            "iteration : 300, loss : 0.0674, accuracy : 97.98\n",
            "iteration : 350, loss : 0.0676, accuracy : 97.97\n",
            "Epoch : 150, training loss : 0.0675, training accuracy : 97.98, test loss : 0.2437, test accuracy : 93.88\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0510, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0537, accuracy : 98.34\n",
            "iteration : 150, loss : 0.0575, accuracy : 98.27\n",
            "iteration : 200, loss : 0.0599, accuracy : 98.16\n",
            "iteration : 250, loss : 0.0624, accuracy : 98.08\n",
            "iteration : 300, loss : 0.0653, accuracy : 98.00\n",
            "iteration : 350, loss : 0.0660, accuracy : 97.97\n",
            "Epoch : 151, training loss : 0.0666, training accuracy : 97.95, test loss : 0.2446, test accuracy : 93.84\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.0631, accuracy : 97.95\n",
            "iteration : 100, loss : 0.0640, accuracy : 98.06\n",
            "iteration : 150, loss : 0.0658, accuracy : 98.03\n",
            "iteration : 200, loss : 0.0652, accuracy : 98.07\n",
            "iteration : 250, loss : 0.0667, accuracy : 98.02\n",
            "iteration : 300, loss : 0.0668, accuracy : 98.02\n",
            "iteration : 350, loss : 0.0670, accuracy : 98.02\n",
            "Epoch : 152, training loss : 0.0663, training accuracy : 98.03, test loss : 0.2543, test accuracy : 94.11\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0652, accuracy : 98.09\n",
            "iteration : 100, loss : 0.0635, accuracy : 98.11\n",
            "iteration : 150, loss : 0.0635, accuracy : 98.14\n",
            "iteration : 200, loss : 0.0634, accuracy : 98.14\n",
            "iteration : 250, loss : 0.0622, accuracy : 98.17\n",
            "iteration : 300, loss : 0.0631, accuracy : 98.17\n",
            "iteration : 350, loss : 0.0646, accuracy : 98.11\n",
            "Epoch : 153, training loss : 0.0651, training accuracy : 98.10, test loss : 0.2477, test accuracy : 94.03\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0647, accuracy : 98.05\n",
            "iteration : 100, loss : 0.0624, accuracy : 98.14\n",
            "iteration : 150, loss : 0.0612, accuracy : 98.20\n",
            "iteration : 200, loss : 0.0636, accuracy : 98.09\n",
            "iteration : 250, loss : 0.0631, accuracy : 98.10\n",
            "iteration : 300, loss : 0.0648, accuracy : 98.05\n",
            "iteration : 350, loss : 0.0649, accuracy : 98.04\n",
            "Epoch : 154, training loss : 0.0646, training accuracy : 98.05, test loss : 0.2446, test accuracy : 94.20\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.0643, accuracy : 97.91\n",
            "iteration : 100, loss : 0.0638, accuracy : 98.01\n",
            "iteration : 150, loss : 0.0623, accuracy : 98.09\n",
            "iteration : 200, loss : 0.0630, accuracy : 98.08\n",
            "iteration : 250, loss : 0.0650, accuracy : 98.01\n",
            "iteration : 300, loss : 0.0678, accuracy : 97.93\n",
            "iteration : 350, loss : 0.0685, accuracy : 97.92\n",
            "Epoch : 155, training loss : 0.0686, training accuracy : 97.91, test loss : 0.2333, test accuracy : 94.16\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0643, accuracy : 98.11\n",
            "iteration : 100, loss : 0.0626, accuracy : 98.11\n",
            "iteration : 150, loss : 0.0621, accuracy : 98.14\n",
            "iteration : 200, loss : 0.0601, accuracy : 98.16\n",
            "iteration : 250, loss : 0.0617, accuracy : 98.13\n",
            "iteration : 300, loss : 0.0620, accuracy : 98.13\n",
            "iteration : 350, loss : 0.0637, accuracy : 98.08\n",
            "Epoch : 156, training loss : 0.0636, training accuracy : 98.08, test loss : 0.2446, test accuracy : 94.06\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0678, accuracy : 97.97\n",
            "iteration : 100, loss : 0.0637, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0630, accuracy : 98.08\n",
            "iteration : 200, loss : 0.0624, accuracy : 98.11\n",
            "iteration : 250, loss : 0.0627, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0642, accuracy : 98.01\n",
            "iteration : 350, loss : 0.0660, accuracy : 97.94\n",
            "Epoch : 157, training loss : 0.0658, training accuracy : 97.95, test loss : 0.2408, test accuracy : 94.12\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.0561, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0622, accuracy : 98.21\n",
            "iteration : 150, loss : 0.0614, accuracy : 98.19\n",
            "iteration : 200, loss : 0.0614, accuracy : 98.20\n",
            "iteration : 250, loss : 0.0611, accuracy : 98.23\n",
            "iteration : 300, loss : 0.0615, accuracy : 98.20\n",
            "iteration : 350, loss : 0.0616, accuracy : 98.20\n",
            "Epoch : 158, training loss : 0.0614, training accuracy : 98.21, test loss : 0.2533, test accuracy : 93.89\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0520, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0550, accuracy : 98.27\n",
            "iteration : 150, loss : 0.0596, accuracy : 98.12\n",
            "iteration : 200, loss : 0.0626, accuracy : 98.05\n",
            "iteration : 250, loss : 0.0607, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0597, accuracy : 98.15\n",
            "iteration : 350, loss : 0.0603, accuracy : 98.15\n",
            "Epoch : 159, training loss : 0.0611, training accuracy : 98.12, test loss : 0.2727, test accuracy : 93.18\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0567, accuracy : 98.52\n",
            "iteration : 100, loss : 0.0609, accuracy : 98.30\n",
            "iteration : 150, loss : 0.0586, accuracy : 98.33\n",
            "iteration : 200, loss : 0.0589, accuracy : 98.30\n",
            "iteration : 250, loss : 0.0593, accuracy : 98.25\n",
            "iteration : 300, loss : 0.0600, accuracy : 98.22\n",
            "iteration : 350, loss : 0.0594, accuracy : 98.23\n",
            "Epoch : 160, training loss : 0.0590, training accuracy : 98.24, test loss : 0.2566, test accuracy : 93.98\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0609, accuracy : 98.28\n",
            "iteration : 100, loss : 0.0628, accuracy : 98.12\n",
            "iteration : 150, loss : 0.0620, accuracy : 98.18\n",
            "iteration : 200, loss : 0.0622, accuracy : 98.13\n",
            "iteration : 250, loss : 0.0625, accuracy : 98.14\n",
            "iteration : 300, loss : 0.0634, accuracy : 98.12\n",
            "iteration : 350, loss : 0.0641, accuracy : 98.09\n",
            "Epoch : 161, training loss : 0.0644, training accuracy : 98.09, test loss : 0.2470, test accuracy : 94.03\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0573, accuracy : 98.25\n",
            "iteration : 100, loss : 0.0594, accuracy : 98.19\n",
            "iteration : 150, loss : 0.0567, accuracy : 98.26\n",
            "iteration : 200, loss : 0.0614, accuracy : 98.11\n",
            "iteration : 250, loss : 0.0619, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0610, accuracy : 98.12\n",
            "iteration : 350, loss : 0.0624, accuracy : 98.08\n",
            "Epoch : 162, training loss : 0.0626, training accuracy : 98.07, test loss : 0.2541, test accuracy : 93.77\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0525, accuracy : 98.56\n",
            "iteration : 100, loss : 0.0546, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0541, accuracy : 98.43\n",
            "iteration : 200, loss : 0.0535, accuracy : 98.43\n",
            "iteration : 250, loss : 0.0535, accuracy : 98.43\n",
            "iteration : 300, loss : 0.0548, accuracy : 98.38\n",
            "iteration : 350, loss : 0.0556, accuracy : 98.36\n",
            "Epoch : 163, training loss : 0.0562, training accuracy : 98.35, test loss : 0.2573, test accuracy : 93.75\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0513, accuracy : 98.56\n",
            "iteration : 100, loss : 0.0541, accuracy : 98.46\n",
            "iteration : 150, loss : 0.0544, accuracy : 98.41\n",
            "iteration : 200, loss : 0.0566, accuracy : 98.35\n",
            "iteration : 250, loss : 0.0571, accuracy : 98.31\n",
            "iteration : 300, loss : 0.0576, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0597, accuracy : 98.22\n",
            "Epoch : 164, training loss : 0.0606, training accuracy : 98.19, test loss : 0.2492, test accuracy : 94.13\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0569, accuracy : 98.30\n",
            "iteration : 100, loss : 0.0548, accuracy : 98.30\n",
            "iteration : 150, loss : 0.0559, accuracy : 98.27\n",
            "iteration : 200, loss : 0.0557, accuracy : 98.29\n",
            "iteration : 250, loss : 0.0560, accuracy : 98.31\n",
            "iteration : 300, loss : 0.0562, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0563, accuracy : 98.30\n",
            "Epoch : 165, training loss : 0.0574, training accuracy : 98.27, test loss : 0.2446, test accuracy : 93.90\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0487, accuracy : 98.52\n",
            "iteration : 100, loss : 0.0519, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0537, accuracy : 98.44\n",
            "iteration : 200, loss : 0.0561, accuracy : 98.37\n",
            "iteration : 250, loss : 0.0569, accuracy : 98.30\n",
            "iteration : 300, loss : 0.0558, accuracy : 98.33\n",
            "iteration : 350, loss : 0.0556, accuracy : 98.31\n",
            "Epoch : 166, training loss : 0.0558, training accuracy : 98.31, test loss : 0.2400, test accuracy : 94.27\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0539, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0519, accuracy : 98.37\n",
            "iteration : 150, loss : 0.0506, accuracy : 98.41\n",
            "iteration : 200, loss : 0.0528, accuracy : 98.36\n",
            "iteration : 250, loss : 0.0540, accuracy : 98.33\n",
            "iteration : 300, loss : 0.0554, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0564, accuracy : 98.25\n",
            "Epoch : 167, training loss : 0.0566, training accuracy : 98.25, test loss : 0.2393, test accuracy : 94.16\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0507, accuracy : 98.38\n",
            "iteration : 100, loss : 0.0557, accuracy : 98.28\n",
            "iteration : 150, loss : 0.0543, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0541, accuracy : 98.32\n",
            "iteration : 250, loss : 0.0536, accuracy : 98.35\n",
            "iteration : 300, loss : 0.0546, accuracy : 98.34\n",
            "iteration : 350, loss : 0.0552, accuracy : 98.31\n",
            "Epoch : 168, training loss : 0.0555, training accuracy : 98.29, test loss : 0.2425, test accuracy : 94.01\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0502, accuracy : 98.47\n",
            "iteration : 100, loss : 0.0532, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0554, accuracy : 98.31\n",
            "iteration : 200, loss : 0.0547, accuracy : 98.38\n",
            "iteration : 250, loss : 0.0549, accuracy : 98.39\n",
            "iteration : 300, loss : 0.0567, accuracy : 98.31\n",
            "iteration : 350, loss : 0.0581, accuracy : 98.28\n",
            "Epoch : 169, training loss : 0.0582, training accuracy : 98.26, test loss : 0.2467, test accuracy : 94.11\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0468, accuracy : 98.44\n",
            "iteration : 100, loss : 0.0449, accuracy : 98.55\n",
            "iteration : 150, loss : 0.0475, accuracy : 98.49\n",
            "iteration : 200, loss : 0.0493, accuracy : 98.45\n",
            "iteration : 250, loss : 0.0520, accuracy : 98.42\n",
            "iteration : 300, loss : 0.0541, accuracy : 98.35\n",
            "iteration : 350, loss : 0.0547, accuracy : 98.33\n",
            "Epoch : 170, training loss : 0.0544, training accuracy : 98.35, test loss : 0.2441, test accuracy : 94.14\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0433, accuracy : 98.47\n",
            "iteration : 100, loss : 0.0449, accuracy : 98.52\n",
            "iteration : 150, loss : 0.0497, accuracy : 98.36\n",
            "iteration : 200, loss : 0.0519, accuracy : 98.35\n",
            "iteration : 250, loss : 0.0541, accuracy : 98.29\n",
            "iteration : 300, loss : 0.0549, accuracy : 98.29\n",
            "iteration : 350, loss : 0.0553, accuracy : 98.29\n",
            "Epoch : 171, training loss : 0.0563, training accuracy : 98.28, test loss : 0.2471, test accuracy : 94.01\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0643, accuracy : 98.25\n",
            "iteration : 100, loss : 0.0551, accuracy : 98.40\n",
            "iteration : 150, loss : 0.0548, accuracy : 98.41\n",
            "iteration : 200, loss : 0.0541, accuracy : 98.40\n",
            "iteration : 250, loss : 0.0567, accuracy : 98.35\n",
            "iteration : 300, loss : 0.0561, accuracy : 98.36\n",
            "iteration : 350, loss : 0.0553, accuracy : 98.38\n",
            "Epoch : 172, training loss : 0.0548, training accuracy : 98.39, test loss : 0.2451, test accuracy : 94.26\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0587, accuracy : 98.27\n",
            "iteration : 100, loss : 0.0525, accuracy : 98.46\n",
            "iteration : 150, loss : 0.0534, accuracy : 98.40\n",
            "iteration : 200, loss : 0.0528, accuracy : 98.42\n",
            "iteration : 250, loss : 0.0522, accuracy : 98.46\n",
            "iteration : 300, loss : 0.0529, accuracy : 98.45\n",
            "iteration : 350, loss : 0.0544, accuracy : 98.41\n",
            "Epoch : 173, training loss : 0.0553, training accuracy : 98.38, test loss : 0.2447, test accuracy : 94.18\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0483, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0508, accuracy : 98.46\n",
            "iteration : 150, loss : 0.0521, accuracy : 98.47\n",
            "iteration : 200, loss : 0.0516, accuracy : 98.51\n",
            "iteration : 250, loss : 0.0511, accuracy : 98.53\n",
            "iteration : 300, loss : 0.0515, accuracy : 98.53\n",
            "iteration : 350, loss : 0.0526, accuracy : 98.49\n",
            "Epoch : 174, training loss : 0.0529, training accuracy : 98.48, test loss : 0.2468, test accuracy : 94.13\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0453, accuracy : 98.55\n",
            "iteration : 100, loss : 0.0499, accuracy : 98.38\n",
            "iteration : 150, loss : 0.0470, accuracy : 98.57\n",
            "iteration : 200, loss : 0.0490, accuracy : 98.50\n",
            "iteration : 250, loss : 0.0502, accuracy : 98.48\n",
            "iteration : 300, loss : 0.0522, accuracy : 98.44\n",
            "iteration : 350, loss : 0.0533, accuracy : 98.40\n",
            "Epoch : 175, training loss : 0.0535, training accuracy : 98.39, test loss : 0.2419, test accuracy : 94.25\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0562, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0536, accuracy : 98.41\n",
            "iteration : 150, loss : 0.0507, accuracy : 98.45\n",
            "iteration : 200, loss : 0.0511, accuracy : 98.43\n",
            "iteration : 250, loss : 0.0516, accuracy : 98.43\n",
            "iteration : 300, loss : 0.0515, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0522, accuracy : 98.40\n",
            "Epoch : 176, training loss : 0.0523, training accuracy : 98.41, test loss : 0.2395, test accuracy : 94.22\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0461, accuracy : 98.70\n",
            "iteration : 100, loss : 0.0476, accuracy : 98.58\n",
            "iteration : 150, loss : 0.0481, accuracy : 98.58\n",
            "iteration : 200, loss : 0.0486, accuracy : 98.59\n",
            "iteration : 250, loss : 0.0493, accuracy : 98.57\n",
            "iteration : 300, loss : 0.0506, accuracy : 98.53\n",
            "iteration : 350, loss : 0.0518, accuracy : 98.50\n",
            "Epoch : 177, training loss : 0.0523, training accuracy : 98.48, test loss : 0.2508, test accuracy : 94.08\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0465, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0455, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0469, accuracy : 98.57\n",
            "iteration : 200, loss : 0.0454, accuracy : 98.58\n",
            "iteration : 250, loss : 0.0452, accuracy : 98.58\n",
            "iteration : 300, loss : 0.0458, accuracy : 98.57\n",
            "iteration : 350, loss : 0.0473, accuracy : 98.52\n",
            "Epoch : 178, training loss : 0.0481, training accuracy : 98.50, test loss : 0.2495, test accuracy : 94.11\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0477, accuracy : 98.48\n",
            "iteration : 100, loss : 0.0487, accuracy : 98.49\n",
            "iteration : 150, loss : 0.0475, accuracy : 98.54\n",
            "iteration : 200, loss : 0.0476, accuracy : 98.55\n",
            "iteration : 250, loss : 0.0472, accuracy : 98.55\n",
            "iteration : 300, loss : 0.0474, accuracy : 98.56\n",
            "iteration : 350, loss : 0.0484, accuracy : 98.54\n",
            "Epoch : 179, training loss : 0.0488, training accuracy : 98.54, test loss : 0.2440, test accuracy : 94.31\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0415, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0467, accuracy : 98.64\n",
            "iteration : 150, loss : 0.0477, accuracy : 98.58\n",
            "iteration : 200, loss : 0.0481, accuracy : 98.55\n",
            "iteration : 250, loss : 0.0495, accuracy : 98.48\n",
            "iteration : 300, loss : 0.0513, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0510, accuracy : 98.44\n",
            "Epoch : 180, training loss : 0.0509, training accuracy : 98.44, test loss : 0.2387, test accuracy : 94.27\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0475, accuracy : 98.50\n",
            "iteration : 100, loss : 0.0476, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0481, accuracy : 98.54\n",
            "iteration : 200, loss : 0.0473, accuracy : 98.59\n",
            "iteration : 250, loss : 0.0497, accuracy : 98.51\n",
            "iteration : 300, loss : 0.0495, accuracy : 98.51\n",
            "iteration : 350, loss : 0.0488, accuracy : 98.55\n",
            "Epoch : 181, training loss : 0.0486, training accuracy : 98.56, test loss : 0.2394, test accuracy : 94.34\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0453, accuracy : 98.56\n",
            "iteration : 100, loss : 0.0465, accuracy : 98.62\n",
            "iteration : 150, loss : 0.0428, accuracy : 98.76\n",
            "iteration : 200, loss : 0.0444, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0460, accuracy : 98.60\n",
            "iteration : 300, loss : 0.0468, accuracy : 98.56\n",
            "iteration : 350, loss : 0.0475, accuracy : 98.53\n",
            "Epoch : 182, training loss : 0.0475, training accuracy : 98.54, test loss : 0.2437, test accuracy : 94.38\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0446, accuracy : 98.62\n",
            "iteration : 100, loss : 0.0418, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0419, accuracy : 98.71\n",
            "iteration : 200, loss : 0.0454, accuracy : 98.61\n",
            "iteration : 250, loss : 0.0455, accuracy : 98.60\n",
            "iteration : 300, loss : 0.0459, accuracy : 98.57\n",
            "iteration : 350, loss : 0.0457, accuracy : 98.57\n",
            "Epoch : 183, training loss : 0.0457, training accuracy : 98.57, test loss : 0.2481, test accuracy : 94.14\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0514, accuracy : 98.28\n",
            "iteration : 100, loss : 0.0506, accuracy : 98.39\n",
            "iteration : 150, loss : 0.0504, accuracy : 98.46\n",
            "iteration : 200, loss : 0.0484, accuracy : 98.52\n",
            "iteration : 250, loss : 0.0480, accuracy : 98.56\n",
            "iteration : 300, loss : 0.0489, accuracy : 98.52\n",
            "iteration : 350, loss : 0.0496, accuracy : 98.50\n",
            "Epoch : 184, training loss : 0.0502, training accuracy : 98.48, test loss : 0.2532, test accuracy : 93.93\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0455, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0455, accuracy : 98.70\n",
            "iteration : 150, loss : 0.0459, accuracy : 98.68\n",
            "iteration : 200, loss : 0.0467, accuracy : 98.62\n",
            "iteration : 250, loss : 0.0470, accuracy : 98.59\n",
            "iteration : 300, loss : 0.0467, accuracy : 98.62\n",
            "iteration : 350, loss : 0.0488, accuracy : 98.56\n",
            "Epoch : 185, training loss : 0.0491, training accuracy : 98.55, test loss : 0.2562, test accuracy : 93.98\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0487, accuracy : 98.44\n",
            "iteration : 100, loss : 0.0472, accuracy : 98.49\n",
            "iteration : 150, loss : 0.0464, accuracy : 98.55\n",
            "iteration : 200, loss : 0.0467, accuracy : 98.53\n",
            "iteration : 250, loss : 0.0460, accuracy : 98.55\n",
            "iteration : 300, loss : 0.0460, accuracy : 98.57\n",
            "iteration : 350, loss : 0.0468, accuracy : 98.56\n",
            "Epoch : 186, training loss : 0.0467, training accuracy : 98.55, test loss : 0.2330, test accuracy : 94.43\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0463, accuracy : 98.56\n",
            "iteration : 100, loss : 0.0465, accuracy : 98.61\n",
            "iteration : 150, loss : 0.0473, accuracy : 98.56\n",
            "iteration : 200, loss : 0.0465, accuracy : 98.61\n",
            "iteration : 250, loss : 0.0467, accuracy : 98.61\n",
            "iteration : 300, loss : 0.0485, accuracy : 98.53\n",
            "iteration : 350, loss : 0.0482, accuracy : 98.53\n",
            "Epoch : 187, training loss : 0.0483, training accuracy : 98.53, test loss : 0.2456, test accuracy : 94.09\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0398, accuracy : 98.83\n",
            "iteration : 100, loss : 0.0411, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0408, accuracy : 98.79\n",
            "iteration : 200, loss : 0.0403, accuracy : 98.79\n",
            "iteration : 250, loss : 0.0420, accuracy : 98.76\n",
            "iteration : 300, loss : 0.0433, accuracy : 98.74\n",
            "iteration : 350, loss : 0.0449, accuracy : 98.67\n",
            "Epoch : 188, training loss : 0.0456, training accuracy : 98.65, test loss : 0.2421, test accuracy : 94.37\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0415, accuracy : 98.66\n",
            "iteration : 100, loss : 0.0391, accuracy : 98.75\n",
            "iteration : 150, loss : 0.0395, accuracy : 98.79\n",
            "iteration : 200, loss : 0.0389, accuracy : 98.80\n",
            "iteration : 250, loss : 0.0400, accuracy : 98.77\n",
            "iteration : 300, loss : 0.0397, accuracy : 98.78\n",
            "iteration : 350, loss : 0.0398, accuracy : 98.78\n",
            "Epoch : 189, training loss : 0.0400, training accuracy : 98.80, test loss : 0.2466, test accuracy : 94.15\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0468, accuracy : 98.59\n",
            "iteration : 100, loss : 0.0426, accuracy : 98.67\n",
            "iteration : 150, loss : 0.0396, accuracy : 98.80\n",
            "iteration : 200, loss : 0.0399, accuracy : 98.76\n",
            "iteration : 250, loss : 0.0409, accuracy : 98.74\n",
            "iteration : 300, loss : 0.0424, accuracy : 98.69\n",
            "iteration : 350, loss : 0.0435, accuracy : 98.68\n",
            "Epoch : 190, training loss : 0.0433, training accuracy : 98.69, test loss : 0.2402, test accuracy : 94.19\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0406, accuracy : 98.78\n",
            "iteration : 100, loss : 0.0428, accuracy : 98.72\n",
            "iteration : 150, loss : 0.0449, accuracy : 98.68\n",
            "iteration : 200, loss : 0.0429, accuracy : 98.75\n",
            "iteration : 250, loss : 0.0428, accuracy : 98.72\n",
            "iteration : 300, loss : 0.0426, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0421, accuracy : 98.75\n",
            "Epoch : 191, training loss : 0.0422, training accuracy : 98.75, test loss : 0.2458, test accuracy : 94.07\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0401, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0382, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0397, accuracy : 98.82\n",
            "iteration : 200, loss : 0.0407, accuracy : 98.77\n",
            "iteration : 250, loss : 0.0394, accuracy : 98.83\n",
            "iteration : 300, loss : 0.0420, accuracy : 98.76\n",
            "iteration : 350, loss : 0.0427, accuracy : 98.72\n",
            "Epoch : 192, training loss : 0.0425, training accuracy : 98.73, test loss : 0.2535, test accuracy : 94.14\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0445, accuracy : 98.62\n",
            "iteration : 100, loss : 0.0430, accuracy : 98.68\n",
            "iteration : 150, loss : 0.0437, accuracy : 98.73\n",
            "iteration : 200, loss : 0.0411, accuracy : 98.81\n",
            "iteration : 250, loss : 0.0409, accuracy : 98.80\n",
            "iteration : 300, loss : 0.0411, accuracy : 98.78\n",
            "iteration : 350, loss : 0.0408, accuracy : 98.78\n",
            "Epoch : 193, training loss : 0.0406, training accuracy : 98.79, test loss : 0.2396, test accuracy : 94.41\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0427, accuracy : 98.69\n",
            "iteration : 100, loss : 0.0418, accuracy : 98.71\n",
            "iteration : 150, loss : 0.0433, accuracy : 98.66\n",
            "iteration : 200, loss : 0.0431, accuracy : 98.67\n",
            "iteration : 250, loss : 0.0439, accuracy : 98.66\n",
            "iteration : 300, loss : 0.0451, accuracy : 98.65\n",
            "iteration : 350, loss : 0.0460, accuracy : 98.62\n",
            "Epoch : 194, training loss : 0.0463, training accuracy : 98.63, test loss : 0.2404, test accuracy : 94.21\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0350, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0354, accuracy : 98.91\n",
            "iteration : 150, loss : 0.0366, accuracy : 98.93\n",
            "iteration : 200, loss : 0.0380, accuracy : 98.87\n",
            "iteration : 250, loss : 0.0387, accuracy : 98.86\n",
            "iteration : 300, loss : 0.0383, accuracy : 98.86\n",
            "iteration : 350, loss : 0.0387, accuracy : 98.86\n",
            "Epoch : 195, training loss : 0.0384, training accuracy : 98.86, test loss : 0.2366, test accuracy : 94.43\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0328, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0366, accuracy : 99.00\n",
            "iteration : 150, loss : 0.0395, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0414, accuracy : 98.76\n",
            "iteration : 250, loss : 0.0417, accuracy : 98.75\n",
            "iteration : 300, loss : 0.0427, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0433, accuracy : 98.73\n",
            "Epoch : 196, training loss : 0.0430, training accuracy : 98.74, test loss : 0.2370, test accuracy : 94.24\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0364, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0344, accuracy : 98.99\n",
            "iteration : 150, loss : 0.0356, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0372, accuracy : 98.88\n",
            "iteration : 250, loss : 0.0397, accuracy : 98.79\n",
            "iteration : 300, loss : 0.0412, accuracy : 98.78\n",
            "iteration : 350, loss : 0.0420, accuracy : 98.74\n",
            "Epoch : 197, training loss : 0.0420, training accuracy : 98.74, test loss : 0.2357, test accuracy : 94.45\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0378, accuracy : 98.94\n",
            "iteration : 100, loss : 0.0388, accuracy : 98.89\n",
            "iteration : 150, loss : 0.0391, accuracy : 98.90\n",
            "iteration : 200, loss : 0.0382, accuracy : 98.93\n",
            "iteration : 250, loss : 0.0368, accuracy : 98.99\n",
            "iteration : 300, loss : 0.0381, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0391, accuracy : 98.91\n",
            "Epoch : 198, training loss : 0.0398, training accuracy : 98.88, test loss : 0.2613, test accuracy : 93.75\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0345, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0358, accuracy : 99.00\n",
            "iteration : 150, loss : 0.0355, accuracy : 98.98\n",
            "iteration : 200, loss : 0.0349, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0346, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0360, accuracy : 98.98\n",
            "iteration : 350, loss : 0.0373, accuracy : 98.94\n",
            "Epoch : 199, training loss : 0.0376, training accuracy : 98.93, test loss : 0.2469, test accuracy : 94.25\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0364, accuracy : 98.81\n",
            "iteration : 100, loss : 0.0385, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0409, accuracy : 98.73\n",
            "iteration : 200, loss : 0.0406, accuracy : 98.71\n",
            "iteration : 250, loss : 0.0416, accuracy : 98.71\n",
            "iteration : 300, loss : 0.0410, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0415, accuracy : 98.74\n",
            "Epoch : 200, training loss : 0.0411, training accuracy : 98.75, test loss : 0.2470, test accuracy : 94.35\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0332, accuracy : 99.11\n",
            "iteration : 100, loss : 0.0331, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0325, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0331, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0359, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0362, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0367, accuracy : 98.93\n",
            "Epoch : 201, training loss : 0.0368, training accuracy : 98.94, test loss : 0.2493, test accuracy : 94.18\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0368, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0364, accuracy : 98.88\n",
            "iteration : 150, loss : 0.0359, accuracy : 98.91\n",
            "iteration : 200, loss : 0.0374, accuracy : 98.89\n",
            "iteration : 250, loss : 0.0386, accuracy : 98.86\n",
            "iteration : 300, loss : 0.0383, accuracy : 98.88\n",
            "iteration : 350, loss : 0.0379, accuracy : 98.90\n",
            "Epoch : 202, training loss : 0.0377, training accuracy : 98.91, test loss : 0.2531, test accuracy : 94.05\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0366, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0357, accuracy : 98.97\n",
            "iteration : 150, loss : 0.0386, accuracy : 98.87\n",
            "iteration : 200, loss : 0.0385, accuracy : 98.87\n",
            "iteration : 250, loss : 0.0390, accuracy : 98.86\n",
            "iteration : 300, loss : 0.0394, accuracy : 98.87\n",
            "iteration : 350, loss : 0.0391, accuracy : 98.87\n",
            "Epoch : 203, training loss : 0.0392, training accuracy : 98.86, test loss : 0.2422, test accuracy : 94.41\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0316, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0312, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0314, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0328, accuracy : 99.00\n",
            "iteration : 250, loss : 0.0340, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0349, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0355, accuracy : 98.94\n",
            "Epoch : 204, training loss : 0.0356, training accuracy : 98.95, test loss : 0.2507, test accuracy : 94.32\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0310, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0335, accuracy : 98.99\n",
            "iteration : 150, loss : 0.0338, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0344, accuracy : 98.95\n",
            "iteration : 250, loss : 0.0340, accuracy : 98.97\n",
            "iteration : 300, loss : 0.0340, accuracy : 98.96\n",
            "iteration : 350, loss : 0.0353, accuracy : 98.93\n",
            "Epoch : 205, training loss : 0.0354, training accuracy : 98.94, test loss : 0.2484, test accuracy : 94.29\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0310, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0318, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0349, accuracy : 98.98\n",
            "iteration : 200, loss : 0.0344, accuracy : 98.95\n",
            "iteration : 250, loss : 0.0349, accuracy : 98.92\n",
            "iteration : 300, loss : 0.0340, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0347, accuracy : 98.94\n",
            "Epoch : 206, training loss : 0.0346, training accuracy : 98.95, test loss : 0.2427, test accuracy : 94.42\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0313, accuracy : 99.06\n",
            "iteration : 100, loss : 0.0313, accuracy : 99.11\n",
            "iteration : 150, loss : 0.0323, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0326, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0330, accuracy : 99.04\n",
            "iteration : 300, loss : 0.0334, accuracy : 99.01\n",
            "iteration : 350, loss : 0.0342, accuracy : 98.97\n",
            "Epoch : 207, training loss : 0.0341, training accuracy : 98.97, test loss : 0.2484, test accuracy : 94.17\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0317, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0346, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0338, accuracy : 99.12\n",
            "iteration : 200, loss : 0.0345, accuracy : 99.07\n",
            "iteration : 250, loss : 0.0338, accuracy : 99.07\n",
            "iteration : 300, loss : 0.0338, accuracy : 99.06\n",
            "iteration : 350, loss : 0.0338, accuracy : 99.06\n",
            "Epoch : 208, training loss : 0.0341, training accuracy : 99.06, test loss : 0.2448, test accuracy : 94.48\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0273, accuracy : 99.08\n",
            "iteration : 100, loss : 0.0304, accuracy : 99.06\n",
            "iteration : 150, loss : 0.0346, accuracy : 98.96\n",
            "iteration : 200, loss : 0.0361, accuracy : 98.91\n",
            "iteration : 250, loss : 0.0369, accuracy : 98.89\n",
            "iteration : 300, loss : 0.0360, accuracy : 98.90\n",
            "iteration : 350, loss : 0.0359, accuracy : 98.91\n",
            "Epoch : 209, training loss : 0.0364, training accuracy : 98.89, test loss : 0.2440, test accuracy : 94.37\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0326, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0366, accuracy : 98.90\n",
            "iteration : 150, loss : 0.0368, accuracy : 98.91\n",
            "iteration : 200, loss : 0.0360, accuracy : 98.93\n",
            "iteration : 250, loss : 0.0358, accuracy : 98.96\n",
            "iteration : 300, loss : 0.0357, accuracy : 98.99\n",
            "iteration : 350, loss : 0.0349, accuracy : 99.00\n",
            "Epoch : 210, training loss : 0.0348, training accuracy : 99.02, test loss : 0.2367, test accuracy : 94.55\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0310, accuracy : 98.91\n",
            "iteration : 100, loss : 0.0325, accuracy : 98.97\n",
            "iteration : 150, loss : 0.0318, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0311, accuracy : 99.07\n",
            "iteration : 250, loss : 0.0307, accuracy : 99.10\n",
            "iteration : 300, loss : 0.0321, accuracy : 99.06\n",
            "iteration : 350, loss : 0.0331, accuracy : 99.05\n",
            "Epoch : 211, training loss : 0.0329, training accuracy : 99.06, test loss : 0.2372, test accuracy : 94.58\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0313, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0307, accuracy : 99.17\n",
            "iteration : 150, loss : 0.0299, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0311, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0311, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0320, accuracy : 99.07\n",
            "iteration : 350, loss : 0.0317, accuracy : 99.08\n",
            "Epoch : 212, training loss : 0.0318, training accuracy : 99.06, test loss : 0.2380, test accuracy : 94.47\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0298, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0298, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0315, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0318, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0322, accuracy : 99.02\n",
            "iteration : 300, loss : 0.0323, accuracy : 99.05\n",
            "iteration : 350, loss : 0.0321, accuracy : 99.07\n",
            "Epoch : 213, training loss : 0.0326, training accuracy : 99.06, test loss : 0.2437, test accuracy : 94.41\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0336, accuracy : 98.95\n",
            "iteration : 100, loss : 0.0332, accuracy : 99.03\n",
            "iteration : 150, loss : 0.0326, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0327, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0319, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0310, accuracy : 99.12\n",
            "iteration : 350, loss : 0.0305, accuracy : 99.14\n",
            "Epoch : 214, training loss : 0.0307, training accuracy : 99.13, test loss : 0.2449, test accuracy : 94.38\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0281, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0288, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0298, accuracy : 99.15\n",
            "iteration : 200, loss : 0.0317, accuracy : 99.09\n",
            "iteration : 250, loss : 0.0307, accuracy : 99.14\n",
            "iteration : 300, loss : 0.0300, accuracy : 99.15\n",
            "iteration : 350, loss : 0.0298, accuracy : 99.14\n",
            "Epoch : 215, training loss : 0.0301, training accuracy : 99.13, test loss : 0.2370, test accuracy : 94.58\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0305, accuracy : 99.03\n",
            "iteration : 100, loss : 0.0296, accuracy : 99.15\n",
            "iteration : 150, loss : 0.0301, accuracy : 99.16\n",
            "iteration : 200, loss : 0.0303, accuracy : 99.14\n",
            "iteration : 250, loss : 0.0322, accuracy : 99.06\n",
            "iteration : 300, loss : 0.0320, accuracy : 99.06\n",
            "iteration : 350, loss : 0.0320, accuracy : 99.06\n",
            "Epoch : 216, training loss : 0.0317, training accuracy : 99.07, test loss : 0.2402, test accuracy : 94.50\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0330, accuracy : 99.03\n",
            "iteration : 100, loss : 0.0308, accuracy : 99.04\n",
            "iteration : 150, loss : 0.0319, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0310, accuracy : 99.07\n",
            "iteration : 250, loss : 0.0306, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0303, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0305, accuracy : 99.09\n",
            "Epoch : 217, training loss : 0.0308, training accuracy : 99.08, test loss : 0.2483, test accuracy : 94.39\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0255, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0264, accuracy : 99.32\n",
            "iteration : 150, loss : 0.0268, accuracy : 99.29\n",
            "iteration : 200, loss : 0.0268, accuracy : 99.26\n",
            "iteration : 250, loss : 0.0272, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0266, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0268, accuracy : 99.25\n",
            "Epoch : 218, training loss : 0.0265, training accuracy : 99.27, test loss : 0.2397, test accuracy : 94.65\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0278, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0301, accuracy : 99.15\n",
            "iteration : 150, loss : 0.0307, accuracy : 99.16\n",
            "iteration : 200, loss : 0.0298, accuracy : 99.17\n",
            "iteration : 250, loss : 0.0304, accuracy : 99.16\n",
            "iteration : 300, loss : 0.0301, accuracy : 99.18\n",
            "iteration : 350, loss : 0.0301, accuracy : 99.16\n",
            "Epoch : 219, training loss : 0.0301, training accuracy : 99.16, test loss : 0.2496, test accuracy : 94.31\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0250, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0287, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0285, accuracy : 99.21\n",
            "iteration : 200, loss : 0.0278, accuracy : 99.22\n",
            "iteration : 250, loss : 0.0277, accuracy : 99.22\n",
            "iteration : 300, loss : 0.0281, accuracy : 99.21\n",
            "iteration : 350, loss : 0.0287, accuracy : 99.19\n",
            "Epoch : 220, training loss : 0.0288, training accuracy : 99.18, test loss : 0.2367, test accuracy : 94.57\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0313, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0301, accuracy : 99.12\n",
            "iteration : 150, loss : 0.0291, accuracy : 99.14\n",
            "iteration : 200, loss : 0.0268, accuracy : 99.24\n",
            "iteration : 250, loss : 0.0273, accuracy : 99.21\n",
            "iteration : 300, loss : 0.0279, accuracy : 99.18\n",
            "iteration : 350, loss : 0.0287, accuracy : 99.17\n",
            "Epoch : 221, training loss : 0.0294, training accuracy : 99.15, test loss : 0.2553, test accuracy : 94.06\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0275, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0235, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0246, accuracy : 99.26\n",
            "iteration : 250, loss : 0.0247, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0257, accuracy : 99.22\n",
            "iteration : 350, loss : 0.0259, accuracy : 99.21\n",
            "Epoch : 222, training loss : 0.0256, training accuracy : 99.23, test loss : 0.2464, test accuracy : 94.47\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0220, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0246, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0241, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0251, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0258, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0261, accuracy : 99.26\n",
            "iteration : 350, loss : 0.0261, accuracy : 99.26\n",
            "Epoch : 223, training loss : 0.0267, training accuracy : 99.24, test loss : 0.2395, test accuracy : 94.48\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0248, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0273, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0268, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0278, accuracy : 99.16\n",
            "iteration : 250, loss : 0.0284, accuracy : 99.15\n",
            "iteration : 300, loss : 0.0290, accuracy : 99.13\n",
            "iteration : 350, loss : 0.0297, accuracy : 99.13\n",
            "Epoch : 224, training loss : 0.0298, training accuracy : 99.12, test loss : 0.2460, test accuracy : 94.41\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0288, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0293, accuracy : 99.22\n",
            "iteration : 150, loss : 0.0275, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0278, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0290, accuracy : 99.19\n",
            "iteration : 300, loss : 0.0282, accuracy : 99.20\n",
            "iteration : 350, loss : 0.0284, accuracy : 99.19\n",
            "Epoch : 225, training loss : 0.0284, training accuracy : 99.18, test loss : 0.2405, test accuracy : 94.52\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0253, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0250, accuracy : 99.30\n",
            "iteration : 150, loss : 0.0231, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0227, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0230, accuracy : 99.37\n",
            "iteration : 300, loss : 0.0227, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0232, accuracy : 99.36\n",
            "Epoch : 226, training loss : 0.0233, training accuracy : 99.35, test loss : 0.2463, test accuracy : 94.46\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0216, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0201, accuracy : 99.47\n",
            "iteration : 150, loss : 0.0211, accuracy : 99.45\n",
            "iteration : 200, loss : 0.0229, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0243, accuracy : 99.30\n",
            "iteration : 300, loss : 0.0245, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0246, accuracy : 99.29\n",
            "Epoch : 227, training loss : 0.0247, training accuracy : 99.28, test loss : 0.2289, test accuracy : 94.76\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0221, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0227, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0209, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0212, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0223, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0224, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0231, accuracy : 99.34\n",
            "Epoch : 228, training loss : 0.0226, training accuracy : 99.36, test loss : 0.2437, test accuracy : 94.61\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0265, accuracy : 99.19\n",
            "iteration : 100, loss : 0.0236, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0230, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0225, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0232, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0228, accuracy : 99.35\n",
            "iteration : 350, loss : 0.0234, accuracy : 99.33\n",
            "Epoch : 229, training loss : 0.0238, training accuracy : 99.31, test loss : 0.2495, test accuracy : 94.35\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0238, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0245, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0249, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0248, accuracy : 99.29\n",
            "iteration : 250, loss : 0.0252, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0251, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0249, accuracy : 99.28\n",
            "Epoch : 230, training loss : 0.0252, training accuracy : 99.28, test loss : 0.2383, test accuracy : 94.69\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0216, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0234, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0241, accuracy : 99.29\n",
            "iteration : 200, loss : 0.0233, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0235, accuracy : 99.29\n",
            "iteration : 300, loss : 0.0237, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0238, accuracy : 99.27\n",
            "Epoch : 231, training loss : 0.0238, training accuracy : 99.28, test loss : 0.2410, test accuracy : 94.42\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0199, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0207, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0219, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0227, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0240, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0246, accuracy : 99.30\n",
            "iteration : 350, loss : 0.0241, accuracy : 99.33\n",
            "Epoch : 232, training loss : 0.0242, training accuracy : 99.32, test loss : 0.2316, test accuracy : 94.81\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0202, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0208, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0206, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0210, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0210, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0208, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0216, accuracy : 99.34\n",
            "Epoch : 233, training loss : 0.0217, training accuracy : 99.34, test loss : 0.2418, test accuracy : 94.49\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0224, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0205, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0191, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0184, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0193, accuracy : 99.46\n",
            "iteration : 300, loss : 0.0204, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0213, accuracy : 99.39\n",
            "Epoch : 234, training loss : 0.0212, training accuracy : 99.39, test loss : 0.2421, test accuracy : 94.61\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0206, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0224, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0220, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0212, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0212, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0207, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0208, accuracy : 99.47\n",
            "Epoch : 235, training loss : 0.0209, training accuracy : 99.46, test loss : 0.2444, test accuracy : 94.53\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0216, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0216, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0215, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0220, accuracy : 99.40\n",
            "iteration : 250, loss : 0.0214, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0218, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0215, accuracy : 99.41\n",
            "Epoch : 236, training loss : 0.0216, training accuracy : 99.41, test loss : 0.2398, test accuracy : 94.69\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0224, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0219, accuracy : 99.27\n",
            "iteration : 150, loss : 0.0218, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0230, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0233, accuracy : 99.32\n",
            "iteration : 300, loss : 0.0237, accuracy : 99.30\n",
            "iteration : 350, loss : 0.0232, accuracy : 99.33\n",
            "Epoch : 237, training loss : 0.0231, training accuracy : 99.33, test loss : 0.2343, test accuracy : 94.69\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0189, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0196, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0183, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0182, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0180, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0187, accuracy : 99.50\n",
            "Epoch : 238, training loss : 0.0189, training accuracy : 99.49, test loss : 0.2445, test accuracy : 94.53\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0145, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0182, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0209, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0206, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0200, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0202, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0209, accuracy : 99.41\n",
            "Epoch : 239, training loss : 0.0210, training accuracy : 99.41, test loss : 0.2494, test accuracy : 94.43\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0187, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0186, accuracy : 99.39\n",
            "iteration : 150, loss : 0.0183, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0185, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0194, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0194, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0198, accuracy : 99.43\n",
            "Epoch : 240, training loss : 0.0195, training accuracy : 99.44, test loss : 0.2373, test accuracy : 94.69\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0167, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0164, accuracy : 99.57\n",
            "iteration : 150, loss : 0.0174, accuracy : 99.55\n",
            "iteration : 200, loss : 0.0174, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0187, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0188, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0190, accuracy : 99.46\n",
            "Epoch : 241, training loss : 0.0189, training accuracy : 99.46, test loss : 0.2414, test accuracy : 94.68\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0205, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0196, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0193, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0193, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0196, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0197, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0203, accuracy : 99.45\n",
            "Epoch : 242, training loss : 0.0202, training accuracy : 99.46, test loss : 0.2378, test accuracy : 94.68\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0226, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0233, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0212, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0208, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0201, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0208, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0214, accuracy : 99.42\n",
            "Epoch : 243, training loss : 0.0213, training accuracy : 99.42, test loss : 0.2460, test accuracy : 94.73\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0191, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0184, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0180, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0172, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0171, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0176, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0176, accuracy : 99.50\n",
            "Epoch : 244, training loss : 0.0179, training accuracy : 99.49, test loss : 0.2422, test accuracy : 94.71\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0189, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0185, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0187, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0186, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0186, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0185, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0181, accuracy : 99.48\n",
            "Epoch : 245, training loss : 0.0179, training accuracy : 99.49, test loss : 0.2347, test accuracy : 94.81\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0187, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0183, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0177, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0167, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0171, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0177, accuracy : 99.48\n",
            "Epoch : 246, training loss : 0.0176, training accuracy : 99.49, test loss : 0.2379, test accuracy : 94.76\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0186, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0171, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0172, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0174, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0174, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0174, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0174, accuracy : 99.51\n",
            "Epoch : 247, training loss : 0.0176, training accuracy : 99.50, test loss : 0.2411, test accuracy : 94.71\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0158, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0152, accuracy : 99.56\n",
            "iteration : 150, loss : 0.0154, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0158, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0165, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0163, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0163, accuracy : 99.53\n",
            "Epoch : 248, training loss : 0.0162, training accuracy : 99.52, test loss : 0.2400, test accuracy : 94.69\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0188, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0181, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0180, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0181, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0173, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0177, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0183, accuracy : 99.47\n",
            "Epoch : 249, training loss : 0.0181, training accuracy : 99.48, test loss : 0.2407, test accuracy : 94.71\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0189, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0178, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0174, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0187, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0195, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0204, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0195, accuracy : 99.44\n",
            "Epoch : 250, training loss : 0.0192, training accuracy : 99.45, test loss : 0.2402, test accuracy : 94.72\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0155, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0156, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0158, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0164, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0158, accuracy : 99.60\n",
            "iteration : 300, loss : 0.0151, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0151, accuracy : 99.61\n",
            "Epoch : 251, training loss : 0.0151, training accuracy : 99.60, test loss : 0.2400, test accuracy : 94.76\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0170, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0180, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0183, accuracy : 99.50\n",
            "iteration : 200, loss : 0.0174, accuracy : 99.53\n",
            "iteration : 250, loss : 0.0169, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0167, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0167, accuracy : 99.52\n",
            "Epoch : 252, training loss : 0.0166, training accuracy : 99.52, test loss : 0.2359, test accuracy : 94.74\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0166, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0174, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0193, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0194, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0182, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0179, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0173, accuracy : 99.54\n",
            "Epoch : 253, training loss : 0.0170, training accuracy : 99.55, test loss : 0.2381, test accuracy : 94.63\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0149, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0152, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0143, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0153, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0149, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0149, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0153, accuracy : 99.62\n",
            "Epoch : 254, training loss : 0.0156, training accuracy : 99.61, test loss : 0.2380, test accuracy : 94.81\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0145, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0140, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0145, accuracy : 99.60\n",
            "iteration : 200, loss : 0.0151, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0148, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0153, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0153, accuracy : 99.56\n",
            "Epoch : 255, training loss : 0.0158, training accuracy : 99.55, test loss : 0.2381, test accuracy : 94.78\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0157, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0161, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0148, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0152, accuracy : 99.60\n",
            "iteration : 250, loss : 0.0157, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0156, accuracy : 99.60\n",
            "iteration : 350, loss : 0.0154, accuracy : 99.60\n",
            "Epoch : 256, training loss : 0.0154, training accuracy : 99.60, test loss : 0.2374, test accuracy : 94.78\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0130, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0134, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0138, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0143, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0137, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0139, accuracy : 99.64\n",
            "Epoch : 257, training loss : 0.0141, training accuracy : 99.62, test loss : 0.2437, test accuracy : 94.63\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0147, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0136, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0137, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0139, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0138, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0137, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0140, accuracy : 99.62\n",
            "Epoch : 258, training loss : 0.0139, training accuracy : 99.62, test loss : 0.2342, test accuracy : 94.73\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0156, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0158, accuracy : 99.53\n",
            "iteration : 150, loss : 0.0149, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0153, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0153, accuracy : 99.61\n",
            "iteration : 300, loss : 0.0148, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0152, accuracy : 99.61\n",
            "Epoch : 259, training loss : 0.0153, training accuracy : 99.60, test loss : 0.2404, test accuracy : 94.76\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0155, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0163, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0151, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0150, accuracy : 99.61\n",
            "iteration : 250, loss : 0.0145, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0149, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0155, accuracy : 99.59\n",
            "Epoch : 260, training loss : 0.0155, training accuracy : 99.59, test loss : 0.2377, test accuracy : 94.75\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0142, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0150, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0150, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0148, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0136, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0140, accuracy : 99.61\n",
            "iteration : 350, loss : 0.0141, accuracy : 99.61\n",
            "Epoch : 261, training loss : 0.0140, training accuracy : 99.61, test loss : 0.2333, test accuracy : 94.87\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0126, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0124, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0128, accuracy : 99.66\n",
            "iteration : 250, loss : 0.0124, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0124, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0125, accuracy : 99.68\n",
            "Epoch : 262, training loss : 0.0124, training accuracy : 99.67, test loss : 0.2339, test accuracy : 94.91\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0129, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0138, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0135, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0137, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0150, accuracy : 99.59\n",
            "iteration : 300, loss : 0.0142, accuracy : 99.62\n",
            "iteration : 350, loss : 0.0144, accuracy : 99.61\n",
            "Epoch : 263, training loss : 0.0144, training accuracy : 99.61, test loss : 0.2370, test accuracy : 94.89\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0098, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0113, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0115, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0116, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0119, accuracy : 99.66\n",
            "Epoch : 264, training loss : 0.0120, training accuracy : 99.66, test loss : 0.2394, test accuracy : 94.72\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0125, accuracy : 99.63\n",
            "iteration : 150, loss : 0.0121, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0128, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0133, accuracy : 99.63\n",
            "iteration : 300, loss : 0.0134, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0129, accuracy : 99.65\n",
            "Epoch : 265, training loss : 0.0129, training accuracy : 99.65, test loss : 0.2372, test accuracy : 94.83\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0133, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0121, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0131, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0130, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0134, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0130, accuracy : 99.64\n",
            "iteration : 350, loss : 0.0127, accuracy : 99.65\n",
            "Epoch : 266, training loss : 0.0126, training accuracy : 99.65, test loss : 0.2388, test accuracy : 94.88\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0124, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0139, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0136, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0138, accuracy : 99.63\n",
            "iteration : 250, loss : 0.0140, accuracy : 99.62\n",
            "iteration : 300, loss : 0.0134, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0135, accuracy : 99.64\n",
            "Epoch : 267, training loss : 0.0137, training accuracy : 99.63, test loss : 0.2266, test accuracy : 95.14\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0136, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0130, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0135, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0127, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0127, accuracy : 99.67\n",
            "iteration : 300, loss : 0.0126, accuracy : 99.67\n",
            "iteration : 350, loss : 0.0125, accuracy : 99.68\n",
            "Epoch : 268, training loss : 0.0125, training accuracy : 99.68, test loss : 0.2351, test accuracy : 94.82\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0126, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0125, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0121, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0129, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0126, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0123, accuracy : 99.69\n",
            "Epoch : 269, training loss : 0.0122, training accuracy : 99.69, test loss : 0.2304, test accuracy : 94.97\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0159, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0137, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0139, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0144, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0143, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0144, accuracy : 99.63\n",
            "iteration : 350, loss : 0.0138, accuracy : 99.65\n",
            "Epoch : 270, training loss : 0.0138, training accuracy : 99.64, test loss : 0.2326, test accuracy : 94.90\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0119, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0119, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0111, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0110, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0108, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.69\n",
            "Epoch : 271, training loss : 0.0110, training accuracy : 99.70, test loss : 0.2337, test accuracy : 94.78\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0112, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0120, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0112, accuracy : 99.68\n",
            "iteration : 200, loss : 0.0118, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.69\n",
            "iteration : 300, loss : 0.0115, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0121, accuracy : 99.68\n",
            "Epoch : 272, training loss : 0.0122, training accuracy : 99.68, test loss : 0.2399, test accuracy : 94.79\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0137, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0137, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0125, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0119, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0115, accuracy : 99.70\n",
            "Epoch : 273, training loss : 0.0115, training accuracy : 99.70, test loss : 0.2277, test accuracy : 95.06\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0116, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0131, accuracy : 99.61\n",
            "iteration : 150, loss : 0.0130, accuracy : 99.64\n",
            "iteration : 200, loss : 0.0131, accuracy : 99.62\n",
            "iteration : 250, loss : 0.0126, accuracy : 99.64\n",
            "iteration : 300, loss : 0.0122, accuracy : 99.65\n",
            "iteration : 350, loss : 0.0126, accuracy : 99.65\n",
            "Epoch : 274, training loss : 0.0126, training accuracy : 99.65, test loss : 0.2358, test accuracy : 94.89\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0097, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0095, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0100, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0107, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0110, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0110, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0109, accuracy : 99.71\n",
            "Epoch : 275, training loss : 0.0108, training accuracy : 99.71, test loss : 0.2377, test accuracy : 94.88\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0104, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0121, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0117, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0115, accuracy : 99.70\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.71\n",
            "Epoch : 276, training loss : 0.0109, training accuracy : 99.72, test loss : 0.2408, test accuracy : 94.65\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0104, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0112, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0121, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0126, accuracy : 99.65\n",
            "iteration : 250, loss : 0.0122, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0125, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0122, accuracy : 99.69\n",
            "Epoch : 277, training loss : 0.0121, training accuracy : 99.70, test loss : 0.2367, test accuracy : 94.88\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0115, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0126, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0124, accuracy : 99.63\n",
            "iteration : 200, loss : 0.0122, accuracy : 99.64\n",
            "iteration : 250, loss : 0.0123, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0120, accuracy : 99.66\n",
            "iteration : 350, loss : 0.0118, accuracy : 99.67\n",
            "Epoch : 278, training loss : 0.0115, training accuracy : 99.68, test loss : 0.2331, test accuracy : 94.96\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0142, accuracy : 99.62\n",
            "iteration : 100, loss : 0.0118, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0123, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0115, accuracy : 99.71\n",
            "Epoch : 279, training loss : 0.0113, training accuracy : 99.72, test loss : 0.2342, test accuracy : 95.06\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0112, accuracy : 99.73\n",
            "iteration : 100, loss : 0.0107, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0105, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0104, accuracy : 99.76\n",
            "iteration : 250, loss : 0.0105, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.76\n",
            "Epoch : 280, training loss : 0.0104, training accuracy : 99.76, test loss : 0.2378, test accuracy : 94.86\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0094, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0114, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0118, accuracy : 99.69\n",
            "iteration : 200, loss : 0.0108, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0111, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0107, accuracy : 99.75\n",
            "Epoch : 281, training loss : 0.0106, training accuracy : 99.75, test loss : 0.2336, test accuracy : 95.03\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0093, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0103, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0103, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0105, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0110, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0113, accuracy : 99.70\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.70\n",
            "Epoch : 282, training loss : 0.0110, training accuracy : 99.71, test loss : 0.2329, test accuracy : 94.94\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0135, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0120, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0119, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0117, accuracy : 99.67\n",
            "iteration : 250, loss : 0.0126, accuracy : 99.65\n",
            "iteration : 300, loss : 0.0120, accuracy : 99.68\n",
            "iteration : 350, loss : 0.0114, accuracy : 99.69\n",
            "Epoch : 283, training loss : 0.0113, training accuracy : 99.69, test loss : 0.2354, test accuracy : 94.94\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0090, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0108, accuracy : 99.68\n",
            "iteration : 150, loss : 0.0108, accuracy : 99.71\n",
            "iteration : 200, loss : 0.0108, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0103, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0109, accuracy : 99.72\n",
            "Epoch : 284, training loss : 0.0108, training accuracy : 99.73, test loss : 0.2341, test accuracy : 94.89\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0141, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0127, accuracy : 99.66\n",
            "iteration : 150, loss : 0.0121, accuracy : 99.66\n",
            "iteration : 200, loss : 0.0112, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0111, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0107, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.73\n",
            "Epoch : 285, training loss : 0.0104, training accuracy : 99.73, test loss : 0.2359, test accuracy : 94.99\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0066, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0093, accuracy : 99.75\n",
            "iteration : 150, loss : 0.0099, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0106, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0104, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.75\n",
            "Epoch : 286, training loss : 0.0107, training accuracy : 99.75, test loss : 0.2356, test accuracy : 94.91\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0099, accuracy : 99.81\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0098, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0100, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0096, accuracy : 99.76\n",
            "iteration : 300, loss : 0.0098, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0099, accuracy : 99.75\n",
            "Epoch : 287, training loss : 0.0097, training accuracy : 99.75, test loss : 0.2330, test accuracy : 95.08\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0142, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0123, accuracy : 99.72\n",
            "iteration : 150, loss : 0.0115, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0110, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0105, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0104, accuracy : 99.75\n",
            "Epoch : 288, training loss : 0.0101, training accuracy : 99.76, test loss : 0.2357, test accuracy : 94.86\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0120, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0097, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0095, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0098, accuracy : 99.75\n",
            "iteration : 300, loss : 0.0095, accuracy : 99.76\n",
            "iteration : 350, loss : 0.0099, accuracy : 99.75\n",
            "Epoch : 289, training loss : 0.0102, training accuracy : 99.73, test loss : 0.2329, test accuracy : 95.01\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0121, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0097, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0097, accuracy : 99.75\n",
            "iteration : 250, loss : 0.0101, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0101, accuracy : 99.74\n",
            "iteration : 350, loss : 0.0099, accuracy : 99.74\n",
            "Epoch : 290, training loss : 0.0100, training accuracy : 99.73, test loss : 0.2346, test accuracy : 94.86\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0129, accuracy : 99.67\n",
            "iteration : 100, loss : 0.0127, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0123, accuracy : 99.65\n",
            "iteration : 200, loss : 0.0115, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0106, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0105, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0106, accuracy : 99.72\n",
            "Epoch : 291, training loss : 0.0107, training accuracy : 99.72, test loss : 0.2339, test accuracy : 94.93\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0084, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0098, accuracy : 99.69\n",
            "iteration : 150, loss : 0.0107, accuracy : 99.70\n",
            "iteration : 200, loss : 0.0110, accuracy : 99.71\n",
            "iteration : 250, loss : 0.0109, accuracy : 99.70\n",
            "iteration : 300, loss : 0.0106, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0109, accuracy : 99.72\n",
            "Epoch : 292, training loss : 0.0108, training accuracy : 99.72, test loss : 0.2341, test accuracy : 94.92\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0096, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0100, accuracy : 99.80\n",
            "iteration : 150, loss : 0.0093, accuracy : 99.82\n",
            "iteration : 200, loss : 0.0094, accuracy : 99.80\n",
            "iteration : 250, loss : 0.0102, accuracy : 99.77\n",
            "iteration : 300, loss : 0.0104, accuracy : 99.75\n",
            "iteration : 350, loss : 0.0101, accuracy : 99.76\n",
            "Epoch : 293, training loss : 0.0100, training accuracy : 99.76, test loss : 0.2410, test accuracy : 94.89\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0103, accuracy : 99.72\n",
            "iteration : 100, loss : 0.0099, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0102, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0108, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0108, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0110, accuracy : 99.72\n",
            "Epoch : 294, training loss : 0.0107, training accuracy : 99.73, test loss : 0.2331, test accuracy : 95.03\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0082, accuracy : 99.83\n",
            "iteration : 100, loss : 0.0094, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0104, accuracy : 99.73\n",
            "iteration : 200, loss : 0.0107, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.71\n",
            "iteration : 300, loss : 0.0110, accuracy : 99.71\n",
            "iteration : 350, loss : 0.0115, accuracy : 99.71\n",
            "Epoch : 295, training loss : 0.0113, training accuracy : 99.71, test loss : 0.2333, test accuracy : 95.01\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0103, accuracy : 99.78\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.77\n",
            "iteration : 150, loss : 0.0114, accuracy : 99.75\n",
            "iteration : 200, loss : 0.0116, accuracy : 99.74\n",
            "iteration : 250, loss : 0.0114, accuracy : 99.74\n",
            "iteration : 300, loss : 0.0114, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0116, accuracy : 99.72\n",
            "Epoch : 296, training loss : 0.0115, training accuracy : 99.72, test loss : 0.2367, test accuracy : 94.86\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0137, accuracy : 99.66\n",
            "iteration : 100, loss : 0.0123, accuracy : 99.71\n",
            "iteration : 150, loss : 0.0116, accuracy : 99.72\n",
            "iteration : 200, loss : 0.0112, accuracy : 99.73\n",
            "iteration : 250, loss : 0.0112, accuracy : 99.72\n",
            "iteration : 300, loss : 0.0107, accuracy : 99.73\n",
            "iteration : 350, loss : 0.0105, accuracy : 99.73\n",
            "Epoch : 297, training loss : 0.0103, training accuracy : 99.73, test loss : 0.2295, test accuracy : 95.06\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0076, accuracy : 99.80\n",
            "iteration : 100, loss : 0.0102, accuracy : 99.70\n",
            "iteration : 150, loss : 0.0120, accuracy : 99.67\n",
            "iteration : 200, loss : 0.0115, accuracy : 99.68\n",
            "iteration : 250, loss : 0.0119, accuracy : 99.68\n",
            "iteration : 300, loss : 0.0116, accuracy : 99.69\n",
            "iteration : 350, loss : 0.0113, accuracy : 99.70\n",
            "Epoch : 298, training loss : 0.0114, training accuracy : 99.69, test loss : 0.2374, test accuracy : 94.91\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0138, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0130, accuracy : 99.73\n",
            "iteration : 150, loss : 0.0114, accuracy : 99.74\n",
            "iteration : 200, loss : 0.0113, accuracy : 99.72\n",
            "iteration : 250, loss : 0.0108, accuracy : 99.73\n",
            "iteration : 300, loss : 0.0110, accuracy : 99.72\n",
            "iteration : 350, loss : 0.0112, accuracy : 99.72\n",
            "Epoch : 299, training loss : 0.0109, training accuracy : 99.72, test loss : 0.2310, test accuracy : 95.03\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0085, accuracy : 99.77\n",
            "iteration : 100, loss : 0.0100, accuracy : 99.74\n",
            "iteration : 150, loss : 0.0094, accuracy : 99.76\n",
            "iteration : 200, loss : 0.0088, accuracy : 99.79\n",
            "iteration : 250, loss : 0.0084, accuracy : 99.80\n",
            "iteration : 300, loss : 0.0089, accuracy : 99.79\n",
            "iteration : 350, loss : 0.0090, accuracy : 99.79\n",
            "Epoch : 300, training loss : 0.0089, training accuracy : 99.79, test loss : 0.2348, test accuracy : 94.93\n"
          ]
        }
      ],
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iUQVIKR-X3v6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.16281128386218174, 96.30454824830977)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7CNz1iabSB21"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : RandAugment\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "E9qb9ItHSC5U"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : RandAugment\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3eiY3bTlWipW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_loss_list_rand = [1.3079642264985134, 0.41276279398742405, 0.3641065050996739, 0.33621111706020385, 0.32227574123276603, 0.3105190498880578, 0.2985378698645245, 0.2845963137703859, 0.27256943354115576, 0.2628671395867498, 0.25574022410361746, 0.24721774489172105, 0.23312961429276763, 0.2307181321791194, 0.22826595868925414, 0.21507334950453222, 0.21533063833468005, 0.20384218360027323, 0.20217743917774703, 0.2004433953503606, 0.18824751913385987, 0.1920065844285133, 0.187250267825314, 0.18458147579173084, 0.17769597254713698, 0.17646904064589722, 0.17025284326779164, 0.17242334513523713, 0.16955551344568168, 0.1653165416766796, 0.16184331506975297, 0.16270417981556437, 0.15991212547754208, 0.15523525618198442, 0.15527952850107254, 0.158528862697726, 0.15280010509822103, 0.15449475903861568, 0.15078019666760595, 0.15182626324132853, 0.1501983254340283, 0.1461215344810389, 0.14986899500172635, 0.14550365379892874, 0.1403558284175509, 0.14542140672361947, 0.1405470239094441, 0.14365218938669053, 0.13810036563110062, 0.14059043776617464, 0.13908706698819065, 0.13695130306409625, 0.1333991897300007, 0.1321409744359452, 0.13560862030561377, 0.1312812770133823, 0.13144924573923353, 0.13186921439257257, 0.1303779829020907, 0.12855140429727108, 0.12881272610751432, 0.12862061339160452, 0.12293643534668093, 0.12759860502530726, 0.12760409868264264, 0.12412538664127754, 0.12261246541025354, 0.12362489930598879, 0.11684692019046483, 0.12635664669115368, 0.11725350842889408, 0.1202228733800291, 0.11977898440043616, 0.11819690775863201, 0.11406317815124019, 0.12160748152975952, 0.1135327710257717, 0.11757346191178493, 0.11692589301827962, 0.11527446912838837, 0.11390696881909357, 0.11128266612547362, 0.1084120699525041, 0.11216231442604285, 0.10896211025383333, 0.1105140705963945, 0.11013614156139576, 0.11042292131146361, 0.1066427088511709, 0.10727387718190022, 0.10637671745768408, 0.10528922357614125, 0.10317711308497562, 0.10493126684209196, 0.10703022915839665, 0.10020091456328305, 0.10443359014747265, 0.10327813833911567, 0.10253999184887745, 0.10005321747151413, 0.09775119930818717, 0.09895929236590943, 0.09956198874457177, 0.09663621041176684, 0.09820591854966348, 0.0958199133102083, 0.09834191584227693, 0.10007545721734847, 0.09484737071645293, 0.09604210347482343, 0.09365542412440224, 0.09515454136037084, 0.0915899477939457, 0.08897347942620597, 0.09372411305867237, 0.09134252944855185, 0.08858357435016613, 0.08518513028073278, 0.08997638049083674, 0.09038671507906461, 0.08876956635101782, 0.08737949878330718, 0.08710510870769902, 0.08606145591512929, 0.0835256307994124, 0.08409009710317705, 0.08514642417632791, 0.08500815152889749, 0.07965256468592298, 0.08329004821365038, 0.07942846951738687, 0.08278998299463977, 0.08198488589507455, 0.07826127050075993, 0.07908188051930289, 0.076796955444161, 0.07810755274674999, 0.07818539347499609, 0.07601612444643971, 0.07647686129875057, 0.07603131658915016, 0.07302434632913045, 0.07223893472354426, 0.07550516255391726, 0.07080602620634929, 0.07053892541585899, 0.07113152286315433, 0.07030726358708408, 0.07296305139193206, 0.0674845933848563, 0.0666364010973634, 0.06634756198864643, 0.06508453956080808, 0.06460393779358364, 0.06859736059486947, 0.06357380861875811, 0.06576774500532526, 0.06143479931816903, 0.06113883249491938, 0.05898804291527526, 0.06436446549870618, 0.062575965680606, 0.05618904328710705, 0.06056681182614066, 0.057390773533856076, 0.055834507522978354, 0.05655236725839294, 0.05552395822626708, 0.058229990793154815, 0.05438301761743176, 0.05628294004304057, 0.05478681649300989, 0.05533521129878435, 0.052947258238546895, 0.0534550957836029, 0.052291425183506275, 0.052318296121398045, 0.04810188965604396, 0.04881041513397522, 0.05085457057558344, 0.04860204716054041, 0.04748570773344194, 0.04570310991030277, 0.050219918913548676, 0.04907680233577039, 0.046715650073982026, 0.0483139068832202, 0.0456187082542899, 0.039992414365666425, 0.04334353352707576, 0.04216374684476008, 0.042458749894900776, 0.04058421820687347, 0.04625414172692229, 0.03840240793173936, 0.04297558541174948, 0.0419943235463748, 0.039768285462557056, 0.037575757089335984, 0.04107870026378006, 0.03676055792433924, 0.03773308889296784, 0.03924703417941524, 0.03560067948615204, 0.035394394668046295, 0.034579224325464106, 0.034057870963436924, 0.034051362065431794, 0.036438416197471984, 0.034808342434657705, 0.03285826306012749, 0.03179724503189911, 0.03259620334458384, 0.03070403641180658, 0.030086495885802155, 0.0316626976205031, 0.03077701206708978, 0.026507464029288632, 0.030121172760641024, 0.028750687549244057, 0.029356615014656934, 0.025606725985805195, 0.026744502662809762, 0.029752478933878, 0.028448736762342757, 0.02331312912987005, 0.024699779668429154, 0.022614457626854823, 0.023837428991455675, 0.02521528576773296, 0.023793862451771453, 0.024236088944220444, 0.021694000673778153, 0.021232543444717783, 0.02092921472063697, 0.021585240682491118, 0.023119772801132806, 0.01890946720092025, 0.0209501470829711, 0.019511741401153165, 0.018914120605588967, 0.02016598089796246, 0.021338878327231036, 0.01794326564027022, 0.017943615567060263, 0.01755146857011599, 0.01759808525054395, 0.016239320796215276, 0.018075850542677633, 0.019246497568182225, 0.015080467639124994, 0.016554365190424955, 0.017023207680991064, 0.01558076386462372, 0.0157793698015908, 0.015399959696164017, 0.014076877912559464, 0.013944049486650664, 0.015304512502676063, 0.015483319247015286, 0.014038200535607047, 0.012375226085008625, 0.014426614435485546, 0.011954178459132166, 0.012934604996618458, 0.012563755948078846, 0.013720255432181, 0.012539297958449802, 0.012156369318544542, 0.0137886715602372, 0.010996153222921943, 0.012231470978513442, 0.011482611671153223, 0.012633665982850885, 0.0108387998392336, 0.01094505579263189, 0.012128652638798644, 0.011514820721313398, 0.011337356862415203, 0.01035400173881755, 0.010573380134891223, 0.01104129131234738, 0.011334962008059085, 0.010769810515574525, 0.010359876090660691, 0.010690325047684515, 0.009734501771636428, 0.010143442243321225, 0.010232979197024815, 0.010022813219231298, 0.010693188950729867, 0.010847516659505887, 0.009985776471894053, 0.010729328628886885, 0.011304906725903676, 0.011516948485163957, 0.010286911307117298, 0.011423589299851902, 0.010949008481963196, 0.008929194090320024]\n",
            "train_acc_list_rand = [54.77395447326628, 87.3943885653785, 88.9655902593965, 89.9142403388036, 90.33986236103759, 90.63419798835362, 91.13393329804128, 91.6209634727369, 91.94706193753309, 92.18634197988354, 92.53149814716781, 92.69878242456326, 93.09052408681842, 93.29380624669137, 93.34886183165696, 93.76813128639492, 93.82742191635786, 94.04552673372154, 94.03705664372684, 94.13658020116463, 94.56855479089465, 94.44997353096876, 94.61725780836422, 94.51773425092641, 94.82265749073584, 94.97723663313923, 95.03652726310217, 95.01111699311805, 95.08099523557438, 95.28215987294865, 95.36897829539438, 95.29062996294336, 95.37321334039174, 95.57861302276336, 95.57649550026468, 95.43038644785601, 95.65272631021705, 95.46426680783483, 95.62308099523557, 95.6019057702488, 95.74166225516146, 95.8009528851244, 95.74166225516146, 95.8369507676019, 95.97035468501853, 95.8094229751191, 95.88777130757015, 95.784012705135, 95.9915299100053, 95.86024351508735, 95.95341450502912, 96.09952355743779, 96.12493382742191, 96.19481206987824, 95.99576495500264, 96.2710428798306, 96.1990471148756, 96.22022233986236, 96.10799364743251, 96.27527792482795, 96.24563260984648, 96.28163049232398, 96.47432503970354, 96.18845950238222, 96.31974589730017, 96.4065643197459, 96.3980942297512, 96.42773954473266, 96.70301746956062, 96.21386977236634, 96.51879301217575, 96.52302805717311, 96.52302805717311, 96.54843832715723, 96.70513499205929, 96.4700899947062, 96.67548967707782, 96.54843832715723, 96.63949179460032, 96.66913710958178, 96.67125463208046, 96.68395976707252, 96.8131286394918, 96.74748544203283, 96.79407093700371, 96.76230809952355, 96.85971413446268, 96.74536791953415, 96.80465854949709, 96.81736368448915, 96.89782953943886, 96.86818422445738, 96.99735309687665, 96.94229751191106, 96.84489147697194, 97.12228692429856, 96.83853890947591, 96.91688724192694, 97.01852832186341, 97.09687665431446, 97.10322922181048, 97.1011116993118, 97.13499205929062, 97.07358390682901, 97.17734250926416, 97.20275277924829, 97.03335097935415, 97.0206458443621, 97.13922710428798, 97.18581259925887, 97.27898358920064, 97.2006352567496, 97.28533615669666, 97.35733192165166, 97.23875066172577, 97.30227633668608, 97.41238750661726, 97.5796717840127, 97.3276866066702, 97.28110111169931, 97.41238750661726, 97.3996823716252, 97.3446267866596, 97.4356802541027, 97.56484912652196, 97.5076760190577, 97.41662255161461, 97.47379565907887, 97.59025939650608, 97.57543673901536, 97.66649020645845, 97.499205929063, 97.53943885653786, 97.61778718898888, 97.63472736897829, 97.73213340391742, 97.66013763896241, 97.70248808893595, 97.77448385389094, 97.76601376389624, 97.68554790894653, 97.8147167813658, 97.87188988883007, 97.7427210164108, 97.86341979883537, 97.89306511381683, 97.8507146638433, 97.95235574377978, 97.78930651138168, 97.9777660137639, 97.94600317628375, 98.03282159872948, 98.10058231868713, 98.04552673372154, 97.90577024880889, 98.07728957120169, 97.9502382212811, 98.21492853361568, 98.12175754367391, 98.23610375860244, 98.08999470619375, 98.06670195870831, 98.34833245103229, 98.18951826363156, 98.2657490735839, 98.31021704605611, 98.25304393859184, 98.29115934356803, 98.26151402858656, 98.35256749602965, 98.27845420857597, 98.39491794600318, 98.38009528851244, 98.47961884595024, 98.3928004235045, 98.40762308099524, 98.48173636844892, 98.50291159343568, 98.53890947591319, 98.44362096347274, 98.55584965590259, 98.53679195341451, 98.56643726839597, 98.48385389094759, 98.54949708840657, 98.54737956590789, 98.5283218634198, 98.65113816834304, 98.79724722075171, 98.68501852832186, 98.74642668078349, 98.72525145579672, 98.79089465325569, 98.62572789835892, 98.85653785071466, 98.74007411328745, 98.74219163578613, 98.8798305982001, 98.9348861831657, 98.75277924827951, 98.93700370566437, 98.9073583906829, 98.85865537321334, 98.94970884065643, 98.93700370566437, 98.9518263631551, 98.96876654314453, 99.05770248808894, 98.89465325569084, 99.02170460561143, 99.05558496559026, 99.0619375330863, 99.05770248808894, 99.12546320804658, 99.12969825304394, 99.07464266807835, 99.08311275807306, 99.26521969295923, 99.16146109052409, 99.17628374801482, 99.1508734780307, 99.22710428798305, 99.23980942297511, 99.1233456855479, 99.18051879301217, 99.35415563790366, 99.28427739544733, 99.36474325039704, 99.31180518793012, 99.2779248279513, 99.28215987294865, 99.32451032292218, 99.33721545791424, 99.39015352038115, 99.46426680783483, 99.40709370037057, 99.33086289041822, 99.49179460031763, 99.41132874536792, 99.43885653785071, 99.45579671784013, 99.45579671784013, 99.42403388035999, 99.48755955532027, 99.48755955532027, 99.4854420328216, 99.50238221281101, 99.52355743779778, 99.48332451032292, 99.44944415034409, 99.6019057702488, 99.5214399152991, 99.5489677077819, 99.61461090524087, 99.55320275277924, 99.6019057702488, 99.62308099523557, 99.6209634727369, 99.60402329274748, 99.5934356802541, 99.61037586024352, 99.67178401270513, 99.60825833774484, 99.65907887771307, 99.65272631021705, 99.65484383271573, 99.62731604023293, 99.67601905770249, 99.69084171519323, 99.64213869772367, 99.69931180518793, 99.68237162519851, 99.70354685018528, 99.65060878771837, 99.71201694017999, 99.72260455267337, 99.69507676019057, 99.68025410269983, 99.71836950767602, 99.76283748014822, 99.7480148226575, 99.70566437268396, 99.69084171519323, 99.7289571201694, 99.7289571201694, 99.74589730015882, 99.75013234515616, 99.75860243515088, 99.73319216516676, 99.73319216516676, 99.71625198517734, 99.72260455267337, 99.76071995764956, 99.73319216516676, 99.70989941768131, 99.7204870301747, 99.73107464266808, 99.69084171519323, 99.72472207517205, 99.79036527263102]\n",
            "test_loss_list_rand = [1.2242924027583177, 0.4766860316489257, 0.4852036163210869, 0.4449080118507731, 0.3844677253681071, 0.3908769353201576, 0.33308059773316573, 0.3329603055528566, 0.3105304033002433, 0.2816346858208086, 0.2945175337806052, 0.277948475059341, 0.3032138998455861, 0.2831221893429756, 0.27575827159864064, 0.26203144776324433, 0.31333797563816984, 0.26917517349562226, 0.253851704628152, 0.2555090201032512, 0.2524060876492192, 0.2693595473161515, 0.2468755625042261, 0.24303570393399865, 0.29847537346330344, 0.23665455542504787, 0.24601385775296128, 0.26269593287040205, 0.241777080385124, 0.2399024426498834, 0.2372869025258457, 0.238308228114072, 0.2611099873468572, 0.2243046425100343, 0.24302090302694077, 0.2619777427730607, 0.23600055122127137, 0.2472808970247998, 0.23857459320011093, 0.23040143489910692, 0.23448149746685637, 0.23396167033078039, 0.23945477546430102, 0.24786251283013352, 0.24238866938314602, 0.23962249183187298, 0.25362784189044263, 0.22922139680560896, 0.2559051115211903, 0.23962331296620415, 0.23080364543506326, 0.22837937785787324, 0.247460591625057, 0.24376308035982006, 0.2347778041353997, 0.24077762334662325, 0.23032311426804347, 0.23188659777024798, 0.23087097901631803, 0.2372709251575026, 0.23744144363730563, 0.2374997875634946, 0.2297166377744254, 0.26560663293097536, 0.23270482321580252, 0.23472226854852019, 0.23707742974454282, 0.24279768688275533, 0.23788784751120737, 0.2408738124239094, 0.23345124054992317, 0.23940513425451868, 0.23605135612774128, 0.25073060765862465, 0.24483406474339028, 0.2369142651704012, 0.23664433430588128, 0.23283696320711397, 0.2526469752009885, 0.2293763486303243, 0.2379093081051228, 0.24137731407787286, 0.22815602656234713, 0.23056939514536484, 0.23905373445036365, 0.23256402484634342, 0.2537045240511789, 0.25104906607200117, 0.24098055192506782, 0.23426727585348428, 0.24968813399912096, 0.2255474401491822, 0.25570899116642337, 0.2658700168351917, 0.22768254453937212, 0.24381475685639123, 0.2371177078023845, 0.24363475305703924, 0.23996025838834398, 0.2301647606141427, 0.2312738028373204, 0.23250958069647645, 0.2470913831436751, 0.24546197549823454, 0.23853133864445136, 0.23482449931622135, 0.23961933846494146, 0.2363752949453306, 0.2366328848452837, 0.25023742631881263, 0.2523412738433656, 0.23386995731761642, 0.2235797121207796, 0.24124343740735568, 0.23789687029213882, 0.25894979935358553, 0.2407712943705858, 0.2473073466041801, 0.24556619305090577, 0.23585436084106856, 0.23382639826512805, 0.24988360268374285, 0.24265338982656307, 0.2321539395762717, 0.24257982144241824, 0.2509613745827593, 0.264335074900266, 0.2360795667102816, 0.24269597353778927, 0.2401655425920206, 0.2393073042964234, 0.24763254971042567, 0.23858816633183583, 0.2453654737704817, 0.24818577219312096, 0.2519976277379136, 0.24066407218867658, 0.24328664805301847, 0.2436335614754581, 0.23726862157676734, 0.2504358967291374, 0.24724281604821777, 0.24097077188757704, 0.23132461413521976, 0.24096911921001532, 0.23906312499414473, 0.23964464410628175, 0.2407097773553402, 0.24008852922741106, 0.24373903726318888, 0.24455829663202167, 0.25426618283724084, 0.24766666304283574, 0.24456659613140658, 0.23325322789377442, 0.2446374527050876, 0.24081618506826608, 0.2533364076824749, 0.2727439739083981, 0.2566343823301734, 0.24697071872651577, 0.25414097462506857, 0.25726331458153096, 0.24916001133547694, 0.24456613417714834, 0.24002879870799826, 0.23932889017148637, 0.24253216307318093, 0.24666448398584537, 0.24409404376923455, 0.2470602400820045, 0.24511258984806344, 0.24469052970993752, 0.24681322200808162, 0.2418613201737696, 0.2395190042717492, 0.25079511499543694, 0.24951477871075564, 0.24402216294159493, 0.23870123306051919, 0.23937643794160263, 0.2437281411360292, 0.2480512512665169, 0.25320648938855705, 0.2562090756517707, 0.23295449858129608, 0.24561562255828404, 0.24209332596693262, 0.2466022344853948, 0.24022627408232758, 0.24576352584157504, 0.25353608178157433, 0.2395598959068165, 0.24042998858745776, 0.23662985444945447, 0.23703962266810386, 0.235718719916893, 0.2613097871562429, 0.2468722050863446, 0.24700676372238234, 0.2492630069929303, 0.2531062271811214, 0.2422399223946473, 0.2507193155732809, 0.24836585325572422, 0.2427369980506745, 0.24839953073829996, 0.24479242154926645, 0.24397121939588995, 0.23672433973600468, 0.23719421193441925, 0.2379821332938531, 0.24366306029625384, 0.24493778684158243, 0.2369944912600605, 0.24019140792170576, 0.2482641932152796, 0.23967477823516317, 0.24958386357791504, 0.2367109964492128, 0.25533891494805905, 0.24639802287314452, 0.23953964434308456, 0.24597930709155752, 0.24053751825190642, 0.24633424845980664, 0.22887894597050606, 0.24372562420937946, 0.2494921569949856, 0.2383151944525832, 0.24101026823707655, 0.23164858017116785, 0.24176706885005914, 0.24208627506981, 0.24435219391449994, 0.23975776935763218, 0.23425774537392108, 0.24447299043337503, 0.24938677952570074, 0.23728075723949016, 0.24136556608273702, 0.2378267114982009, 0.24595940985954276, 0.24221335073896483, 0.23467198031607503, 0.23794158814730598, 0.2411233695023054, 0.23995255867895834, 0.2407341397221328, 0.24021167078000658, 0.2400192185305059, 0.23594458068848825, 0.23809927128547548, 0.2379592138470388, 0.2380814891052889, 0.23737910618165545, 0.24371199371001007, 0.23417508491661912, 0.24041478861780727, 0.2377036967333041, 0.2333276258671985, 0.2338991296824579, 0.23701730861748552, 0.23940507240374298, 0.2372228802893968, 0.23878907943235747, 0.22658240720720998, 0.23505923559195271, 0.23041910179616773, 0.23255765283315935, 0.23369695666227855, 0.23989456293501837, 0.2277499797626161, 0.23583768852347253, 0.2377351895339933, 0.24077455684378304, 0.2367346158853787, 0.23311336334867805, 0.2341520840064714, 0.23782339239693887, 0.233605924838533, 0.23291249989586718, 0.23535046233868628, 0.2341424756473405, 0.23586027203219048, 0.2356172766098205, 0.2329787541655641, 0.23569418476693624, 0.23289725104091213, 0.23461141331376983, 0.2339027374434997, 0.23413826437557445, 0.24099776943615986, 0.23312585029349314, 0.23334580722867565, 0.23665643463312996, 0.22950803500819295, 0.23740327323549518, 0.23103500169464478, 0.23477486165368236]\n",
            "test_acc_list_rand = [63.2759680393362, 85.48325138291334, 85.77519975414874, 86.6740934234788, 88.42962507682851, 88.21834665027659, 89.89320835894284, 90.08143822987093, 90.69606637984019, 91.61032575291948, 91.36447449293178, 91.82544560540873, 91.16856177012907, 91.79471419791027, 91.80623847572218, 92.490012292563, 91.03027043638599, 92.30946527350953, 92.62062077443147, 92.64751075599263, 92.9240934234788, 92.35556238475722, 93.00092194222495, 93.06622618315919, 91.64105716041794, 93.3082360172096, 93.09311616472034, 92.47080516287646, 93.21988322065151, 93.03549477566072, 93.44652735095268, 93.35049170251997, 92.45159803318992, 93.64244007375538, 93.43884449907806, 92.70129071911494, 93.63475722188076, 93.02397049784881, 93.28902888752305, 93.3620159803319, 93.28518746158574, 93.45036877688999, 93.42732022126613, 93.21220036877689, 93.40043023970497, 93.38506453595575, 92.9740319606638, 93.5118315918869, 92.77043638598647, 93.39274738783037, 93.73079287031346, 93.69237861094038, 93.28134603564843, 93.27366318377382, 93.5118315918869, 93.5118315918869, 93.75, 93.76152427781193, 93.63475722188076, 93.39658881376766, 93.48878303626306, 93.58097725875845, 93.56945298094652, 92.59757221880763, 93.69622003687769, 93.58866011063307, 93.65012292563, 93.31207744314689, 93.60018438844499, 93.53103872157345, 93.71926859250154, 93.6539643515673, 93.700061462815, 93.39658881376766, 93.33512599877075, 93.69622003687769, 93.67701290719116, 93.799938537185, 93.22756607252612, 93.78841425937308, 93.72695144437616, 93.52719729563614, 93.9420712968654, 93.81146281499693, 93.66933005531654, 93.82298709280884, 93.21988322065151, 93.3159188690842, 93.63859864781807, 93.75768285187462, 93.33512599877075, 93.93054701905348, 93.02397049784881, 92.98555623847572, 93.98432698217579, 93.37738168408113, 93.91902274124155, 93.76152427781193, 93.75384142593731, 93.91518131530424, 94.01505838967425, 93.8959741856177, 93.57329440688383, 93.56945298094652, 93.87292562999386, 93.89981561155501, 93.58866011063307, 93.76536570374923, 93.66548862937923, 93.4618930547019, 93.4618930547019, 93.86140135218193, 94.14950829748003, 93.92286416717886, 93.79609711124769, 93.34665027658266, 93.74231714812538, 93.71542716656423, 93.64244007375538, 93.83451137062077, 93.98048555623848, 93.44268592501537, 93.80762138905962, 94.0419483712354, 93.68085433312845, 93.4580516287646, 93.49646588813768, 94.05731407498463, 93.65012292563, 93.66548862937923, 93.92670559311617, 93.9958512599877, 93.92286416717886, 93.85371850030731, 93.79609711124769, 93.799938537185, 93.93054701905348, 93.57329440688383, 93.84987707437, 94.07652120467118, 93.68469575906576, 93.82682851874615, 94.20328826060233, 94.23786109403811, 93.88444990780577, 93.99969268592501, 94.03426551936079, 94.0918869084204, 93.93438844499079, 93.88060848186846, 93.8421942224954, 94.11109403810694, 94.03042409342348, 94.19560540872772, 94.16103257529196, 94.05731407498463, 94.11877688998156, 93.8921327596804, 93.18146896127843, 93.98048555623848, 94.03426551936079, 93.76920712968654, 93.75384142593731, 94.13414259373079, 93.8959741856177, 94.26859250153657, 94.16487400122925, 94.00737553779963, 94.11493546404425, 94.1418254456054, 94.01121696373694, 94.25706822372464, 94.1840811309158, 94.13030116779349, 94.25322679778733, 94.21865396435157, 94.07652120467118, 94.10725261216963, 94.31468961278426, 94.26859250153657, 94.34157959434542, 94.38383527965581, 94.1418254456054, 93.93054701905348, 93.98432698217579, 94.42609096496619, 94.0918869084204, 94.37231100184388, 94.15334972341734, 94.1917639827904, 94.07267977873387, 94.1379840196681, 94.41456668715428, 94.21097111247695, 94.42609096496619, 94.23786109403811, 94.44913952059004, 93.75384142593731, 94.24554394591273, 94.34926244622004, 94.17639827904118, 94.04578979717272, 94.41456668715428, 94.31853103872157, 94.29164105716042, 94.42224953902888, 94.16871542716656, 94.4798709280885, 94.37231100184388, 94.55285802089736, 94.57974800245852, 94.4721880762139, 94.40688383527966, 94.3761524277812, 94.57974800245852, 94.49523663183774, 94.39151813153042, 94.64505224339274, 94.31084818684695, 94.5720651505839, 94.06499692685925, 94.4721880762139, 94.48371235402581, 94.41456668715428, 94.51828518746159, 94.46450522433928, 94.75645359557468, 94.60663798401967, 94.35310387215735, 94.68730792870313, 94.42224953902888, 94.80639213275968, 94.49139520590043, 94.61047940995698, 94.52980946527352, 94.69114935464044, 94.69114935464044, 94.52980946527352, 94.4299323909035, 94.68730792870313, 94.68346650276582, 94.68346650276582, 94.72572218807622, 94.71419791026429, 94.8140749846343, 94.75645359557468, 94.71035648432698, 94.68730792870313, 94.71419791026429, 94.7180393362016, 94.76029502151198, 94.74492931776275, 94.63352796558083, 94.8140749846343, 94.77950215119853, 94.77950215119853, 94.63352796558083, 94.72572218807622, 94.76413644744929, 94.74877074370006, 94.86785494775661, 94.9139520590043, 94.89090350338046, 94.7180393362016, 94.82944068838353, 94.87553779963122, 95.14059618930547, 94.8179164105716, 94.97157344806392, 94.90242778119237, 94.77950215119853, 94.79486785494775, 95.059926244622, 94.89474492931777, 94.88322065150584, 94.65273509526736, 94.87553779963122, 94.9562077443147, 95.0560848186847, 94.85633066994468, 95.02535341118623, 94.94468346650277, 94.94468346650277, 94.89090350338046, 94.99078057775046, 94.9139520590043, 95.08297480024585, 94.8640135218193, 95.009987707437, 94.85633066994468, 94.92931776275353, 94.91779348494161, 94.89474492931777, 95.03303626306085, 95.01382913337432, 94.8640135218193, 95.059926244622, 94.91011063306699, 95.02535341118623, 94.93315918869084]\n"
          ]
        }
      ],
      "source": [
        "print(f\"train_loss_list_rand = {train_loss_list}\") \n",
        "print(f\"train_acc_list_rand = {train_acc_list}\")\n",
        "print(f\"test_loss_list_rand = {test_loss_list}\")\n",
        "print(f\"test_acc_list_rand = {test_acc_list}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ilmYYfIElcAU"
      },
      "outputs": [],
      "source": [
        "train_loss_list_01 = [2.3849518770770977, 2.2417600981911345, 2.2415069635644516, 2.2409557133186153, 2.2419579268147953, 2.240125378942102, 2.240931454066662, 2.2417839348800785, 2.242252948807507, 2.241471426273749, 2.241231945472035, 2.2413479971691843, 2.241036834432504, 2.2407813569717616, 2.2415969093963706]\n",
        "train_acc_list_01 = [18.56855479089465, 18.617257808364215, 18.746426680783483, 18.60243515087348, 18.61937533086289, 18.886183165696135, 18.60243515087348, 18.598200105876124, 18.604552673372154, 18.740074113287452, 18.731604023292746, 18.814187400741133, 18.848067760719957, 18.82901005823187, 18.752779248279513]\n",
        "test_loss_list_01 = [2.2387831538331273, 2.241503697984359, 2.241926829020182, 2.240557459055209, 2.2406100852816713, 2.252836311564726, 2.2468298743752873, 2.2446437150824305, 2.2425780202828203, 2.240177970306546, 2.243702617346072, 2.2441832843948815, 2.253918958645241, 2.245230858232461, 2.2435202879064224]\n",
        "test_acc_list_01 = [18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 14.27858020897357, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463, 18.95743700061463]\n",
        "train_loss_list_001 = [2.2888378896687414, 2.232192633274771, 1.4893088792236193, 0.5001831288098643, 0.3830500937251218, 0.32867970416539405, 0.29770232389774426, 0.27895135900919354, 0.2627035691970732, 0.24814978033950336, 0.23438045485475198, 0.2243682525668364, 0.21301924496848731, 0.20818865677811266, 0.1958482328166322]\n",
        "train_acc_list_001 = [18.50926416093171, 18.968766543144522, 47.80307040762308, 84.23292747485442, 88.15881418740074, 89.9142403388036, 91.04923239809423, 91.63790365272631, 92.08893594494441, 92.58020116463737, 93.03758602435151, 93.36791953414505, 93.715193223928, 93.84012705134992, 94.35044997353097]\n",
        "test_loss_list_001 = [2.2451091665847627, 2.2302937355695986, 0.684039752711268, 0.4374444575286379, 0.39873581839834943, 0.36398082489476485, 0.3313831433507742, 0.3210300371854329, 0.2847569804346445, 0.29315854806233854, 0.27853756525791157, 0.26896954926789973, 0.2596830692069203, 0.26028463620619446, 0.2491153629460171]\n",
        "test_acc_list_001 = [18.88060848186847, 19.053472649047325, 78.20759065765212, 86.33988936693301, 87.90334972341734, 88.90596189305471, 90.08143822987093, 90.81515058389674, 91.70636140135218, 91.54118008604794, 91.8638598647818, 92.2480024585126, 92.43239090350338, 92.50537799631223, 92.82037492317149]\n",
        "train_loss_list_0001 = [1.8565612323885041, 0.5532636212785715, 0.4003713336094285, 0.3402645644860539, 0.309145948165639, 0.2848975209968523, 0.262982070708501, 0.24855145600026216, 0.2386487888167221, 0.2278864942390098, 0.21327269485164788, 0.20556094100683686, 0.19517452138549268, 0.18913837452608395, 0.18232752032436653]\n",
        "train_acc_list_0001 = [34.7993647432504, 82.35468501852831, 87.65272631021705, 89.51614610905241, 90.6723133933298, 91.51932239280042, 92.26892535733192, 92.59925886712546, 92.97617787188989, 93.34674430915828, 93.69401799894123, 93.97353096876654, 94.31868713605083, 94.53255690841715, 94.71254632080466]\n",
        "test_loss_list_0001 = [1.0861167063315709, 0.46979708385233787, 0.41340532643245714, 0.3364271931350231, 0.3262409484561752, 0.34040282589986043, 0.28803076817854945, 0.2942232322678262, 0.2779932311169949, 0.27514038454083833, 0.2478603608906269, 0.2512748020463714, 0.26197264781769586, 0.2462065773194327, 0.24985540344142446]\n",
        "test_acc_list_0001 = [64.00583896742471, 85.58312845728334, 87.41933005531654, 89.81637984019667, 89.970036877689, 89.99308543331284, 91.59111862323294, 91.35295021511985, 91.82928703134604, 92.03672403196066, 92.76275353411187, 92.83574062692071, 92.90488629379226, 92.98555623847572, 92.87031346035648]\n",
        "train_loss_list_00001 = [1.2440541174192092, 0.4583125376927497, 0.36052036624613815, 0.3151768069603256, 0.2875107263081119, 0.26835051384883196, 0.25187577066948097, 0.23390740957767336, 0.22118305101950317, 0.21190600004299545, 0.20555683115350845, 0.1952596850249018, 0.18612936185546683, 0.18178928815090883, 0.17480877608181986]\n",
        "train_acc_list_00001 = [57.18581259925887, 85.53943885653786, 88.79618845950239, 90.25304393859184, 91.214399152991, 92.01058761249338, 92.36421386977237, 93.09899417681312, 93.43991529910005, 93.80412916887242, 93.86765484383271, 94.25092641609317, 94.500794070937, 94.64478560084702, 94.82477501323451]\n",
        "test_loss_list_00001 = [0.5935118007017117, 0.43328142947718207, 0.3668157114994292, 0.34575528556517526, 0.3590214093964474, 0.3134572241893586, 0.2903973020467104, 0.28062032824199573, 0.26905518266208034, 0.26447016406146917, 0.27132851287138227, 0.26421160440818936, 0.25491809147391836, 0.24584030433028353, 0.2630316608895858]\n",
        "test_acc_list_00001 = [80.93500307314075, 86.63567916410571, 88.97510755992624, 89.3438844499078, 88.99431468961278, 90.58466502765826, 91.54502151198525, 91.56422864167179, 92.02135832821143, 92.25184388444991, 91.97910264290104, 92.37861094038107, 92.72433927473878, 92.96250768285188, 92.59757221880763]\n",
        "\n",
        "\n",
        "train_loss_list_const = [1.5944857293674293, 0.4869092239677745, 0.3710624326454592, 0.3296906750215101, 0.29884549410039496, 0.27146996971633697, 0.2604337949053382, 0.24417246496532022, 0.23419785655045575, 0.22457996957751147, 0.21173857996457315, 0.20202396321425917, 0.1943116241051414, 0.18745123495052501, 0.1774632519257424, 0.17214223601650902, 0.16552220173886797, 0.15473239550866733, 0.15404348591276948, 0.14213740165398372, 0.13690419029192066, 0.13265049481779578, 0.12643728150322348, 0.1223953123176647, 0.11572830584960256, 0.10872174589453028, 0.10607895330301306, 0.10143619665729645, 0.09846852844464908, 0.09155762590831373, 0.09176684167932689, 0.08320119946962853, 0.07973214024854547, 0.07931727142424037, 0.06983993944351063, 0.07091535234995448, 0.0671700671211713, 0.06470882359558974, 0.061619590856841586, 0.061459206172131346, 0.057121708876198427, 0.053374216585170206, 0.0498615506466044, 0.05073856983490308, 0.04987424655961312, 0.046790412364042994, 0.04711244383266544, 0.04263703334889801, 0.043034394194061555, 0.04126133985860339, 0.03625233271121373, 0.03783284285014904, 0.0379838087573284, 0.03416189216178284, 0.03383261713717981, 0.03370409071676403, 0.033968668481206325, 0.03165312097212274, 0.03038872497441464, 0.02841417055683044, 0.029356409022635036, 0.02889881345480024, 0.028939039615515447, 0.02568263415692672, 0.027656220918051838, 0.026301205131101617, 0.023402816848356156, 0.024891200900357974, 0.022950206206843942, 0.022271349775869285, 0.0250707899006071, 0.022344395426495713, 0.019562920480307198, 0.022191016989943715, 0.02281398802152065, 0.0203801996838415, 0.020550807893653537, 0.02117716848076041, 0.019771566791103416, 0.022764958889409142, 0.017198871523580392, 0.02078044665365311, 0.020109122048629333, 0.01588759385191313, 0.01774437841210012, 0.020397339600326397, 0.01534856222032823, 0.016435430918308794, 0.01711455919842304, 0.01770936636729633, 0.018040290616609177, 0.01564757464108749, 0.015251886521385311, 0.01534154344099917, 0.015427147975282334, 0.01419120683843651, 0.01811075882562171, 0.016642653122089984, 0.01305790530750528, 0.013949767823940267, 0.014656124457392462, 0.013457846312966957, 0.014343252129156173, 0.013850259383991146, 0.012460459422866397, 0.012413865320129272, 0.013889214684304953, 0.01221698466477806, 0.015144580592092777, 0.010421825547786688, 0.01315658220278792, 0.014559990600058128, 0.012665790851758906, 0.01143317199561781, 0.011717887349175301, 0.011158559258571676, 0.012634062059679534, 0.011741106513152792, 0.012515452659958159, 0.011751638104695717, 0.011457235534427129, 0.011438692509360596, 0.012512708038015468, 0.008602326916451698, 0.010682771495822786, 0.011196336384384224, 0.009268063090769713, 0.009998142540840234, 0.012729091897495166, 0.010267185127698763, 0.010566324319321028, 0.010713477439735606, 0.010119398856430022, 0.012654226244056406, 0.009628543746825273, 0.0102562899387879, 0.010169452542220153, 0.011113815507671713, 0.01246779066452764, 0.007518461719419533, 0.007389902458939484, 0.011104748547043264, 0.01135712010794771, 0.008941290669380526, 0.009924649224834431, 0.009838336234147059, 0.007612355080635734, 0.009282812758648675, 0.008423602285464519, 0.009065308554876006, 0.010365516194014629, 0.006115070432431249, 0.006490228478146479, 0.011122480412586441, 0.00872685812082587, 0.008791860558152852, 0.009787340952780856, 0.008418811355189983, 0.007423892029066022, 0.009898100130695444, 0.008828991230531168, 0.007722080036888794, 0.009378696144602659, 0.00829515335842403, 0.006554383687879633, 0.006126663065066873, 0.010910632581322311, 0.007716069375264186, 0.006358072765228986, 0.007697185452836498, 0.008877029283653514, 0.008559577807589543, 0.00707696825589556, 0.00850764569837624, 0.0062317561151193695, 0.0066259507287960455, 0.008618629247509826, 0.007878521594295732, 0.007498281048782551, 0.006598806819186967, 0.006230976371933824, 0.006141664580167378, 0.007923200858393221, 0.008045455975570557, 0.00792612456981016, 0.007601726206049413, 0.007419711852838272, 0.004584900550783639, 0.00685006011318293, 0.008050930358640324, 0.008370204849807444, 0.005507358266772813, 0.006857401374734981, 0.008172955556791895, 0.004244687181667273, 0.008633549227632837, 0.008705449396538356, 0.0067560189635043925, 0.006710546794999935, 0.00550614756518684, 0.0069180619830691474, 0.00835907400430254, 0.005712247417545953, 0.006098340349643973, 0.007154704712266156, 0.0061693518397019, 0.005750073719051822, 0.005536363746094307, 0.006909997050809464, 0.006392437509654407, 0.004530645426771153, 0.00935107147158372, 0.005991038718238898, 0.004413133404240297, 0.007152540924743997, 0.006123794079590131, 0.004265024197029019, 0.007488072243164834, 0.006849384789138319, 0.006199823599835201, 0.004939858833603685, 0.006542191565922548, 0.006134184821023406, 0.006127372573951168, 0.006120713498504846, 0.005705225819374859, 0.0054501006014126396, 0.00476737067947397, 0.005204442173390228, 0.006888031104115721, 0.005438639611901732, 0.005198591940753804, 0.006226397409455738, 0.00398913007948585, 0.006725981141281091, 0.0057350191533619786, 0.004547678196042624, 0.003760278201966155, 0.004862335098237806, 0.0053206211415940214, 0.004511028374038128, 0.006714381331292853, 0.006507630908786355, 0.004150821972775074, 0.005154698122662017, 0.005662354828146746, 0.004606740834664833, 0.004731273036621596, 0.004916682220897782, 0.0048261530498232945, 0.006001339212270287, 0.007263619651073496, 0.0038986454153652154, 0.005036929771996826, 0.005049693082181207, 0.00644224065052205, 0.004578751418381182, 0.004686958738437559, 0.003585059260631736, 0.003101582141557616, 0.007221638428928184, 0.005000114876925785, 0.0051700827908760325, 0.005254589704386773, 0.00406367993565528, 0.0036232792127412505, 0.006171727926178867, 0.005875237415135579, 0.00392443083374783, 0.006561909474143628, 0.0037487958143322068, 0.004443046452993911, 0.005447318903908573, 0.004017829913798502, 0.00472008602112813, 0.0055074588290326305, 0.005279580161203713, 0.0036886348879267314, 0.0035813810519778975, 0.004883374933941502, 0.004846986711028166, 0.004054715485154408, 0.004949977601104352, 0.0046826732979178415, 0.004384392334953691, 0.004187341836835158, 0.004243570542040143, 0.005868256121943751, 0.00557879406272601, 0.0037290168971954394, 0.0024883511193469366, 0.004939023949055762, 0.004748034648033541, 0.0038421762496379727, 0.003486280636854575, 0.0036575751641310756, 0.00670746078328728, 0.004645855993902198, 0.004636629221823842, 0.004005611049817734]\n",
        "train_acc_list_const = [44.03176283748015, 84.49761778718899, 88.61196400211752, 89.87400741132875, 91.01958708311275, 91.79460031762838, 92.2350449973531, 92.80889359449444, 93.03546850185283, 93.37427210164108, 93.80412916887242, 94.0052938062467, 94.38009528851244, 94.6278454208576, 94.88618316569614, 95.01323451561673, 95.1868713605082, 95.66543144520911, 95.57226045526734, 95.91953414505029, 96.01905770248808, 96.16093170989942, 96.35150873478031, 96.54632080465855, 96.74113287453679, 96.86183165696136, 96.96347273689783, 97.10746426680784, 97.21757543673901, 97.32133403917416, 97.3276866066702, 97.63896241397565, 97.72154579142403, 97.7342509264161, 97.88883006881949, 97.87400741132875, 97.96506087877184, 98.09211222869243, 98.24245632609846, 98.17257808364214, 98.28692429857067, 98.34833245103229, 98.44997353096876, 98.49444150344097, 98.49655902593965, 98.54102699841185, 98.55796717840127, 98.67443091582848, 98.63419798835362, 98.74219163578613, 98.87559555320276, 98.77607199576495, 98.78030704076231, 98.9793541556379, 98.94970884065643, 98.9433562731604, 98.90947591318158, 98.98782424563261, 98.96453149814717, 99.08099523557438, 99.05770248808894, 99.05134992059291, 99.0428798305982, 99.1784012705135, 99.13393329804128, 99.1148755955532, 99.20381154049761, 99.18051879301217, 99.27580730545262, 99.23980942297511, 99.1784012705135, 99.25674960296453, 99.3499205929063, 99.26310217046056, 99.2503970354685, 99.32874536791954, 99.28215987294865, 99.27157226045527, 99.29486500794071, 99.24404446797247, 99.42615140285865, 99.2779248279513, 99.31180518793012, 99.46214928533615, 99.40921122286925, 99.36262572789836, 99.49602964531498, 99.43673901535203, 99.42615140285865, 99.40921122286925, 99.41132874536792, 99.47697194282689, 99.49602964531498, 99.49814716781366, 99.47273689782953, 99.5044997353097, 99.39015352038115, 99.45156167284277, 99.57649550026468, 99.55320275277924, 99.5404976177872, 99.5659078877713, 99.53414505029116, 99.54685018528322, 99.58073054526204, 99.5849655902594, 99.49391212281631, 99.58920063525674, 99.51720487030175, 99.66543144520911, 99.5574377977766, 99.56167284277396, 99.56802541026998, 99.60402329274748, 99.62519851773425, 99.6019057702488, 99.5934356802541, 99.62519851773425, 99.60402329274748, 99.61461090524087, 99.61461090524087, 99.63790365272631, 99.6019057702488, 99.70566437268396, 99.66119640021175, 99.64002117522499, 99.69507676019057, 99.68025410269983, 99.58920063525674, 99.6569613552144, 99.62519851773425, 99.6569613552144, 99.66754896770779, 99.54261514028586, 99.69719428268925, 99.63366860772896, 99.67178401270513, 99.60825833774484, 99.6019057702488, 99.75436739015352, 99.75436739015352, 99.59555320275278, 99.62731604023293, 99.6929592376919, 99.65484383271573, 99.66966649020645, 99.7289571201694, 99.71836950767602, 99.69084171519323, 99.67601905770249, 99.66119640021175, 99.79671784012704, 99.78824775013234, 99.62519851773425, 99.71836950767602, 99.72683959767072, 99.68872419269455, 99.73107464266808, 99.73107464266808, 99.67601905770249, 99.70989941768131, 99.73319216516676, 99.68025410269983, 99.7204870301747, 99.78824775013234, 99.80518793012176, 99.64849126521969, 99.73107464266808, 99.79883536262572, 99.74377977766014, 99.69719428268925, 99.69719428268925, 99.77130757014294, 99.71625198517734, 99.7924827951297, 99.784012705135, 99.70354685018528, 99.75860243515088, 99.73319216516676, 99.79036527263102, 99.78189518263632, 99.79036527263102, 99.73530968766543, 99.69719428268925, 99.75013234515616, 99.72472207517205, 99.76071995764956, 99.84330333509793, 99.784012705135, 99.73530968766543, 99.69084171519323, 99.82212811011117, 99.76919004764426, 99.73319216516676, 99.85812599258867, 99.70989941768131, 99.70142932768661, 99.76071995764956, 99.78824775013234, 99.8284806776072, 99.7924827951297, 99.7204870301747, 99.8009528851244, 99.81577554261514, 99.75436739015352, 99.7924827951297, 99.81365802011646, 99.8009528851244, 99.77554261514028, 99.77977766013764, 99.85389094759132, 99.67601905770249, 99.81789306511382, 99.86871360508205, 99.74377977766014, 99.78189518263632, 99.84965590259397, 99.77130757014294, 99.78824775013234, 99.82001058761249, 99.81365802011646, 99.77130757014294, 99.78189518263632, 99.79883536262572, 99.80730545262044, 99.82636315510852, 99.79036527263102, 99.83059820010588, 99.81154049761778, 99.7564849126522, 99.8094229751191, 99.8284806776072, 99.81154049761778, 99.88353626257279, 99.75224986765484, 99.79671784012704, 99.85389094759132, 99.88565378507147, 99.83059820010588, 99.80730545262044, 99.84118581259926, 99.77130757014294, 99.81154049761778, 99.85177342509265, 99.83271572260455, 99.8094229751191, 99.8284806776072, 99.86236103758603, 99.8369507676019, 99.82424563260984, 99.8009528851244, 99.78824775013234, 99.84330333509793, 99.82636315510852, 99.8284806776072, 99.78189518263632, 99.85812599258867, 99.85177342509265, 99.89624139756485, 99.89835892006353, 99.77130757014294, 99.84118581259926, 99.81577554261514, 99.8369507676019, 99.85600847008999, 99.86659608258337, 99.81154049761778, 99.79671784012704, 99.87718369507677, 99.79883536262572, 99.85812599258867, 99.85389094759132, 99.83906829010058, 99.88353626257279, 99.84753838009529, 99.8369507676019, 99.83906829010058, 99.88565378507147, 99.87506617257809, 99.83271572260455, 99.84330333509793, 99.85812599258867, 99.86659608258337, 99.8369507676019, 99.8284806776072, 99.84753838009529, 99.85177342509265, 99.7924827951297, 99.79671784012704, 99.88141874007411, 99.92376919004765, 99.86024351508735, 99.84753838009529, 99.85600847008999, 99.87506617257809, 99.87718369507677, 99.79671784012704, 99.86024351508735, 99.85177342509265, 99.87083112758073]\n",
        "test_loss_list_const = [0.9403148495099124, 0.46971940738605517, 0.34306909633325594, 0.3384028169892582, 0.3164549315823059, 0.31764499748162195, 0.282772644879479, 0.27239831069520876, 0.2788072020618939, 0.2605946859454407, 0.2569786167758353, 0.25151387514436946, 0.23040011815507622, 0.23389688687508597, 0.23323347488893012, 0.22432381940969065, 0.23885389055837603, 0.22965874502837075, 0.23512107754747072, 0.23369459731175618, 0.22183155821745887, 0.2491116562639089, 0.23399925456546686, 0.25359821337841304, 0.23499618255186314, 0.22481991891183106, 0.2324665375966944, 0.23745987874766192, 0.24356824685545528, 0.24952035153503804, 0.23930090873082185, 0.25436064657554325, 0.25367995643732594, 0.26609678615761156, 0.2672480665622096, 0.2746706861893044, 0.25824960023529975, 0.26025373894063863, 0.2791316908072023, 0.26918739248432366, 0.2763673086997633, 0.2997446244436444, 0.2916372886438872, 0.3003798500981693, 0.2806282534501424, 0.2931029381979184, 0.29610180335265457, 0.3087221756942716, 0.32797835285172744, 0.30948590080929445, 0.31205729929292025, 0.3264701991711798, 0.31612680869761345, 0.3249107254029927, 0.3218845636081681, 0.3128845189679779, 0.3158182442261308, 0.3337518404621412, 0.34533743259003935, 0.32409434777447116, 0.3321887091642209, 0.3324712138604738, 0.33405348602864965, 0.3346349073199592, 0.3316200801226146, 0.35435040097446274, 0.3272651647738017, 0.3605334343612377, 0.3315225986252521, 0.36446072681642633, 0.37520757036320135, 0.3792235794586732, 0.3674935271379118, 0.3611337201596767, 0.35125305392213313, 0.3664919300844856, 0.3579704279956572, 0.358177753384499, 0.37499817195074525, 0.3516562903372973, 0.36848902222061275, 0.37098927200570997, 0.37174441270968495, 0.3886868230103716, 0.36849470644751015, 0.3531273581982389, 0.3660184144471571, 0.37421249364520986, 0.3748011891273599, 0.3667490633350669, 0.3643053767691348, 0.37450089440772344, 0.37444961906465535, 0.3669995936729452, 0.37039543743080955, 0.38952702733085437, 0.39265704210208474, 0.3686957350274658, 0.3655954713695774, 0.39540933398529887, 0.37815322328870205, 0.3858970995974161, 0.37387897242682383, 0.37425000457020074, 0.38926156392941874, 0.3786647673787586, 0.37843504400156874, 0.4000659274078869, 0.38383779517721894, 0.38883355182513374, 0.38116612786189746, 0.39886108679952575, 0.4039541971048011, 0.4085461224575399, 0.4040325807553588, 0.40590017215878355, 0.38623136059180196, 0.38693335685221586, 0.41144327609343273, 0.3980357802459313, 0.4007313655798926, 0.40366758222636934, 0.38523547610669745, 0.42097171660804866, 0.4246416387294291, 0.40921079142786126, 0.4052840040850581, 0.4231068941344525, 0.38739459854824576, 0.4194799537094785, 0.41747657055327414, 0.4164134435739149, 0.3939945743985328, 0.393845445202554, 0.4172966992665155, 0.4208380446276244, 0.408174136686641, 0.43686796033608855, 0.3842315211042981, 0.4022488394277353, 0.41533097460427704, 0.4123304054533661, 0.3947430606748836, 0.4344836208585869, 0.42245670334509045, 0.4120090134880122, 0.4084896664774301, 0.4208405370477076, 0.4342218170389898, 0.4342225538966173, 0.414541769489719, 0.42016579197290554, 0.43672865715023934, 0.3970657474470927, 0.4120631425404081, 0.4072342333657777, 0.4361969401840778, 0.40728934426043256, 0.4082725720443562, 0.43000999789721533, 0.4488575828864294, 0.4490402939628956, 0.42708765610358584, 0.4119907574152903, 0.4171651719850214, 0.46127884272559017, 0.424621216852364, 0.42663686249551236, 0.443727373143238, 0.4234333980655042, 0.4163527505554478, 0.41962003758446514, 0.43725314623146666, 0.4135830312939909, 0.4485339165719993, 0.438206387026345, 0.42548432996423513, 0.4139385414218493, 0.422614497990877, 0.44258191624619797, 0.5020544356643679, 0.42181872726217207, 0.45136614169414135, 0.42586187040889817, 0.4617275644605066, 0.42785005136758236, 0.4181436715811929, 0.43456211805745376, 0.456493051905258, 0.4419752404130265, 0.439949865836431, 0.458016514120733, 0.45355927485370023, 0.4266304138993077, 0.4774111132959233, 0.43147677636942733, 0.42984621565533326, 0.4257584124226888, 0.41347907219703, 0.44227188734301165, 0.44509480125270784, 0.42359101505694435, 0.45620652116542937, 0.4532003666402078, 0.4544208156419735, 0.4288948582868804, 0.45772276510365817, 0.4652134206776014, 0.4481767811538542, 0.45044724102926387, 0.45808432965228957, 0.4832801581086481, 0.4330355538119215, 0.450099578350965, 0.44405912482818843, 0.4245691280629413, 0.47918898603130206, 0.4441465862088508, 0.4232571859679678, 0.42079631317699073, 0.46119900758140814, 0.4447093260310152, 0.45832808096619215, 0.4615354106998911, 0.46208279893970955, 0.44396297247651234, 0.44644354116719437, 0.4640259245666219, 0.466397661642701, 0.46452038604811785, 0.4498248620908342, 0.4491576017337103, 0.465446551038208, 0.47035092626716574, 0.4713521593012938, 0.4597488452506927, 0.4512224776633814, 0.48416280633240355, 0.4734927043455708, 0.46729338517887337, 0.46055449181357305, 0.44595291037751617, 0.45059747884606977, 0.4550898942740305, 0.4553583972247354, 0.484807043427638, 0.47273430763972085, 0.45941312947109636, 0.4590342458538419, 0.4751177701332113, 0.47533442642466694, 0.45637929342760175, 0.4544383191900766, 0.4463540438526109, 0.45517611408414427, 0.4642113188975582, 0.44581558580930325, 0.4676268981386195, 0.46861990691874833, 0.47670730448090565, 0.4613685188541079, 0.46828112538502203, 0.47280027833310706, 0.44121663249097764, 0.4606796685479554, 0.5259090004415781, 0.48354477134040175, 0.469667333146265, 0.4692415113266393, 0.45400641179944884, 0.47124824484846756, 0.4692198993460111, 0.44249545135900525, 0.4556816854969571, 0.4647824307538423, 0.47515893393360514, 0.44086019356972445, 0.47874322776481804, 0.48824521492911027, 0.4832109012732319, 0.45331234720138397, 0.46669590011166007, 0.4659509803990231, 0.4592359331253843, 0.4794528412395248, 0.4734935767528619, 0.4946094546649678, 0.47707713397183255, 0.4516702479244593, 0.44164761501893984, 0.48553112554637823, 0.4603754159190929, 0.4560155915005096, 0.48063205852739366, 0.4870615708185177, 0.503982417563926, 0.4785489882890354, 0.44539653152848285, 0.4539821001874539, 0.47518135870204253]\n",
        "test_acc_list_const = [71.6080208973571, 85.50245851259987, 89.67040565457899, 89.58589428395821, 90.46173939766442, 90.5577750460971, 91.50276582667486, 92.0520897357099, 91.96757836508912, 92.47464658881377, 92.6820835894284, 92.88952059004302, 93.56177012907192, 93.4580516287646, 93.66548862937923, 93.80377996312231, 93.34665027658266, 93.71158574062692, 93.5041487400123, 93.57329440688383, 94.06499692685925, 93.33896742470804, 93.72311001843885, 92.94330055316533, 93.68853718500307, 94.18792255685311, 93.96127842655194, 93.91518131530424, 93.82682851874615, 94.03042409342348, 94.1917639827904, 93.77304855562384, 93.9420712968654, 93.68853718500307, 94.08420405654579, 93.77304855562384, 93.91518131530424, 94.14566687154272, 93.93822987092808, 94.14566687154272, 94.06499692685925, 93.73079287031346, 93.8921327596804, 94.07267977873387, 94.01889981561156, 94.22633681622618, 93.86140135218193, 93.90749846342962, 93.97664413030117, 94.07652120467118, 94.03042409342348, 93.97280270436386, 94.21865396435157, 94.07267977873387, 94.09956976029503, 94.21865396435157, 94.30700676090964, 93.88444990780577, 94.06883835279656, 94.30316533497235, 94.23017824216349, 94.19944683466503, 94.05731407498463, 94.36846957590657, 94.26475107559926, 94.04578979717272, 94.31468961278426, 94.13030116779349, 94.3262138905962, 94.06499692685925, 93.98432698217579, 94.34926244622004, 94.3262138905962, 94.10341118623234, 94.35694529809466, 93.99969268592501, 94.21481253841426, 94.41840811309157, 94.13414259373079, 94.23017824216349, 94.39920098340504, 94.10341118623234, 94.0381069452981, 94.36846957590657, 94.44529809465274, 94.29548248309773, 94.39920098340504, 94.2839582052858, 94.34542102028273, 94.51444376152428, 94.34926244622004, 94.1418254456054, 94.45682237246466, 94.5221266133989, 94.41072526121697, 94.45298094652735, 93.9958512599877, 94.24938537185002, 94.57974800245852, 94.1418254456054, 94.18792255685311, 94.34926244622004, 94.48371235402581, 94.39535955746773, 94.39151813153042, 94.39535955746773, 94.41072526121697, 94.1418254456054, 94.24554394591273, 94.39535955746773, 94.49907805777505, 94.3262138905962, 94.22633681622618, 94.34542102028273, 94.42224953902888, 94.27627535341118, 94.46450522433928, 94.1840811309158, 94.1917639827904, 94.50291948371235, 94.09956976029503, 94.27243392747388, 94.38383527965581, 94.40304240934235, 94.10725261216963, 94.36462814996926, 94.52980946527352, 94.36078672403197, 94.50676090964966, 94.1917639827904, 94.25706822372464, 94.31084818684695, 94.43761524277812, 94.36462814996926, 94.42224953902888, 94.21097111247695, 94.38383527965581, 94.18023970497849, 94.21865396435157, 94.5259680393362, 94.53365089121081, 94.21865396435157, 94.56822372464659, 94.21865396435157, 94.31468961278426, 94.41072526121697, 94.50291948371235, 94.35310387215735, 94.51060233558697, 94.26090964966195, 94.3338967424708, 94.4299323909035, 94.40688383527966, 94.66425937307929, 94.41072526121697, 94.39535955746773, 94.23786109403811, 94.44145666871543, 94.30316533497235, 94.3761524277812, 94.04578979717272, 94.36846957590657, 94.3300553165335, 94.44145666871543, 94.64505224339274, 93.99200983405039, 94.53365089121081, 94.52980946527352, 94.3799938537185, 94.38767670559312, 94.54133374308543, 94.36078672403197, 94.44913952059004, 94.28779963122311, 94.48755377996312, 94.51828518746159, 94.4721880762139, 94.45682237246466, 94.39920098340504, 94.51444376152428, 94.10341118623234, 94.61432083589429, 94.41840811309157, 94.21865396435157, 94.46450522433928, 94.44913952059004, 94.4798709280885, 94.61432083589429, 94.22633681622618, 94.63352796558083, 94.51060233558697, 94.44145666871543, 94.5221266133989, 94.50291948371235, 93.93438844499079, 94.4721880762139, 94.48371235402581, 94.63736939151813, 94.5759065765212, 94.61047940995698, 94.39920098340504, 94.56822372464659, 94.21481253841426, 94.59895513214505, 94.32237246465888, 94.59127228027043, 94.25322679778733, 94.4299323909035, 94.54517516902274, 94.3338967424708, 94.40688383527966, 94.08420405654579, 94.5259680393362, 94.59895513214505, 94.6681007990166, 94.61047940995698, 94.44529809465274, 94.48371235402581, 94.48755377996312, 94.67962507682851, 94.36846957590657, 94.09572833435772, 94.35694529809466, 94.43377381684081, 94.55669944683467, 94.61047940995698, 94.36462814996926, 94.6220036877689, 94.69499078057775, 94.39535955746773, 94.47602950215119, 94.43761524277812, 94.3300553165335, 94.68730792870313, 94.56822372464659, 94.45298094652735, 94.61816226183159, 94.40304240934235, 94.48371235402581, 94.62968653964352, 94.54517516902274, 94.51828518746159, 94.60663798401967, 94.55669944683467, 94.50676090964966, 94.52980946527352, 94.59511370620774, 94.46066379840197, 94.40304240934235, 94.49139520590043, 94.4299323909035, 94.34157959434542, 94.6681007990166, 94.58358942839583, 94.61047940995698, 94.34926244622004, 94.57974800245852, 94.50676090964966, 94.71035648432698, 94.70651505838967, 94.5221266133989, 94.3761524277812, 94.50291948371235, 94.59127228027043, 94.52980946527352, 94.41456668715428, 94.53749231714812, 94.29932390903504, 94.5720651505839, 94.68730792870313, 94.61816226183159, 94.55285802089736, 94.6681007990166, 94.70651505838967, 94.59127228027043, 94.40304240934235, 94.69114935464044, 94.68730792870313, 94.5221266133989, 94.45682237246466, 94.51828518746159, 94.57974800245852, 94.61432083589429, 94.6220036877689, 94.34542102028273, 94.53749231714812, 94.40688383527966, 94.49523663183774, 94.42609096496619, 94.6757836508912, 94.42224953902888, 94.43377381684081, 94.56438229870928, 94.65273509526736, 94.49139520590043, 94.4721880762139, 94.3338967424708, 94.56054087277197, 94.70651505838967, 94.45682237246466]\n",
        "train_loss_list_cosine = [1.4922631353059113, 0.4829037673670425, 0.3722163860390826, 0.33177220926375245, 0.30060175129876227, 0.2739472426776964, 0.2571369293057499, 0.24421742562517565, 0.23246381484315323, 0.22585582048670064, 0.2115350698794776, 0.2027042203842786, 0.19336028488953586, 0.18796989869901803, 0.17635167322467335, 0.17350608562712425, 0.16392932351688705, 0.15824737119157786, 0.15103116432623812, 0.14280527628211148, 0.13911624957976465, 0.12778460819445814, 0.12645664817696503, 0.12117257786202963, 0.11404008718162048, 0.11024573898614261, 0.10301367604352918, 0.09962807324661957, 0.09590673247266429, 0.09140933892338822, 0.08890839097617563, 0.08312069913650028, 0.07912878103322049, 0.07539688910895247, 0.0703017507051307, 0.06730901317161112, 0.06545131962347604, 0.06006853946239806, 0.05934957680948644, 0.055661115047034776, 0.05501487654656535, 0.04808283364715756, 0.048917624036854686, 0.04978224897863177, 0.04599738403836765, 0.043476243917943865, 0.04154425250810538, 0.044289368833225914, 0.039924735993999975, 0.038134503976484824, 0.03573395804436448, 0.03633099462935413, 0.03407486775469762, 0.03466019954098273, 0.03175460545031914, 0.030751291346965928, 0.02861118235254312, 0.026954070920645837, 0.026345375887917098, 0.027870583739497655, 0.02564559653584358, 0.02595152790995118, 0.02664908538916373, 0.017370514487667858, 0.02682043477352759, 0.02288949422978072, 0.023653404380517905, 0.021177587269342865, 0.01825827019425427, 0.020935813946993946, 0.02142309610224594, 0.021006980622225117, 0.017297085196421504, 0.020975425832167893, 0.016085938927383088, 0.019187606733535582, 0.016529617174421272, 0.017658664713450265, 0.017461861724943822, 0.01473946469002416, 0.013888732157426767, 0.018813401251478592, 0.018245284999345322, 0.013429369967483517, 0.013213716673361565, 0.014179550147890478, 0.013165565478723676, 0.013442610755386424, 0.011938090494525969, 0.013952531346849035, 0.012777711567658235, 0.01361946394184095, 0.01415981548655886, 0.0132249127186142, 0.008202303269795845, 0.013768190907590946, 0.01187117434464689, 0.011072834069390408, 0.011375907309212407, 0.008056683960140176, 0.010813868758558904, 0.010755819991023294, 0.011055263281323623, 0.009854590676922565, 0.009897416544746738, 0.007694702545891319, 0.010974907935630917, 0.009073475042950686, 0.00957946586983009, 0.008521491170432235, 0.009181163735296882, 0.008033073926435217, 0.0075308985053745, 0.007736907337884813, 0.008905222457779166, 0.0075535059801524005, 0.008956405532912782, 0.00652504011866664, 0.007371700082288889, 0.007105531808951359, 0.009608815764438242, 0.006718999051766881, 0.006360923469938631, 0.006009158956909616, 0.005984226889401126, 0.006414240613540077, 0.008046470651133564, 0.006813095370703897, 0.004941387222356937, 0.006961080895634556, 0.0061504495415253034, 0.0044804993527444515, 0.005129817030924186, 0.0051584241011990275, 0.005904302868943308, 0.007040096893751609, 0.006497695825108162, 0.006231063241578372, 0.0045172427992303645, 0.004859256595798965, 0.005357292924210756, 0.0036115553579709734, 0.004059441841032762, 0.003976077818785122, 0.004686579279045762, 0.0047765986038821135, 0.0052203105924466305, 0.004452093922426263, 0.00366650621972908, 0.004813907496713725, 0.003619882242262452, 0.003379426909955734, 0.003642032724296165, 0.003996376050219234, 0.0027063930590780893, 0.0038246032934270147, 0.0035842454059730284, 0.0031291756938454703, 0.003011805406823518, 0.004053137657470737, 0.003489878551598058, 0.002363212944410382, 0.003003648613003568, 0.002955618500260455, 0.002723948648240625, 0.0037992216377342554, 0.0022372855783796795, 0.002972214323449715, 0.0029076020603400095, 0.002062946104120749, 0.0018581666937362376, 0.002313281514427136, 0.0030319557007007047, 0.0023327526890917524, 0.001145549262753536, 0.0016048090303144983, 0.0012414230531554533, 0.0032581396387386516, 0.0029362018629135516, 0.001846583708184242, 0.0012200965248589121, 0.001480849612214921, 0.002236391388236972, 0.0015825234965880586, 0.0009787926420108245, 0.001177633173575748, 0.0017891079141621141, 0.001687879517755926, 0.0027874750058864543, 0.0015881228617325382, 0.0013229923813940598, 0.0013549324787301051, 0.0013446073458035835, 0.0015000026920124656, 0.0012523066383357765, 0.000858102498131121, 0.001035546338261591, 0.0019881639752974524, 0.0016324728715111814, 0.001434380197954095, 0.0009871888154247414, 0.0008098082351434123, 0.0006027996978315856, 0.0008483088922217938, 0.0007788997386943644, 0.0014305688407895846, 0.0007751793022755421, 0.0005314110757599325, 0.0008008613197451418, 0.0005263145778683319, 0.0005657160187561332, 0.0008550505100606119, 0.0004061993359216422, 0.0008238756363850807, 0.0004022425374623086, 0.0007475109217440491, 0.0008010522570415786, 0.0008719455779264424, 0.0008102537067081855, 0.0006836875085969626, 0.000508847326879101, 0.00048076220441707043, 0.0003452522178527452, 0.0005887742460472115, 0.0007575123468680768, 0.0007316459814930954, 0.0006069562958639393, 0.00040482217438505993, 0.000413289390024444, 0.0002512041357034269, 0.00035321101188540014, 0.0002289985919775472, 0.00031013075728369315, 0.00048718760862263036, 0.00024815363102110633, 0.0002127035124524638, 0.00040036594692248164, 0.00022304668872061623, 0.0002539508311412804, 0.00033001078329677927, 0.0005046333238317957, 0.0002472649033615957, 0.00045928622355630194, 0.00022344069632032276, 0.00026002821110843074, 0.00015903297622683538, 0.00040398000405654787, 0.0002816041986039678, 0.00024018690979448482, 0.00029882328487923265, 0.00034577957207554183, 0.00028206157859465874, 0.00017730721445880266, 0.00011439717115794378, 0.00023043857095496602, 0.00013369225500067483, 0.00022097409422235014, 0.00013705560435689807, 0.000107482878806249, 0.0002181373778419168, 9.028549420162597e-05, 8.780925534434591e-05, 0.00015708037552040194, 0.00013996531600239778, 0.00015848159901898015, 9.64823931015096e-05, 0.00021168788567095436, 0.000136050276398834, 0.00010472044780222292, 0.0002206477002030788, 0.00013934975601720095, 0.00010074895917807085, 9.32180454911494e-05, 0.0001472607405713623, 0.00011569483834266491, 7.036176643037457e-05, 0.0001351957850342979, 0.000159376835863944, 0.00010674569268919185, 0.0001185344251126515, 8.718404802724013e-05, 0.00011900450702660869, 0.00012125460894170754, 0.00011230571605721275, 0.00010454212349596384, 0.0001487989837777661, 6.830416546227417e-05, 6.393169394098347e-05, 0.00020005663507281302, 0.00010431355027100938, 0.00017086659031600928, 7.99108349451789e-05, 0.00011386882648142751, 9.478551902827757e-05, 0.00010300004370941282, 8.344318782375742e-05, 0.0001274786575664539, 8.525987476745569e-05, 7.452874058836112e-05, 0.00011550953129777401]\n",
        "train_acc_list_cosine = [47.73742721016411, 84.54632080465855, 88.4425622022234, 89.96717840127052, 90.86712546320804, 91.64849126521969, 92.33245103229221, 92.79195341450503, 93.21757543673901, 93.32980412916888, 93.78295394388566, 94.04129168872419, 94.3928004235045, 94.53679195341451, 94.81206987824245, 95.05134992059291, 95.20169401799895, 95.48755955532027, 95.65484383271573, 95.92165166754897, 95.9640021175225, 96.39385918475384, 96.32186341979883, 96.55479089465325, 96.74748544203283, 96.74748544203283, 97.07146638433034, 97.24722075172049, 97.19640021175225, 97.32980412916888, 97.39332980412917, 97.62413975648491, 97.67496029645315, 97.79777660137638, 97.90365272631021, 97.96717840127052, 98.07940709370037, 98.21492853361568, 98.19163578613023, 98.32715722604553, 98.32292218104817, 98.55584965590259, 98.57702488088935, 98.43938591847538, 98.58973001588141, 98.68713605082054, 98.66807834833246, 98.61302276336686, 98.80359978824775, 98.78454208575967, 98.83748014822658, 98.80783483324511, 98.86077289571202, 98.89253573319216, 98.9433562731604, 99.02805717310747, 99.05982001058761, 99.13816834303864, 99.14452091053468, 99.0979354155638, 99.13393329804128, 99.1148755955532, 99.10217046056114, 99.45579671784013, 99.15510852302806, 99.22710428798305, 99.22498676548439, 99.32451032292218, 99.39650608787719, 99.32662784542086, 99.27368978295394, 99.3774483853891, 99.41132874536792, 99.29698253043938, 99.45367919534145, 99.3668607728957, 99.47061937533087, 99.43885653785071, 99.43250397035469, 99.51932239280042, 99.5404976177872, 99.33509793541556, 99.3774483853891, 99.5299100052938, 99.54473266278454, 99.5129698253044, 99.57014293276866, 99.57014293276866, 99.6294335627316, 99.48967707781895, 99.55320275277924, 99.5659078877713, 99.53202752779248, 99.58073054526204, 99.73742721016411, 99.53838009528852, 99.61461090524087, 99.6294335627316, 99.63366860772896, 99.7289571201694, 99.63155108523029, 99.64213869772367, 99.61884595023822, 99.71201694017999, 99.69084171519323, 99.74589730015882, 99.63790365272631, 99.72260455267337, 99.69719428268925, 99.72260455267337, 99.66754896770779, 99.70354685018528, 99.73954473266278, 99.72683959767072, 99.67601905770249, 99.73319216516676, 99.71625198517734, 99.80518793012176, 99.7289571201694, 99.77130757014294, 99.6929592376919, 99.7924827951297, 99.77766013763896, 99.784012705135, 99.80730545262044, 99.78613022763366, 99.76071995764956, 99.79036527263102, 99.83906829010058, 99.76707252514558, 99.7924827951297, 99.86659608258337, 99.84118581259926, 99.8284806776072, 99.79883536262572, 99.77977766013764, 99.79671784012704, 99.79671784012704, 99.83059820010588, 99.84965590259397, 99.8369507676019, 99.86871360508205, 99.89412387506617, 99.87083112758073, 99.85177342509265, 99.85177342509265, 99.81365802011646, 99.85389094759132, 99.87294865007941, 99.84118581259926, 99.87294865007941, 99.89200635256749, 99.88353626257279, 99.87294865007941, 99.90682901005823, 99.87506617257809, 99.87506617257809, 99.89624139756485, 99.90047644256221, 99.85812599258867, 99.88988883006881, 99.91953414505029, 99.90894653255691, 99.89412387506617, 99.90047644256221, 99.88988883006881, 99.92165166754897, 99.88777130757015, 99.90047644256221, 99.92588671254632, 99.93223928004235, 99.91953414505029, 99.88565378507147, 99.91953414505029, 99.95764955002647, 99.94917946003176, 99.95341450502912, 99.90471148755955, 99.91529910005293, 99.94070937003706, 99.95764955002647, 99.95764955002647, 99.93435680254103, 99.94706193753309, 99.9724722075172, 99.96611964002118, 99.9364743250397, 99.94282689253573, 99.91953414505029, 99.94706193753309, 99.94917946003176, 99.96188459502382, 99.95341450502912, 99.94917946003176, 99.95129698253044, 99.96823716251986, 99.97458973001588, 99.9364743250397, 99.9555320275278, 99.95129698253044, 99.9640021175225, 99.97458973001588, 99.97458973001588, 99.9640021175225, 99.97882477501324, 99.94706193753309, 99.97458973001588, 99.97882477501324, 99.9724722075172, 99.98094229751192, 99.98305982001058, 99.96188459502382, 99.98305982001058, 99.97035468501853, 99.98517734250926, 99.97458973001588, 99.95976707252514, 99.9640021175225, 99.9640021175225, 99.97670725251456, 99.98729486500794, 99.97670725251456, 99.98305982001058, 99.97670725251456, 99.97882477501324, 99.97882477501324, 99.97670725251456, 99.98305982001058, 99.98729486500794, 99.98517734250926, 99.98941238750662, 99.98941238750662, 99.98941238750662, 99.98729486500794, 99.98941238750662, 99.9915299100053, 99.98094229751192, 99.9915299100053, 99.98517734250926, 99.98094229751192, 99.98729486500794, 99.98941238750662, 99.98094229751192, 99.98941238750662, 99.98305982001058, 99.99364743250398, 99.98517734250926, 99.98729486500794, 99.98941238750662, 99.9915299100053, 99.98094229751192, 99.98305982001058, 99.9915299100053, 99.99576495500264, 99.98517734250926, 99.9915299100053, 99.98941238750662, 99.99576495500264, 99.99576495500264, 99.98941238750662, 99.99364743250398, 100.0, 99.9915299100053, 99.9915299100053, 99.98941238750662, 99.99576495500264, 99.98729486500794, 99.99576495500264, 99.99364743250398, 99.9915299100053, 99.99364743250398, 99.99364743250398, 99.99576495500264, 99.98941238750662, 99.9915299100053, 99.99788247750132, 99.98941238750662, 99.9915299100053, 99.99788247750132, 99.99576495500264, 99.99576495500264, 99.99364743250398, 99.9915299100053, 99.99576495500264, 99.99364743250398, 99.99364743250398, 99.99788247750132, 100.0, 99.98941238750662, 99.99576495500264, 99.99364743250398, 99.99576495500264, 99.9915299100053, 99.99576495500264, 99.99364743250398, 99.99576495500264, 99.9915299100053, 99.99788247750132, 99.99576495500264, 99.99576495500264]\n",
        "test_loss_list_cosine = [0.8128060245630788, 0.43769084055926283, 0.3670157931160693, 0.3468363919094497, 0.320717461845454, 0.29874755628407, 0.2880513122414841, 0.27089574385215254, 0.2712526289636598, 0.25813622246770296, 0.27607545629143715, 0.2508813175281473, 0.23157318663217275, 0.23963026514313385, 0.23259784851004095, 0.21799716919514478, 0.2553474395992417, 0.23297278822271847, 0.23411084861293727, 0.24059130224015782, 0.22228851220479198, 0.23491562519441633, 0.23474602972832964, 0.24284341398115253, 0.2531394691699568, 0.2231898330809439, 0.23778781842659502, 0.2260036576612323, 0.2408501879476449, 0.2531653588601187, 0.2437402953521586, 0.25621635543511195, 0.25155149374668506, 0.25073284860335143, 0.24820336959708264, 0.2527083886933385, 0.265060295743466, 0.2638632501977697, 0.27953584988911945, 0.28330178079469237, 0.2898024614425559, 0.285278165183377, 0.28805743263322203, 0.29073836449898927, 0.3027702011898452, 0.3096775676069014, 0.30173745603921515, 0.3021223259468873, 0.30385272365574745, 0.3038429137702812, 0.31642839670473455, 0.31861640270068947, 0.3238279426963452, 0.3290770745598802, 0.3349987664123011, 0.3213929162525079, 0.33029763011590524, 0.34525904689422426, 0.3447993094028503, 0.330581444653445, 0.321795464146371, 0.3393226447423883, 0.33919910747813536, 0.36219359443102983, 0.34984895151437206, 0.34508059543155717, 0.3312660858683361, 0.3478064838449891, 0.37045160054649207, 0.36637233636871563, 0.3615635437069132, 0.34741060503338483, 0.36895488022698786, 0.3510999241478595, 0.3899413192069487, 0.3609655104285362, 0.3731252212287383, 0.38873775874940203, 0.3712333329387155, 0.37048297439354894, 0.39439879441816433, 0.373478519492874, 0.36712522848564033, 0.3914665420420979, 0.38469330903471394, 0.3913668092230664, 0.38145966287337096, 0.37784753624788103, 0.38290000464036766, 0.3881754014622785, 0.39466715339279057, 0.38420211229765533, 0.3928751589150588, 0.3871026979576723, 0.39617241206852827, 0.42028651262323063, 0.38088225707521334, 0.3810242618087168, 0.38620046643978534, 0.41002833922667536, 0.40598774530157883, 0.4145270490324965, 0.4107616166036357, 0.39369509952124576, 0.3983155651949346, 0.4124191829971239, 0.40144294117778245, 0.4075972944425017, 0.41423957538791, 0.4235737093589634, 0.4338786544680011, 0.41868019625818464, 0.4211389536400983, 0.42915419865783083, 0.43854167406428973, 0.44444839669135855, 0.4239410953870153, 0.4140438513154639, 0.4214930013826519, 0.4469692508014394, 0.42807763744620425, 0.4140391830473627, 0.40559402319109616, 0.4296429092794949, 0.447968863443855, 0.4537148197985017, 0.4160180985562357, 0.42932759126757875, 0.4655364652474721, 0.44017627959450084, 0.4476152099646153, 0.45807484502666723, 0.4661675389518267, 0.46650424552624864, 0.4483312015951264, 0.43905310305839806, 0.4307253006571794, 0.4221340075886224, 0.43024291792957514, 0.4534414633743319, 0.439005637879246, 0.455770535284982, 0.4492703357884916, 0.46906485925337266, 0.4463952654680493, 0.4687405661497192, 0.4440763243108842, 0.4453616064936653, 0.466629167502819, 0.4585120371433304, 0.45512742501017, 0.4547710154278606, 0.46565503765847166, 0.47993873200361054, 0.46249571734783695, 0.47604353346076667, 0.4764781709778689, 0.4798887720623729, 0.47362343249294686, 0.477798045156341, 0.475341152007162, 0.46233028874677773, 0.47649588846765895, 0.4905768512090778, 0.49597619045023605, 0.4826731762720966, 0.46256263993963526, 0.4635071344308409, 0.4836937281412675, 0.4737923650753538, 0.47685492698641896, 0.49306667890107514, 0.5079087016615542, 0.47638218596294596, 0.47670910018496215, 0.5044073596980203, 0.5164688170211864, 0.5278736670528922, 0.4981932266542286, 0.4995226736783105, 0.5136372742845732, 0.47614785384697217, 0.5164846121424845, 0.4969261799825985, 0.504426286427998, 0.5065735648545966, 0.5321676667674682, 0.5085760319283615, 0.5058678923307133, 0.505933805387102, 0.5226816843154237, 0.5136628734705714, 0.49541927424862103, 0.5014571096729853, 0.5036171923942782, 0.4995590589264883, 0.5177521720453275, 0.5068038706028578, 0.5099713094623796, 0.4981300873846254, 0.5104206896560523, 0.5262805088377539, 0.5344062875499767, 0.5306157725908812, 0.5357992586351055, 0.5134817360370767, 0.5160781835849562, 0.5250964877469575, 0.5374465012448091, 0.5358220952946473, 0.5378625886514783, 0.5421092164064711, 0.5452210189199403, 0.5411839835076392, 0.5549019162050065, 0.5455894065546054, 0.5472353847934773, 0.5522625445497825, 0.5455132025983367, 0.538114388518985, 0.5509232937326363, 0.536594347077577, 0.5530879087015694, 0.5591187921909652, 0.5578411847065368, 0.5505201403601163, 0.564152417302716, 0.5473365803345052, 0.5458330702511411, 0.5650019636080947, 0.5671965913600562, 0.5513512968934853, 0.5562908629117552, 0.5620296227371356, 0.5617975970529312, 0.5581122653555426, 0.5675084015614736, 0.5657756982859699, 0.5526409040846606, 0.5721038452220634, 0.5690990948042942, 0.5608643038853474, 0.5609114988775556, 0.560604001846692, 0.5697500870121397, 0.5728723204822517, 0.5595448732640886, 0.5570606883866632, 0.5764513230818671, 0.556780993596048, 0.5647389884220948, 0.5543315944429624, 0.5563175826808255, 0.5460904273889301, 0.5574481183030716, 0.552725079560689, 0.5607348622220075, 0.5605667959076955, 0.5626120614498515, 0.5649992083380507, 0.5637176143309083, 0.5681553194373526, 0.5555440176634446, 0.5632415316604059, 0.5714907293874478, 0.5667051952526284, 0.5770144956528812, 0.5655391789224072, 0.57731314366444, 0.5691993516018413, 0.5740196594384079, 0.5696190810367446, 0.5625503671834332, 0.5508576962696251, 0.5676857517648708, 0.575283505166333, 0.5721206341997958, 0.5749247564988977, 0.5699750101832929, 0.580029735533411, 0.5632244463092374, 0.5753009883919731, 0.5712149130710054, 0.5708004219147066, 0.5698616763567734, 0.5771289975888495, 0.577879674463332, 0.5692181185472245, 0.591953951887102, 0.5678756476480368, 0.5756764801849118, 0.578166978474816, 0.5680953554498652, 0.5808629620039616, 0.5650976837804431, 0.5722748214018294, 0.5711645722097042, 0.5751072838172024, 0.5794820262486681, 0.5856684064892047]\n",
        "test_acc_list_cosine = [75.21511985248924, 86.48970497848802, 88.72157344806392, 89.39766441303011, 90.33113091579594, 91.21850030731407, 91.4259373079287, 92.23647818070067, 92.30562384757222, 92.6859250153657, 91.92532267977873, 92.7819606637984, 93.44652735095268, 93.21988322065151, 93.53872157344806, 93.85371850030731, 92.85494775660726, 93.56561155500921, 93.33512599877075, 93.48494161032575, 93.96896127842655, 93.75768285187462, 93.72311001843885, 93.5579287031346, 93.30439459127228, 94.19560540872772, 93.9459127228027, 94.2340196681008, 93.98048555623848, 93.700061462815, 94.24554394591273, 93.68085433312845, 93.82298709280884, 94.05731407498463, 94.24938537185002, 94.29164105716042, 93.83835279655808, 94.26090964966195, 94.03042409342348, 93.97280270436386, 94.00353411186232, 94.06883835279656, 94.13414259373079, 94.00737553779963, 94.12261831591887, 93.93822987092808, 94.16871542716656, 93.9958512599877, 93.96127842655194, 94.22249539028887, 93.93438844499079, 93.91518131530424, 94.0918869084204, 94.01121696373694, 94.15719114935465, 94.29164105716042, 94.31853103872157, 94.01889981561156, 94.15719114935465, 93.88060848186846, 94.54133374308543, 94.34542102028273, 94.14950829748003, 94.09572833435772, 94.36078672403197, 94.21481253841426, 94.3262138905962, 94.27627535341118, 93.92286416717886, 94.05731407498463, 94.09572833435772, 94.41456668715428, 94.24554394591273, 94.31853103872157, 94.24170251997542, 94.06115550092194, 94.0419483712354, 93.95359557467732, 94.30316533497235, 94.42224953902888, 94.1917639827904, 94.02274124154886, 94.43761524277812, 94.12645974185618, 94.20712968653964, 94.40304240934235, 94.35694529809466, 94.29164105716042, 94.48371235402581, 94.13030116779349, 94.31853103872157, 94.3338967424708, 94.41840811309157, 94.66041794714198, 94.48755377996312, 94.09956976029503, 94.26090964966195, 94.34157959434542, 94.38767670559312, 94.4798709280885, 94.38767670559312, 94.36078672403197, 94.16871542716656, 94.64505224339274, 94.36846957590657, 94.48371235402581, 94.28011677934849, 94.46450522433928, 94.28779963122311, 94.36078672403197, 94.35694529809466, 94.55285802089736, 94.35310387215735, 94.48755377996312, 94.34926244622004, 94.47602950215119, 94.64889366933005, 94.46450522433928, 94.44145666871543, 94.54133374308543, 94.52980946527352, 94.54901659496005, 94.69883220651506, 94.58743085433314, 94.16871542716656, 94.41456668715428, 94.56438229870928, 94.5759065765212, 94.21097111247695, 94.54517516902274, 94.44529809465274, 94.46834665027659, 94.27627535341118, 94.34157959434542, 94.71035648432698, 94.49139520590043, 94.60663798401967, 94.75645359557468, 94.74108789182544, 94.66041794714198, 94.59895513214505, 94.66041794714198, 94.57974800245852, 94.57974800245852, 94.71035648432698, 94.54901659496005, 94.7679778733866, 94.7679778733866, 94.56438229870928, 94.67194222495391, 94.66425937307929, 94.74492931776275, 94.79486785494775, 94.44529809465274, 94.73724646588813, 94.43761524277812, 94.72956361401353, 94.72572218807622, 94.74877074370006, 94.66041794714198, 94.4798709280885, 94.80639213275968, 94.61432083589429, 94.50676090964966, 94.70651505838967, 94.70651505838967, 94.59895513214505, 94.78334357713584, 94.76413644744929, 94.79870928088506, 94.69883220651506, 94.59511370620774, 94.42609096496619, 94.87169637369392, 94.86785494775661, 94.74492931776275, 94.74108789182544, 94.29932390903504, 94.86785494775661, 94.84864781807006, 94.68346650276582, 94.94852489244008, 94.69499078057775, 94.82175783650891, 94.86017209588199, 94.89090350338046, 94.6258451137062, 94.82559926244622, 94.83328211432084, 94.90242778119237, 94.88322065150584, 94.89090350338046, 94.96004917025199, 94.87553779963122, 94.9139520590043, 94.91779348494161, 94.99462200368777, 94.94852489244008, 94.85248924400737, 94.83712354025815, 94.90626920712968, 94.76029502151198, 94.78334357713584, 94.99078057775046, 94.88706207744315, 95.02151198524892, 94.97925629993854, 94.97541487400123, 94.87553779963122, 94.97541487400123, 94.88706207744315, 94.76413644744929, 94.84096496619546, 94.9638905961893, 94.96004917025199, 95.01382913337432, 94.99846342962508, 94.91011063306699, 94.92163491087892, 94.93315918869084, 94.9139520590043, 95.12907191149354, 95.03687768899816, 94.8640135218193, 94.82559926244622, 94.98693915181315, 94.90626920712968, 94.95236631837739, 94.98309772587585, 94.92931776275353, 94.74877074370006, 95.11754763368162, 95.02151198524892, 95.15596189305471, 94.98693915181315, 94.89858635525508, 94.99462200368777, 94.98693915181315, 95.07145052243392, 94.9638905961893, 94.97925629993854, 94.9638905961893, 94.98309772587585, 94.99462200368777, 94.95236631837739, 95.059926244622, 95.01767055931161, 95.11754763368162, 95.02919483712354, 95.13291333743085, 95.09834050399509, 95.109864781807, 95.159803318992, 95.16748617086662, 94.98693915181315, 95.22126613398893, 95.06376767055932, 95.12138905961893, 95.1521204671174, 95.04071911493547, 95.16364474492931, 95.17132759680393, 95.09834050399509, 95.1521204671174, 95.08681622618316, 95.059926244622, 95.20974185617702, 95.059926244622, 95.02151198524892, 95.14443761524278, 95.109864781807, 95.12907191149354, 95.17901044867855, 95.20974185617702, 95.17132759680393, 95.17132759680393, 95.19437615242778, 95.02151198524892, 95.17132759680393, 95.12907191149354, 95.19437615242778, 95.19821757836509, 95.12907191149354, 95.09449907805778, 95.109864781807, 95.24431468961278, 95.12907191149354, 95.16748617086662, 95.01382913337432, 95.22894898586355, 95.06760909649662, 95.11370620774431, 95.21742470805162, 95.159803318992, 95.22894898586355, 95.14059618930547, 95.109864781807, 95.14827904118009, 95.13675476336816, 95.09449907805778]\n",
        "train_loss_list_step = [1.5044204033810271, 0.49208608810817644, 0.37830603914209177, 0.33356322168415475, 0.30051283241529775, 0.2731129997548695, 0.25931339448464275, 0.24751879757416603, 0.23341218238762076, 0.22551919120114025, 0.21245744068246225, 0.20461816869696303, 0.19351729513385754, 0.18888680500591673, 0.1790498158513369, 0.17313903582794718, 0.16563977062863708, 0.15938490460599017, 0.1541340289319434, 0.14407484958246147, 0.13983443950815252, 0.13245242257630277, 0.12638695052443805, 0.12125968426304458, 0.11788717211605249, 0.11147532753424269, 0.10493720971996869, 0.10114793143257862, 0.0986312177905505, 0.09325535373060603, 0.06156983597173802, 0.050082290781091464, 0.04566037622696864, 0.042090283896077454, 0.037587769078418896, 0.03525592588446025, 0.034040165755286976, 0.031216950466235478, 0.030938032096463855, 0.029107897011765093, 0.02691679777953582, 0.024817724369887797, 0.02327188092970408, 0.022008026162917072, 0.021083637177603455, 0.020199724932410693, 0.020445302296316296, 0.01811823503557229, 0.016885934063002497, 0.016305384605991728, 0.015734232263797315, 0.01582022506202922, 0.01498541162761943, 0.013855328222454038, 0.012501976389670114, 0.011526370763357194, 0.012135067156394237, 0.011748958516110058, 0.01063293406628148, 0.010577014861956521, 0.009060478683824204, 0.00822005160518879, 0.007903504668233972, 0.0073660259777244665, 0.007953700653634155, 0.006759841606805863, 0.0069558188274406844, 0.007026840372753726, 0.006894602971896024, 0.0063807274687209935, 0.006584282455781946, 0.006663436767551165, 0.006154599042466102, 0.006103539849245962, 0.006048463034234704, 0.00614482418276391, 0.006322457585395453, 0.006028014029933677, 0.006245754332408476, 0.00581059281254909, 0.005709030747089944, 0.005923201890116833, 0.00537758718227695, 0.005807922808844647, 0.005243748521360034, 0.004974289045318631, 0.005323306708443634, 0.005775554073696229, 0.005093053230784864, 0.005536427160324011, 0.0052108629591284105, 0.004749226128792285, 0.005127630246161776, 0.005065120122888249, 0.004685268356975264, 0.005243820296808683, 0.0049121043785080205, 0.005093204319377071, 0.005370734221114888, 0.005008859195898547, 0.005249464887712999, 0.004638611593474268, 0.004728934781005616, 0.005035177127017733, 0.004871168427265863, 0.005197190184529169, 0.004969428170602996, 0.004836069228517858, 0.005308753678475464, 0.004877718586694237, 0.0046394178739258, 0.004715173941094246, 0.005071522070231865, 0.005326596872854554, 0.004782932753934018, 0.00452435254143787, 0.005128751716483132, 0.004405792564614916, 0.005072576416472445, 0.004683375086275378, 0.004745950062742184, 0.005199869099014116, 0.004980171865863766, 0.004840858398257126, 0.0046297258561474545, 0.004515150684752434, 0.005211585615480793, 0.0046955638231178315, 0.004485690257187711, 0.004824536544108968, 0.004908301922912914, 0.005288558260384356, 0.004324457162535601, 0.004497020012851448, 0.004832310812420976, 0.004629586771150122, 0.00469785524222389, 0.0051049072852895816, 0.004760202388700317, 0.00467975922823293, 0.004756802660750659, 0.004577458516745399, 0.0046352100029977686, 0.0045855785586362005, 0.0045302773612948275, 0.00527981271794404, 0.004842230399004512, 0.005128007588957292, 0.0047336897144657575, 0.00452429031405685, 0.004871112167589785, 0.004707888855899686, 0.004987144301444176, 0.004576647475924656, 0.0050210763291275344, 0.004869554478370365, 0.005145938421950075, 0.0049324849194394005, 0.005007630027365255, 0.004694439974090723, 0.005161294266647788, 0.005086089323139447, 0.0045723088694465674, 0.0049315163145030455, 0.005043683056833177, 0.004782498019489051, 0.005208640262013842, 0.004906343821498318, 0.004504839811582297, 0.004765471186647242, 0.004636792729359163, 0.0048650642755604795, 0.00440384292277286, 0.005331020985203442, 0.004309070902481564, 0.005132160800841049, 0.0048202084057818, 0.0042395592619208135, 0.004579358937877841, 0.004854094481356265, 0.004861267090400903, 0.004641431140822869, 0.004752275634260134, 0.005280723348089562, 0.0049866622946490744, 0.004713922302353166, 0.0049851976106240516, 0.0049522482596579456, 0.0047414097434517405, 0.005198805679831853, 0.005367297160049249, 0.004801289511232152, 0.004936103088396973, 0.004513620775553264, 0.005461517685133368, 0.005038318795404084, 0.004621399451298942, 0.004861979541443869, 0.004728190580562456, 0.00424972596940481, 0.004389661508080736, 0.004675524760544651, 0.004678315578823907, 0.005468495250993464, 0.004904287783485815, 0.004628283689890138, 0.004957924133196958, 0.004207717516179686, 0.004497190112673026, 0.0054004669855449725, 0.004738672002041044, 0.004832978636525801, 0.004593934473861766, 0.00428500973297249, 0.004800252157330026, 0.004579809123577946, 0.0047766751335504105, 0.004927973183699149, 0.003962443585616099, 0.005313499298157754, 0.005448946752569696, 0.004626204171514537, 0.00486521731347835, 0.00464669697920995, 0.00473364783223967, 0.0051715146905747635, 0.004559630797677459, 0.004807513805560901, 0.004414798407140251, 0.0055058369074104515, 0.004735360263094689, 0.00523965430700253, 0.004729446927025576, 0.0046202187999946675, 0.004473174700221971, 0.005083680975845281, 0.00473606128646736, 0.004358955291625135, 0.004462744230992882, 0.00445035025452959, 0.004661599569843557, 0.004881452044679059, 0.004753526896741532, 0.004433994836329803, 0.004789544894031266, 0.004920090539939323, 0.005374149948520697, 0.004787621462456112, 0.004773729829575745, 0.0044936602159881545, 0.004565336937149469, 0.004793149001421886, 0.004954056282359589, 0.00447947610881707, 0.004911790105014495, 0.004962967739427566, 0.004856595992693377, 0.0049429370942105965, 0.004495315723300048, 0.005116432885005868, 0.004663471084030195, 0.005105866048162296, 0.0048063792890729005, 0.004996042520567828, 0.0048235019935209174, 0.0052012274302681025, 0.004979272351725164, 0.004979745390991236, 0.005170751338667516, 0.004858217863982788, 0.004663840651548111, 0.005053441917670759, 0.004584236903877065, 0.004780888221675258, 0.004612757356412744, 0.004703525326821731, 0.005128545914503725, 0.005097627184756964, 0.004888794860495936, 0.004546376856262884, 0.0049637655482588335, 0.004738951851973477, 0.005256142429007358, 0.004507868475276601, 0.004676521270572799, 0.0042853716939203765, 0.004519075942008067, 0.004505532296253924, 0.004830542344546013, 0.004257976696643534, 0.004718432969060035, 0.004490681710655531, 0.004793198622190861, 0.004690485046958793, 0.004381172440937899, 0.004597538895479108, 0.0050219146489527905, 0.004799934776746264, 0.004984567811975109, 0.004093347564194513]\n",
        "train_acc_list_step = [47.394388565378506, 84.38962413975648, 88.32609846479619, 89.65802011646373, 90.93912122816305, 91.76071995764956, 92.30492323980943, 92.69878242456326, 93.1371095817893, 93.283218634198, 93.84859714134463, 93.97988353626258, 94.4203282159873, 94.56643726839597, 94.83324510322922, 95.06405505558496, 95.14452091053468, 95.38168343038645, 95.61461090524087, 95.86871360508205, 95.94282689253573, 96.23928004235044, 96.33456855479089, 96.51667548967708, 96.58231868713605, 96.79618845950239, 96.97829539438857, 97.11805187930122, 97.2006352567496, 97.23451561672843, 98.32927474854421, 98.71254632080466, 98.81630492323981, 98.94970884065643, 99.04499735309687, 99.05558496559026, 99.12969825304394, 99.22075172048703, 99.16993118051879, 99.24616199047115, 99.32027527792482, 99.33721545791424, 99.38168343038645, 99.40285865537321, 99.45156167284277, 99.46426680783483, 99.43885653785071, 99.49814716781366, 99.51932239280042, 99.55320275277924, 99.58073054526204, 99.56379036527264, 99.56379036527264, 99.6209634727369, 99.65907887771307, 99.68237162519851, 99.61884595023822, 99.62731604023293, 99.69507676019057, 99.72260455267337, 99.73742721016411, 99.7564849126522, 99.76919004764426, 99.79883536262572, 99.7649550026469, 99.83906829010058, 99.80307040762308, 99.81154049761778, 99.81154049761778, 99.83271572260455, 99.8009528851244, 99.82424563260984, 99.8369507676019, 99.82424563260984, 99.83906829010058, 99.83483324510323, 99.8369507676019, 99.84965590259397, 99.82636315510852, 99.83059820010588, 99.83906829010058, 99.84118581259926, 99.86236103758603, 99.85600847008999, 99.86236103758603, 99.87083112758073, 99.8369507676019, 99.8284806776072, 99.84753838009529, 99.85600847008999, 99.85389094759132, 99.87083112758073, 99.85812599258867, 99.86024351508735, 99.88353626257279, 99.85600847008999, 99.87506617257809, 99.85600847008999, 99.86236103758603, 99.84753838009529, 99.85600847008999, 99.87930121757543, 99.86236103758603, 99.8644785600847, 99.85177342509265, 99.86024351508735, 99.85389094759132, 99.85812599258867, 99.84330333509793, 99.86024351508735, 99.87718369507677, 99.86871360508205, 99.85812599258867, 99.84753838009529, 99.86659608258337, 99.87083112758073, 99.85389094759132, 99.88988883006881, 99.86024351508735, 99.87718369507677, 99.86659608258337, 99.84118581259926, 99.87083112758073, 99.85812599258867, 99.88777130757015, 99.88988883006881, 99.84542085759661, 99.87294865007941, 99.87083112758073, 99.85177342509265, 99.86024351508735, 99.86236103758603, 99.87506617257809, 99.8644785600847, 99.86024351508735, 99.86659608258337, 99.86871360508205, 99.85600847008999, 99.87083112758073, 99.87294865007941, 99.87294865007941, 99.88353626257279, 99.86236103758603, 99.87930121757543, 99.87930121757543, 99.86024351508735, 99.86024351508735, 99.83483324510323, 99.8644785600847, 99.89200635256749, 99.87083112758073, 99.84965590259397, 99.84965590259397, 99.88141874007411, 99.84753838009529, 99.87506617257809, 99.85389094759132, 99.85812599258867, 99.86236103758603, 99.85600847008999, 99.85600847008999, 99.84118581259926, 99.87930121757543, 99.86024351508735, 99.85812599258867, 99.86871360508205, 99.85389094759132, 99.86236103758603, 99.87718369507677, 99.85812599258867, 99.87083112758073, 99.85600847008999, 99.87506617257809, 99.87506617257809, 99.88777130757015, 99.85812599258867, 99.85600847008999, 99.88777130757015, 99.86659608258337, 99.87718369507677, 99.86659608258337, 99.86659608258337, 99.88141874007411, 99.85177342509265, 99.85600847008999, 99.87718369507677, 99.86236103758603, 99.87930121757543, 99.87506617257809, 99.85177342509265, 99.84118581259926, 99.86236103758603, 99.85812599258867, 99.87506617257809, 99.82636315510852, 99.85600847008999, 99.87718369507677, 99.88353626257279, 99.87294865007941, 99.89624139756485, 99.88353626257279, 99.87718369507677, 99.87083112758073, 99.84542085759661, 99.8644785600847, 99.88141874007411, 99.85600847008999, 99.89412387506617, 99.88141874007411, 99.85177342509265, 99.85389094759132, 99.87930121757543, 99.88565378507147, 99.88353626257279, 99.86871360508205, 99.86871360508205, 99.8644785600847, 99.86871360508205, 99.89624139756485, 99.84330333509793, 99.84542085759661, 99.88353626257279, 99.86236103758603, 99.87930121757543, 99.87506617257809, 99.87083112758073, 99.87294865007941, 99.87294865007941, 99.86871360508205, 99.84330333509793, 99.86659608258337, 99.84542085759661, 99.87506617257809, 99.8644785600847, 99.89200635256749, 99.84542085759661, 99.86871360508205, 99.87506617257809, 99.88141874007411, 99.87083112758073, 99.87083112758073, 99.86659608258337, 99.86659608258337, 99.86871360508205, 99.88565378507147, 99.87083112758073, 99.84753838009529, 99.88777130757015, 99.87718369507677, 99.88353626257279, 99.87083112758073, 99.86236103758603, 99.87718369507677, 99.88777130757015, 99.84965590259397, 99.85600847008999, 99.86236103758603, 99.85812599258867, 99.87718369507677, 99.8644785600847, 99.86024351508735, 99.87506617257809, 99.8644785600847, 99.8644785600847, 99.87930121757543, 99.85177342509265, 99.86024351508735, 99.85177342509265, 99.84118581259926, 99.86871360508205, 99.87718369507677, 99.86024351508735, 99.88988883006881, 99.8644785600847, 99.88141874007411, 99.87506617257809, 99.85177342509265, 99.86871360508205, 99.87506617257809, 99.87506617257809, 99.87930121757543, 99.86871360508205, 99.85812599258867, 99.88988883006881, 99.88353626257279, 99.89412387506617, 99.87930121757543, 99.86659608258337, 99.86659608258337, 99.88988883006881, 99.87083112758073, 99.87506617257809, 99.87083112758073, 99.87718369507677, 99.87930121757543, 99.87083112758073, 99.84330333509793, 99.87718369507677, 99.87294865007941, 99.88565378507147]\n",
        "test_loss_list_step = [0.7521707053278007, 0.41922322909037274, 0.3967930445191907, 0.35680990534670215, 0.3250766685049908, 0.30643289908766747, 0.27803952721696273, 0.26461896110399097, 0.26403681638047977, 0.2668783114309989, 0.2609864034708224, 0.25131638298797254, 0.24851202544774495, 0.24535505795011334, 0.23515790532909187, 0.23713360856488055, 0.2406249167215006, 0.24633904404061682, 0.22870721204169825, 0.23081522924350759, 0.23210598513776182, 0.23380596216256713, 0.23264140811036615, 0.2492445185597913, 0.24672274596477842, 0.23322659500819795, 0.2332557491848574, 0.2325650899324055, 0.2574915187433362, 0.23942345477567584, 0.21860766188953729, 0.22490006466122234, 0.22599033247588166, 0.2330661828027052, 0.23813083913980745, 0.2410290090677639, 0.25265564182408007, 0.25388093285408675, 0.25897152009694013, 0.2653597001095905, 0.267472947769634, 0.2757880237756991, 0.28352711534164116, 0.2891207381023788, 0.2937827925471699, 0.3016136303891008, 0.29683313752506296, 0.3059604737939605, 0.31242154799766986, 0.313900034299449, 0.3219801964347853, 0.33177046875889393, 0.3355082166837711, 0.33843776536192377, 0.34364481102309974, 0.3455076480613035, 0.34563379169569586, 0.35321818167051555, 0.35429473890576, 0.35551488774317297, 0.36010582940470354, 0.35364405492631096, 0.3636273516743791, 0.36364757276012327, 0.35665451670887277, 0.3549169595261999, 0.3703063836905594, 0.36219835961146246, 0.3605355029998749, 0.3578162646861564, 0.36774474406140106, 0.3633771698702784, 0.3653042366746448, 0.3691165424956411, 0.36504066177625577, 0.3715900047225695, 0.377250660302154, 0.3705682765610297, 0.3703488810170515, 0.37351591648606985, 0.37677567382343113, 0.37160208523638694, 0.3709423184650494, 0.3721590631769276, 0.3763345616011351, 0.3800408639488559, 0.37680745827874135, 0.3771674718214747, 0.3848353975142042, 0.3798498407590623, 0.3773468048494382, 0.3819653425073507, 0.3785710880149375, 0.37396970703654614, 0.37354924148131236, 0.38423583746029466, 0.3775230319622685, 0.36990976165614875, 0.3731101816017911, 0.3812584620443921, 0.3834761079017292, 0.38286398570327196, 0.3858671422372116, 0.3799955192816389, 0.3770175949378195, 0.3851937071338077, 0.38235137271968755, 0.38205398502303106, 0.38025479872400564, 0.3843826920378442, 0.38355313671533675, 0.3811237645043316, 0.37951354816665545, 0.3756716687428564, 0.38028262046110983, 0.38163883488296585, 0.38113248269712807, 0.38293040138395396, 0.38826611508414444, 0.3821931903378344, 0.3927168071534777, 0.3814124613087259, 0.38783683029788674, 0.384523262598497, 0.3777104146395098, 0.3834854347001323, 0.37351655306331083, 0.37643494215958256, 0.38091917734081837, 0.3816218973667014, 0.38416977602915436, 0.3851156278796421, 0.3824646536297366, 0.37491797317988146, 0.3818769982979432, 0.37861419655382633, 0.3842346754938583, 0.37655236199498177, 0.37527537754024654, 0.37987153332077844, 0.3830455071018899, 0.3794875234803733, 0.38347619624041457, 0.39262797689868834, 0.37537328259763764, 0.3836479137067263, 0.3833252173236699, 0.37210422399563386, 0.387080483238998, 0.3719306717816211, 0.37743640282446994, 0.3764378684846794, 0.37271541687568616, 0.37936363588361177, 0.3804472682075392, 0.371879458409168, 0.38374935142586336, 0.3781576618488294, 0.3732976428844838, 0.3825277503210065, 0.37921793430167083, 0.38190348801550034, 0.3811836604795912, 0.3839620574760963, 0.3847266497711341, 0.3755092674263698, 0.38182204500680755, 0.3782161524446279, 0.383978412504874, 0.3785792852560168, 0.37529283202728075, 0.38586872027200814, 0.37986113718144743, 0.3781509461410928, 0.385330743640296, 0.3866525995362477, 0.3780062812672672, 0.38378071400574315, 0.38093069549102115, 0.38920295537065935, 0.38671026251041424, 0.3797818386251582, 0.3798417299438049, 0.3740828680422376, 0.38145504804218516, 0.38200287843196123, 0.3875016655645096, 0.38280217152308016, 0.38698648213974984, 0.3808485120260978, 0.38649215569317924, 0.3831135995238654, 0.3805198727321683, 0.38389426964682105, 0.38486584148132336, 0.3807648518761876, 0.3849117392856701, 0.38554816227406263, 0.38357163151251333, 0.3802410184734446, 0.3815109624947403, 0.382021243195506, 0.3853015545938237, 0.377494075719048, 0.38711871966427447, 0.3774435311857173, 0.3905071147407095, 0.3806642469603057, 0.38146250298721535, 0.3838621310104488, 0.39097126777849944, 0.38649382303450625, 0.3796885114930132, 0.38229777593183895, 0.3814927838633166, 0.37877432031410874, 0.37605211639082897, 0.37402715621625676, 0.3860787514177169, 0.3734693876285033, 0.3757060868665576, 0.3836308258823028, 0.3818031575019453, 0.3754675344144012, 0.3817046451035376, 0.3788763102984019, 0.38524902579100695, 0.3789480691410455, 0.383543092720941, 0.39350814419780294, 0.3787268260402568, 0.3791520138837251, 0.3838299135951435, 0.3793616016559741, 0.3767985271874304, 0.3790097143866268, 0.37912912847583785, 0.38894346707007466, 0.3742955804382469, 0.38342078903909116, 0.3848988146238102, 0.37459168269061577, 0.38242888645561157, 0.3820874952601598, 0.38645922241951614, 0.37736037103276626, 0.37371054134450343, 0.3787742013148233, 0.3864014833873394, 0.38398269388605566, 0.3823276355716528, 0.38692381239368345, 0.38753888685731036, 0.37855833295878827, 0.37346514290673477, 0.37298866356814314, 0.3816173101830132, 0.3767242920311058, 0.3841047031056209, 0.38750297252965327, 0.369205586767445, 0.3753658247490724, 0.38275528518373475, 0.3800859667141648, 0.3870932775020015, 0.37551862878414494, 0.3840912418421723, 0.3726249620291021, 0.37941730108486454, 0.38384662088298915, 0.37976392629720707, 0.37775822057772207, 0.38208118103006306, 0.3800909294399853, 0.37907081163104844, 0.3812881442672555, 0.3761177863743083, 0.38076628101350485, 0.37963548230518607, 0.3693312196718419, 0.37269937157557875, 0.3810739266987452, 0.37098004634254705, 0.38030854867332997, 0.3809964715083148, 0.37681868622152537, 0.3750186215574835, 0.3816620926098788, 0.3736031973351012, 0.38245252669588026, 0.3819549302798787, 0.38419706475756626, 0.3735217913966991, 0.3787470100191878, 0.37715579814040195, 0.3867923977976555, 0.3815506379960068, 0.3814866198135503, 0.3853403181933305, 0.3877864530602214]\n",
        "test_acc_list_step = [75.81054087277197, 86.9660417947142, 87.58451137062077, 89.28242163491088, 90.1160110633067, 91.05331899200984, 91.73325138291334, 92.37092808850646, 92.390135218193, 92.24031960663798, 92.37092808850646, 92.9778733866011, 93.1737861094038, 93.04701905347265, 93.41963736939152, 93.37738168408113, 93.3159188690842, 93.0278119237861, 93.7077443146896, 93.72695144437616, 93.68853718500307, 93.78073140749846, 93.73847572218807, 93.26213890596189, 93.30055316533497, 93.93438844499079, 94.06883835279656, 94.11877688998156, 93.6040258143823, 94.0419483712354, 94.87553779963122, 94.83712354025815, 94.81023355869699, 94.87169637369392, 94.91779348494161, 94.89474492931777, 94.86785494775661, 94.84096496619546, 94.71419791026429, 94.56054087277197, 94.75645359557468, 94.73724646588813, 94.64889366933005, 94.64889366933005, 94.69499078057775, 94.72956361401353, 94.6757836508912, 94.74492931776275, 94.6220036877689, 94.65273509526736, 94.54901659496005, 94.53749231714812, 94.63352796558083, 94.61816226183159, 94.61432083589429, 94.5221266133989, 94.64121081745544, 94.56054087277197, 94.74492931776275, 94.61047940995698, 94.59127228027043, 94.75645359557468, 94.75645359557468, 94.66425937307929, 94.73340503995082, 94.6757836508912, 94.53365089121081, 94.64505224339274, 94.66041794714198, 94.70267363245236, 94.6258451137062, 94.75645359557468, 94.80255070682237, 94.67962507682851, 94.58358942839583, 94.54517516902274, 94.6258451137062, 94.7180393362016, 94.66425937307929, 94.6681007990166, 94.61432083589429, 94.78718500307313, 94.76029502151198, 94.68730792870313, 94.64121081745544, 94.62968653964352, 94.69114935464044, 94.58743085433314, 94.58358942839583, 94.64889366933005, 94.69114935464044, 94.70651505838967, 94.70267363245236, 94.66041794714198, 94.69883220651506, 94.61432083589429, 94.71419791026429, 94.82559926244622, 94.69883220651506, 94.72572218807622, 94.49523663183774, 94.75261216963737, 94.61047940995698, 94.64505224339274, 94.64889366933005, 94.66041794714198, 94.70267363245236, 94.55669944683467, 94.65657652120467, 94.6681007990166, 94.77566072526122, 94.85633066994468, 94.66041794714198, 94.78334357713584, 94.79102642901044, 94.63352796558083, 94.78718500307313, 94.64505224339274, 94.56438229870928, 94.73340503995082, 94.61047940995698, 94.68730792870313, 94.70267363245236, 94.61432083589429, 94.67194222495391, 94.77181929932391, 94.73724646588813, 94.64505224339274, 94.69499078057775, 94.65273509526736, 94.69883220651506, 94.6757836508912, 94.71035648432698, 94.69883220651506, 94.67194222495391, 94.66425937307929, 94.6757836508912, 94.76029502151198, 94.75645359557468, 94.70267363245236, 94.64505224339274, 94.74108789182544, 94.54133374308543, 94.54901659496005, 94.74877074370006, 94.67962507682851, 94.67194222495391, 94.67194222495391, 94.69883220651506, 94.8140749846343, 94.64121081745544, 94.7180393362016, 94.74492931776275, 94.78334357713584, 94.65273509526736, 94.5759065765212, 94.61816226183159, 94.61047940995698, 94.76413644744929, 94.79870928088506, 94.67962507682851, 94.69114935464044, 94.72572218807622, 94.60663798401967, 94.5720651505839, 94.75261216963737, 94.80255070682237, 94.70267363245236, 94.66041794714198, 94.66425937307929, 94.69114935464044, 94.72188076213891, 94.68730792870313, 94.55285802089736, 94.75645359557468, 94.64505224339274, 94.7180393362016, 94.53749231714812, 94.72572218807622, 94.65657652120467, 94.6258451137062, 94.61432083589429, 94.72188076213891, 94.77566072526122, 94.62968653964352, 94.66041794714198, 94.54901659496005, 94.6757836508912, 94.66425937307929, 94.55669944683467, 94.7679778733866, 94.74877074370006, 94.68346650276582, 94.63352796558083, 94.5759065765212, 94.61047940995698, 94.66425937307929, 94.71419791026429, 94.59895513214505, 94.61816226183159, 94.72956361401353, 94.7180393362016, 94.67194222495391, 94.68346650276582, 94.59127228027043, 94.76029502151198, 94.69499078057775, 94.66041794714198, 94.6681007990166, 94.69499078057775, 94.64889366933005, 94.67962507682851, 94.78718500307313, 94.67962507682851, 94.77950215119853, 94.6757836508912, 94.74492931776275, 94.84096496619546, 94.73340503995082, 94.72188076213891, 94.69883220651506, 94.61047940995698, 94.5720651505839, 94.72572218807622, 94.70651505838967, 94.6681007990166, 94.6258451137062, 94.64121081745544, 94.65657652120467, 94.48371235402581, 94.6757836508912, 94.73724646588813, 94.59895513214505, 94.70651505838967, 94.76029502151198, 94.75645359557468, 94.76413644744929, 94.6220036877689, 94.72188076213891, 94.7180393362016, 94.66425937307929, 94.6681007990166, 94.62968653964352, 94.71419791026429, 94.68346650276582, 94.69883220651506, 94.69114935464044, 94.70267363245236, 94.60663798401967, 94.67194222495391, 94.55285802089736, 94.59895513214505, 94.66041794714198, 94.61432083589429, 94.59895513214505, 94.73724646588813, 94.63352796558083, 94.73724646588813, 94.77566072526122, 94.60279655808236, 94.76413644744929, 94.74108789182544, 94.65273509526736, 94.58743085433314, 94.59511370620774, 94.80255070682237, 94.61047940995698, 94.71035648432698, 94.65273509526736, 94.6681007990166, 94.75261216963737, 94.72188076213891, 94.61816226183159, 94.65657652120467, 94.79870928088506, 94.64121081745544, 94.66425937307929, 94.69883220651506, 94.64889366933005, 94.83712354025815, 94.69883220651506, 94.71419791026429, 94.77181929932391, 94.63352796558083, 94.74492931776275, 94.69883220651506, 94.61816226183159, 94.73340503995082, 94.73724646588813, 94.67194222495391, 94.72956361401353, 94.69114935464044, 94.77181929932391, 94.65657652120467, 94.68346650276582, 94.60663798401967, 94.64889366933005, 94.72188076213891, 94.70651505838967, 94.70267363245236]\n",
        "train_loss_list_linear = [1.0505396097010067, 0.41031305952285363, 0.34015994405924144, 0.3038505317194029, 0.27671615290771007, 0.2539030461090044, 0.24293845730422312, 0.2313264879753919, 0.22205986383403867, 0.21279181394635177, 0.20017173882260877, 0.19267895106014196, 0.18516845405505603, 0.18218227440546844, 0.17281363284604012, 0.16757936995264475, 0.1608044776490064, 0.1526946394343363, 0.1503949542921087, 0.14460325269861435, 0.1379334318267338, 0.13074936200347212, 0.12652073063834654, 0.12361395870888137, 0.11955785878043027, 0.11495751238915171, 0.10918548092407586, 0.10549298947964741, 0.10151599690843081, 0.09746526491886312, 0.09452159306524084, 0.09095732697353857, 0.08772212413056352, 0.08348307219462667, 0.08420775942554884, 0.07539896097104885, 0.07707164484154443, 0.07315899130855473, 0.07151561531902047, 0.06873627238071506, 0.06587666986752452, 0.060232834865876696, 0.059981180607343754, 0.06175297403930204, 0.05708458852451951, 0.05296678836066225, 0.057111382709063976, 0.053133565748266894, 0.05266129370491073, 0.050845057070684346, 0.04849550221115351, 0.0469664877891379, 0.04695639163596419, 0.04652371517443136, 0.04273645039004221, 0.042981587787635805, 0.040151388551610795, 0.03779632822411049, 0.04038962408859879, 0.04276739005321198, 0.035970243897294085, 0.0389290425395164, 0.034595519276827996, 0.034754975707721986, 0.03550548981976138, 0.034968214722219, 0.033513369056052934, 0.033747713374943145, 0.03146642715614011, 0.03088276023026253, 0.03096695340504784, 0.030255960882325888, 0.029922214430077934, 0.026458163596317426, 0.030674233368527256, 0.026803990912770092, 0.028383832357679988, 0.029045142340322395, 0.026921882543196714, 0.025758919344589033, 0.027054627095336514, 0.026907914471629563, 0.02701804478293749, 0.026367548554233428, 0.0261913293705708, 0.02126370853306549, 0.023870387598386333, 0.02343241305678558, 0.026290576351723865, 0.024439686884867598, 0.02106365902915024, 0.022621461096346544, 0.022915108798849105, 0.026723837544939667, 0.021479926218290063, 0.022599881692684752, 0.02025709556021947, 0.019150306689010323, 0.0245601841217654, 0.02191897199554167, 0.01751039288937257, 0.019330726730524307, 0.019164910448808477, 0.024538845911642237, 0.015059424295682626, 0.020382445553086728, 0.022040061389306198, 0.018767614002140155, 0.019355695793985044, 0.019107737408797596, 0.017818392037986942, 0.017417942697963446, 0.021768966118750215, 0.017869346996114355, 0.017951215525590914, 0.01640004834187341, 0.01777675135988997, 0.019044938084980482, 0.015788522874557873, 0.022552578193564623, 0.019296770346089745, 0.014811130384146363, 0.015027894850487148, 0.018469126043487265, 0.018098258266041212, 0.014053442152699729, 0.015989337644331865, 0.018125206405116463, 0.016060621967932922, 0.014774189744400855, 0.01385672290374157, 0.018077892058706895, 0.01727216446030504, 0.014268980369684513, 0.015724211079724697, 0.016640492273927766, 0.014277730624145401, 0.016971004403142326, 0.014619164786088283, 0.016830329990812937, 0.01374343925469295, 0.012524507506886185, 0.01549116191725552, 0.013598875393053052, 0.0151209402418879, 0.014543565561276227, 0.01439005304061679, 0.01173475706005888, 0.016134371224896974, 0.011917180406560032, 0.012485065904832735, 0.0135999512963786, 0.01591276152397482, 0.011587380154317354, 0.009772636422980987, 0.01344329923276031, 0.015756404174912624, 0.014378462400149205, 0.014014234783006126, 0.012014912663851542, 0.01611480384725613, 0.012844832076313767, 0.013578993814619104, 0.011672752483787808, 0.014098815763512592, 0.012935566670426325, 0.015032568787632202, 0.012224024389351091, 0.014372231721552152, 0.011108738588597336, 0.011174887407980897, 0.016397042357398143, 0.007919598232576864, 0.011932518933157513, 0.012465056306042874, 0.012968409797498517, 0.012325529049170838, 0.011099239793170491, 0.011867815633636468, 0.012848886264346762, 0.012182228031996032, 0.011562556203482023, 0.010714672763688681, 0.013554639555182596, 0.012572727286869529, 0.01000095399527329, 0.011995266647266443, 0.011573533052920085, 0.012627172824654724, 0.013339461832399402, 0.010677528724291735, 0.011543188395016422, 0.010650319310586942, 0.009674460366150607, 0.010641892690030602, 0.012230279986791725, 0.01145689182891605, 0.01050705676828164, 0.00970218700554869, 0.011088927940358716, 0.010147759896973753, 0.0144252521625949, 0.009280225094541092, 0.0071248399373703905, 0.013807516299508516, 0.01255990872180107, 0.011576532107942895, 0.008876261175843192, 0.009988300831966212, 0.00985363164915767, 0.01084158176923064, 0.011923880645647825, 0.010890684340816107, 0.011154608579179295, 0.010417346872992216, 0.00848533340091224, 0.009280508183635208, 0.012399288785308195, 0.0072954312946646145, 0.007857306191068237, 0.013135696996073112, 0.007985612870672613, 0.009203406998578557, 0.011825267453099425, 0.009498726522558978, 0.010254976841146922, 0.012558721663513307, 0.007996204799592626, 0.007471766532690587, 0.01177365344033834, 0.010632048703338191, 0.01163114502161057, 0.008471069070895856, 0.008329393120841747, 0.010546008776983772, 0.007213423388353836, 0.009344507342992069, 0.01120725078885267, 0.008723668579122892, 0.008438797396795254, 0.009862698597215867, 0.011641287900165775, 0.009182740705942006, 0.00702334287046593, 0.010164535000019891, 0.008744396335386742, 0.009129313499368456, 0.008084669785307412, 0.010208096987843342, 0.011367271971373962, 0.007117051272343865, 0.008831601460626934, 0.00922884266564445, 0.009587360389017516, 0.008383878850809764, 0.007664171105706518, 0.010088089144474219, 0.007183663350923383, 0.010528553138330638, 0.010388069153280813, 0.008873608166863086, 0.007134666277026189, 0.00842848956719368, 0.008079520347828268, 0.009280271344362845, 0.008955260842407897, 0.010437573963957794, 0.007464409461843186, 0.00868264229136562, 0.008687793158964795, 0.008216140737795655, 0.007891892086514802, 0.007512410591053153, 0.008665417990315836, 0.008976994133145053, 0.008162402152780846, 0.008810677293791461, 0.006835815719566462, 0.007850608918254282, 0.010575828147088042, 0.008934358710317107, 0.0076927311314263745, 0.007455576667292384, 0.005771527193367357, 0.008088972039366073, 0.011438270648105747, 0.005829402718917685, 0.009437662637821316, 0.006043440636937449, 0.007854135620251515, 0.009180552848274084, 0.010004485826416422, 0.007954190094466415, 0.0069836680795018575, 0.006469878575636705, 0.008808609063802835, 0.007791922163181544, 0.006387679697369947, 0.009282423594551585, 0.007415674569260415]\n",
        "train_acc_list_linear = [64.05293806246691, 87.14875595553202, 89.55002646903124, 90.75913181577555, 91.62308099523557, 92.33456855479089, 92.87030174695606, 93.13922710428798, 93.25780836421387, 93.82106934886183, 94.23822128110112, 94.36315510852303, 94.60667019587083, 94.74007411328745, 95.01111699311805, 95.09158284806776, 95.25674960296453, 95.67178401270513, 95.61461090524087, 95.74377977766014, 95.9915299100053, 96.22869242985706, 96.29645314981472, 96.45526733721546, 96.42985706723134, 96.65431445209106, 96.86818422445738, 96.87241926945474, 97.08628904182108, 97.16040232927475, 97.2641609317099, 97.3361566966649, 97.35309687665432, 97.51191106405506, 97.51191106405506, 97.76601376389624, 97.7342509264161, 97.88035997882477, 97.82953943885654, 97.96717840127052, 98.00317628374802, 98.16622551614611, 98.13658020116463, 98.11540497617787, 98.1937533086289, 98.37374272101641, 98.19798835362626, 98.44150344097406, 98.33562731604023, 98.41821069348862, 98.43303335097936, 98.4923239809423, 98.5643197458973, 98.4923239809423, 98.66807834833246, 98.64902064584436, 98.69348861831656, 98.76971942826893, 98.68925357331922, 98.64690312334568, 98.82053996823716, 98.74219163578613, 98.81842244573849, 98.86500794070938, 98.88194812069878, 98.78877713075701, 98.90100582318688, 98.90312334568554, 98.96664902064585, 98.97723663313923, 98.98358920063525, 99.0428798305982, 99.01111699311805, 99.12122816304924, 98.98570672313393, 99.10852302805718, 99.09158284806776, 99.05134992059291, 99.1233456855479, 99.1868713605082, 99.0873478030704, 99.10217046056114, 99.08523028057174, 99.13181577554262, 99.11064055055584, 99.33721545791424, 99.22710428798305, 99.17628374801482, 99.0979354155638, 99.20169401799895, 99.2694547379566, 99.2419269454738, 99.25463208046585, 99.08099523557438, 99.26733721545791, 99.2419269454738, 99.30968766543144, 99.3223928004235, 99.1868713605082, 99.25463208046585, 99.40921122286925, 99.36262572789836, 99.3499205929063, 99.16357861302276, 99.52355743779778, 99.31815775542616, 99.2779248279513, 99.37109581789306, 99.3499205929063, 99.37321334039174, 99.36050820539968, 99.42826892535733, 99.22075172048703, 99.41979883536263, 99.3943885653785, 99.46426680783483, 99.39015352038115, 99.33509793541556, 99.47697194282689, 99.28427739544733, 99.36050820539968, 99.5574377977766, 99.5044997353097, 99.41556379036527, 99.38803599788248, 99.5489677077819, 99.49179460031763, 99.37956590788777, 99.47697194282689, 99.5214399152991, 99.55108523028058, 99.42403388035999, 99.45156167284277, 99.51720487030175, 99.47273689782953, 99.42403388035999, 99.5299100052938, 99.41132874536792, 99.48755955532027, 99.46638433033351, 99.53202752779248, 99.57014293276866, 99.49179460031763, 99.53626257278984, 99.50026469031233, 99.50026469031233, 99.5299100052938, 99.61884595023822, 99.45579671784013, 99.59978824775013, 99.55108523028058, 99.55532027527792, 99.47273689782953, 99.62519851773425, 99.67813658020117, 99.57437797776602, 99.45579671784013, 99.5299100052938, 99.50873478030704, 99.61672842773955, 99.46214928533615, 99.59555320275278, 99.57014293276866, 99.6209634727369, 99.53414505029116, 99.58920063525674, 99.5044997353097, 99.57014293276866, 99.53626257278984, 99.58073054526204, 99.6209634727369, 99.46850185283219, 99.72260455267337, 99.58920063525674, 99.55955532027528, 99.56379036527264, 99.60825833774484, 99.64849126521969, 99.60402329274748, 99.5574377977766, 99.60825833774484, 99.61884595023822, 99.63790365272631, 99.56802541026998, 99.58284806776072, 99.66754896770779, 99.58708311275808, 99.62519851773425, 99.57226045526734, 99.59767072525146, 99.64213869772367, 99.60614081524616, 99.63366860772896, 99.66331392271043, 99.65907887771307, 99.58920063525674, 99.61884595023822, 99.67813658020117, 99.66119640021175, 99.62731604023293, 99.68237162519851, 99.57437797776602, 99.66543144520911, 99.80307040762308, 99.5574377977766, 99.5659078877713, 99.63366860772896, 99.69931180518793, 99.66966649020645, 99.67178401270513, 99.63578613022763, 99.59767072525146, 99.63790365272631, 99.66119640021175, 99.64425622022235, 99.72683959767072, 99.68660667019587, 99.61249338274219, 99.7734250926416, 99.7480148226575, 99.56379036527264, 99.74589730015882, 99.67813658020117, 99.62731604023293, 99.69931180518793, 99.65484383271573, 99.59767072525146, 99.7649550026469, 99.73319216516676, 99.61037586024352, 99.65060878771837, 99.62731604023293, 99.72260455267337, 99.73107464266808, 99.66543144520911, 99.76071995764956, 99.71413446267867, 99.62731604023293, 99.74166225516146, 99.71201694017999, 99.67178401270513, 99.58708311275808, 99.69507676019057, 99.73954473266278, 99.68025410269983, 99.71836950767602, 99.70566437268396, 99.73107464266808, 99.65272631021705, 99.61037586024352, 99.79460031762838, 99.71201694017999, 99.68872419269455, 99.70778189518263, 99.7564849126522, 99.7564849126522, 99.66543144520911, 99.76919004764426, 99.64213869772367, 99.66966649020645, 99.71836950767602, 99.76071995764956, 99.73319216516676, 99.75224986765484, 99.6929592376919, 99.67813658020117, 99.65907887771307, 99.72472207517205, 99.68872419269455, 99.71413446267867, 99.72472207517205, 99.76071995764956, 99.7924827951297, 99.69719428268925, 99.68448914769719, 99.73107464266808, 99.73742721016411, 99.76283748014822, 99.75224986765484, 99.66543144520911, 99.70354685018528, 99.7564849126522, 99.76283748014822, 99.79460031762838, 99.72260455267337, 99.64002117522499, 99.79036527263102, 99.69084171519323, 99.8009528851244, 99.75860243515088, 99.73107464266808, 99.6569613552144, 99.75013234515616, 99.77977766013764, 99.79883536262572, 99.70142932768661, 99.69507676019057, 99.7734250926416, 99.70989941768131, 99.7480148226575]\n",
        "test_loss_list_linear = [0.6001535044873462, 0.4386981564993952, 0.32313791202271686, 0.3436826935001448, 0.3047397654576629, 0.28500792238057826, 0.26378655251042515, 0.2683426228297107, 0.27285771396960695, 0.2532431707516605, 0.25363739693135606, 0.25782927618745494, 0.2381599084200228, 0.24098271157081222, 0.23853215436432876, 0.22876223501767598, 0.2459365823762674, 0.24074384005849853, 0.23645990466078123, 0.23522190126937395, 0.23930488064812094, 0.23977934446770185, 0.22856533881642072, 0.2503986795633739, 0.2523015177096514, 0.2286284007497278, 0.2522817155821066, 0.23435633048853455, 0.25106761294106644, 0.23668714861075082, 0.25767872133748787, 0.2464016547937896, 0.24990028208669493, 0.25964026233437015, 0.2509775684014255, 0.25643125852095144, 0.25149472640352505, 0.2636343431750349, 0.2690670864802657, 0.2728121512952973, 0.2877597091719508, 0.2628066184400928, 0.289077399882908, 0.2637001336643509, 0.2718517802947876, 0.30822780637034014, 0.2853905560412243, 0.3029168626914422, 0.2886001042948634, 0.30125397385335434, 0.2918206257928236, 0.31290997323744435, 0.3016124471894228, 0.3364011060409978, 0.30226883032888757, 0.30097986143264993, 0.3160975992350894, 0.324009220079318, 0.2896068644545534, 0.28848243194321793, 0.30809056048518885, 0.31027250127026845, 0.31317287329219134, 0.3204711799728958, 0.323861251740406, 0.30745547297684583, 0.313345615642474, 0.31225476020435783, 0.32664194748755176, 0.3649349614393477, 0.3289772248781268, 0.34484266127715363, 0.34519970382326376, 0.3562348909566508, 0.34551431653181125, 0.3507686852495752, 0.3557186407749267, 0.34874562685396154, 0.3626312471414898, 0.32565479080977977, 0.3328241787111277, 0.3502718099739914, 0.35965223179436195, 0.33991847547026827, 0.32748869253212914, 0.3568636128751963, 0.366436490642966, 0.36107295554350405, 0.34370773008056715, 0.343383331070928, 0.3387923375794701, 0.33830804925631075, 0.3589262347485797, 0.3675877243079537, 0.3818419510498643, 0.35536084654649686, 0.35281344309595286, 0.3716038850629154, 0.35846942082485733, 0.34315043032242387, 0.36869288876871853, 0.3778895241226636, 0.38406652862242624, 0.352233542257226, 0.3620971898712656, 0.37449508531055614, 0.37493528568131085, 0.36601132105159406, 0.36861560567665624, 0.3717369966793294, 0.369726698087784, 0.3781300349087984, 0.35664984070714195, 0.3536169459751132, 0.406203392372631, 0.3737115380445532, 0.36973171939562055, 0.37459768973948326, 0.40706704461983606, 0.3881958369896108, 0.37344259142364356, 0.3670044747710812, 0.3696818688847855, 0.37015472998952165, 0.38189552233134416, 0.3749138325744984, 0.3891030573742647, 0.3678013856029686, 0.36681280497863306, 0.3926687809620418, 0.4037954283184281, 0.4014408321065061, 0.3803745116673264, 0.38791271775741787, 0.37617705127808687, 0.3729255223537193, 0.36608954668775495, 0.3800407153572522, 0.37864029130843635, 0.3903196998415332, 0.39593674976597815, 0.4233312764953749, 0.41837985076092404, 0.4037898524353902, 0.39368393412772934, 0.40711697678574743, 0.3831657852119237, 0.39936033349630295, 0.4122653530234946, 0.4055917443908459, 0.391819414573119, 0.3998463283493823, 0.3856644867325896, 0.38932094629853964, 0.45811930652159977, 0.4117749379582557, 0.3846923142269838, 0.38948512334814844, 0.3886060610632686, 0.4141315968488069, 0.39312611405244646, 0.40893315564037536, 0.3917342342217179, 0.4033093715232669, 0.40686407040658534, 0.402124198153615, 0.3760559221218322, 0.41189432279298116, 0.3945868524777539, 0.4064665962668026, 0.4281328501380688, 0.3944055610610282, 0.4145982712644207, 0.39619766517231864, 0.4008463170269833, 0.4100334781368135, 0.3959438812681565, 0.4048078361277779, 0.3898382809506181, 0.3940078625638111, 0.41857934771699135, 0.40016016322553305, 0.42573420590191496, 0.4076968614815497, 0.40349324788971275, 0.41680151768320917, 0.4404344489323158, 0.4126104597257925, 0.41489401694464806, 0.39322311091510687, 0.39995222047482637, 0.4001541677862406, 0.40168759471955984, 0.4003755842317261, 0.4116448443930815, 0.3964004161512004, 0.40526396525092423, 0.40040231844885094, 0.436148803023731, 0.42100324081804824, 0.42921065237811384, 0.4016612854548821, 0.4237121128860642, 0.439825722326835, 0.40309608828586835, 0.4022408921143734, 0.40526854241376414, 0.4139130270638156, 0.42377193091327653, 0.4159748113506437, 0.43089511207140546, 0.41587188575124623, 0.42511460325662415, 0.42877695572507735, 0.4061464395730154, 0.4108063448129185, 0.4237177971266575, 0.4147245909142144, 0.4100258317867331, 0.44727529514599224, 0.40763759292552576, 0.4096112189969669, 0.4398777195818576, 0.4397024883450392, 0.42169872507014694, 0.4525160300362782, 0.4039532243748944, 0.42747145336048276, 0.41558541071500776, 0.4243580105614063, 0.4347197997529863, 0.40417124533697085, 0.4213481634563091, 0.42768024219492196, 0.41689553120922224, 0.42403014976640835, 0.43499788091353636, 0.40865986559576556, 0.39625785907949596, 0.41788156942793114, 0.42710175029203, 0.39860118992453186, 0.40182735008534554, 0.43774937425612237, 0.4086107819651564, 0.42035614554861594, 0.42495340735231546, 0.42730405055943477, 0.41829457241749646, 0.4210991954211803, 0.39986408456210415, 0.43789072635163573, 0.4252349973893633, 0.4179426681058591, 0.4189265777229094, 0.42167082347327334, 0.4152061831133038, 0.4278368429582128, 0.42154261065354826, 0.39703031598279875, 0.42471260858663157, 0.4301079974747172, 0.4101321147526523, 0.4378960633869557, 0.412081252308745, 0.4338206772089881, 0.4061669551530013, 0.4237026169088067, 0.4324107744557527, 0.4283796655671561, 0.4480068746077664, 0.42207027151815446, 0.4178310304186216, 0.42660820346289113, 0.423226847276822, 0.44835605301127274, 0.4472741484733335, 0.4282900923771747, 0.4323618656535651, 0.4345184702519784, 0.4196194001455225, 0.4023636024691822, 0.4263699218977754, 0.45100487594940136, 0.4451090616046214, 0.4251364004779972, 0.4428678105822673, 0.42934233425459, 0.4463761615374025, 0.42917647955519167, 0.43650849757935195, 0.40754444616865, 0.4072841090405835, 0.43068673907249583, 0.42376545924857695, 0.4177722467438263, 0.41679786920105794, 0.4319006550381435, 0.42252690807057947, 0.4652784176572573]\n",
        "test_acc_list_linear = [81.16548862937923, 86.56653349723418, 90.2581438229871, 89.47065150583897, 90.86893054701905, 91.77934849416103, 92.38629379225569, 92.26336816226183, 92.23263675476336, 92.60525507068223, 92.71665642286416, 92.73586355255071, 93.29287031346036, 93.27366318377382, 93.4081130915796, 93.58481868469576, 93.13921327596803, 93.3620159803319, 93.6078672403196, 93.46957590657652, 93.5540872771973, 93.73847572218807, 93.77688998156115, 93.21220036877689, 93.30055316533497, 94.06883835279656, 93.46957590657652, 94.04963122311001, 93.73847572218807, 93.8959741856177, 93.44652735095268, 93.91518131530424, 93.73847572218807, 93.6578057775046, 93.9881684081131, 93.98432698217579, 93.95743700061463, 93.98432698217579, 93.8921327596804, 93.92670559311617, 93.68469575906576, 94.3338967424708, 93.6040258143823, 94.10341118623234, 94.14950829748003, 93.79609711124769, 94.17255685310387, 93.64244007375538, 93.9958512599877, 93.56945298094652, 94.00353411186232, 93.67317148125385, 93.96127842655194, 93.50799016594961, 93.99969268592501, 94.3300553165335, 93.97664413030117, 93.78457283343577, 94.16871542716656, 94.10341118623234, 94.16103257529196, 94.16487400122925, 94.05731407498463, 94.11877688998156, 94.12645974185618, 94.16487400122925, 94.17639827904118, 94.17639827904118, 94.18792255685311, 94.02658266748617, 94.1379840196681, 93.90365703749232, 93.80762138905962, 93.93054701905348, 94.00353411186232, 93.98432698217579, 93.97280270436386, 94.06499692685925, 93.99969268592501, 94.25706822372464, 94.41840811309157, 94.01505838967425, 93.78457283343577, 94.14566687154272, 94.25706822372464, 94.04963122311001, 93.8959741856177, 93.97280270436386, 94.13030116779349, 94.41072526121697, 94.20712968653964, 94.31084818684695, 93.77304855562384, 93.85755992624462, 93.78073140749846, 94.21865396435157, 94.3262138905962, 93.96896127842655, 94.20328826060233, 94.43377381684081, 94.19944683466503, 94.17639827904118, 93.82298709280884, 94.3761524277812, 94.31084818684695, 94.13030116779349, 94.18023970497849, 94.27627535341118, 94.14950829748003, 94.30700676090964, 94.41456668715428, 94.35310387215735, 94.10341118623234, 94.3262138905962, 93.75, 94.23786109403811, 94.28779963122311, 93.94975414874001, 93.8921327596804, 93.83066994468346, 94.08420405654579, 94.37231100184388, 94.17255685310387, 94.44913952059004, 94.14950829748003, 94.3761524277812, 94.22249539028887, 94.21865396435157, 94.53365089121081, 94.14566687154272, 94.16487400122925, 93.91133988936693, 93.98432698217579, 94.2839582052858, 94.19944683466503, 94.21481253841426, 94.3338967424708, 93.98432698217579, 94.18792255685311, 94.03042409342348, 94.27627535341118, 93.83066994468346, 94.12261831591887, 94.02274124154886, 94.13030116779349, 94.0880454824831, 94.52980946527352, 94.0880454824831, 94.02658266748617, 94.3262138905962, 94.31084818684695, 94.42609096496619, 94.39535955746773, 94.42224953902888, 93.99969268592501, 94.16871542716656, 94.35310387215735, 94.23017824216349, 94.26475107559926, 94.39535955746773, 94.49139520590043, 94.26090964966195, 94.51444376152428, 94.40304240934235, 94.15719114935465, 94.16103257529196, 94.41840811309157, 94.17639827904118, 94.31853103872157, 94.31853103872157, 94.00737553779963, 94.3338967424708, 94.34926244622004, 94.3338967424708, 94.24554394591273, 94.51060233558697, 94.39151813153042, 94.27243392747388, 94.43377381684081, 94.3262138905962, 94.23017824216349, 94.24170251997542, 93.86908420405655, 94.13030116779349, 94.29164105716042, 94.35694529809466, 94.19560540872772, 94.3799938537185, 94.3338967424708, 94.25322679778733, 94.2839582052858, 94.2340196681008, 94.26859250153657, 94.69114935464044, 94.16487400122925, 94.46834665027659, 94.27243392747388, 94.36846957590657, 94.10725261216963, 94.43377381684081, 94.23786109403811, 94.46450522433928, 94.32237246465888, 94.28779963122311, 94.11493546404425, 94.16871542716656, 94.4299323909035, 94.30700676090964, 94.19560540872772, 94.46066379840197, 94.17639827904118, 94.53365089121081, 94.16871542716656, 94.29164105716042, 94.29548248309773, 94.49523663183774, 94.25706822372464, 94.10341118623234, 94.64889366933005, 94.30316533497235, 94.42609096496619, 94.44529809465274, 94.19944683466503, 94.1917639827904, 94.44529809465274, 94.16103257529196, 94.3338967424708, 94.54901659496005, 94.4721880762139, 94.31084818684695, 94.39920098340504, 94.49139520590043, 94.3761524277812, 94.45298094652735, 94.31468961278426, 94.45682237246466, 94.30316533497235, 94.55285802089736, 94.4299323909035, 94.40688383527966, 94.34542102028273, 94.4299323909035, 94.49139520590043, 94.27243392747388, 94.47602950215119, 94.26090964966195, 94.24554394591273, 94.54517516902274, 94.3300553165335, 94.29548248309773, 94.61432083589429, 94.11877688998156, 94.40304240934235, 94.27243392747388, 94.59511370620774, 94.2839582052858, 94.39151813153042, 94.39535955746773, 94.21865396435157, 94.49523663183774, 94.24554394591273, 94.51060233558697, 94.59127228027043, 94.43377381684081, 94.31853103872157, 94.46834665027659, 94.43761524277812, 94.43761524277812, 94.37231100184388, 94.34157959434542, 94.00353411186232, 94.38383527965581, 94.58358942839583, 94.5759065765212, 94.41840811309157, 94.07652120467118, 94.09956976029503, 94.3799938537185, 94.30700676090964, 94.29932390903504, 94.38383527965581, 94.5259680393362, 94.26859250153657, 94.35310387215735, 94.36462814996926, 94.37231100184388, 94.38767670559312, 94.35694529809466, 94.36846957590657, 94.34542102028273, 94.40304240934235, 94.28779963122311, 94.46834665027659, 94.56054087277197, 94.62968653964352, 94.35310387215735, 94.43761524277812, 94.39535955746773, 94.26475107559926, 93.79609711124769]\n",
        "train_loss_list_exp = [1.5108453227575556, 0.45569133027620756, 0.38955149030297753, 0.38201055714108434, 0.3791554563736851, 0.37846633355791975, 0.37789585104156637, 0.37833547551780533, 0.37885424543202406, 0.3798585463830126, 0.3801606303146538, 0.3785690202864851, 0.380058984244419, 0.38191306041831247, 0.3755665107309657, 0.37845069464790787, 0.3810173268240642, 0.37906573632060675, 0.38025176412044825, 0.37710853672124506, 0.37910957913088605, 0.3795646985129612, 0.37768419939004955, 0.37915716253645054, 0.3763544963304266, 0.37615088787343764, 0.3767496614356028, 0.3780435439209305, 0.37970296291477956, 0.3768931365190806, 0.3782231926433439, 0.37863624968179843, 0.3801447201146666, 0.38008787327504095, 0.3784985868184547, 0.3775513489152681, 0.3797982176387213, 0.378436125835106, 0.37775709227656284, 0.3784049479618951, 0.3812676381047179, 0.377607957215167, 0.37778531927564923, 0.3785970114110931, 0.3763440459320539, 0.37659880587563604, 0.38260392961786366, 0.3788121801404772, 0.3782783241937477, 0.37851579224837184, 0.37789375260270386, 0.37802613358995135, 0.3780238080800064, 0.38144587775879113, 0.3779912972595634, 0.3791698661600025, 0.3766854087996289, 0.37804330437163997, 0.3786587099718854, 0.37654282059772876, 0.3787634488609102, 0.3799068733524824, 0.37713607115958764, 0.37751451530430696, 0.3801226066299247, 0.37911422368956776, 0.3805197463610631, 0.37700172774190827, 0.3788724349847008, 0.3784166157326401, 0.38057625192775313, 0.37780569122251134, 0.38091700400924944, 0.3794297932528545, 0.3814112742338077, 0.37960589101644066, 0.37835461256626823, 0.37888884483798735, 0.3795582245439695, 0.37849634607148364, 0.37682519744082194, 0.37698636250444223, 0.3814956169464401, 0.3775139360570003, 0.379286357057773, 0.37696852730864755, 0.3763741816123973, 0.37921798265561824, 0.3787178158921601, 0.3787029756682352, 0.37783306570557074, 0.3772085670452454, 0.377733004771597, 0.3765660253401371, 0.3802021978912638, 0.37975252381346736, 0.3792028047528047, 0.3785102287039847, 0.3774594428739574, 0.3806928960773034, 0.3792232088441771, 0.37827566011649805, 0.38008303063994825, 0.3778988100325835, 0.3803016372974003, 0.3791544923614357, 0.38108806501882186, 0.37818956795100594, 0.37843327103106955, 0.38068289166382013, 0.38067130910025704, 0.37755737811084683, 0.37938234137325755, 0.3798758971497295, 0.37914535011540906, 0.37674440685811084, 0.37839110369281714, 0.3803737539505248, 0.3769939240966709, 0.37618244623104086, 0.3802966764625818, 0.376914295521855, 0.38093100847590583, 0.3785027291797364, 0.38015424458153524, 0.3799492670753138, 0.3770360491140102, 0.3781222916311688, 0.37643939103214397, 0.37902817707559283, 0.37899113222350916, 0.3788869072428241, 0.37866583002130516, 0.38024979207896925, 0.37998249028433306, 0.37665241209633626, 0.3791340853221371, 0.3768656629776244, 0.3781943121738227, 0.37956079872966136, 0.3790555945660687, 0.3767165967280949, 0.3785593254860178, 0.3785436793071468, 0.3786095209157241, 0.37977585394369556, 0.37895426606421223, 0.37726105127715803, 0.3811360982456181, 0.3777372095239195, 0.37985739951857384, 0.3775850888190231, 0.37942486108964696, 0.3774096647575296, 0.37866538746893247, 0.37956950698441605, 0.37765448986676325, 0.3765703885535884, 0.37493878506063444, 0.3787174464322041, 0.3790939331539278, 0.3798395115024029, 0.3802033216972661, 0.3778447469237051, 0.37922620373528176, 0.3788863822696655, 0.3776389633010073, 0.37965354569720705, 0.37847416466329153, 0.38210083584636856, 0.3769269231858292, 0.376283608316406, 0.3791458204105941, 0.3800827085891067, 0.3800524467940576, 0.379729083156198, 0.3786869500145357, 0.37784998191566, 0.37798632447150987, 0.37959555187199495, 0.3772686913326827, 0.37737370587299834, 0.3774894487647829, 0.3773925192149351, 0.37866038096144916, 0.3799699583673865, 0.37930802181161194, 0.37929151229419034, 0.37872534558217374, 0.38051237880699035, 0.3785012537224829, 0.37907243408969427, 0.3786347356188265, 0.3772817448306536, 0.37789186423386983, 0.377803510807071, 0.37891541296227516, 0.3768928705596019, 0.3764786839808229, 0.37735731827049723, 0.37620092403436417, 0.37594923223583354, 0.37946950848186567, 0.37934725721515616, 0.3799315985383057, 0.37742675118006985, 0.3800657158137014, 0.38043200723362486, 0.3790072623468673, 0.37812678907621844, 0.380248889528962, 0.3795400029679301, 0.37716066687895355, 0.37926767851279036, 0.3811982564002195, 0.3785968935344277, 0.37829239452434427, 0.3791591095003655, 0.37750050801087204, 0.3755794913788152, 0.37707439347657407, 0.3798365030023787, 0.37901620768757693, 0.3787338199815776, 0.3768027005237616, 0.378330920527621, 0.3773879486774688, 0.37904529988281127, 0.3793277707364824, 0.37877471587522243, 0.3773436505135482, 0.38000925620235404, 0.37747299065434836, 0.3783611831707037, 0.38008849536823386, 0.3802644786069064, 0.37820831300604957, 0.37898807813158525, 0.37832132050500006, 0.37651883493755567, 0.3780955279745707, 0.3794448867157546, 0.37681427719147226, 0.37846876878725483, 0.37892088152690306, 0.3757547806433546, 0.3790287462272618, 0.3771775669764051, 0.37960463952081314, 0.38023487892415786, 0.3773157599818739, 0.3790636116734688, 0.37847773605568946, 0.3790109474969104, 0.3778621018094422, 0.3795850692483468, 0.38052020777208695, 0.3777503734681664, 0.38023589062819957, 0.37878056504539037, 0.3778544519813403, 0.3769152445117956, 0.37817029428837423, 0.3794189135879682, 0.3814815942268708, 0.3767595124357761, 0.376037456559618, 0.3787438214067521, 0.37781988387185383, 0.3780568046621514, 0.3789186481295562, 0.3792363849031893, 0.3795603767723895, 0.38027571985715125, 0.37710057130350977, 0.37790428061633896, 0.3758348726887044, 0.37957349484205893, 0.3801340398184329, 0.37570041014250055, 0.37696901714898706, 0.3780564808748602, 0.37865450675409984, 0.37989892315896867, 0.37623435913062675, 0.3776249932402841, 0.3777484252120098, 0.37681428954853274, 0.3777670302203677, 0.37880285431537525, 0.3775242522641572, 0.37820892516513505, 0.37701137581976446, 0.378751233704691, 0.37953954490865793, 0.3773617081364319, 0.3809620241324107, 0.37960273697770386, 0.3808248356428896, 0.37698043551709914]\n",
        "train_acc_list_exp = [47.21651667548968, 85.66437268395977, 87.928004235045, 88.11858125992589, 88.25410269984118, 88.07623080995235, 88.23928004235044, 88.18422445738486, 88.14187400741133, 88.09740603493913, 88.06564319745897, 88.14399152991001, 88.0084700899947, 87.9915299100053, 88.28163049232398, 88.06776071995765, 88.09105346744309, 88.11434621492853, 87.96188459502382, 88.11222869242985, 88.12916887241927, 88.13975648491265, 88.2435150873478, 88.16728427739545, 88.09105346744309, 88.20116463737428, 88.20328215987296, 88.01482265749074, 88.0084700899947, 88.20116463737428, 88.15034409740603, 88.22445738485972, 88.04870301746956, 88.06776071995765, 88.28586553732133, 88.19269454737956, 88.11011116993119, 88.18422445738486, 88.20539968237162, 88.08046585494971, 88.071995764955, 88.23292747485442, 88.23292747485442, 88.15881418740074, 88.26680783483324, 88.32398094229751, 88.06140815246162, 88.18210693488618, 88.17363684489148, 88.15881418740074, 88.13975648491265, 88.15034409740603, 88.12281630492323, 88.01694017998942, 88.19481206987824, 88.18634197988354, 88.3070407623081, 88.16093170989942, 88.16516675489677, 88.19481206987824, 88.25622022233986, 88.12493382742191, 88.1355214399153, 88.16093170989942, 88.09105346744309, 88.24563260984648, 88.11858125992589, 88.10799364743251, 88.09317098994177, 88.09528851244045, 88.0359978824775, 88.17787188988883, 88.10375860243515, 88.1355214399153, 88.08470089994707, 88.09105346744309, 88.23928004235044, 88.12916887241927, 88.10375860243515, 88.26045526733722, 88.24775013234516, 88.16093170989942, 88.05717310746427, 88.11858125992589, 88.15034409740603, 88.24563260984648, 88.14822657490735, 88.05293806246691, 88.1630492323981, 88.12281630492323, 88.32821598729487, 88.21386977236634, 88.23928004235044, 88.26892535733192, 88.23928004235044, 87.91529910005293, 88.05293806246691, 88.16093170989942, 88.26892535733192, 88.10587612493383, 88.13128639491795, 88.28374801482266, 88.21810481736368, 88.20328215987296, 88.18422445738486, 88.09740603493913, 88.00635256749602, 88.08681842244575, 88.10375860243515, 88.09528851244045, 88.06140815246162, 88.16093170989942, 88.10799364743251, 88.12493382742191, 88.08681842244575, 88.29433562731604, 88.08258337744839, 88.0084700899947, 88.17363684489148, 88.27527792482795, 88.10587612493383, 88.12069878242457, 87.98941238750662, 88.22022233986236, 88.12916887241927, 88.12281630492323, 88.14399152991001, 88.16516675489677, 88.14399152991001, 88.09528851244045, 88.18845950238222, 88.19692959237692, 88.15246161990471, 88.06564319745897, 88.12069878242457, 88.25833774483854, 88.15246161990471, 88.25622022233986, 88.11646373742721, 88.10375860243515, 88.11434621492853, 88.18422445738486, 88.14610905240868, 88.27316040232928, 88.15881418740074, 88.16728427739545, 88.13763896241397, 88.08681842244575, 88.03811540497618, 88.15457914240339, 88.15034409740603, 88.28586553732133, 88.13975648491265, 88.29010058231869, 88.25622022233986, 88.21810481736368, 88.22445738485972, 88.27316040232928, 88.2710428798306, 88.11222869242985, 88.02541026998412, 88.02541026998412, 88.08258337744839, 88.14610905240868, 88.09740603493913, 88.18634197988354, 88.18634197988354, 88.17998941238751, 88.11646373742721, 88.06140815246162, 88.05293806246691, 88.2710428798306, 88.071995764955, 88.22233986236104, 88.09317098994177, 88.12493382742191, 88.18634197988354, 88.11434621492853, 88.21810481736368, 88.00211752249868, 88.11858125992589, 88.27527792482795, 88.30280571731075, 88.24563260984648, 88.15246161990471, 88.12493382742191, 88.11434621492853, 88.02117522498676, 88.24563260984648, 88.06987824245633, 88.23716251985178, 88.0635256749603, 88.14399152991001, 88.22445738485972, 88.19269454737956, 88.26045526733722, 88.1630492323981, 88.1990471148756, 88.14399152991001, 88.27527792482795, 88.24563260984648, 88.32609846479619, 88.02117522498676, 88.15669666490207, 88.071995764955, 88.16728427739545, 88.15034409740603, 88.12281630492323, 88.15881418740074, 88.17787188988883, 88.06987824245633, 88.08470089994707, 88.11646373742721, 88.13763896241397, 88.1355214399153, 88.18845950238222, 88.18845950238222, 88.14399152991001, 88.25410269984118, 88.22445738485972, 88.17998941238751, 88.07623080995235, 88.14187400741133, 88.09952355743779, 88.31127580730545, 88.19481206987824, 88.2265749073584, 88.10164107993647, 88.12705134992059, 88.1905770248809, 88.11434621492853, 88.13128639491795, 88.22233986236104, 88.17998941238751, 88.13128639491795, 88.12705134992059, 88.24563260984648, 88.16940179989412, 88.06987824245633, 88.24986765484384, 88.12493382742191, 88.15457914240339, 88.22869242985706, 88.06987824245633, 88.08470089994707, 88.23928004235044, 88.07411328745368, 88.215987294865, 88.15881418740074, 88.14822657490735, 88.12069878242457, 88.10799364743251, 88.27527792482795, 88.00423504499736, 88.17363684489148, 88.18422445738486, 88.1355214399153, 88.18634197988354, 88.08893594494441, 88.11011116993119, 88.09105346744309, 88.2265749073584, 88.20539968237162, 88.13763896241397, 88.11646373742721, 88.29221810481737, 88.20116463737428, 88.18845950238222, 88.14399152991001, 88.0359978824775, 88.15246161990471, 88.14399152991001, 88.07834833245103, 88.30068819481207, 88.21386977236634, 88.1715193223928, 88.34515616728427, 87.99788247750132, 87.91106405505559, 88.15034409740603, 88.15669666490207, 88.1630492323981, 88.13975648491265, 88.08470089994707, 88.3705664372684, 88.22869242985706, 88.1355214399153, 88.17787188988883, 88.14610905240868, 88.18210693488618, 88.27527792482795, 88.17363684489148, 88.15034409740603, 88.26469031233457, 88.14187400741133, 88.26045526733722, 88.08681842244575, 88.09952355743779, 88.16728427739545, 88.12916887241927]\n",
        "test_loss_list_exp = [0.7599668520338395, 0.41391250622623105, 0.392150557216476, 0.3908533724207504, 0.3910430484980929, 0.3915113131059151, 0.3893689420439449, 0.39278720669886646, 0.3913472319642703, 0.3898209324654411, 0.3919214537622882, 0.3930370972729197, 0.3875842901567618, 0.3887199363579937, 0.3931986349178295, 0.3915082841527228, 0.39034265929869577, 0.38962281656031517, 0.39410267273585003, 0.3923792628680958, 0.3913208600498882, 0.3940731910806076, 0.3901035269978, 0.3910912872091228, 0.38958175787154364, 0.3905664257266942, 0.3923723243323027, 0.3930160653795682, 0.3910279740013328, 0.39261947053612445, 0.39096589437594603, 0.391885179076709, 0.3932027472730945, 0.39465218753206965, 0.38952960351518556, 0.3913427036182553, 0.39291733357251857, 0.38893387598149914, 0.3907097995865579, 0.39178018352272465, 0.38997077613192443, 0.38897235888768644, 0.39309656094102297, 0.3918713939686616, 0.38930126476813764, 0.3919200319431576, 0.3909892667742336, 0.3936515018782195, 0.3921337866900014, 0.3897011807444049, 0.389769053050116, 0.3902200594106141, 0.38991651070468564, 0.3911624582228707, 0.3900841569491461, 0.38808364508783116, 0.39078373330480914, 0.3911517692693308, 0.3905084923494096, 0.392090796986047, 0.3915708720245782, 0.3868667685664168, 0.3898552243469977, 0.3904008638186782, 0.38936665559224054, 0.3908520120323873, 0.3909977381574173, 0.3894340494538055, 0.3913114227938886, 0.38867319550584345, 0.3922208594340904, 0.39250611046365663, 0.3933124074748918, 0.3923281282916957, 0.3923263843445217, 0.3901771641537255, 0.39134884019400556, 0.38913627334085166, 0.39259269424513277, 0.39072176375809836, 0.3921530346806143, 0.39100976697370116, 0.39079241250075547, 0.3899397780643959, 0.3929170014373228, 0.3889102427398457, 0.38894158966985404, 0.39256049473496046, 0.3897372295020842, 0.39233241295989824, 0.39279353041567056, 0.394793822411813, 0.3923672336865874, 0.38789207575952306, 0.391120845853698, 0.3914312515042576, 0.3890334714715387, 0.38947177671042144, 0.3914659348334752, 0.39052898429480254, 0.38922284959870224, 0.3926088816541083, 0.3930806327684253, 0.392626012584158, 0.3940140974580073, 0.3908073946687521, 0.3892480945762466, 0.3914746798428835, 0.3904998555925547, 0.39119182118013796, 0.39440940546931, 0.39126060596283746, 0.39380357363352586, 0.3891267492344566, 0.3960636570027061, 0.3884448780879086, 0.39201007885675804, 0.3887008518418845, 0.3946489935704306, 0.39012787235425966, 0.3926868131201641, 0.391174876587648, 0.39185114438627283, 0.3916316426121721, 0.39595334516728625, 0.3925237079315326, 0.3923376457510041, 0.3896145831574412, 0.3903454670719072, 0.38917045835770814, 0.3901947463552157, 0.38894958538459795, 0.39138510881685745, 0.3923698270729944, 0.391075956543871, 0.3886808121905607, 0.3895190705855687, 0.3937717227666986, 0.3892171628334943, 0.3928420579462659, 0.3929640487128613, 0.39179296280239145, 0.39022867213569434, 0.3915944700585861, 0.3908092556338684, 0.38998721832153843, 0.3919305083360158, 0.38930609106433156, 0.3915616778620318, 0.39198073642510994, 0.3898476078083702, 0.3942314390750492, 0.39122209778311207, 0.39321639089315547, 0.39556199356037025, 0.39069073004465477, 0.3901906652631713, 0.3918502590089452, 0.3921655998656563, 0.3897442833027419, 0.39050330440787706, 0.39026205139417275, 0.3908781868716081, 0.39245978443353785, 0.3903481831007144, 0.39445868105280635, 0.3892170712351799, 0.390042986282531, 0.3925706706941128, 0.39200084298556925, 0.3911351076528138, 0.39240403268851487, 0.3938527383348521, 0.3900003031480546, 0.3922225654709573, 0.3932921053001694, 0.39253465655971975, 0.3926194997540876, 0.3874349548097919, 0.39200115612908903, 0.3948955665908608, 0.3898700127998988, 0.38938687602971117, 0.3864065995257275, 0.39098570473930416, 0.3909385864641152, 0.3916171494067884, 0.39381099733359676, 0.3898175518740626, 0.3911994169740116, 0.39155133265782804, 0.39230194549058, 0.39114902234252763, 0.39017920955723406, 0.3944379236622184, 0.3916288679283039, 0.39200210498244153, 0.39363550146420795, 0.3911365672361617, 0.39186333141782703, 0.39117470471297994, 0.38984077394593, 0.39264219977399883, 0.3900822729018389, 0.38898184559508864, 0.3910406724202867, 0.3907987319809549, 0.39212526841198697, 0.39124271427007284, 0.3914998725202738, 0.38839635442869336, 0.3937654222781752, 0.3902626017875531, 0.3955241435883092, 0.39323169840317146, 0.39330630157800284, 0.38655056892072454, 0.39017754106544983, 0.3952399448436849, 0.39261257144458156, 0.3932858498073092, 0.39499423879326556, 0.3910665978111473, 0.3922448316041161, 0.39132243789294185, 0.3905644398547855, 0.39066209877822916, 0.39234474681171716, 0.3916584701806891, 0.39128884439374884, 0.38904376978091165, 0.39183107588221044, 0.3900741520611679, 0.3930635173969409, 0.38995324082526506, 0.39311547854951784, 0.3887742017121876, 0.3909773443113355, 0.38996322007448064, 0.38965144855718986, 0.39268765284442436, 0.390111534545819, 0.3925885803559247, 0.39064669915858435, 0.3895316495030534, 0.3907163414154567, 0.3907886814399093, 0.39067917981860684, 0.3914143468673323, 0.39072976911477014, 0.39038341521632436, 0.39487345085716713, 0.39242782728636966, 0.39340144921751585, 0.391638440317383, 0.38982952050134245, 0.39072682591629965, 0.39341319571523103, 0.39195014886996327, 0.3917578032203749, 0.38969211536003096, 0.3894471714601797, 0.3898716885961738, 0.39170538400318106, 0.39051353675769823, 0.39179608466870647, 0.3921622397998969, 0.39439454747765673, 0.39445579307628614, 0.39132673534400325, 0.3867330071972866, 0.3894542139388767, 0.39122700530524346, 0.3883669243431559, 0.38984382444737004, 0.3917064682817927, 0.3907441727670969, 0.39336200226463525, 0.3893578349083078, 0.3913763885696729, 0.3897980788320887, 0.3937839002293699, 0.39068301957027585, 0.3921215317997278, 0.39208485865417647, 0.3916279115513259, 0.3911278141918136, 0.39544760121726524, 0.38938475297946556, 0.39043475139667005, 0.39209672136634005, 0.39414026142627584, 0.3913420901871195, 0.39028101735839654, 0.3931575333516972, 0.39237413212072614, 0.38880399651094977, 0.3887612786801422, 0.3894933131979961, 0.3917112931901333]\n",
        "test_acc_list_exp = [75.58773816840811, 87.06976029502151, 87.53073140749846, 87.8918254456054, 87.76889981561156, 87.74969268592501, 87.98786109403811, 87.76121696373694, 87.87645974185618, 87.71896127842655, 87.74969268592501, 87.79578979717272, 87.96481253841426, 87.81115550092194, 87.81499692685925, 87.80731407498463, 87.9417639827904, 87.93023970497849, 87.76505838967425, 87.78042409342348, 87.66133988936693, 87.7458512599877, 87.7458512599877, 87.85725261216963, 87.66518131530424, 87.71127842655194, 87.65365703749232, 87.66133988936693, 87.74200983405039, 87.7458512599877, 87.85725261216963, 87.89566687154272, 87.64981561155501, 87.61140135218193, 88.02243392747388, 87.85725261216963, 87.50384142593731, 87.91871542716656, 87.82652120467118, 87.80731407498463, 87.82652120467118, 87.93792255685311, 87.6920712968654, 87.72280270436386, 87.82267977873387, 87.81499692685925, 87.87645974185618, 87.66133988936693, 87.8418869084204, 87.76889981561156, 87.88030116779349, 87.78042409342348, 87.7919483712354, 87.79963122311001, 87.81883835279656, 87.8918254456054, 87.92639827904118, 87.94944683466503, 87.78426551936079, 87.8918254456054, 87.7881069452981, 88.00322679778733, 87.87645974185618, 87.7881069452981, 87.68054701905348, 87.81499692685925, 87.71896127842655, 87.82267977873387, 87.7919483712354, 87.81883835279656, 87.83036263060848, 87.78042409342348, 87.78042409342348, 87.71511985248924, 87.72664413030117, 87.82267977873387, 87.80731407498463, 87.88030116779349, 87.83036263060848, 87.84956976029503, 87.91487400122925, 87.73048555623848, 87.81115550092194, 87.73048555623848, 87.7381684081131, 87.72664413030117, 87.8918254456054, 87.66518131530424, 87.8418869084204, 87.7919483712354, 87.69975414874001, 87.79578979717272, 87.77658266748617, 87.90334972341734, 87.75353411186232, 87.83036263060848, 87.86877688998156, 87.86877688998156, 87.7919483712354, 87.84572833435772, 87.91871542716656, 87.84956976029503, 87.83036263060848, 87.80347264904732, 87.70743700061463, 87.75353411186232, 87.9840196681008, 87.70359557467732, 87.84956976029503, 87.87645974185618, 87.7381684081131, 87.81115550092194, 87.6459741856177, 87.7458512599877, 87.51920712968654, 88.05700676090964, 87.79578979717272, 87.91871542716656, 87.56914566687155, 87.82267977873387, 87.73432698217579, 87.75737553779963, 87.81115550092194, 87.85341118623234, 87.74969268592501, 87.68054701905348, 87.63444990780577, 87.83420405654579, 87.91103257529196, 87.76505838967425, 87.80731407498463, 88.04548248309773, 87.81883835279656, 87.82652120467118, 87.6920712968654, 87.76121696373694, 87.74200983405039, 87.80347264904732, 87.96481253841426, 87.8380454824831, 87.6459741856177, 87.85341118623234, 87.92255685310387, 87.68438844499079, 87.7458512599877, 87.78426551936079, 87.81883835279656, 87.98017824216349, 87.88030116779349, 87.6421327596804, 87.9340811309158, 87.65365703749232, 87.68438844499079, 87.60371850030731, 87.66133988936693, 87.6459741856177, 87.78042409342348, 87.77658266748617, 87.58451137062077, 87.97633681622618, 87.83420405654579, 87.73048555623848, 87.84572833435772, 87.75737553779963, 87.76121696373694, 87.63829133374308, 87.91103257529196, 87.75353411186232, 87.57298709280884, 87.70743700061463, 87.73048555623848, 87.77658266748617, 87.76505838967425, 88.02243392747388, 87.84572833435772, 87.73048555623848, 87.88414259373079, 87.69975414874001, 87.88030116779349, 87.88030116779349, 87.68054701905348, 87.92639827904118, 87.95712968653964, 87.91103257529196, 87.68054701905348, 87.76121696373694, 87.78426551936079, 87.67286416717886, 87.67670559311617, 87.88414259373079, 87.79578979717272, 87.66133988936693, 87.83036263060848, 87.86493546404425, 87.78426551936079, 87.67286416717886, 87.81115550092194, 87.6421327596804, 87.70359557467732, 87.71127842655194, 87.84956976029503, 87.82267977873387, 87.7919483712354, 87.71127842655194, 87.83036263060848, 87.8879840196681, 87.76121696373694, 87.73048555623848, 87.76121696373694, 87.8380454824831, 87.75353411186232, 87.72280270436386, 87.84956976029503, 87.59987707437, 87.6959127228027, 87.62292562999386, 88.03779963122311, 87.77658266748617, 87.7381684081131, 87.80731407498463, 87.56530424093424, 87.63060848186846, 87.69975414874001, 87.71896127842655, 87.89566687154272, 87.83420405654579, 87.76121696373694, 87.63060848186846, 87.79578979717272, 87.65365703749232, 87.82267977873387, 87.85341118623234, 87.85341118623234, 87.80347264904732, 87.87645974185618, 87.84572833435772, 87.96097111247695, 87.84572833435772, 87.71511985248924, 87.85725261216963, 87.78426551936079, 87.78426551936079, 87.61908420405655, 87.86493546404425, 87.96097111247695, 87.86109403810694, 87.75737553779963, 87.84956976029503, 87.7881069452981, 87.84572833435772, 87.70359557467732, 87.71127842655194, 87.65749846342962, 87.62292562999386, 87.71896127842655, 87.96865396435157, 87.97633681622618, 87.76121696373694, 87.75353411186232, 87.81115550092194, 87.8879840196681, 87.82652120467118, 87.87645974185618, 87.68438844499079, 87.8918254456054, 87.63060848186846, 87.76505838967425, 87.5960356484327, 87.66133988936693, 87.9840196681008, 87.88030116779349, 87.86877688998156, 87.77274124154886, 87.9417639827904, 87.90719114935465, 87.7881069452981, 87.69975414874001, 87.77658266748617, 87.84572833435772, 87.67286416717886, 87.80731407498463, 87.74200983405039, 87.71896127842655, 87.6459741856177, 87.68438844499079, 87.86877688998156, 87.82267977873387, 87.78426551936079, 87.72664413030117, 87.98017824216349, 87.66518131530424, 87.6959127228027, 87.75353411186232, 87.91487400122925, 87.89566687154272, 87.72280270436386, 87.91487400122925, 87.88030116779349, 87.91103257529196, 87.75353411186232]\n",
        "\n",
        "train_loss_list_5e4 = [1.4802386524877573, 0.5145602391825782, 0.4270873037860969, 0.3911595640143728, 0.3666531684275888, 0.3449077741482717, 0.3271806535278232, 0.31609291052269095, 0.30408265799041684, 0.29365021432560634, 0.28160075124524797, 0.2770334360439603, 0.26884961707724464, 0.2641961875202533, 0.25375436896554177, 0.2502837141112583, 0.2470766868048567, 0.23852058467745457, 0.23988390942091542, 0.23230066057546997, 0.2278682370051782, 0.22565480602304464, 0.22504844034953816, 0.2194394195467476, 0.2184035734028674, 0.21561946675464067, 0.21413457837854297, 0.21124843120332656, 0.21243113762923696, 0.20603898459374098, 0.20471800083350036, 0.20373490892936222, 0.20350655851769578, 0.20246598469774899, 0.1970768437334677, 0.1986252810896897, 0.19743034299109685, 0.19816612322034874, 0.19047524803254987, 0.19524715533909112, 0.19178811626301873, 0.1882726750904467, 0.19070912812783467, 0.18814858636720394, 0.18752294930258417, 0.1876332449012494, 0.18488309697972405, 0.1866223762634647, 0.1871858422712582, 0.18362822461378606, 0.18218793275798886, 0.18321568313935585, 0.17826895162382422, 0.18373284461658176, 0.17807404935198426, 0.1832454036466959, 0.17776847138072094, 0.17643137354918612, 0.17395331942776676, 0.17643668946777255, 0.17554401157065458, 0.17678272510124093, 0.17290620969199552, 0.16925092947575943, 0.16994382533923721, 0.17394802385189023, 0.17083977395037647, 0.16816641190430012, 0.16901177336489606, 0.17049305643734894, 0.1686860634806518, 0.16724471368279237, 0.16594552272945884, 0.16281739412365245, 0.16591493490588696, 0.16363012161721704, 0.1613770334055107, 0.1623856468898494, 0.16301845652139607, 0.162652157650892, 0.1592392718626393, 0.15867647268180926, 0.16240244100329676, 0.15764614089636944, 0.15857719487654484, 0.1579075544810069, 0.15415261916028775, 0.15517796557850955, 0.15625821325554434, 0.15458142586799495, 0.15438464393985626, 0.1572210744003132, 0.15048929238262862, 0.14945686451060985, 0.1505291250440971, 0.15305787657556658, 0.14894246368550348, 0.147898023902643, 0.15114493573180382, 0.14610998743897693, 0.15086569631002783, 0.1484981526729214, 0.1449999701039901, 0.14578966223975506, 0.14083756948631954, 0.14295084347448697, 0.14506198940319098, 0.14481029806905968, 0.14044388525470647, 0.13931026286508655, 0.13716323383980328, 0.14206144404484006, 0.14102018442822667, 0.13867486305635796, 0.13811369409385896, 0.1344010781062651, 0.13408899530364568, 0.1343176202560828, 0.13219609678417524, 0.13159260965459715, 0.13269671327295665, 0.13183512560377114, 0.12735726753487012, 0.12682472022002952, 0.12942703321656884, 0.1311747801154448, 0.12721967680990534, 0.1226911582961315, 0.12565258846474373, 0.12294719365695467, 0.12425805079601807, 0.12707497591410227, 0.12184429030895717, 0.11833101225799823, 0.11996741157762081, 0.12062076490806532, 0.11433951715184099, 0.11977466401267989, 0.11421744397217988, 0.11513779362345614, 0.11420854406688756, 0.11369123289659418, 0.11147084164869818, 0.10956912912014458, 0.10847692516680332, 0.10791541966369886, 0.11187728162006877, 0.1076874076956656, 0.10829882281340235, 0.10742072247190851, 0.10462035163530328, 0.10231590462308428, 0.10307718535789306, 0.10392249831399782, 0.10356237866277214, 0.10049116989057562, 0.10070552909620571, 0.10252807007188881, 0.0956033191373597, 0.09626625927523545, 0.09980903398548442, 0.09713503368814705, 0.09384269924546645, 0.0914715800489877, 0.08955621895276838, 0.09167020563995289, 0.090305259370505, 0.09060482793616328, 0.08901907593203673, 0.08711527488610851, 0.08550666729467833, 0.08462664417589341, 0.08277387312313082, 0.0831775262416782, 0.08437745708992407, 0.07903462006309168, 0.08039322875561268, 0.08088428793136827, 0.07809663470834494, 0.07452205053169714, 0.07565905422547727, 0.07784699146360724, 0.07322079027046356, 0.07456694824272782, 0.07434099188786213, 0.07195496950349026, 0.06945457162340563, 0.06995407763495032, 0.06968635793139295, 0.06799854681420779, 0.0665388950500082, 0.06632828803324117, 0.06345813088421899, 0.06316864148266917, 0.06253555392074149, 0.06164193874588463, 0.0611763400048381, 0.06035886153573711, 0.05943589109351965, 0.05881024452638093, 0.05468251183250449, 0.0560565908366264, 0.05653399054899933, 0.05562049048248588, 0.051311384115448895, 0.052225344276767435, 0.05155897112739926, 0.049526256553293645, 0.0512430043146519, 0.04822691060415896, 0.047050642753196606, 0.04643308828764047, 0.04417288107519873, 0.04520953228056108, 0.04404221094140233, 0.04265529490744841, 0.042559341516164015, 0.04379070952893071, 0.03938076019963517, 0.03966092142758532, 0.04086068194664934, 0.03843482981150753, 0.03796145588356426, 0.03604420552757091, 0.03480926173332786, 0.035225326502645204, 0.032808877683565184, 0.03266389738152505, 0.03565281066162698, 0.03340759336365372, 0.031172389340609674, 0.030425866454685747, 0.03243158847853279, 0.028973121385324777, 0.026384176789886422, 0.026978798047197744, 0.026420692340937774, 0.0263953241601657, 0.024609150329029293, 0.025792538209220138, 0.02489618113110483, 0.02391088762519967, 0.023590878200172107, 0.023072245570535704, 0.021719697244505735, 0.01978167379306513, 0.0224422716837235, 0.01906526322807198, 0.02043990451882929, 0.0194835271951326, 0.01808247663810102, 0.018989686450673657, 0.019306088054456765, 0.017153792940761622, 0.016933396825223636, 0.015568477072599054, 0.015502154842576865, 0.016233396913596307, 0.015708776872332502, 0.01585502114947027, 0.014598894385724433, 0.013940555258366332, 0.013421826192990531, 0.013425068274836957, 0.011932213828257893, 0.013165008134731069, 0.012398852705501201, 0.013773291333588622, 0.011650380652604831, 0.011782062857671765, 0.011110670982890316, 0.011588883696175698, 0.011087752127802874, 0.010806514879342845, 0.009987936503137777, 0.01071135072052721, 0.010115013357193833, 0.010417424021213035, 0.010015223428940142, 0.009483683681138224, 0.008984847573833514, 0.009285728387196414, 0.009138723183864965, 0.009764598564845498, 0.00961481691192997, 0.009357751404228027, 0.009121658366620237, 0.00870354351181244, 0.008720945385802124, 0.008100192143321845, 0.008175653403910094, 0.008284241586373387, 0.008199483759103085, 0.008166708315275913, 0.008518998447406865, 0.008448232254922026, 0.009054402698987343, 0.008894852471574484, 0.008632234484777561, 0.008205045137029434]\n",
        "train_acc_list_5e4 = [48.277395447326626, 83.64002117522499, 86.79724722075171, 88.12493382742191, 88.89147697194282, 89.55214399152992, 89.9947061937533, 90.69772366331392, 90.96029645314981, 91.36050820539968, 91.55955532027528, 91.68660667019587, 92.08046585494971, 92.15457914240339, 92.65643197458974, 92.62466913710958, 92.80254102699841, 93.1011116993118, 93.08840656431974, 93.11593435680254, 93.28110111169931, 93.37850714663843, 93.46956061408153, 93.68131286394917, 93.59661196400212, 93.65166754896771, 93.93118051879301, 93.87612493382743, 93.82953943885654, 94.09846479618847, 94.14716781365802, 94.02858655373214, 94.13022763366861, 94.16410799364743, 94.31445209105347, 94.212811011117, 94.20434092112228, 94.25939650608788, 94.53255690841715, 94.46056114346214, 94.500794070937, 94.56855479089465, 94.40974060349392, 94.58125992588671, 94.53255690841715, 94.62361037586024, 94.77183695076761, 94.66807834833246, 94.64690312334568, 94.67866596082584, 94.7443091582848, 94.71042879830598, 94.96029645314981, 94.68290100582318, 94.88406564319746, 94.70619375330863, 94.8268925357332, 94.91371095817892, 94.92429857067232, 94.99417681312865, 94.94759131815776, 94.89253573319216, 95.142403388036, 95.1148755955532, 95.13816834303864, 94.97511911064055, 95.08946532556908, 95.23133933298041, 95.18475383800953, 95.00264690312335, 95.13393329804128, 95.06617257808364, 95.18475383800953, 95.32874536791954, 95.26521969295923, 95.21651667548967, 95.358390682901, 95.29274748544204, 95.39862361037586, 95.3223928004235, 95.47908946532557, 95.358390682901, 95.3499205929063, 95.42826892535733, 95.46638433033351, 95.43462149285337, 95.57014293276866, 95.4219163578613, 95.45579671784013, 95.63366860772896, 95.55532027527792, 95.5214399152991, 95.74166225516146, 95.71836950767602, 95.66543144520911, 95.5849655902594, 95.67178401270513, 95.75860243515088, 95.6569613552144, 95.80518793012176, 95.6929592376919, 95.71836950767602, 95.84542085759661, 95.8644785600847, 95.95764955002647, 95.92165166754897, 95.84965590259397, 95.78613022763366, 96.01694017998942, 95.98941238750662, 96.02117522498676, 95.97670725251456, 95.93859184753838, 96.09317098994177, 96.00211752249868, 96.11858125992589, 96.14822657490735, 96.15669666490207, 96.22022233986236, 96.18845950238222, 96.2075172048703, 96.16516675489677, 96.35997882477501, 96.38538909475913, 96.26257278983589, 96.2350449973531, 96.36421386977237, 96.56961355214399, 96.3790365272631, 96.54843832715723, 96.38750661725781, 96.3430386447856, 96.56749602964531, 96.65431445209106, 96.64584436209634, 96.6056114346215, 96.66490206458444, 96.52302805717311, 96.76654314452091, 96.68395976707252, 96.72631021704606, 96.67337215457914, 96.80254102699841, 96.92535733192165, 96.93170989941768, 96.84277395447327, 96.79195341450503, 96.9020645844362, 96.929592376919, 96.86394917946004, 97.01429327686607, 97.07146638433034, 97.05452620434092, 96.98041291688725, 97.05029115934357, 97.19428268925357, 97.12228692429856, 97.07993647432504, 97.32556908417152, 97.30227633668608, 97.18581259925887, 97.25357331921651, 97.28533615669666, 97.3446267866596, 97.43991529910005, 97.44415034409741, 97.41450502911593, 97.43144520910535, 97.46744309158285, 97.45262043409211, 97.56484912652196, 97.57755426151402, 97.60508205399682, 97.66225516146109, 97.53732133403918, 97.7067231339333, 97.66013763896241, 97.65378507146639, 97.76601376389624, 97.87612493382743, 97.8507146638433, 97.80836421386977, 97.89306511381683, 97.83800952885125, 97.86553732133405, 97.91847538380095, 98.0052938062467, 97.92906299629433, 97.97141344626786, 98.07517204870302, 98.04552673372154, 98.08152461619905, 98.14505029115935, 98.18951826363156, 98.1852832186342, 98.24033880359978, 98.18316569613552, 98.27633668607729, 98.26786659608258, 98.24669137109582, 98.44150344097406, 98.41821069348862, 98.30598200105877, 98.37374272101641, 98.54314452091053, 98.52196929592377, 98.51561672842774, 98.61725780836422, 98.53890947591319, 98.66807834833246, 98.59820010587613, 98.64690312334568, 98.74854420328217, 98.68925357331922, 98.70407623080995, 98.74642668078349, 98.76548438327157, 98.67019587083112, 98.89677077818952, 98.80359978824775, 98.77818951826363, 98.854420328216, 98.86924298570672, 98.94123875066173, 98.97511911064055, 98.97511911064055, 99.07252514557968, 99.06617257808364, 98.96029645314981, 99.04711487559555, 99.09158284806776, 99.08523028057174, 99.02593965060879, 99.16569613552144, 99.26098464796189, 99.214399152991, 99.2419269454738, 99.2503970354685, 99.31604023292748, 99.22498676548439, 99.30333509793542, 99.29062996294336, 99.28427739544733, 99.32874536791954, 99.36897829539438, 99.44944415034409, 99.34568554790894, 99.46638433033351, 99.40709370037057, 99.43673901535203, 99.47273689782953, 99.44944415034409, 99.46003176283747, 99.50661725780836, 99.51085230280572, 99.56167284277396, 99.58708311275808, 99.5299100052938, 99.56379036527264, 99.56802541026998, 99.59978824775013, 99.61461090524087, 99.65272631021705, 99.62519851773425, 99.7204870301747, 99.63578613022763, 99.67813658020117, 99.64002117522499, 99.69931180518793, 99.69719428268925, 99.72472207517205, 99.68448914769719, 99.70566437268396, 99.7204870301747, 99.76283748014822, 99.71625198517734, 99.75860243515088, 99.72683959767072, 99.74377977766014, 99.76707252514558, 99.78613022763366, 99.78189518263632, 99.7564849126522, 99.75224986765484, 99.75224986765484, 99.75436739015352, 99.7649550026469, 99.79460031762838, 99.7924827951297, 99.7924827951297, 99.81154049761778, 99.82424563260984, 99.79460031762838, 99.82001058761249, 99.80307040762308, 99.7649550026469, 99.73954473266278, 99.77766013763896, 99.78613022763366, 99.81365802011646]\n",
        "test_loss_list_5e4 = [0.8332259766027039, 0.6243491267748907, 0.4589659714815663, 0.4804437569543427, 0.427488262965983, 0.4090569673800001, 0.3808782717906961, 0.35566168862814995, 0.33532728992548644, 0.3060293437949583, 0.3283534451734786, 0.2954925663184886, 0.28179204358043625, 0.30891463029034, 0.3015652174750964, 0.28739879868340257, 0.30414057471880723, 0.2780561326254232, 0.2854650225125107, 0.2774708117632305, 0.2591885347649747, 0.2489084253343297, 0.26453685285706147, 0.2969295278775926, 0.2676425111644408, 0.24623633818883522, 0.2657030882569505, 0.2502379405367024, 0.2519026789814234, 0.24595809209288336, 0.2657459572907172, 0.24606278589835354, 0.26561722005991373, 0.26659162207415293, 0.27839325434144807, 0.24987868000479305, 0.24812665541528486, 0.24487176610558642, 0.2487423630321727, 0.2408910162162547, 0.23539556405853992, 0.24755567443721435, 0.24770869559371003, 0.24718565263730638, 0.239624624816226, 0.24662797083603402, 0.2383566615445649, 0.2449228444374075, 0.23434106121752776, 0.2598630274627723, 0.2621555033998162, 0.23332552688525005, 0.23622890707908892, 0.24026584826117636, 0.2526375870494282, 0.23738020169092158, 0.2334751014177706, 0.2508628443796553, 0.2439220617068749, 0.23279762205978236, 0.24812749104903026, 0.23510865852528928, 0.24088096268036785, 0.2441782963772615, 0.2372785997120481, 0.2591664406160514, 0.24067971285651713, 0.2356143380497016, 0.22471256245512003, 0.23980964548593642, 0.22926432496922858, 0.22651415298163308, 0.23500783566166372, 0.24451460265645794, 0.24563370374780075, 0.238621749385607, 0.24734771609598516, 0.24317575092701352, 0.2357527948002897, 0.23691468079592667, 0.2317277501318969, 0.2389378704167172, 0.2415773031100923, 0.22682394105575832, 0.24305828332024462, 0.23989992117618814, 0.24055047181672326, 0.2310189416851191, 0.2320305799663651, 0.24022304533305122, 0.23773051294333794, 0.23404084142370551, 0.22795141963105575, 0.24373077162924936, 0.2519711878004612, 0.24464675623412227, 0.24204467719092088, 0.23641339631057254, 0.22852404189168238, 0.23696348417148577, 0.23560348832431963, 0.2293583228636314, 0.22483952014761813, 0.2419880603750547, 0.24878904088308998, 0.23500451945937148, 0.23449834377742282, 0.23925709554596858, 0.22304683715543328, 0.23272172509528258, 0.23005880462918796, 0.2274422672230239, 0.24031657513742352, 0.23025121483221359, 0.23231310611042907, 0.23103023649138563, 0.23504440533910312, 0.23217533609154178, 0.24353400623316274, 0.23417713490369566, 0.23423893881194732, 0.228971739591775, 0.23540909750861863, 0.22729575031382196, 0.24729683045663087, 0.23187032206824013, 0.23728142879611136, 0.2368104361983783, 0.23822332604550847, 0.24225895598019456, 0.24053165952072425, 0.23070352232339336, 0.23100480022748895, 0.23646043049281135, 0.2497626494835405, 0.2328693575280554, 0.2476191526169286, 0.24926081999186792, 0.22955395642887144, 0.23540200713072337, 0.24256359033432662, 0.23535256659356402, 0.22585722259884955, 0.23929224061030968, 0.2362090051356776, 0.2323842137759807, 0.23957762155024445, 0.23328206293723164, 0.24211033893858686, 0.24111823025433457, 0.2357599771234627, 0.23172222019410602, 0.23386546501926347, 0.2514914059828894, 0.2433166304037121, 0.2378106768261276, 0.22987670018611586, 0.230959424767278, 0.23733513216104576, 0.24432600466717108, 0.23943918337132417, 0.24938077077853912, 0.24655740848724164, 0.2478333150456641, 0.25572608141045944, 0.24093728490612087, 0.24338949404145574, 0.24488756612089335, 0.2545141223452839, 0.24692704464655882, 0.2547990579669382, 0.2425614733468084, 0.24526120959689804, 0.2513910967254025, 0.24536202011593417, 0.24960096059914896, 0.24901917171390617, 0.24787598632860416, 0.25261163421194344, 0.24638482389569866, 0.2529066298522201, 0.24463685903259935, 0.2410413659926431, 0.2477684386403245, 0.2515937814708142, 0.25298880261606443, 0.2537755832423036, 0.24700629240011468, 0.25862112336372045, 0.2538327400974345, 0.25513705633142414, 0.25248319325128604, 0.25129741044970705, 0.25710856879823935, 0.2602397995252235, 0.2535753774401896, 0.25966272229219184, 0.26258349593947916, 0.2575982420467863, 0.25366368794850275, 0.26840075843182265, 0.2604089367623423, 0.26335741690926107, 0.26115994502370266, 0.27586783806556, 0.2699058622340946, 0.2690262606038767, 0.26124058914023873, 0.2695645693068703, 0.269948998539179, 0.2699922961274199, 0.2781037180410588, 0.26918452950742316, 0.2656496547746892, 0.27793413524826366, 0.2727473574854872, 0.26761945906807394, 0.28111935063612226, 0.2739635939691581, 0.2791407521741063, 0.27640176702327296, 0.27548765678269166, 0.2720885994460653, 0.2716641928087555, 0.29375703590830754, 0.2741913403402649, 0.2834579655352761, 0.2800045997799173, 0.2746915383556602, 0.28199352176093, 0.2871078792502921, 0.2875168366430729, 0.2797919814200962, 0.2876746460211043, 0.2826278824541791, 0.2888666771662732, 0.28122600651912244, 0.2926978058565189, 0.2832188007111351, 0.2851658707082856, 0.2897379945543613, 0.2852640647404626, 0.29130700086334754, 0.29031389254127066, 0.301364694575907, 0.29234155917576715, 0.30245087251943703, 0.29240411739138994, 0.2921989212068273, 0.29711635103997064, 0.30123716481395213, 0.29383554357085745, 0.2976747955944316, 0.2940556710385078, 0.29956718912238584, 0.2989022258906534, 0.3001505616760137, 0.29694770219023614, 0.296321282619793, 0.30678381798241067, 0.2958103663415885, 0.29974740378412545, 0.29243658106847137, 0.29601444117724895, 0.2992933266522253, 0.3056794012914978, 0.3159245145963688, 0.3084262056714472, 0.3033023014774217, 0.2999487395635715, 0.305078714501624, 0.29950604752144394, 0.3001469790424202, 0.3019553049303153, 0.3047376123220459, 0.30005093909087865, 0.2961901391984201, 0.3029952965421127, 0.2991081877867235, 0.3034919787508746, 0.30540474100659293, 0.29712304374312654, 0.304328731279455, 0.30321371049492385, 0.3067686218019648, 0.30573491976760764, 0.30279137281810536, 0.304144932066693, 0.3118132336941712, 0.3035398056803673, 0.3062666428193231, 0.3056575841106036, 0.30131836995190264, 0.3063342159285265, 0.31325177424678613, 0.2976547159327596, 0.3059330361475255, 0.2988617525655119, 0.29846939126796584, 0.3085705796804498]\n",
        "test_acc_list_5e4 = [73.40580823601721, 79.92086662569146, 86.0479409956976, 85.25276582667486, 87.02750460971113, 87.60755992624462, 88.40273509526736, 89.31699446834665, 89.8740012292563, 90.96496619545175, 90.20052243392747, 91.4259373079287, 91.59496004917025, 91.09557467732022, 91.30301167793485, 91.72172710510141, 91.09557467732022, 91.82928703134604, 91.69483712354025, 91.82160417947142, 92.55147510755992, 92.82805777504609, 92.3939766441303, 91.19161032575292, 92.22111247695145, 92.82037492317149, 92.24031960663798, 92.8242163491088, 92.70513214505225, 92.94330055316533, 92.41318377381684, 92.88952059004302, 92.42086662569146, 92.1980639213276, 92.02519975414874, 92.71665642286416, 92.86647203441917, 92.96250768285188, 92.83958205285802, 93.14305470190534, 93.32744314689613, 92.98939766441303, 93.01244622003688, 93.0278119237861, 93.2160417947142, 92.97019053472648, 93.29287031346036, 93.04317762753534, 93.25829748002458, 92.65903503380454, 92.59373079287032, 93.4119545175169, 93.1238475722188, 93.28134603564843, 92.85494775660726, 93.31207744314689, 93.42347879532882, 93.00476336816226, 93.08927473878303, 93.51951444376152, 93.00860479409957, 93.36585740626921, 93.20835894283958, 92.98555623847572, 93.39274738783037, 92.60141364474492, 93.18146896127843, 93.35049170251997, 93.68469575906576, 93.27750460971113, 93.51951444376152, 93.6578057775046, 93.51951444376152, 93.1238475722188, 92.98555623847572, 93.27366318377382, 93.15457897971727, 93.38890596189306, 93.45421020282728, 93.32360172095882, 93.44268592501537, 93.38122311001844, 93.4618930547019, 93.59634296250768, 93.42347879532882, 93.34665027658266, 93.31976029502151, 93.7077443146896, 93.42347879532882, 93.34665027658266, 93.49646588813768, 93.48110018438844, 93.6578057775046, 93.37354025814382, 93.1238475722188, 93.1737861094038, 93.24677320221267, 93.45421020282728, 93.71542716656423, 93.39658881376766, 93.5041487400123, 93.58097725875845, 93.9881684081131, 93.26982175783651, 93.0201290719115, 93.61555009219423, 93.52335586969883, 93.39274738783037, 93.91518131530424, 93.71926859250154, 93.86140135218193, 93.77688998156115, 93.37354025814382, 93.69237861094038, 93.6578057775046, 93.80762138905962, 93.49262446220037, 93.66548862937923, 93.40427166564228, 93.6578057775046, 93.68469575906576, 93.66548862937923, 93.61939151813154, 94.08420405654579, 93.59634296250768, 93.65012292563, 93.6539643515673, 93.61939151813154, 93.61170866625692, 93.48878303626306, 93.58097725875845, 93.73847572218807, 93.73079287031346, 93.66164720344192, 93.4119545175169, 93.75, 93.48110018438844, 93.58097725875845, 93.86524277811924, 93.73079287031346, 93.47725875845114, 93.90749846342962, 94.19944683466503, 93.66933005531654, 93.66933005531654, 93.79609711124769, 93.700061462815, 93.88060848186846, 93.80762138905962, 93.76920712968654, 93.75, 93.86524277811924, 93.88829133374308, 93.6078672403196, 93.75768285187462, 93.76152427781193, 93.87292562999386, 94.0419483712354, 93.93054701905348, 93.85371850030731, 93.91902274124155, 93.6539643515673, 93.9881684081131, 93.66548862937923, 93.53872157344806, 93.89981561155501, 94.01505838967425, 94.06115550092194, 93.7077443146896, 93.799938537185, 93.59250153657038, 93.91902274124155, 94.02658266748617, 93.82298709280884, 94.01505838967425, 93.90749846342962, 93.96511985248924, 93.91133988936693, 93.76536570374923, 94.0918869084204, 93.78841425937308, 93.96896127842655, 94.09956976029503, 94.14566687154272, 93.8921327596804, 93.91518131530424, 93.8921327596804, 94.26090964966195, 93.80762138905962, 93.88060848186846, 94.01889981561156, 94.07652120467118, 94.14566687154272, 94.10341118623234, 93.8460356484327, 94.02658266748617, 93.91902274124155, 93.86908420405655, 94.12645974185618, 94.1379840196681, 93.93822987092808, 94.07652120467118, 94.08420405654579, 94.0381069452981, 93.85755992624462, 94.02658266748617, 94.00353411186232, 94.18792255685311, 94.02274124154886, 94.0918869084204, 94.21865396435157, 93.99200983405039, 94.06499692685925, 94.28011677934849, 93.99969268592501, 93.96896127842655, 94.28011677934849, 93.97664413030117, 94.22633681622618, 94.24170251997542, 94.05731407498463, 94.13414259373079, 94.1917639827904, 94.49139520590043, 93.99200983405039, 94.28779963122311, 94.03042409342348, 94.38767670559312, 94.19944683466503, 94.1840811309158, 94.11109403810694, 94.07267977873387, 94.23017824216349, 94.38383527965581, 94.32237246465888, 94.21481253841426, 94.22249539028887, 94.24938537185002, 94.31853103872157, 94.34926244622004, 94.21865396435157, 94.29932390903504, 94.25322679778733, 94.26090964966195, 94.29548248309773, 94.41840811309157, 94.17639827904118, 94.36078672403197, 94.37231100184388, 94.13414259373079, 94.19944683466503, 94.34157959434542, 94.34157959434542, 94.36846957590657, 94.35310387215735, 94.24170251997542, 94.37231100184388, 94.36462814996926, 94.39920098340504, 94.25706822372464, 94.43761524277812, 94.50676090964966, 94.53749231714812, 94.40304240934235, 94.40688383527966, 94.24554394591273, 94.41072526121697, 94.35310387215735, 94.45298094652735, 94.41072526121697, 94.36846957590657, 94.35310387215735, 94.34926244622004, 94.48755377996312, 94.30316533497235, 94.5259680393362, 94.54133374308543, 94.36846957590657, 94.4299323909035, 94.4299323909035, 94.34157959434542, 94.45682237246466, 94.36462814996926, 94.5259680393362, 94.38383527965581, 94.34542102028273, 94.43761524277812, 94.35694529809466, 94.34157959434542, 94.35310387215735, 94.39920098340504, 94.3761524277812, 94.44913952059004, 94.39920098340504, 94.25706822372464, 94.40688383527966, 94.40304240934235, 94.53749231714812, 94.49139520590043, 94.39920098340504]\n",
        "train_loss_list_1e2 = [1.5527255651427478, 0.7627066397246952, 0.6569193178399145, 0.5921073180388629, 0.546498658453546, 0.5105415238597528, 0.47226018775607836, 0.45309981219167633, 0.43924911138488026, 0.43100208492298436, 0.4212728382610693, 0.41300465610776815, 0.4045538008213043, 0.4054832284043475, 0.3981784124603763, 0.3888350862070797, 0.39557689973493904, 0.3849596505565695, 0.38653959386393955, 0.3762374546146651, 0.3782367709529432, 0.378456503718203, 0.3755849917245105, 0.3749620059560631, 0.36809034039819144, 0.36870617429576913, 0.3730806762448494, 0.36712608452088785, 0.36660385567967485, 0.36959863246133334, 0.3632444916217308, 0.3644418810118182, 0.36485538633696757, 0.36698362255484107, 0.3613756539295036, 0.35718589920177046, 0.36429271227137505, 0.36132887983063694, 0.3543453055022532, 0.3603528007097684, 0.3564891892720044, 0.35404422895372073, 0.35419524224793036, 0.35227534304143293, 0.3567796440790016, 0.3542361634614345, 0.35447870001075715, 0.34879108730370434, 0.3530310347958955, 0.3487641865887293, 0.3490374646778029, 0.34873911532444685, 0.34579125224413265, 0.3513515398592807, 0.3442573002602673, 0.3463528130435685, 0.3472748042606726, 0.3470753237483947, 0.34206526155071204, 0.3467505073482751, 0.34544345078268024, 0.3452807960067661, 0.34138486039670823, 0.3389452373997629, 0.3426176652029601, 0.34132564726076153, 0.3424493167296981, 0.3418385875095843, 0.3418787742856395, 0.34008422926027926, 0.339485853748916, 0.33518301576456727, 0.3393149601653985, 0.34077820496830513, 0.33797502247137107, 0.3379082687017394, 0.33854370699503883, 0.34068398940853956, 0.3345345121815922, 0.3382403756464077, 0.3322211721644492, 0.3343696817957612, 0.33672725802999204, 0.33137381682551004, 0.33107420422520417, 0.33307342098011233, 0.3294808776559545, 0.33175269314428657, 0.33532339277952344, 0.3320228717514493, 0.3307226697200036, 0.32816360587996196, 0.32942063278622097, 0.32721746008247543, 0.3277757565584286, 0.3272739693643601, 0.32677768360631576, 0.32788037005963366, 0.3252505469855254, 0.323792631427447, 0.3279054834025339, 0.324119375973213, 0.3267514456093796, 0.31973684213671905, 0.3212464982416572, 0.3202035142800349, 0.32931598875580764, 0.32346599341084964, 0.32207406047721543, 0.3172987587245176, 0.3217875505204446, 0.31829690812079886, 0.31830294115271995, 0.31893069468701113, 0.31877699518591407, 0.312669252032833, 0.31463882803593873, 0.31603938769195783, 0.3125667206155575, 0.3170066020756879, 0.31546638635438956, 0.31003526991944974, 0.31431218396195876, 0.3105714437034395, 0.30887893088626345, 0.3134051806881499, 0.31058979543243964, 0.30319389363695287, 0.3121243864701692, 0.3095417679325352, 0.30828995017339866, 0.3101163512565256, 0.3107531458381715, 0.3027265773717627, 0.3056968258325323, 0.3024863699668145, 0.30128581620005734, 0.3016197922107004, 0.299526641968143, 0.2986661980387964, 0.2984643379645296, 0.29741050064725283, 0.2990728767745217, 0.2990959255191369, 0.29619891364076917, 0.29736113919798274, 0.2955238256593384, 0.29230425500772833, 0.2962869802706933, 0.29052087436442775, 0.29694146858805887, 0.29178089206296254, 0.29490946855163835, 0.29239766099830955, 0.2924875763212116, 0.28727431777046947, 0.2880558754168552, 0.2857201242753807, 0.2816028292909224, 0.2807034290014567, 0.28417113209319955, 0.28439420837212387, 0.2830575307008374, 0.2813569333777841, 0.27838333346817873, 0.27398664932344663, 0.2771222206958264, 0.27646606219654807, 0.2802742558927717, 0.2772935264034646, 0.2740311811004228, 0.2723778088483707, 0.2723173482752428, 0.2726296799858088, 0.27150820464218856, 0.26980353556674347, 0.26999763235813234, 0.271178680888521, 0.2675590464052792, 0.2703916107816748, 0.266955815921954, 0.2677417722982443, 0.2664734733096629, 0.2616598152007837, 0.2618987378348826, 0.26313713015256535, 0.2639927453182254, 0.259921546025974, 0.258447800689758, 0.25932324954326236, 0.25735331044045245, 0.2553924481841284, 0.2541026953193877, 0.25215241785456494, 0.2530908481013484, 0.25340088620418455, 0.25041529901709336, 0.2508946477478436, 0.2468922998766266, 0.2466819979311005, 0.24553922820220472, 0.24616448865915702, 0.24352020812713035, 0.245112945454392, 0.24095632723434185, 0.23944184220418696, 0.2382999281007746, 0.23802665427206007, 0.23865492385496614, 0.23545282233132903, 0.2377435686142464, 0.23764096109605418, 0.2318824281900879, 0.23145699398062095, 0.23044203516590564, 0.22966974194295361, 0.2281639202823484, 0.22935683029857754, 0.2232396638974911, 0.22491911413062232, 0.22704380019731962, 0.2246162830410288, 0.22387785173770858, 0.22307075927574138, 0.21958557272022008, 0.21611802863477045, 0.2176091348737236, 0.21643275698914438, 0.21815684972820568, 0.21690630444343173, 0.2144953469596904, 0.21045533636317343, 0.21018598590116838, 0.20738654356540703, 0.21073267158615558, 0.20730509647347417, 0.20378858273026096, 0.2044903948034859, 0.20360999324215137, 0.1994028746233723, 0.20446652236588925, 0.20125386017932478, 0.19629940685379474, 0.19656375153520242, 0.19888904985535114, 0.19253174583036403, 0.19341871907636726, 0.1911584874236487, 0.19066347582593843, 0.19120384700253082, 0.18559699137076777, 0.1865291543365494, 0.18703436189227635, 0.18597213580679442, 0.1833313486076952, 0.18305706753721082, 0.1841687772897524, 0.18172660851623954, 0.17979243877255496, 0.1783425477017878, 0.17759921925702715, 0.17537852715912874, 0.1756007778571873, 0.17712873846373262, 0.1747176744104401, 0.17266693390239546, 0.16993015551268248, 0.17234542241022194, 0.17027409417115577, 0.17333801079612116, 0.16846228924991316, 0.16776102249255673, 0.17062995226442976, 0.169317345773463, 0.16778956143675136, 0.16644412503252184, 0.16641084437932424, 0.16423629589762467, 0.16514317738775638, 0.16319842011665264, 0.16312887173317636, 0.16379846247836827, 0.16102075314295647, 0.16114530194459892, 0.16031344735687017, 0.1607335551180006, 0.1630021522381926, 0.1587733793246552, 0.16005276916876718, 0.15837314123222176, 0.1588358394296835, 0.1603796430699386, 0.16015164662142434, 0.15751453530457285, 0.15853346335047952, 0.15670412618333732, 0.16078843703357185, 0.15981154410335108, 0.1568133581258094, 0.1594764508286789]\n",
        "train_acc_list_1e2 = [46.18951826363155, 75.3859184753838, 79.5214399152991, 82.13446267866595, 83.8369507676019, 84.98676548438327, 86.23186871360508, 86.93276866066702, 87.21651667548967, 87.58284806776072, 87.91953414505029, 88.02541026998412, 88.43620963472736, 88.48914769719428, 88.59925886712546, 88.89994706193754, 88.69666490206458, 89.00582318687135, 88.91053467443092, 89.20910534674431, 89.17734250926416, 89.13075701429328, 89.19428268925357, 89.38909475913182, 89.55637903652726, 89.499205929063, 89.42721016410799, 89.4907358390683, 89.41450502911593, 89.40603493912123, 89.69401799894123, 89.6156696664902, 89.5436739015352, 89.5796717840127, 89.7427210164108, 89.89306511381683, 89.7511911064055, 89.74483853890948, 89.93965060878772, 89.74060349391212, 89.93118051879301, 89.99682371625198, 89.97564849126522, 89.99258867125464, 89.88459502382213, 90.10693488618317, 90.01588141874008, 90.13234515616729, 89.9777660137639, 90.18316569613552, 90.14716781365802, 90.19163578613023, 90.26151402858656, 90.0582318687136, 90.34409740603493, 90.23186871360508, 90.30174695606141, 90.29539438856538, 90.37162519851773, 90.16410799364743, 90.25092641609317, 90.20434092112228, 90.40338803599788, 90.49867654843833, 90.26151402858656, 90.33774483853891, 90.25092641609317, 90.47750132345156, 90.31021704605611, 90.4372683959767, 90.39491794600318, 90.67866596082584, 90.47114875595553, 90.35044997353097, 90.40550555849656, 90.57278983589201, 90.46691371095818, 90.46056114346214, 90.64902064584436, 90.43091582848068, 90.65749073583906, 90.53890947591319, 90.6003176283748, 90.6807834833245, 90.61302276336686, 90.65325569084172, 90.62996294335628, 90.7993647432504, 90.58337744838539, 90.66807834833246, 90.716781365802, 90.78242456326099, 90.7993647432504, 90.81842244573849, 90.94123875066173, 90.78454208575967, 90.83748014822658, 90.72313393329804, 90.87347803070408, 90.8628904182107, 90.74642668078349, 91.11064055055584, 90.8798305982001, 91.0619375330863, 90.98570672313393, 91.02805717310747, 90.86924298570672, 90.98782424563261, 91.01535203811541, 91.1784012705135, 90.95606140815246, 91.12122816304924, 91.19745897300159, 91.08523028057174, 91.12758073054526, 91.27157226045527, 91.08523028057174, 91.14452091053468, 91.28851244044468, 91.07676019057702, 91.21863419798835, 91.36897829539438, 91.16146109052409, 91.29486500794071, 91.27368978295394, 91.19534145050291, 91.23133933298041, 91.56167284277396, 91.24827951296983, 91.33933298041292, 91.43250397035469, 91.41556379036527, 91.32027527792482, 91.68025410269983, 91.61672842773955, 91.68448914769719, 91.64002117522499, 91.61249338274219, 91.63790365272631, 91.7564849126522, 91.73954473266278, 91.7289571201694, 91.77977766013764, 91.66966649020645, 91.83059820010588, 91.80518793012176, 91.76919004764426, 91.96823716251986, 91.82636315510852, 92.06776071995765, 91.928004235045, 91.95129698253044, 91.87294865007941, 91.93435680254103, 91.9724722075172, 92.0635256749603, 92.14822657490735, 92.10164107993647, 92.33245103229221, 92.26257278983589, 92.11858125992589, 92.11858125992589, 92.2795129698253, 92.27527792482795, 92.4955002646903, 92.51032292218105, 92.36633139227104, 92.41926945473796, 92.2435150873478, 92.47220751720486, 92.55690841715193, 92.66913710958178, 92.55267337215457, 92.52726310217047, 92.55902593965061, 92.69242985706722, 92.6945473795659, 92.57384859714135, 92.76019057702489, 92.68395976707252, 92.78348332451033, 92.67972472207518, 92.69878242456326, 92.93170989941768, 92.92535733192165, 92.81948120698783, 92.6945473795659, 92.91688724192694, 92.95288512440445, 93.0291159343568, 93.07146638433034, 93.11805187930122, 93.10322922181048, 93.13499205929062, 93.12652196929592, 93.19216516675489, 93.25145579671783, 93.10958178930652, 93.24510322922181, 93.37427210164108, 93.38485971413446, 93.37003705664372, 93.54155637903652, 93.41450502911593, 93.60084700899947, 93.55214399152992, 93.7067231339333, 93.63472736897829, 93.67284277395447, 93.7702488088936, 93.63260984647962, 93.65590259396507, 93.86341979883537, 93.8147167813658, 93.86130227633669, 93.92694547379566, 94.01164637374272, 94.00952885124404, 94.08364213869773, 94.02646903123346, 93.92482795129698, 94.01799894123874, 94.11752249867655, 94.21069348861832, 94.17046056114346, 94.36315510852303, 94.39068290100582, 94.32292218104817, 94.20434092112228, 94.24669137109582, 94.3928004235045, 94.56643726839597, 94.54102699841185, 94.59820010587613, 94.54102699841185, 94.72101641079936, 94.72948650079407, 94.6998411858126, 94.67654843832716, 94.84171519322393, 94.716781365802, 94.9348861831657, 94.98147167813659, 94.98147167813659, 94.87347803070408, 95.05558496559026, 95.16569613552144, 95.06405505558496, 95.21228163049233, 95.14875595553202, 95.23557437797777, 95.23980942297511, 95.30333509793542, 95.25886712546321, 95.39650608787719, 95.40921122286925, 95.44944415034409, 95.40497617787189, 95.54261514028586, 95.56802541026998, 95.59131815775542, 95.58920063525674, 95.61037586024352, 95.6019057702488, 95.64637374272101, 95.76707252514558, 95.84330333509793, 95.68872419269455, 95.80518793012176, 95.64002117522499, 95.86659608258337, 95.90047644256221, 95.7924827951297, 95.80730545262044, 95.92165166754897, 95.91318157755425, 95.91741662255161, 95.99576495500264, 95.9364743250397, 96.02117522498676, 96.10587612493383, 95.99576495500264, 96.09105346744309, 96.06140815246162, 96.1905770248809, 96.06564319745897, 95.94070937003706, 96.08893594494441, 96.11858125992589, 96.20963472736898, 96.23716251985178, 96.15669666490207, 96.11858125992589, 96.215987294865, 96.13340391741663, 96.22445738485972, 96.09105346744309, 96.1630492323981, 96.2519851773425, 96.12916887241927]\n",
        "test_loss_list_1e2 = [1.3247391093595355, 1.4372946111594929, 1.221768849620632, 1.04543336437029, 0.5333422891357366, 0.5233590896515286, 0.49802757405182896, 0.527402958010926, 0.4824988966186841, 0.4684229656761768, 0.49865198369119684, 0.47028423013056025, 0.4917339387477613, 0.4712933559774184, 0.4579176530241966, 0.4159662657800843, 0.4274352864891875, 0.4261161432254548, 0.4882675524727971, 0.4069283462914766, 0.5291290985895138, 0.43474191529493705, 0.4278999815384547, 0.39875933359943183, 0.4106285313473028, 0.43170712793282434, 0.41362819207065243, 0.3644701678524999, 0.3623922984389698, 0.40609714147799153, 0.4058787767939708, 0.3751636910087922, 0.4418983731199713, 0.4179941722575356, 0.41956588603994427, 0.46951715706610214, 0.3965024639140157, 0.5922169619623352, 0.40990259790537403, 0.4151836939009966, 0.42209712372106667, 0.41093988658166397, 0.35882656948239194, 0.4029843812333603, 0.43423887977705283, 0.38498282746649254, 0.39403144240963695, 0.4007577213148276, 0.3736706005007613, 0.37586646884971975, 0.3973163742499024, 0.6469484598321074, 0.40791156398607237, 0.3950306963832939, 0.3600991340536697, 0.3767053528013183, 0.3982468478965993, 0.4106949169700052, 0.43118151138518374, 0.38861456491491375, 0.3672837637657044, 0.39455970508210797, 0.39955709195312333, 0.4675620217241493, 0.4306795852733593, 0.36596085978489296, 0.35671870439660314, 0.36200251970805375, 0.3765243383160993, 0.34345025160149034, 0.3757677159326918, 0.5893007164784506, 0.40471649535146415, 0.3984757918937534, 0.4169327455262343, 0.5200155050146813, 0.3654365451896892, 0.3924478087939468, 0.35877613043960405, 0.5022338476835513, 0.38222038358742116, 0.3505580706485346, 0.3415709840608578, 0.3706067300894681, 0.377026793492191, 0.35964643889490294, 0.39072127464939566, 0.4306023905382437, 0.3463652852703543, 0.4006179951715703, 0.3697551385883023, 0.3627208395331514, 0.3898671181032471, 0.4780514804171581, 0.3634938892780566, 0.5188167922052682, 0.3604150237698181, 0.35250089288342235, 0.3252626089196579, 0.3479796185651246, 0.3554346494961019, 0.3606822587111417, 0.3899602895127792, 0.40885376608839225, 0.3573276655492829, 0.350634348816147, 0.3364679439395082, 0.3656962395590894, 0.5394042597389689, 0.3279435099193863, 0.3306477459184095, 0.3268139467519872, 0.36625869447986287, 0.39838596696362777, 0.381319522857666, 0.351467617002188, 0.33824310422528026, 0.34666523709893227, 0.45219722740790425, 0.31658349412621234, 0.3582795552325015, 0.3590943407924736, 0.34482148333507423, 0.33396274413840443, 0.34916307185502615, 0.4292598191429587, 0.32467805952125905, 0.3357189829443015, 0.4115390076356776, 0.31629938288938764, 0.39742738264156324, 0.3100176707786672, 0.30909344707341757, 0.412040871074971, 0.31328197568655014, 0.31859667730682034, 0.3150135146490499, 0.31053604923334777, 0.3349029737360337, 0.5725868261035751, 0.33193637963895706, 0.3257182556770596, 0.3234951227319007, 0.33131444804808674, 0.32013773888933894, 0.3087677399755693, 0.31707401840271904, 0.3055169947226258, 0.30974703742300763, 0.32284443306864474, 0.3504650902514364, 0.30639906819252405, 0.32764800407868977, 0.34437243019541103, 0.3272252177783087, 0.3307260185041848, 0.30390654708824905, 0.30922777417535874, 0.29515569072728065, 0.30915866412368476, 0.30551093005958724, 0.3073367017741297, 0.30962466521590365, 0.31125049318606945, 0.3018519765898293, 0.3057772036568791, 0.30710070960077585, 0.3110234323231613, 0.3019946907080856, 0.30974323379204555, 0.3310978485673082, 0.3041924396712406, 0.28668475308108565, 0.29636092403647946, 0.3006317568029843, 0.2844046983940929, 0.35042893543255094, 0.2765379489636889, 0.2871919143579754, 0.30185085483918, 0.2806559734265594, 0.29328389626507667, 0.2850948043313681, 0.34038400401671726, 0.29658816148545225, 0.2900318577885628, 0.3164708414936767, 0.2704349430460556, 0.28909379587161776, 0.2766348912070195, 0.2954838537410194, 0.2803557366132736, 0.2698567771151954, 0.28018723702167764, 0.2962662287275581, 0.27532919876131356, 0.2694316993595338, 0.2662475112022138, 0.28343176304855766, 0.29671202501391664, 0.27179979993139997, 0.28223006507637455, 0.2900701365707552, 0.2721700471844159, 0.26092747346881556, 0.26539998760848654, 0.26519521442698496, 0.27638812151317504, 0.2700666151575598, 0.2631940585652403, 0.2667978351533997, 0.2727283788662331, 0.26423550035585375, 0.2590555296297751, 0.27035205230555115, 0.26533449850246016, 0.2622304866273029, 0.2551094347282368, 0.26004181678096455, 0.27162871688750445, 0.24523562127176454, 0.2569774056912637, 0.251955021099717, 0.25539296215363577, 0.25087627918258604, 0.25988455777805225, 0.26432519333035337, 0.25944741738631444, 0.2533169457068046, 0.2480630805606351, 0.2482597777145166, 0.24768747327228388, 0.2565581485921261, 0.27113300364683657, 0.24470692774390473, 0.24457374613220786, 0.24245896723632718, 0.24782768728248045, 0.24563301789263883, 0.25142964742639484, 0.24496393921036347, 0.24507040318612958, 0.2429210834716465, 0.24630526723522767, 0.24146442790972253, 0.24734521755839095, 0.24448578213067615, 0.24797514888147512, 0.24284489299444592, 0.24469150555338345, 0.24450236584479904, 0.24315517479736432, 0.2357017386741206, 0.24220527233738526, 0.23520723346839933, 0.23920954018831253, 0.23878256140240267, 0.23733027867388493, 0.23719553802819812, 0.24080834782444963, 0.23478803265036322, 0.23879089633769848, 0.23768455305081956, 0.24077472737168565, 0.23638445438415395, 0.23551501680676842, 0.24104665650748738, 0.2356582859202343, 0.23666301727587102, 0.23688774943059565, 0.23403703056129754, 0.235051355598604, 0.23781126664549695, 0.23505273757173734, 0.2296296937719864, 0.23563272381822267, 0.2333506334609553, 0.23327350784458367, 0.2340209804037038, 0.23465112187698775, 0.23493007486503498, 0.2349644734652019, 0.2349888317506103, 0.2317731349783785, 0.2339920843129649, 0.23434527083208748, 0.23276801688560084, 0.23693077834140436, 0.2345955311959865, 0.23683660813406401, 0.23327333925693644, 0.23412100777176081, 0.23264528898631825, 0.23628339854379496, 0.23472745530307293, 0.23727154311742268, 0.23173269472431904, 0.23639510246906795, 0.23554416570593328, 0.23380914703011513]\n",
        "test_acc_list_1e2 = [56.81084818684696, 54.15258143822987, 59.11954517516902, 67.14428395820529, 85.01843884449907, 85.47172710510141, 85.13368162261831, 84.16564228641671, 86.33988936693301, 86.0978795328826, 85.54087277197296, 86.13245236631838, 85.2681315304241, 86.41287645974185, 86.75476336816226, 87.79963122311001, 87.76121696373694, 88.2221880762139, 85.62538414259373, 88.20298094652735, 84.53826060233558, 87.7458512599877, 87.51920712968654, 89.18638598647819, 88.2721266133989, 87.17732022126613, 87.8418869084204, 89.44376152427782, 89.52827289489859, 88.41425937307929, 88.28749231714812, 89.37461585740627, 87.20421020282728, 87.83036263060848, 87.66133988936693, 86.71250768285188, 88.33358942839583, 82.68669330055316, 88.54870928088506, 87.51152427781193, 87.83420405654579, 88.10310387215735, 89.62814996926859, 88.24523663183774, 87.93792255685311, 89.12492317148126, 88.72157344806392, 88.62553779963122, 89.69345421020283, 89.18638598647819, 88.24907805777505, 80.7659803318992, 88.48724646588813, 88.57559926244622, 89.98924400737553, 89.1479717271051, 88.84449907805778, 88.09157959434542, 87.25799016594961, 88.89059618930547, 89.37461585740627, 88.57944068838353, 88.48724646588813, 87.0582360172096, 87.6421327596804, 89.19406883835279, 90.0660725261217, 89.82022126613398, 88.93669330055316, 90.22357098955132, 89.32467732022127, 83.88137676705593, 88.01090964966195, 89.02120467117393, 88.24523663183774, 84.43070067609096, 89.69345421020283, 88.7062077443147, 90.0660725261217, 85.70221266133989, 88.79840196681008, 89.91241548862938, 90.34265519360787, 89.30931161647203, 89.48217578365089, 90.0660725261217, 88.67163491087892, 87.88414259373079, 90.2158881376767, 88.36432083589429, 89.48217578365089, 89.68961278426552, 88.49492931776275, 86.10556238475722, 89.4860172095882, 85.2220344191764, 89.60510141364475, 89.93930547019053, 90.77289489858636, 90.18131530424094, 89.85479409956976, 89.64351567301783, 88.859864781807, 87.96481253841426, 89.85095267363245, 89.98156115550093, 90.58082360172097, 89.44760295021511, 83.9582052858021, 90.56545789797173, 90.71527350952674, 90.63460356484327, 89.3899815611555, 88.23755377996312, 88.84449907805778, 89.64735709895513, 90.29655808236018, 90.08143822987093, 86.79317762753534, 91.07636754763368, 89.51290719114935, 89.83942839582053, 90.20052243392747, 90.61155500921942, 90.20436385986478, 87.51920712968654, 90.85740626920713, 90.80746773202213, 88.31822372464659, 91.16087891825445, 88.64090350338046, 91.20697602950216, 91.18776889981561, 88.29901659496005, 91.08789182544561, 91.02258758451137, 90.99569760295022, 91.32221880762138, 90.33497234173325, 83.41272280270437, 90.75368776889981, 90.65381069452981, 90.89966195451751, 90.79978488014751, 90.9419176398279, 91.30301167793485, 91.03027043638599, 91.64874001229256, 91.40288875230486, 90.91886908420406, 90.13905961893055, 91.13014751075599, 90.70374923171481, 90.13905961893055, 90.51551936078673, 90.67685925015365, 91.379840196681, 91.22234173325138, 91.6180086047941, 91.24923171481254, 91.329901659496, 91.28380454824831, 91.45282728948986, 90.97264904732637, 91.51044867854948, 91.3759987707437, 91.38752304855562, 91.16856177012907, 91.57191149354641, 91.18776889981561, 90.64228641671788, 91.29917025199754, 91.88306699446835, 91.58343577135832, 91.50660725261217, 91.91763982790411, 89.92393976644131, 92.40550092194222, 91.87922556853104, 91.72940995697603, 92.17885679164105, 92.0098340503995, 91.98294406883835, 90.3119237861094, 91.68715427166565, 91.76782421634911, 91.08020897357099, 92.5361094038107, 91.83312845728334, 92.08282114320836, 91.83696988322065, 92.11739397664412, 92.65903503380454, 92.23263675476336, 91.71788567916411, 92.31714812538414, 92.50921942224954, 92.65519360786725, 92.14044253226798, 91.75245851259987, 92.42086662569146, 92.15964966195452, 91.71788567916411, 92.55531653349723, 92.84726490473264, 92.57452366318377, 92.5860479409957, 92.32483097725876, 92.52842655193608, 92.68976644130301, 92.58220651505839, 92.440073755378, 92.74738783036263, 92.8242163491088, 92.59757221880763, 92.5361094038107, 92.7819606637984, 92.86647203441917, 92.79732636754764, 92.46696373693915, 93.28134603564843, 92.98555623847572, 93.04701905347265, 93.26213890596189, 93.1737861094038, 92.78580208973571, 92.80885064535956, 92.91256914566686, 93.11616472034419, 93.13153042409343, 93.15457897971727, 93.3581745543946, 92.99323909035034, 92.47848801475108, 93.25061462814998, 93.39658881376766, 93.43116164720344, 93.28134603564843, 93.37738168408113, 93.15457897971727, 93.24677320221267, 93.23524892440074, 93.30055316533497, 93.2160417947142, 93.44652735095268, 93.26982175783651, 93.39658881376766, 93.31976029502151, 93.51567301782421, 93.25061462814998, 93.42347879532882, 93.29671173939767, 93.6539643515673, 93.4081130915796, 93.72311001843885, 93.65012292563, 93.53103872157345, 93.37738168408113, 93.57329440688383, 93.62707437000614, 93.6578057775046, 93.63475722188076, 93.63091579594345, 93.61170866625692, 93.63475722188076, 93.69237861094038, 93.55024585125999, 93.75768285187462, 93.71158574062692, 93.66933005531654, 93.85755992624462, 93.71158574062692, 93.58097725875845, 93.5579287031346, 93.7922556853104, 93.61939151813154, 93.799938537185, 93.8921327596804, 93.75, 93.88829133374308, 93.88444990780577, 93.77688998156115, 93.77304855562384, 93.87676705593117, 93.83066994468346, 93.81146281499693, 93.97280270436386, 93.72311001843885, 93.75, 93.70390288875231, 93.76920712968654, 93.75768285187462, 93.82682851874615, 93.78073140749846, 93.88829133374308, 93.74231714812538, 93.87676705593117, 93.81530424093424, 93.80762138905962, 93.8460356484327]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ubm3Y5CSnZ0D"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_acc_list_5e4)), train_acc_list_5e4, 'b')\n",
        "plt.plot(range(len(train_acc_list_1e2)), train_acc_list_1e2, 'r')\n",
        "#plt.plot(range(len(train_acc_list_step)), train_acc_list_step, 'g')\n",
        "#plt.plot(range(len(train_acc_list_linear)), train_acc_list_linear, 'y')\n",
        "#plt.plot(range(len(train_acc_list_exp)), train_acc_list_exp, 'purple')\n",
        "\n",
        "plt.plot(range(len(test_acc_list_5e4)), test_acc_list_5e4, color='b', linestyle='--')\n",
        "plt.plot(range(len(test_acc_list_1e2)), test_acc_list_1e2,color='r', linestyle='--')\n",
        "#plt.plot(range(len(test_acc_list_step)), test_acc_list_step, color='g', linestyle='--')\n",
        "#plt.plot(range(len(test_acc_list_linear)), test_acc_list_linear, color='y', linestyle='--')\n",
        "#plt.plot(range(len(test_acc_list_exp)), test_acc_list_exp, color='purple', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "#plt.ylim([85, 101])\n",
        "plt.title(\"Combined accuracy\")\n",
        "plt.legend(['train with weight decay coefficient = 5e-4', 'train with weight decay coefficient = 1e-2',\n",
        "            'test with weight decay coefficient = 5e-4', 'test with weight decay coefficient = 1e-2'])\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QnOTaNoesBEb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_loss_list_5e4)), train_loss_list_5e4, 'b')\n",
        "plt.plot(range(len(train_loss_list_1e2)), train_loss_list_1e2, 'r')\n",
        "#plt.plot(range(len(train_loss_list_step)), train_loss_list_step, 'g')\n",
        "#plt.plot(range(len(train_loss_list_linear)), train_loss_list_linear, 'y')\n",
        "#plt.plot(range(len(train_loss_list_exp)), train_loss_list_exp, 'purple')\n",
        "\n",
        "plt.plot(range(len(test_loss_list_5e4)), test_loss_list_5e4, color='b', linestyle='--')\n",
        "plt.plot(range(len(test_loss_list_1e2)), test_loss_list_1e2,color='r', linestyle='--')\n",
        "#plt.plot(range(len(test_loss_list_step)), test_loss_list_step, color='g', linestyle='--')\n",
        "#plt.plot(range(len(test_loss_list_linear)), test_loss_list_linear, color='y', linestyle='--')\n",
        "#plt.plot(range(len(test_loss_list_exp)), test_loss_list_exp, color='purple', linestyle='--')\n",
        "\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "#plt.ylim([85, 101])\n",
        "plt.title(\"Combined loss\")\n",
        "plt.legend(['train with weight decay coefficient = 5e-4', 'train with weight decay coefficient = 1e-2',\n",
        "            'test with weight decay coefficient = 5e-4', 'test with weight decay coefficient = 1e-2'])\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "2d35a857fdeedecb30594b1c7eb95a8c0480700735195f416faf3d51f501baa5"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "012c4fccfa124eb294393069e02f09d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02696ee9f9ce4417ae638d5844c2ec73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1878c24981c347adb95e0e89c72fe9f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d96196040e641699e8a78db79cf7b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3c542fe55a24623ba09e0f6cbcf6857",
              "IPY_MODEL_dcc6414fe83041d8b5ebd3934cb1ca90",
              "IPY_MODEL_92dff67c5c9049298e504f042450a0fc"
            ],
            "layout": "IPY_MODEL_e13f87e33fad4a1e95fb438dd982ee13"
          }
        },
        "5ae2aba4cc914176a5ccfc663eb6310a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcc36657b3d44ea8a74b5ee30f4677d9",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d563a2f339ec4d0e896caf5b765d2be5",
            "value": 64275384
          }
        },
        "5c3384e0e4044a3d82e3e66f3682220a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "722efa37812c450c8997c8a0b916a30c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02696ee9f9ce4417ae638d5844c2ec73",
            "placeholder": "",
            "style": "IPY_MODEL_5c3384e0e4044a3d82e3e66f3682220a",
            "value": " 64275384/64275384 [00:06&lt;00:00, 20759009.68it/s]"
          }
        },
        "78da9a8381fe41d6b4341e3603daf12e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84d2ff23e4634816b1c3291e34c4661b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92dff67c5c9049298e504f042450a0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d2ff23e4634816b1c3291e34c4661b",
            "placeholder": "",
            "style": "IPY_MODEL_1878c24981c347adb95e0e89c72fe9f7",
            "value": " 182040794/182040794 [00:11&lt;00:00, 21134256.13it/s]"
          }
        },
        "969dedb7d7e74432919aaee552cc4cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a57bfb0e83374f4493cc9062e20a718b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc3ee902af6e47b7b518a1aabaf7ba85",
            "placeholder": "",
            "style": "IPY_MODEL_b19fb9dad8774b1c8161b72430accf56",
            "value": "100%"
          }
        },
        "a7a1a99f011e4610ad5ac35d32b324fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a57bfb0e83374f4493cc9062e20a718b",
              "IPY_MODEL_5ae2aba4cc914176a5ccfc663eb6310a",
              "IPY_MODEL_722efa37812c450c8997c8a0b916a30c"
            ],
            "layout": "IPY_MODEL_f51e75cc2ff347969ebd105e903a8952"
          }
        },
        "b19fb9dad8774b1c8161b72430accf56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc3ee902af6e47b7b518a1aabaf7ba85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcc36657b3d44ea8a74b5ee30f4677d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3c542fe55a24623ba09e0f6cbcf6857": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa88d9d680484321a6889b511696efff",
            "placeholder": "",
            "style": "IPY_MODEL_78da9a8381fe41d6b4341e3603daf12e",
            "value": "100%"
          }
        },
        "d563a2f339ec4d0e896caf5b765d2be5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dcc6414fe83041d8b5ebd3934cb1ca90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_012c4fccfa124eb294393069e02f09d2",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_969dedb7d7e74432919aaee552cc4cde",
            "value": 182040794
          }
        },
        "e13f87e33fad4a1e95fb438dd982ee13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f51e75cc2ff347969ebd105e903a8952": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa88d9d680484321a6889b511696efff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
