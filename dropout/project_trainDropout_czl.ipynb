{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4b456e27ad664c38b0b9556cb3888ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98a1d76310e04a66990a590299971985",
              "IPY_MODEL_995c31f4be894c0fa4ccfd4bab30e589",
              "IPY_MODEL_f61b221a3a334144ab1d72d3a3d3bb07"
            ],
            "layout": "IPY_MODEL_211635636d0b426f86ed3338b97de7d6"
          }
        },
        "98a1d76310e04a66990a590299971985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ad9887c9ff5431e9e2f74eaf2b1afcf",
            "placeholder": "​",
            "style": "IPY_MODEL_93126859379f4562babd40066e716fcb",
            "value": "100%"
          }
        },
        "995c31f4be894c0fa4ccfd4bab30e589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e83351e16abe4dc6adf8cabd40155b46",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_681d4f84c5f345ec97ca7e4e5c9da8d7",
            "value": 182040794
          }
        },
        "f61b221a3a334144ab1d72d3a3d3bb07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5e93001d54b466c95d41d771bb2d0c8",
            "placeholder": "​",
            "style": "IPY_MODEL_3767951841bd46f1b99ff0656a1f48ed",
            "value": " 182040794/182040794 [00:02&lt;00:00, 113388991.68it/s]"
          }
        },
        "211635636d0b426f86ed3338b97de7d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ad9887c9ff5431e9e2f74eaf2b1afcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93126859379f4562babd40066e716fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e83351e16abe4dc6adf8cabd40155b46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "681d4f84c5f345ec97ca7e4e5c9da8d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5e93001d54b466c95d41d771bb2d0c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3767951841bd46f1b99ff0656a1f48ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9f54f48f03f4ebb86b41130123ff75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27738e8f05a143e59125ac9b6a9abe1a",
              "IPY_MODEL_b0e928470608487098909d58a1772a6e",
              "IPY_MODEL_257a16f61d6f475b868a47acb68b5017"
            ],
            "layout": "IPY_MODEL_8c6f97c301844676a239474488e0676f"
          }
        },
        "27738e8f05a143e59125ac9b6a9abe1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4edeeec17bd84773bc99106c0780f451",
            "placeholder": "​",
            "style": "IPY_MODEL_4a4fd7e4107a4050933815ecfd91b49b",
            "value": "100%"
          }
        },
        "b0e928470608487098909d58a1772a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07d617bfbe394062b5e4918b2490caad",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39a319cdd7fe4abfb8c97c84ad4d0e1e",
            "value": 64275384
          }
        },
        "257a16f61d6f475b868a47acb68b5017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dfd96e37745422892e65ca9d77d1d71",
            "placeholder": "​",
            "style": "IPY_MODEL_52d0f290b83746f692e814bd2e9d81e9",
            "value": " 64275384/64275384 [00:00&lt;00:00, 97922203.87it/s]"
          }
        },
        "8c6f97c301844676a239474488e0676f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4edeeec17bd84773bc99106c0780f451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a4fd7e4107a4050933815ecfd91b49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07d617bfbe394062b5e4918b2490caad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39a319cdd7fe4abfb8c97c84ad4d0e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7dfd96e37745422892e65ca9d77d1d71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52d0f290b83746f692e814bd2e9d81e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "4b456e27ad664c38b0b9556cb3888ac1",
            "98a1d76310e04a66990a590299971985",
            "995c31f4be894c0fa4ccfd4bab30e589",
            "f61b221a3a334144ab1d72d3a3d3bb07",
            "211635636d0b426f86ed3338b97de7d6",
            "4ad9887c9ff5431e9e2f74eaf2b1afcf",
            "93126859379f4562babd40066e716fcb",
            "e83351e16abe4dc6adf8cabd40155b46",
            "681d4f84c5f345ec97ca7e4e5c9da8d7",
            "e5e93001d54b466c95d41d771bb2d0c8",
            "3767951841bd46f1b99ff0656a1f48ed",
            "d9f54f48f03f4ebb86b41130123ff75a",
            "27738e8f05a143e59125ac9b6a9abe1a",
            "b0e928470608487098909d58a1772a6e",
            "257a16f61d6f475b868a47acb68b5017",
            "8c6f97c301844676a239474488e0676f",
            "4edeeec17bd84773bc99106c0780f451",
            "4a4fd7e4107a4050933815ecfd91b49b",
            "07d617bfbe394062b5e4918b2490caad",
            "39a319cdd7fe4abfb8c97c84ad4d0e1e",
            "7dfd96e37745422892e65ca9d77d1d71",
            "52d0f290b83746f692e814bd2e9d81e9"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "3920377b-5890-4175-b8fa-d122ae948bb6"
      },
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.SVHN),\n",
        "    transforms.ToTensor(),\n",
        "    \n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b456e27ad664c38b0b9556cb3888ac1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9f54f48f03f4ebb86b41130123ff75a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))\n",
        "print(trainset[4][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "e362c372-4b70-4517-f4ca-5e15252524be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        out = F.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        out = F.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        out = F.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "e10958f4-77f5-4316-8dbc-8074e3a0caec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "ca6c90b7-366c-4b33-e726-1334d0c8e785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArgupDVRwB8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b395b6a-bda5-4d64-923f-3f95a453dbb7"
      },
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss().to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.4271, accuracy : 12.39\n",
            "iteration : 100, loss : 2.4051, accuracy : 12.67\n",
            "iteration : 150, loss : 2.3919, accuracy : 12.66\n",
            "iteration : 200, loss : 2.3747, accuracy : 12.82\n",
            "iteration : 250, loss : 2.3651, accuracy : 13.06\n",
            "iteration : 300, loss : 2.3555, accuracy : 13.26\n",
            "iteration : 350, loss : 2.3363, accuracy : 14.13\n",
            "Epoch :   1, training loss : 2.3270, training accuracy : 14.52, test loss : 2.1145, test accuracy : 23.24\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 2.0606, accuracy : 26.62\n",
            "iteration : 100, loss : 2.0050, accuracy : 29.33\n",
            "iteration : 150, loss : 1.9596, accuracy : 30.84\n",
            "iteration : 200, loss : 1.9125, accuracy : 32.41\n",
            "iteration : 250, loss : 1.8681, accuracy : 33.95\n",
            "iteration : 300, loss : 1.8193, accuracy : 35.54\n",
            "iteration : 350, loss : 1.7788, accuracy : 36.83\n",
            "Epoch :   2, training loss : 1.7654, training accuracy : 37.16, test loss : 1.7707, test accuracy : 38.25\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 1.4417, accuracy : 47.61\n",
            "iteration : 100, loss : 1.4495, accuracy : 47.35\n",
            "iteration : 150, loss : 1.4329, accuracy : 47.54\n",
            "iteration : 200, loss : 1.4207, accuracy : 47.86\n",
            "iteration : 250, loss : 1.4067, accuracy : 48.23\n",
            "iteration : 300, loss : 1.3995, accuracy : 48.40\n",
            "iteration : 350, loss : 1.3904, accuracy : 48.71\n",
            "Epoch :   3, training loss : 1.3875, training accuracy : 48.72, test loss : 1.3545, test accuracy : 49.44\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 1.3115, accuracy : 50.11\n",
            "iteration : 100, loss : 1.2975, accuracy : 50.91\n",
            "iteration : 150, loss : 1.2985, accuracy : 51.10\n",
            "iteration : 200, loss : 1.2974, accuracy : 51.23\n",
            "iteration : 250, loss : 1.2939, accuracy : 51.23\n",
            "iteration : 300, loss : 1.2911, accuracy : 51.27\n",
            "iteration : 350, loss : 1.2876, accuracy : 51.28\n",
            "Epoch :   4, training loss : 1.2880, training accuracy : 51.23, test loss : 1.3742, test accuracy : 49.14\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 1.2749, accuracy : 51.58\n",
            "iteration : 100, loss : 1.2722, accuracy : 51.95\n",
            "iteration : 150, loss : 1.2674, accuracy : 52.16\n",
            "iteration : 200, loss : 1.2698, accuracy : 51.95\n",
            "iteration : 250, loss : 1.2711, accuracy : 51.74\n",
            "iteration : 300, loss : 1.2684, accuracy : 51.85\n",
            "iteration : 350, loss : 1.2665, accuracy : 51.82\n",
            "Epoch :   5, training loss : 1.2636, training accuracy : 51.82, test loss : 1.2692, test accuracy : 51.41\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 1.2487, accuracy : 52.94\n",
            "iteration : 100, loss : 1.2475, accuracy : 52.67\n",
            "iteration : 150, loss : 1.2471, accuracy : 52.50\n",
            "iteration : 200, loss : 1.2483, accuracy : 52.41\n",
            "iteration : 250, loss : 1.2421, accuracy : 52.58\n",
            "iteration : 300, loss : 1.2390, accuracy : 52.65\n",
            "iteration : 350, loss : 1.2380, accuracy : 52.69\n",
            "Epoch :   6, training loss : 1.2367, training accuracy : 52.71, test loss : 1.2488, test accuracy : 52.12\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 1.2185, accuracy : 52.36\n",
            "iteration : 100, loss : 1.2274, accuracy : 52.38\n",
            "iteration : 150, loss : 1.2245, accuracy : 52.60\n",
            "iteration : 200, loss : 1.2202, accuracy : 52.89\n",
            "iteration : 250, loss : 1.2184, accuracy : 53.02\n",
            "iteration : 300, loss : 1.2158, accuracy : 53.19\n",
            "iteration : 350, loss : 1.2167, accuracy : 53.13\n",
            "Epoch :   7, training loss : 1.2146, training accuracy : 53.21, test loss : 1.3196, test accuracy : 51.33\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 1.2304, accuracy : 52.39\n",
            "iteration : 100, loss : 1.2351, accuracy : 52.25\n",
            "iteration : 150, loss : 1.2292, accuracy : 52.43\n",
            "iteration : 200, loss : 1.2183, accuracy : 52.89\n",
            "iteration : 250, loss : 1.2204, accuracy : 52.71\n",
            "iteration : 300, loss : 1.2166, accuracy : 52.87\n",
            "iteration : 350, loss : 1.2143, accuracy : 52.99\n",
            "Epoch :   8, training loss : 1.2131, training accuracy : 53.03, test loss : 1.2091, test accuracy : 53.23\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 1.1980, accuracy : 53.17\n",
            "iteration : 100, loss : 1.2173, accuracy : 52.67\n",
            "iteration : 150, loss : 1.2104, accuracy : 52.95\n",
            "iteration : 200, loss : 1.2083, accuracy : 53.02\n",
            "iteration : 250, loss : 1.2018, accuracy : 53.17\n",
            "iteration : 300, loss : 1.2004, accuracy : 53.17\n",
            "iteration : 350, loss : 1.2005, accuracy : 53.14\n",
            "Epoch :   9, training loss : 1.1995, training accuracy : 53.16, test loss : 1.2207, test accuracy : 52.41\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 1.1922, accuracy : 53.28\n",
            "iteration : 100, loss : 1.2051, accuracy : 53.05\n",
            "iteration : 150, loss : 1.1998, accuracy : 53.22\n",
            "iteration : 200, loss : 1.2002, accuracy : 53.05\n",
            "iteration : 250, loss : 1.1948, accuracy : 53.28\n",
            "iteration : 300, loss : 1.1940, accuracy : 53.39\n",
            "iteration : 350, loss : 1.1934, accuracy : 53.44\n",
            "Epoch :  10, training loss : 1.1934, training accuracy : 53.44, test loss : 1.2469, test accuracy : 52.34\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 1.1847, accuracy : 53.59\n",
            "iteration : 100, loss : 1.1758, accuracy : 53.97\n",
            "iteration : 150, loss : 1.1773, accuracy : 53.98\n",
            "iteration : 200, loss : 1.1858, accuracy : 53.65\n",
            "iteration : 250, loss : 1.1822, accuracy : 53.84\n",
            "iteration : 300, loss : 1.1816, accuracy : 53.88\n",
            "iteration : 350, loss : 1.1824, accuracy : 53.85\n",
            "Epoch :  11, training loss : 1.1808, training accuracy : 53.98, test loss : 1.2102, test accuracy : 53.13\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 1.1981, accuracy : 53.09\n",
            "iteration : 100, loss : 1.1924, accuracy : 53.19\n",
            "iteration : 150, loss : 1.1876, accuracy : 53.46\n",
            "iteration : 200, loss : 1.1805, accuracy : 53.70\n",
            "iteration : 250, loss : 1.1769, accuracy : 53.85\n",
            "iteration : 300, loss : 1.1764, accuracy : 53.93\n",
            "iteration : 350, loss : 1.1758, accuracy : 53.96\n",
            "Epoch :  12, training loss : 1.1758, training accuracy : 53.95, test loss : 1.1897, test accuracy : 53.36\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 1.1778, accuracy : 53.70\n",
            "iteration : 100, loss : 1.1668, accuracy : 53.93\n",
            "iteration : 150, loss : 1.1657, accuracy : 54.15\n",
            "iteration : 200, loss : 1.1716, accuracy : 54.34\n",
            "iteration : 250, loss : 1.1749, accuracy : 54.06\n",
            "iteration : 300, loss : 1.1755, accuracy : 54.05\n",
            "iteration : 350, loss : 1.1780, accuracy : 53.85\n",
            "Epoch :  13, training loss : 1.1779, training accuracy : 53.87, test loss : 1.1666, test accuracy : 54.59\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 1.1647, accuracy : 54.36\n",
            "iteration : 100, loss : 1.1695, accuracy : 54.16\n",
            "iteration : 150, loss : 1.1596, accuracy : 54.31\n",
            "iteration : 200, loss : 1.1593, accuracy : 54.44\n",
            "iteration : 250, loss : 1.1587, accuracy : 54.49\n",
            "iteration : 300, loss : 1.1608, accuracy : 54.48\n",
            "iteration : 350, loss : 1.1602, accuracy : 54.49\n",
            "Epoch :  14, training loss : 1.1606, training accuracy : 54.52, test loss : 1.1898, test accuracy : 53.53\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 1.1363, accuracy : 54.83\n",
            "iteration : 100, loss : 1.1544, accuracy : 54.84\n",
            "iteration : 150, loss : 1.1624, accuracy : 54.51\n",
            "iteration : 200, loss : 1.1617, accuracy : 54.41\n",
            "iteration : 250, loss : 1.1598, accuracy : 54.38\n",
            "iteration : 300, loss : 1.1568, accuracy : 54.36\n",
            "iteration : 350, loss : 1.1590, accuracy : 54.26\n",
            "Epoch :  15, training loss : 1.1601, training accuracy : 54.18, test loss : 1.1676, test accuracy : 54.08\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 1.1674, accuracy : 53.67\n",
            "iteration : 100, loss : 1.1524, accuracy : 54.70\n",
            "iteration : 150, loss : 1.1538, accuracy : 54.61\n",
            "iteration : 200, loss : 1.1558, accuracy : 54.60\n",
            "iteration : 250, loss : 1.1513, accuracy : 54.82\n",
            "iteration : 300, loss : 1.1481, accuracy : 55.01\n",
            "iteration : 350, loss : 1.1443, accuracy : 55.06\n",
            "Epoch :  16, training loss : 1.1439, training accuracy : 55.00, test loss : 1.1819, test accuracy : 53.64\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 1.1430, accuracy : 55.27\n",
            "iteration : 100, loss : 1.1354, accuracy : 55.35\n",
            "iteration : 150, loss : 1.1447, accuracy : 54.89\n",
            "iteration : 200, loss : 1.1422, accuracy : 54.93\n",
            "iteration : 250, loss : 1.1454, accuracy : 54.77\n",
            "iteration : 300, loss : 1.1471, accuracy : 54.64\n",
            "iteration : 350, loss : 1.1420, accuracy : 54.81\n",
            "Epoch :  17, training loss : 1.1433, training accuracy : 54.77, test loss : 1.1768, test accuracy : 53.97\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 1.1343, accuracy : 55.47\n",
            "iteration : 100, loss : 1.1484, accuracy : 54.74\n",
            "iteration : 150, loss : 1.1517, accuracy : 54.69\n",
            "iteration : 200, loss : 1.1470, accuracy : 54.78\n",
            "iteration : 250, loss : 1.1420, accuracy : 54.82\n",
            "iteration : 300, loss : 1.1417, accuracy : 54.76\n",
            "iteration : 350, loss : 1.1429, accuracy : 54.73\n",
            "Epoch :  18, training loss : 1.1429, training accuracy : 54.69, test loss : 1.1464, test accuracy : 54.47\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 1.1440, accuracy : 54.47\n",
            "iteration : 100, loss : 1.1341, accuracy : 54.94\n",
            "iteration : 150, loss : 1.1368, accuracy : 54.90\n",
            "iteration : 200, loss : 1.1398, accuracy : 54.77\n",
            "iteration : 250, loss : 1.1435, accuracy : 54.74\n",
            "iteration : 300, loss : 1.1413, accuracy : 54.73\n",
            "iteration : 350, loss : 1.1431, accuracy : 54.59\n",
            "Epoch :  19, training loss : 1.1426, training accuracy : 54.59, test loss : 1.1679, test accuracy : 54.11\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 1.1581, accuracy : 54.19\n",
            "iteration : 100, loss : 1.1404, accuracy : 54.19\n",
            "iteration : 150, loss : 1.1440, accuracy : 54.22\n",
            "iteration : 200, loss : 1.1447, accuracy : 54.29\n",
            "iteration : 250, loss : 1.1368, accuracy : 54.60\n",
            "iteration : 300, loss : 1.1307, accuracy : 54.82\n",
            "iteration : 350, loss : 1.1328, accuracy : 54.76\n",
            "Epoch :  20, training loss : 1.1331, training accuracy : 54.75, test loss : 1.1287, test accuracy : 54.75\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 1.1009, accuracy : 56.34\n",
            "iteration : 100, loss : 1.1240, accuracy : 55.54\n",
            "iteration : 150, loss : 1.1213, accuracy : 55.49\n",
            "iteration : 200, loss : 1.1218, accuracy : 55.55\n",
            "iteration : 250, loss : 1.1279, accuracy : 55.34\n",
            "iteration : 300, loss : 1.1323, accuracy : 55.10\n",
            "iteration : 350, loss : 1.1333, accuracy : 55.11\n",
            "Epoch :  21, training loss : 1.1346, training accuracy : 55.00, test loss : 1.1421, test accuracy : 54.19\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 1.1237, accuracy : 54.66\n",
            "iteration : 100, loss : 1.1238, accuracy : 54.91\n",
            "iteration : 150, loss : 1.1332, accuracy : 54.49\n",
            "iteration : 200, loss : 1.1325, accuracy : 54.50\n",
            "iteration : 250, loss : 1.1324, accuracy : 54.65\n",
            "iteration : 300, loss : 1.1335, accuracy : 54.72\n",
            "iteration : 350, loss : 1.1339, accuracy : 54.77\n",
            "Epoch :  22, training loss : 1.1347, training accuracy : 54.74, test loss : 1.1537, test accuracy : 53.84\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 1.1357, accuracy : 54.50\n",
            "iteration : 100, loss : 1.1263, accuracy : 55.17\n",
            "iteration : 150, loss : 1.1238, accuracy : 55.26\n",
            "iteration : 200, loss : 1.1253, accuracy : 55.30\n",
            "iteration : 250, loss : 1.1292, accuracy : 55.12\n",
            "iteration : 300, loss : 1.1301, accuracy : 54.98\n",
            "iteration : 350, loss : 1.1284, accuracy : 55.07\n",
            "Epoch :  23, training loss : 1.1272, training accuracy : 55.11, test loss : 1.1455, test accuracy : 54.61\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 1.1213, accuracy : 54.23\n",
            "iteration : 100, loss : 1.1148, accuracy : 54.99\n",
            "iteration : 150, loss : 1.1114, accuracy : 55.31\n",
            "iteration : 200, loss : 1.1140, accuracy : 55.21\n",
            "iteration : 250, loss : 1.1175, accuracy : 55.23\n",
            "iteration : 300, loss : 1.1186, accuracy : 55.21\n",
            "iteration : 350, loss : 1.1223, accuracy : 55.06\n",
            "Epoch :  24, training loss : 1.1206, training accuracy : 55.12, test loss : 1.1280, test accuracy : 54.74\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 1.1222, accuracy : 56.03\n",
            "iteration : 100, loss : 1.1335, accuracy : 55.33\n",
            "iteration : 150, loss : 1.1233, accuracy : 55.52\n",
            "iteration : 200, loss : 1.1260, accuracy : 55.26\n",
            "iteration : 250, loss : 1.1276, accuracy : 55.19\n",
            "iteration : 300, loss : 1.1270, accuracy : 55.22\n",
            "iteration : 350, loss : 1.1246, accuracy : 55.26\n",
            "Epoch :  25, training loss : 1.1249, training accuracy : 55.25, test loss : 1.1673, test accuracy : 53.77\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 1.1377, accuracy : 54.64\n",
            "iteration : 100, loss : 1.1297, accuracy : 55.03\n",
            "iteration : 150, loss : 1.1316, accuracy : 54.65\n",
            "iteration : 200, loss : 1.1258, accuracy : 54.90\n",
            "iteration : 250, loss : 1.1248, accuracy : 54.98\n",
            "iteration : 300, loss : 1.1252, accuracy : 55.13\n",
            "iteration : 350, loss : 1.1251, accuracy : 55.15\n",
            "Epoch :  26, training loss : 1.1251, training accuracy : 55.10, test loss : 1.1292, test accuracy : 54.92\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 1.1228, accuracy : 53.77\n",
            "iteration : 100, loss : 1.1149, accuracy : 54.48\n",
            "iteration : 150, loss : 1.1141, accuracy : 54.71\n",
            "iteration : 200, loss : 1.1231, accuracy : 54.60\n",
            "iteration : 250, loss : 1.1208, accuracy : 54.76\n",
            "iteration : 300, loss : 1.1210, accuracy : 54.82\n",
            "iteration : 350, loss : 1.1193, accuracy : 54.77\n",
            "Epoch :  27, training loss : 1.1194, training accuracy : 54.80, test loss : 1.1233, test accuracy : 55.24\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 1.0971, accuracy : 55.30\n",
            "iteration : 100, loss : 1.1087, accuracy : 55.52\n",
            "iteration : 150, loss : 1.1081, accuracy : 55.35\n",
            "iteration : 200, loss : 1.1157, accuracy : 55.37\n",
            "iteration : 250, loss : 1.1154, accuracy : 55.29\n",
            "iteration : 300, loss : 1.1139, accuracy : 55.30\n",
            "iteration : 350, loss : 1.1153, accuracy : 55.29\n",
            "Epoch :  28, training loss : 1.1148, training accuracy : 55.32, test loss : 1.1227, test accuracy : 55.13\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 1.1050, accuracy : 56.47\n",
            "iteration : 100, loss : 1.1136, accuracy : 55.80\n",
            "iteration : 150, loss : 1.1155, accuracy : 55.38\n",
            "iteration : 200, loss : 1.1137, accuracy : 55.27\n",
            "iteration : 250, loss : 1.1160, accuracy : 55.26\n",
            "iteration : 300, loss : 1.1185, accuracy : 55.18\n",
            "iteration : 350, loss : 1.1202, accuracy : 55.26\n",
            "Epoch :  29, training loss : 1.1212, training accuracy : 55.25, test loss : 1.1175, test accuracy : 55.10\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 1.1091, accuracy : 55.84\n",
            "iteration : 100, loss : 1.1084, accuracy : 55.55\n",
            "iteration : 150, loss : 1.1039, accuracy : 55.60\n",
            "iteration : 200, loss : 1.1050, accuracy : 55.43\n",
            "iteration : 250, loss : 1.1096, accuracy : 55.43\n",
            "iteration : 300, loss : 1.1109, accuracy : 55.38\n",
            "iteration : 350, loss : 1.1111, accuracy : 55.33\n",
            "Epoch :  30, training loss : 1.1095, training accuracy : 55.40, test loss : 1.1020, test accuracy : 55.58\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 1.1208, accuracy : 54.34\n",
            "iteration : 100, loss : 1.1237, accuracy : 54.25\n",
            "iteration : 150, loss : 1.1155, accuracy : 54.88\n",
            "iteration : 200, loss : 1.1126, accuracy : 55.21\n",
            "iteration : 250, loss : 1.1169, accuracy : 55.13\n",
            "iteration : 300, loss : 1.1154, accuracy : 55.26\n",
            "iteration : 350, loss : 1.1155, accuracy : 55.25\n",
            "Epoch :  31, training loss : 1.1144, training accuracy : 55.38, test loss : 1.1148, test accuracy : 55.09\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 1.0667, accuracy : 56.31\n",
            "iteration : 100, loss : 1.0865, accuracy : 56.31\n",
            "iteration : 150, loss : 1.0958, accuracy : 56.10\n",
            "iteration : 200, loss : 1.1069, accuracy : 55.79\n",
            "iteration : 250, loss : 1.1046, accuracy : 55.86\n",
            "iteration : 300, loss : 1.1078, accuracy : 55.79\n",
            "iteration : 350, loss : 1.1045, accuracy : 55.79\n",
            "Epoch :  32, training loss : 1.1053, training accuracy : 55.74, test loss : 1.1578, test accuracy : 54.25\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 1.1184, accuracy : 54.88\n",
            "iteration : 100, loss : 1.1089, accuracy : 55.12\n",
            "iteration : 150, loss : 1.1155, accuracy : 54.90\n",
            "iteration : 200, loss : 1.1138, accuracy : 55.12\n",
            "iteration : 250, loss : 1.1136, accuracy : 55.29\n",
            "iteration : 300, loss : 1.1116, accuracy : 55.38\n",
            "iteration : 350, loss : 1.1108, accuracy : 55.41\n",
            "Epoch :  33, training loss : 1.1108, training accuracy : 55.40, test loss : 1.1099, test accuracy : 55.68\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 1.1169, accuracy : 55.05\n",
            "iteration : 100, loss : 1.1115, accuracy : 55.31\n",
            "iteration : 150, loss : 1.1114, accuracy : 55.35\n",
            "iteration : 200, loss : 1.1036, accuracy : 55.66\n",
            "iteration : 250, loss : 1.1054, accuracy : 55.52\n",
            "iteration : 300, loss : 1.1065, accuracy : 55.62\n",
            "iteration : 350, loss : 1.1077, accuracy : 55.59\n",
            "Epoch :  34, training loss : 1.1066, training accuracy : 55.63, test loss : 1.1291, test accuracy : 54.62\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 1.0927, accuracy : 55.25\n",
            "iteration : 100, loss : 1.0834, accuracy : 55.77\n",
            "iteration : 150, loss : 1.1005, accuracy : 55.36\n",
            "iteration : 200, loss : 1.1027, accuracy : 55.36\n",
            "iteration : 250, loss : 1.1028, accuracy : 55.52\n",
            "iteration : 300, loss : 1.1030, accuracy : 55.51\n",
            "iteration : 350, loss : 1.1018, accuracy : 55.56\n",
            "Epoch :  35, training loss : 1.1010, training accuracy : 55.61, test loss : 1.1406, test accuracy : 54.73\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 1.1237, accuracy : 54.72\n",
            "iteration : 100, loss : 1.1017, accuracy : 55.88\n",
            "iteration : 150, loss : 1.1028, accuracy : 55.67\n",
            "iteration : 200, loss : 1.0984, accuracy : 55.71\n",
            "iteration : 250, loss : 1.1034, accuracy : 55.61\n",
            "iteration : 300, loss : 1.1019, accuracy : 55.58\n",
            "iteration : 350, loss : 1.1001, accuracy : 55.74\n",
            "Epoch :  36, training loss : 1.0994, training accuracy : 55.82, test loss : 1.1035, test accuracy : 55.83\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 1.0864, accuracy : 55.88\n",
            "iteration : 100, loss : 1.0990, accuracy : 55.51\n",
            "iteration : 150, loss : 1.0927, accuracy : 55.66\n",
            "iteration : 200, loss : 1.0981, accuracy : 55.68\n",
            "iteration : 250, loss : 1.0976, accuracy : 55.77\n",
            "iteration : 300, loss : 1.0947, accuracy : 55.91\n",
            "iteration : 350, loss : 1.0966, accuracy : 55.81\n",
            "Epoch :  37, training loss : 1.0991, training accuracy : 55.76, test loss : 1.1243, test accuracy : 55.18\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 1.0848, accuracy : 56.86\n",
            "iteration : 100, loss : 1.0933, accuracy : 56.09\n",
            "iteration : 150, loss : 1.0959, accuracy : 56.09\n",
            "iteration : 200, loss : 1.0937, accuracy : 56.09\n",
            "iteration : 250, loss : 1.0962, accuracy : 55.81\n",
            "iteration : 300, loss : 1.0977, accuracy : 55.78\n",
            "iteration : 350, loss : 1.0986, accuracy : 55.75\n",
            "Epoch :  38, training loss : 1.0994, training accuracy : 55.79, test loss : 1.1395, test accuracy : 54.95\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 1.0919, accuracy : 55.50\n",
            "iteration : 100, loss : 1.1091, accuracy : 55.09\n",
            "iteration : 150, loss : 1.1164, accuracy : 55.02\n",
            "iteration : 200, loss : 1.1143, accuracy : 55.07\n",
            "iteration : 250, loss : 1.1124, accuracy : 55.10\n",
            "iteration : 300, loss : 1.1109, accuracy : 55.24\n",
            "iteration : 350, loss : 1.1111, accuracy : 55.24\n",
            "Epoch :  39, training loss : 1.1092, training accuracy : 55.32, test loss : 1.1191, test accuracy : 55.00\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 1.1100, accuracy : 55.66\n",
            "iteration : 100, loss : 1.1182, accuracy : 55.09\n",
            "iteration : 150, loss : 1.1150, accuracy : 55.20\n",
            "iteration : 200, loss : 1.1174, accuracy : 55.13\n",
            "iteration : 250, loss : 1.1104, accuracy : 55.27\n",
            "iteration : 300, loss : 1.1136, accuracy : 55.13\n",
            "iteration : 350, loss : 1.1105, accuracy : 55.15\n",
            "Epoch :  40, training loss : 1.1096, training accuracy : 55.17, test loss : 1.1265, test accuracy : 54.70\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 1.0869, accuracy : 56.97\n",
            "iteration : 100, loss : 1.1013, accuracy : 56.44\n",
            "iteration : 150, loss : 1.1004, accuracy : 56.31\n",
            "iteration : 200, loss : 1.1025, accuracy : 56.14\n",
            "iteration : 250, loss : 1.1010, accuracy : 56.17\n",
            "iteration : 300, loss : 1.1008, accuracy : 56.17\n",
            "iteration : 350, loss : 1.1002, accuracy : 56.11\n",
            "Epoch :  41, training loss : 1.0985, training accuracy : 56.09, test loss : 1.1245, test accuracy : 55.19\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 1.0891, accuracy : 56.50\n",
            "iteration : 100, loss : 1.0955, accuracy : 55.84\n",
            "iteration : 150, loss : 1.0994, accuracy : 55.74\n",
            "iteration : 200, loss : 1.0985, accuracy : 55.78\n",
            "iteration : 250, loss : 1.1010, accuracy : 55.58\n",
            "iteration : 300, loss : 1.1010, accuracy : 55.56\n",
            "iteration : 350, loss : 1.1016, accuracy : 55.55\n",
            "Epoch :  42, training loss : 1.1005, training accuracy : 55.59, test loss : 1.0946, test accuracy : 56.01\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 1.0597, accuracy : 56.09\n",
            "iteration : 100, loss : 1.0907, accuracy : 55.61\n",
            "iteration : 150, loss : 1.0982, accuracy : 55.48\n",
            "iteration : 200, loss : 1.0930, accuracy : 55.85\n",
            "iteration : 250, loss : 1.0917, accuracy : 56.00\n",
            "iteration : 300, loss : 1.0936, accuracy : 55.89\n",
            "iteration : 350, loss : 1.0953, accuracy : 55.89\n",
            "Epoch :  43, training loss : 1.0962, training accuracy : 55.87, test loss : 1.1194, test accuracy : 54.79\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 1.1074, accuracy : 55.62\n",
            "iteration : 100, loss : 1.0955, accuracy : 56.00\n",
            "iteration : 150, loss : 1.0982, accuracy : 55.64\n",
            "iteration : 200, loss : 1.0986, accuracy : 55.65\n",
            "iteration : 250, loss : 1.1012, accuracy : 55.50\n",
            "iteration : 300, loss : 1.1011, accuracy : 55.46\n",
            "iteration : 350, loss : 1.0985, accuracy : 55.60\n",
            "Epoch :  44, training loss : 1.0977, training accuracy : 55.62, test loss : 1.1165, test accuracy : 55.34\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 1.0981, accuracy : 55.59\n",
            "iteration : 100, loss : 1.1073, accuracy : 55.84\n",
            "iteration : 150, loss : 1.1042, accuracy : 55.72\n",
            "iteration : 200, loss : 1.1005, accuracy : 55.93\n",
            "iteration : 250, loss : 1.0976, accuracy : 55.90\n",
            "iteration : 300, loss : 1.0946, accuracy : 55.86\n",
            "iteration : 350, loss : 1.0952, accuracy : 55.83\n",
            "Epoch :  45, training loss : 1.0950, training accuracy : 55.80, test loss : 1.1264, test accuracy : 55.29\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 1.1198, accuracy : 55.17\n",
            "iteration : 100, loss : 1.1064, accuracy : 55.69\n",
            "iteration : 150, loss : 1.1045, accuracy : 56.02\n",
            "iteration : 200, loss : 1.0971, accuracy : 56.19\n",
            "iteration : 250, loss : 1.0975, accuracy : 56.12\n",
            "iteration : 300, loss : 1.0960, accuracy : 56.22\n",
            "iteration : 350, loss : 1.0956, accuracy : 56.32\n",
            "Epoch :  46, training loss : 1.0959, training accuracy : 56.28, test loss : 1.1268, test accuracy : 55.16\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 1.1005, accuracy : 54.83\n",
            "iteration : 100, loss : 1.1086, accuracy : 54.67\n",
            "iteration : 150, loss : 1.1036, accuracy : 55.23\n",
            "iteration : 200, loss : 1.1046, accuracy : 55.17\n",
            "iteration : 250, loss : 1.0983, accuracy : 55.60\n",
            "iteration : 300, loss : 1.0978, accuracy : 55.62\n",
            "iteration : 350, loss : 1.0959, accuracy : 55.65\n",
            "Epoch :  47, training loss : 1.0962, training accuracy : 55.72, test loss : 1.1462, test accuracy : 54.09\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 1.1149, accuracy : 55.23\n",
            "iteration : 100, loss : 1.0908, accuracy : 55.78\n",
            "iteration : 150, loss : 1.0901, accuracy : 55.80\n",
            "iteration : 200, loss : 1.0900, accuracy : 55.98\n",
            "iteration : 250, loss : 1.0880, accuracy : 55.97\n",
            "iteration : 300, loss : 1.0920, accuracy : 55.81\n",
            "iteration : 350, loss : 1.0929, accuracy : 55.81\n",
            "Epoch :  48, training loss : 1.0917, training accuracy : 55.90, test loss : 1.1140, test accuracy : 55.44\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 1.0767, accuracy : 56.30\n",
            "iteration : 100, loss : 1.0910, accuracy : 56.27\n",
            "iteration : 150, loss : 1.0905, accuracy : 55.97\n",
            "iteration : 200, loss : 1.0944, accuracy : 55.82\n",
            "iteration : 250, loss : 1.0916, accuracy : 56.03\n",
            "iteration : 300, loss : 1.0926, accuracy : 55.89\n",
            "iteration : 350, loss : 1.0904, accuracy : 56.02\n",
            "Epoch :  49, training loss : 1.0923, training accuracy : 55.89, test loss : 1.1107, test accuracy : 55.67\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 1.0856, accuracy : 55.78\n",
            "iteration : 100, loss : 1.0829, accuracy : 55.99\n",
            "iteration : 150, loss : 1.0804, accuracy : 55.94\n",
            "iteration : 200, loss : 1.0825, accuracy : 56.01\n",
            "iteration : 250, loss : 1.0854, accuracy : 55.87\n",
            "iteration : 300, loss : 1.0856, accuracy : 55.85\n",
            "iteration : 350, loss : 1.0842, accuracy : 55.92\n",
            "Epoch :  50, training loss : 1.0863, training accuracy : 55.87, test loss : 1.1075, test accuracy : 55.25\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 1.0818, accuracy : 56.48\n",
            "iteration : 100, loss : 1.0751, accuracy : 56.48\n",
            "iteration : 150, loss : 1.0847, accuracy : 56.16\n",
            "iteration : 200, loss : 1.0868, accuracy : 55.91\n",
            "iteration : 250, loss : 1.0909, accuracy : 55.72\n",
            "iteration : 300, loss : 1.0938, accuracy : 55.73\n",
            "iteration : 350, loss : 1.0965, accuracy : 55.63\n",
            "Epoch :  51, training loss : 1.0949, training accuracy : 55.71, test loss : 1.1083, test accuracy : 55.72\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 1.0764, accuracy : 56.48\n",
            "iteration : 100, loss : 1.0848, accuracy : 55.94\n",
            "iteration : 150, loss : 1.0900, accuracy : 55.93\n",
            "iteration : 200, loss : 1.0896, accuracy : 56.12\n",
            "iteration : 250, loss : 1.0913, accuracy : 56.11\n",
            "iteration : 300, loss : 1.0918, accuracy : 56.08\n",
            "iteration : 350, loss : 1.0942, accuracy : 55.98\n",
            "Epoch :  52, training loss : 1.0939, training accuracy : 55.94, test loss : 1.0944, test accuracy : 55.89\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 1.0808, accuracy : 56.09\n",
            "iteration : 100, loss : 1.0801, accuracy : 56.13\n",
            "iteration : 150, loss : 1.0850, accuracy : 55.84\n",
            "iteration : 200, loss : 1.0863, accuracy : 55.74\n",
            "iteration : 250, loss : 1.0915, accuracy : 55.64\n",
            "iteration : 300, loss : 1.0918, accuracy : 55.71\n",
            "iteration : 350, loss : 1.0918, accuracy : 55.72\n",
            "Epoch :  53, training loss : 1.0921, training accuracy : 55.71, test loss : 1.1151, test accuracy : 55.03\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 1.1056, accuracy : 55.12\n",
            "iteration : 100, loss : 1.0912, accuracy : 55.93\n",
            "iteration : 150, loss : 1.0898, accuracy : 55.97\n",
            "iteration : 200, loss : 1.0889, accuracy : 56.00\n",
            "iteration : 250, loss : 1.0923, accuracy : 55.90\n",
            "iteration : 300, loss : 1.0917, accuracy : 55.84\n",
            "iteration : 350, loss : 1.0929, accuracy : 55.71\n",
            "Epoch :  54, training loss : 1.0932, training accuracy : 55.74, test loss : 1.0991, test accuracy : 55.63\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 1.0611, accuracy : 57.16\n",
            "iteration : 100, loss : 1.0682, accuracy : 56.91\n",
            "iteration : 150, loss : 1.0840, accuracy : 56.43\n",
            "iteration : 200, loss : 1.0834, accuracy : 56.41\n",
            "iteration : 250, loss : 1.0823, accuracy : 56.37\n",
            "iteration : 300, loss : 1.0830, accuracy : 56.15\n",
            "iteration : 350, loss : 1.0851, accuracy : 56.06\n",
            "Epoch :  55, training loss : 1.0864, training accuracy : 56.05, test loss : 1.0918, test accuracy : 55.77\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 1.0763, accuracy : 56.61\n",
            "iteration : 100, loss : 1.0895, accuracy : 55.96\n",
            "iteration : 150, loss : 1.0896, accuracy : 56.02\n",
            "iteration : 200, loss : 1.0931, accuracy : 55.98\n",
            "iteration : 250, loss : 1.0937, accuracy : 56.01\n",
            "iteration : 300, loss : 1.0962, accuracy : 55.90\n",
            "iteration : 350, loss : 1.0929, accuracy : 56.07\n",
            "Epoch :  56, training loss : 1.0918, training accuracy : 56.09, test loss : 1.0943, test accuracy : 55.45\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 1.0960, accuracy : 55.33\n",
            "iteration : 100, loss : 1.0891, accuracy : 55.72\n",
            "iteration : 150, loss : 1.0948, accuracy : 55.50\n",
            "iteration : 200, loss : 1.0881, accuracy : 55.87\n",
            "iteration : 250, loss : 1.0914, accuracy : 55.67\n",
            "iteration : 300, loss : 1.0896, accuracy : 55.77\n",
            "iteration : 350, loss : 1.0873, accuracy : 55.97\n",
            "Epoch :  57, training loss : 1.0868, training accuracy : 55.97, test loss : 1.1044, test accuracy : 55.20\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 1.0666, accuracy : 56.91\n",
            "iteration : 100, loss : 1.0726, accuracy : 56.78\n",
            "iteration : 150, loss : 1.0698, accuracy : 56.70\n",
            "iteration : 200, loss : 1.0814, accuracy : 56.40\n",
            "iteration : 250, loss : 1.0873, accuracy : 56.04\n",
            "iteration : 300, loss : 1.0922, accuracy : 55.90\n",
            "iteration : 350, loss : 1.0899, accuracy : 56.11\n",
            "Epoch :  58, training loss : 1.0883, training accuracy : 56.17, test loss : 1.1155, test accuracy : 55.48\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 1.0729, accuracy : 56.58\n",
            "iteration : 100, loss : 1.0676, accuracy : 56.69\n",
            "iteration : 150, loss : 1.0804, accuracy : 56.40\n",
            "iteration : 200, loss : 1.0851, accuracy : 56.20\n",
            "iteration : 250, loss : 1.0884, accuracy : 56.09\n",
            "iteration : 300, loss : 1.0887, accuracy : 56.01\n",
            "iteration : 350, loss : 1.0895, accuracy : 55.94\n",
            "Epoch :  59, training loss : 1.0884, training accuracy : 55.99, test loss : 1.0957, test accuracy : 56.08\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 1.1080, accuracy : 55.64\n",
            "iteration : 100, loss : 1.0933, accuracy : 56.16\n",
            "iteration : 150, loss : 1.0900, accuracy : 56.15\n",
            "iteration : 200, loss : 1.0894, accuracy : 56.08\n",
            "iteration : 250, loss : 1.0861, accuracy : 56.10\n",
            "iteration : 300, loss : 1.0833, accuracy : 56.12\n",
            "iteration : 350, loss : 1.0870, accuracy : 56.03\n",
            "Epoch :  60, training loss : 1.0872, training accuracy : 56.07, test loss : 1.1110, test accuracy : 55.19\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 1.1033, accuracy : 55.92\n",
            "iteration : 100, loss : 1.0902, accuracy : 56.41\n",
            "iteration : 150, loss : 1.0900, accuracy : 56.18\n",
            "iteration : 200, loss : 1.0912, accuracy : 55.98\n",
            "iteration : 250, loss : 1.0829, accuracy : 56.17\n",
            "iteration : 300, loss : 1.0877, accuracy : 55.99\n",
            "iteration : 350, loss : 1.0858, accuracy : 56.09\n",
            "Epoch :  61, training loss : 1.0859, training accuracy : 56.08, test loss : 1.0882, test accuracy : 56.26\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 1.0829, accuracy : 55.39\n",
            "iteration : 100, loss : 1.0752, accuracy : 55.95\n",
            "iteration : 150, loss : 1.0764, accuracy : 56.21\n",
            "iteration : 200, loss : 1.0746, accuracy : 56.41\n",
            "iteration : 250, loss : 1.0785, accuracy : 56.36\n",
            "iteration : 300, loss : 1.0821, accuracy : 56.19\n",
            "iteration : 350, loss : 1.0831, accuracy : 56.21\n",
            "Epoch :  62, training loss : 1.0829, training accuracy : 56.17, test loss : 1.1034, test accuracy : 55.43\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 1.1025, accuracy : 56.03\n",
            "iteration : 100, loss : 1.0970, accuracy : 55.75\n",
            "iteration : 150, loss : 1.0960, accuracy : 55.73\n",
            "iteration : 200, loss : 1.0932, accuracy : 55.84\n",
            "iteration : 250, loss : 1.0897, accuracy : 55.85\n",
            "iteration : 300, loss : 1.0857, accuracy : 55.98\n",
            "iteration : 350, loss : 1.0865, accuracy : 55.92\n",
            "Epoch :  63, training loss : 1.0858, training accuracy : 55.95, test loss : 1.1030, test accuracy : 55.77\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 1.0790, accuracy : 56.30\n",
            "iteration : 100, loss : 1.0772, accuracy : 56.23\n",
            "iteration : 150, loss : 1.0755, accuracy : 56.46\n",
            "iteration : 200, loss : 1.0827, accuracy : 56.09\n",
            "iteration : 250, loss : 1.0866, accuracy : 55.97\n",
            "iteration : 300, loss : 1.0890, accuracy : 55.84\n",
            "iteration : 350, loss : 1.0888, accuracy : 55.82\n",
            "Epoch :  64, training loss : 1.0875, training accuracy : 55.86, test loss : 1.0979, test accuracy : 55.84\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 1.0930, accuracy : 55.00\n",
            "iteration : 100, loss : 1.0961, accuracy : 55.54\n",
            "iteration : 150, loss : 1.0958, accuracy : 55.69\n",
            "iteration : 200, loss : 1.0900, accuracy : 55.86\n",
            "iteration : 250, loss : 1.0860, accuracy : 55.97\n",
            "iteration : 300, loss : 1.0872, accuracy : 55.90\n",
            "iteration : 350, loss : 1.0845, accuracy : 55.85\n",
            "Epoch :  65, training loss : 1.0841, training accuracy : 55.87, test loss : 1.1059, test accuracy : 55.70\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 1.0731, accuracy : 57.11\n",
            "iteration : 100, loss : 1.0748, accuracy : 56.34\n",
            "iteration : 150, loss : 1.0746, accuracy : 56.39\n",
            "iteration : 200, loss : 1.0742, accuracy : 56.32\n",
            "iteration : 250, loss : 1.0807, accuracy : 56.25\n",
            "iteration : 300, loss : 1.0828, accuracy : 56.24\n",
            "iteration : 350, loss : 1.0805, accuracy : 56.36\n",
            "Epoch :  66, training loss : 1.0823, training accuracy : 56.31, test loss : 1.1259, test accuracy : 54.89\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 1.0747, accuracy : 56.64\n",
            "iteration : 100, loss : 1.0635, accuracy : 56.67\n",
            "iteration : 150, loss : 1.0655, accuracy : 56.69\n",
            "iteration : 200, loss : 1.0690, accuracy : 56.44\n",
            "iteration : 250, loss : 1.0757, accuracy : 56.22\n",
            "iteration : 300, loss : 1.0802, accuracy : 56.12\n",
            "iteration : 350, loss : 1.0807, accuracy : 56.13\n",
            "Epoch :  67, training loss : 1.0801, training accuracy : 56.14, test loss : 1.0857, test accuracy : 55.91\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 1.0634, accuracy : 57.11\n",
            "iteration : 100, loss : 1.0718, accuracy : 57.06\n",
            "iteration : 150, loss : 1.0741, accuracy : 57.04\n",
            "iteration : 200, loss : 1.0725, accuracy : 56.93\n",
            "iteration : 250, loss : 1.0779, accuracy : 56.75\n",
            "iteration : 300, loss : 1.0830, accuracy : 56.54\n",
            "iteration : 350, loss : 1.0796, accuracy : 56.45\n",
            "Epoch :  68, training loss : 1.0771, training accuracy : 56.58, test loss : 1.0889, test accuracy : 55.80\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 1.1071, accuracy : 55.59\n",
            "iteration : 100, loss : 1.0897, accuracy : 55.67\n",
            "iteration : 150, loss : 1.0987, accuracy : 55.31\n",
            "iteration : 200, loss : 1.0896, accuracy : 55.98\n",
            "iteration : 250, loss : 1.0927, accuracy : 55.84\n",
            "iteration : 300, loss : 1.0922, accuracy : 55.82\n",
            "iteration : 350, loss : 1.0879, accuracy : 55.97\n",
            "Epoch :  69, training loss : 1.0865, training accuracy : 55.99, test loss : 1.1025, test accuracy : 55.87\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 1.1018, accuracy : 55.73\n",
            "iteration : 100, loss : 1.0872, accuracy : 56.26\n",
            "iteration : 150, loss : 1.0828, accuracy : 56.29\n",
            "iteration : 200, loss : 1.0892, accuracy : 55.92\n",
            "iteration : 250, loss : 1.0844, accuracy : 56.09\n",
            "iteration : 300, loss : 1.0821, accuracy : 56.22\n",
            "iteration : 350, loss : 1.0843, accuracy : 56.14\n",
            "Epoch :  70, training loss : 1.0848, training accuracy : 56.08, test loss : 1.0961, test accuracy : 55.62\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 1.0630, accuracy : 56.72\n",
            "iteration : 100, loss : 1.0781, accuracy : 56.11\n",
            "iteration : 150, loss : 1.0797, accuracy : 56.17\n",
            "iteration : 200, loss : 1.0766, accuracy : 56.13\n",
            "iteration : 250, loss : 1.0758, accuracy : 56.38\n",
            "iteration : 300, loss : 1.0748, accuracy : 56.46\n",
            "iteration : 350, loss : 1.0728, accuracy : 56.55\n",
            "Epoch :  71, training loss : 1.0737, training accuracy : 56.53, test loss : 1.0941, test accuracy : 55.68\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 1.0859, accuracy : 55.66\n",
            "iteration : 100, loss : 1.0903, accuracy : 55.78\n",
            "iteration : 150, loss : 1.0834, accuracy : 55.99\n",
            "iteration : 200, loss : 1.0839, accuracy : 56.00\n",
            "iteration : 250, loss : 1.0797, accuracy : 56.13\n",
            "iteration : 300, loss : 1.0801, accuracy : 56.06\n",
            "iteration : 350, loss : 1.0784, accuracy : 56.11\n",
            "Epoch :  72, training loss : 1.0765, training accuracy : 56.19, test loss : 1.0913, test accuracy : 55.90\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 1.0637, accuracy : 56.69\n",
            "iteration : 100, loss : 1.0754, accuracy : 56.33\n",
            "iteration : 150, loss : 1.0820, accuracy : 56.19\n",
            "iteration : 200, loss : 1.0816, accuracy : 56.18\n",
            "iteration : 250, loss : 1.0827, accuracy : 56.11\n",
            "iteration : 300, loss : 1.0787, accuracy : 56.28\n",
            "iteration : 350, loss : 1.0798, accuracy : 56.36\n",
            "Epoch :  73, training loss : 1.0790, training accuracy : 56.38, test loss : 1.0945, test accuracy : 55.84\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 1.0729, accuracy : 57.03\n",
            "iteration : 100, loss : 1.0857, accuracy : 56.48\n",
            "iteration : 150, loss : 1.0827, accuracy : 56.42\n",
            "iteration : 200, loss : 1.0841, accuracy : 56.04\n",
            "iteration : 250, loss : 1.0809, accuracy : 56.11\n",
            "iteration : 300, loss : 1.0788, accuracy : 56.18\n",
            "iteration : 350, loss : 1.0796, accuracy : 56.10\n",
            "Epoch :  74, training loss : 1.0777, training accuracy : 56.22, test loss : 1.0900, test accuracy : 55.95\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 1.0659, accuracy : 57.36\n",
            "iteration : 100, loss : 1.0733, accuracy : 56.81\n",
            "iteration : 150, loss : 1.0721, accuracy : 56.77\n",
            "iteration : 200, loss : 1.0725, accuracy : 56.69\n",
            "iteration : 250, loss : 1.0740, accuracy : 56.42\n",
            "iteration : 300, loss : 1.0770, accuracy : 56.25\n",
            "iteration : 350, loss : 1.0771, accuracy : 56.30\n",
            "Epoch :  75, training loss : 1.0763, training accuracy : 56.31, test loss : 1.0977, test accuracy : 55.82\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 1.0664, accuracy : 56.89\n",
            "iteration : 100, loss : 1.0787, accuracy : 56.82\n",
            "iteration : 150, loss : 1.0757, accuracy : 56.72\n",
            "iteration : 200, loss : 1.0740, accuracy : 56.86\n",
            "iteration : 250, loss : 1.0757, accuracy : 56.62\n",
            "iteration : 300, loss : 1.0770, accuracy : 56.57\n",
            "iteration : 350, loss : 1.0780, accuracy : 56.54\n",
            "Epoch :  76, training loss : 1.0786, training accuracy : 56.51, test loss : 1.0728, test accuracy : 56.47\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 1.0870, accuracy : 56.03\n",
            "iteration : 100, loss : 1.0728, accuracy : 56.37\n",
            "iteration : 150, loss : 1.0740, accuracy : 56.18\n",
            "iteration : 200, loss : 1.0768, accuracy : 56.01\n",
            "iteration : 250, loss : 1.0764, accuracy : 55.98\n",
            "iteration : 300, loss : 1.0799, accuracy : 55.81\n",
            "iteration : 350, loss : 1.0777, accuracy : 55.85\n",
            "Epoch :  77, training loss : 1.0779, training accuracy : 55.92, test loss : 1.1112, test accuracy : 55.26\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 1.0634, accuracy : 57.66\n",
            "iteration : 100, loss : 1.0692, accuracy : 57.19\n",
            "iteration : 150, loss : 1.0768, accuracy : 56.70\n",
            "iteration : 200, loss : 1.0780, accuracy : 56.52\n",
            "iteration : 250, loss : 1.0762, accuracy : 56.43\n",
            "iteration : 300, loss : 1.0812, accuracy : 56.17\n",
            "iteration : 350, loss : 1.0787, accuracy : 56.30\n",
            "Epoch :  78, training loss : 1.0783, training accuracy : 56.27, test loss : 1.0838, test accuracy : 56.27\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 1.0733, accuracy : 56.83\n",
            "iteration : 100, loss : 1.0716, accuracy : 56.80\n",
            "iteration : 150, loss : 1.0671, accuracy : 56.86\n",
            "iteration : 200, loss : 1.0697, accuracy : 56.55\n",
            "iteration : 250, loss : 1.0694, accuracy : 56.59\n",
            "iteration : 300, loss : 1.0711, accuracy : 56.49\n",
            "iteration : 350, loss : 1.0701, accuracy : 56.58\n",
            "Epoch :  79, training loss : 1.0697, training accuracy : 56.58, test loss : 1.0883, test accuracy : 56.22\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 1.0509, accuracy : 57.70\n",
            "iteration : 100, loss : 1.0553, accuracy : 57.06\n",
            "iteration : 150, loss : 1.0645, accuracy : 56.67\n",
            "iteration : 200, loss : 1.0737, accuracy : 56.26\n",
            "iteration : 250, loss : 1.0751, accuracy : 56.24\n",
            "iteration : 300, loss : 1.0747, accuracy : 56.16\n",
            "iteration : 350, loss : 1.0719, accuracy : 56.27\n",
            "Epoch :  80, training loss : 1.0752, training accuracy : 56.19, test loss : 1.0816, test accuracy : 55.98\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 1.0706, accuracy : 56.78\n",
            "iteration : 100, loss : 1.0829, accuracy : 56.04\n",
            "iteration : 150, loss : 1.0842, accuracy : 55.81\n",
            "iteration : 200, loss : 1.0803, accuracy : 55.95\n",
            "iteration : 250, loss : 1.0789, accuracy : 56.10\n",
            "iteration : 300, loss : 1.0778, accuracy : 56.17\n",
            "iteration : 350, loss : 1.0745, accuracy : 56.27\n",
            "Epoch :  81, training loss : 1.0767, training accuracy : 56.18, test loss : 1.1070, test accuracy : 55.32\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 1.0453, accuracy : 57.66\n",
            "iteration : 100, loss : 1.0531, accuracy : 57.45\n",
            "iteration : 150, loss : 1.0544, accuracy : 57.34\n",
            "iteration : 200, loss : 1.0520, accuracy : 57.32\n",
            "iteration : 250, loss : 1.0631, accuracy : 56.88\n",
            "iteration : 300, loss : 1.0683, accuracy : 56.67\n",
            "iteration : 350, loss : 1.0694, accuracy : 56.55\n",
            "Epoch :  82, training loss : 1.0667, training accuracy : 56.63, test loss : 1.1114, test accuracy : 55.13\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 1.0533, accuracy : 56.64\n",
            "iteration : 100, loss : 1.0473, accuracy : 56.66\n",
            "iteration : 150, loss : 1.0614, accuracy : 56.26\n",
            "iteration : 200, loss : 1.0703, accuracy : 56.04\n",
            "iteration : 250, loss : 1.0704, accuracy : 56.19\n",
            "iteration : 300, loss : 1.0737, accuracy : 56.01\n",
            "iteration : 350, loss : 1.0742, accuracy : 56.08\n",
            "Epoch :  83, training loss : 1.0750, training accuracy : 56.02, test loss : 1.0833, test accuracy : 55.88\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 1.0488, accuracy : 56.77\n",
            "iteration : 100, loss : 1.0503, accuracy : 56.88\n",
            "iteration : 150, loss : 1.0555, accuracy : 57.07\n",
            "iteration : 200, loss : 1.0542, accuracy : 57.09\n",
            "iteration : 250, loss : 1.0616, accuracy : 56.68\n",
            "iteration : 300, loss : 1.0646, accuracy : 56.71\n",
            "iteration : 350, loss : 1.0687, accuracy : 56.62\n",
            "Epoch :  84, training loss : 1.0693, training accuracy : 56.61, test loss : 1.1154, test accuracy : 55.50\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 1.0717, accuracy : 56.97\n",
            "iteration : 100, loss : 1.0719, accuracy : 56.82\n",
            "iteration : 150, loss : 1.0680, accuracy : 56.77\n",
            "iteration : 200, loss : 1.0722, accuracy : 56.45\n",
            "iteration : 250, loss : 1.0750, accuracy : 56.37\n",
            "iteration : 300, loss : 1.0718, accuracy : 56.44\n",
            "iteration : 350, loss : 1.0703, accuracy : 56.48\n",
            "Epoch :  85, training loss : 1.0702, training accuracy : 56.46, test loss : 1.0817, test accuracy : 56.03\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 1.0800, accuracy : 55.53\n",
            "iteration : 100, loss : 1.0749, accuracy : 56.15\n",
            "iteration : 150, loss : 1.0715, accuracy : 56.19\n",
            "iteration : 200, loss : 1.0711, accuracy : 56.23\n",
            "iteration : 250, loss : 1.0726, accuracy : 56.14\n",
            "iteration : 300, loss : 1.0760, accuracy : 55.99\n",
            "iteration : 350, loss : 1.0740, accuracy : 56.10\n",
            "Epoch :  86, training loss : 1.0761, training accuracy : 55.97, test loss : 1.0864, test accuracy : 56.34\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 1.0708, accuracy : 56.34\n",
            "iteration : 100, loss : 1.0684, accuracy : 56.77\n",
            "iteration : 150, loss : 1.0730, accuracy : 56.52\n",
            "iteration : 200, loss : 1.0757, accuracy : 56.38\n",
            "iteration : 250, loss : 1.0730, accuracy : 56.39\n",
            "iteration : 300, loss : 1.0729, accuracy : 56.32\n",
            "iteration : 350, loss : 1.0730, accuracy : 56.34\n",
            "Epoch :  87, training loss : 1.0733, training accuracy : 56.35, test loss : 1.0798, test accuracy : 56.04\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 1.0659, accuracy : 56.44\n",
            "iteration : 100, loss : 1.0704, accuracy : 56.24\n",
            "iteration : 150, loss : 1.0646, accuracy : 56.79\n",
            "iteration : 200, loss : 1.0651, accuracy : 56.92\n",
            "iteration : 250, loss : 1.0658, accuracy : 56.87\n",
            "iteration : 300, loss : 1.0646, accuracy : 56.80\n",
            "iteration : 350, loss : 1.0634, accuracy : 56.72\n",
            "Epoch :  88, training loss : 1.0641, training accuracy : 56.72, test loss : 1.0766, test accuracy : 56.13\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 1.0719, accuracy : 55.55\n",
            "iteration : 100, loss : 1.0654, accuracy : 55.58\n",
            "iteration : 150, loss : 1.0608, accuracy : 56.13\n",
            "iteration : 200, loss : 1.0618, accuracy : 56.36\n",
            "iteration : 250, loss : 1.0649, accuracy : 56.36\n",
            "iteration : 300, loss : 1.0624, accuracy : 56.47\n",
            "iteration : 350, loss : 1.0652, accuracy : 56.42\n",
            "Epoch :  89, training loss : 1.0656, training accuracy : 56.38, test loss : 1.0855, test accuracy : 55.75\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 1.0395, accuracy : 57.03\n",
            "iteration : 100, loss : 1.0587, accuracy : 56.90\n",
            "iteration : 150, loss : 1.0601, accuracy : 56.94\n",
            "iteration : 200, loss : 1.0616, accuracy : 56.71\n",
            "iteration : 250, loss : 1.0601, accuracy : 56.87\n",
            "iteration : 300, loss : 1.0649, accuracy : 56.73\n",
            "iteration : 350, loss : 1.0664, accuracy : 56.71\n",
            "Epoch :  90, training loss : 1.0657, training accuracy : 56.70, test loss : 1.0766, test accuracy : 56.37\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 1.0627, accuracy : 56.70\n",
            "iteration : 100, loss : 1.0577, accuracy : 56.48\n",
            "iteration : 150, loss : 1.0573, accuracy : 56.47\n",
            "iteration : 200, loss : 1.0598, accuracy : 56.68\n",
            "iteration : 250, loss : 1.0655, accuracy : 56.50\n",
            "iteration : 300, loss : 1.0690, accuracy : 56.44\n",
            "iteration : 350, loss : 1.0687, accuracy : 56.53\n",
            "Epoch :  91, training loss : 1.0675, training accuracy : 56.59, test loss : 1.0755, test accuracy : 56.07\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 1.0298, accuracy : 57.67\n",
            "iteration : 100, loss : 1.0467, accuracy : 57.28\n",
            "iteration : 150, loss : 1.0564, accuracy : 56.86\n",
            "iteration : 200, loss : 1.0539, accuracy : 56.92\n",
            "iteration : 250, loss : 1.0596, accuracy : 56.70\n",
            "iteration : 300, loss : 1.0640, accuracy : 56.59\n",
            "iteration : 350, loss : 1.0617, accuracy : 56.78\n",
            "Epoch :  92, training loss : 1.0636, training accuracy : 56.74, test loss : 1.1024, test accuracy : 55.51\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 1.0563, accuracy : 56.39\n",
            "iteration : 100, loss : 1.0696, accuracy : 55.96\n",
            "iteration : 150, loss : 1.0596, accuracy : 56.50\n",
            "iteration : 200, loss : 1.0666, accuracy : 56.34\n",
            "iteration : 250, loss : 1.0647, accuracy : 56.26\n",
            "iteration : 300, loss : 1.0629, accuracy : 56.54\n",
            "iteration : 350, loss : 1.0650, accuracy : 56.45\n",
            "Epoch :  93, training loss : 1.0660, training accuracy : 56.39, test loss : 1.0746, test accuracy : 56.46\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 1.0760, accuracy : 56.22\n",
            "iteration : 100, loss : 1.0786, accuracy : 55.97\n",
            "iteration : 150, loss : 1.0810, accuracy : 55.84\n",
            "iteration : 200, loss : 1.0717, accuracy : 56.19\n",
            "iteration : 250, loss : 1.0688, accuracy : 56.20\n",
            "iteration : 300, loss : 1.0688, accuracy : 56.30\n",
            "iteration : 350, loss : 1.0686, accuracy : 56.31\n",
            "Epoch :  94, training loss : 1.0706, training accuracy : 56.24, test loss : 1.0793, test accuracy : 56.26\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 1.0857, accuracy : 56.14\n",
            "iteration : 100, loss : 1.0630, accuracy : 56.65\n",
            "iteration : 150, loss : 1.0626, accuracy : 56.52\n",
            "iteration : 200, loss : 1.0624, accuracy : 56.30\n",
            "iteration : 250, loss : 1.0663, accuracy : 56.20\n",
            "iteration : 300, loss : 1.0654, accuracy : 56.28\n",
            "iteration : 350, loss : 1.0667, accuracy : 56.23\n",
            "Epoch :  95, training loss : 1.0645, training accuracy : 56.27, test loss : 1.0752, test accuracy : 56.19\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 1.0623, accuracy : 55.64\n",
            "iteration : 100, loss : 1.0639, accuracy : 55.71\n",
            "iteration : 150, loss : 1.0682, accuracy : 55.65\n",
            "iteration : 200, loss : 1.0680, accuracy : 56.08\n",
            "iteration : 250, loss : 1.0672, accuracy : 56.27\n",
            "iteration : 300, loss : 1.0659, accuracy : 56.27\n",
            "iteration : 350, loss : 1.0645, accuracy : 56.32\n",
            "Epoch :  96, training loss : 1.0685, training accuracy : 56.19, test loss : 1.0846, test accuracy : 55.79\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 1.0770, accuracy : 56.88\n",
            "iteration : 100, loss : 1.0608, accuracy : 57.12\n",
            "iteration : 150, loss : 1.0592, accuracy : 57.01\n",
            "iteration : 200, loss : 1.0608, accuracy : 56.83\n",
            "iteration : 250, loss : 1.0636, accuracy : 56.76\n",
            "iteration : 300, loss : 1.0645, accuracy : 56.60\n",
            "iteration : 350, loss : 1.0626, accuracy : 56.54\n",
            "Epoch :  97, training loss : 1.0645, training accuracy : 56.50, test loss : 1.0631, test accuracy : 56.34\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 1.0745, accuracy : 56.50\n",
            "iteration : 100, loss : 1.0680, accuracy : 56.75\n",
            "iteration : 150, loss : 1.0677, accuracy : 56.66\n",
            "iteration : 200, loss : 1.0636, accuracy : 56.59\n",
            "iteration : 250, loss : 1.0610, accuracy : 56.61\n",
            "iteration : 300, loss : 1.0593, accuracy : 56.66\n",
            "iteration : 350, loss : 1.0610, accuracy : 56.54\n",
            "Epoch :  98, training loss : 1.0615, training accuracy : 56.46, test loss : 1.1025, test accuracy : 55.63\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 1.0758, accuracy : 55.92\n",
            "iteration : 100, loss : 1.0548, accuracy : 56.67\n",
            "iteration : 150, loss : 1.0660, accuracy : 56.27\n",
            "iteration : 200, loss : 1.0672, accuracy : 56.27\n",
            "iteration : 250, loss : 1.0657, accuracy : 56.46\n",
            "iteration : 300, loss : 1.0663, accuracy : 56.52\n",
            "iteration : 350, loss : 1.0660, accuracy : 56.53\n",
            "Epoch :  99, training loss : 1.0653, training accuracy : 56.59, test loss : 1.0763, test accuracy : 56.35\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 1.0490, accuracy : 57.48\n",
            "iteration : 100, loss : 1.0550, accuracy : 56.99\n",
            "iteration : 150, loss : 1.0506, accuracy : 57.09\n",
            "iteration : 200, loss : 1.0575, accuracy : 56.76\n",
            "iteration : 250, loss : 1.0543, accuracy : 56.85\n",
            "iteration : 300, loss : 1.0610, accuracy : 56.61\n",
            "iteration : 350, loss : 1.0608, accuracy : 56.68\n",
            "Epoch : 100, training loss : 1.0617, training accuracy : 56.60, test loss : 1.0680, test accuracy : 56.65\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 1.0603, accuracy : 56.28\n",
            "iteration : 100, loss : 1.0735, accuracy : 55.97\n",
            "iteration : 150, loss : 1.0791, accuracy : 56.02\n",
            "iteration : 200, loss : 1.0736, accuracy : 56.14\n",
            "iteration : 250, loss : 1.0722, accuracy : 56.27\n",
            "iteration : 300, loss : 1.0713, accuracy : 56.35\n",
            "iteration : 350, loss : 1.0660, accuracy : 56.46\n",
            "Epoch : 101, training loss : 1.0676, training accuracy : 56.39, test loss : 1.0834, test accuracy : 55.95\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 1.0892, accuracy : 55.81\n",
            "iteration : 100, loss : 1.0718, accuracy : 56.20\n",
            "iteration : 150, loss : 1.0666, accuracy : 56.33\n",
            "iteration : 200, loss : 1.0637, accuracy : 56.27\n",
            "iteration : 250, loss : 1.0635, accuracy : 56.31\n",
            "iteration : 300, loss : 1.0639, accuracy : 56.30\n",
            "iteration : 350, loss : 1.0633, accuracy : 56.29\n",
            "Epoch : 102, training loss : 1.0632, training accuracy : 56.33, test loss : 1.0686, test accuracy : 56.34\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 1.0716, accuracy : 56.31\n",
            "iteration : 100, loss : 1.0592, accuracy : 56.48\n",
            "iteration : 150, loss : 1.0666, accuracy : 56.32\n",
            "iteration : 200, loss : 1.0669, accuracy : 56.24\n",
            "iteration : 250, loss : 1.0671, accuracy : 56.30\n",
            "iteration : 300, loss : 1.0672, accuracy : 56.51\n",
            "iteration : 350, loss : 1.0638, accuracy : 56.65\n",
            "Epoch : 103, training loss : 1.0612, training accuracy : 56.71, test loss : 1.0700, test accuracy : 56.42\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 1.0523, accuracy : 57.64\n",
            "iteration : 100, loss : 1.0553, accuracy : 57.12\n",
            "iteration : 150, loss : 1.0530, accuracy : 57.08\n",
            "iteration : 200, loss : 1.0560, accuracy : 56.79\n",
            "iteration : 250, loss : 1.0565, accuracy : 56.92\n",
            "iteration : 300, loss : 1.0561, accuracy : 56.79\n",
            "iteration : 350, loss : 1.0582, accuracy : 56.85\n",
            "Epoch : 104, training loss : 1.0582, training accuracy : 56.81, test loss : 1.0781, test accuracy : 56.48\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 1.0580, accuracy : 57.14\n",
            "iteration : 100, loss : 1.0579, accuracy : 56.59\n",
            "iteration : 150, loss : 1.0578, accuracy : 57.02\n",
            "iteration : 200, loss : 1.0594, accuracy : 56.80\n",
            "iteration : 250, loss : 1.0560, accuracy : 56.83\n",
            "iteration : 300, loss : 1.0597, accuracy : 56.77\n",
            "iteration : 350, loss : 1.0603, accuracy : 56.65\n",
            "Epoch : 105, training loss : 1.0584, training accuracy : 56.69, test loss : 1.0631, test accuracy : 56.61\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 1.0582, accuracy : 55.95\n",
            "iteration : 100, loss : 1.0521, accuracy : 56.16\n",
            "iteration : 150, loss : 1.0544, accuracy : 56.58\n",
            "iteration : 200, loss : 1.0617, accuracy : 56.30\n",
            "iteration : 250, loss : 1.0639, accuracy : 56.29\n",
            "iteration : 300, loss : 1.0625, accuracy : 56.37\n",
            "iteration : 350, loss : 1.0663, accuracy : 56.35\n",
            "Epoch : 106, training loss : 1.0670, training accuracy : 56.36, test loss : 1.0884, test accuracy : 56.07\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 1.0538, accuracy : 56.69\n",
            "iteration : 100, loss : 1.0537, accuracy : 56.69\n",
            "iteration : 150, loss : 1.0517, accuracy : 56.71\n",
            "iteration : 200, loss : 1.0574, accuracy : 56.59\n",
            "iteration : 250, loss : 1.0522, accuracy : 56.68\n",
            "iteration : 300, loss : 1.0543, accuracy : 56.66\n",
            "iteration : 350, loss : 1.0552, accuracy : 56.76\n",
            "Epoch : 107, training loss : 1.0533, training accuracy : 56.81, test loss : 1.0854, test accuracy : 55.44\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 1.0593, accuracy : 57.23\n",
            "iteration : 100, loss : 1.0465, accuracy : 57.38\n",
            "iteration : 150, loss : 1.0418, accuracy : 57.32\n",
            "iteration : 200, loss : 1.0436, accuracy : 57.47\n",
            "iteration : 250, loss : 1.0498, accuracy : 57.10\n",
            "iteration : 300, loss : 1.0507, accuracy : 57.11\n",
            "iteration : 350, loss : 1.0508, accuracy : 57.08\n",
            "Epoch : 108, training loss : 1.0499, training accuracy : 57.05, test loss : 1.0706, test accuracy : 56.13\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 1.0332, accuracy : 57.70\n",
            "iteration : 100, loss : 1.0435, accuracy : 57.27\n",
            "iteration : 150, loss : 1.0471, accuracy : 56.98\n",
            "iteration : 200, loss : 1.0521, accuracy : 56.80\n",
            "iteration : 250, loss : 1.0526, accuracy : 56.95\n",
            "iteration : 300, loss : 1.0533, accuracy : 56.94\n",
            "iteration : 350, loss : 1.0538, accuracy : 56.94\n",
            "Epoch : 109, training loss : 1.0540, training accuracy : 56.96, test loss : 1.0690, test accuracy : 56.57\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 1.0713, accuracy : 55.81\n",
            "iteration : 100, loss : 1.0673, accuracy : 56.37\n",
            "iteration : 150, loss : 1.0653, accuracy : 56.50\n",
            "iteration : 200, loss : 1.0695, accuracy : 56.45\n",
            "iteration : 250, loss : 1.0703, accuracy : 56.50\n",
            "iteration : 300, loss : 1.0682, accuracy : 56.43\n",
            "iteration : 350, loss : 1.0643, accuracy : 56.60\n",
            "Epoch : 110, training loss : 1.0630, training accuracy : 56.67, test loss : 1.0768, test accuracy : 56.25\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 1.0561, accuracy : 57.11\n",
            "iteration : 100, loss : 1.0528, accuracy : 57.02\n",
            "iteration : 150, loss : 1.0550, accuracy : 56.91\n",
            "iteration : 200, loss : 1.0542, accuracy : 56.92\n",
            "iteration : 250, loss : 1.0598, accuracy : 56.60\n",
            "iteration : 300, loss : 1.0585, accuracy : 56.66\n",
            "iteration : 350, loss : 1.0597, accuracy : 56.59\n",
            "Epoch : 111, training loss : 1.0592, training accuracy : 56.63, test loss : 1.0848, test accuracy : 55.65\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 1.0512, accuracy : 57.22\n",
            "iteration : 100, loss : 1.0590, accuracy : 56.78\n",
            "iteration : 150, loss : 1.0521, accuracy : 57.02\n",
            "iteration : 200, loss : 1.0515, accuracy : 56.80\n",
            "iteration : 250, loss : 1.0503, accuracy : 56.85\n",
            "iteration : 300, loss : 1.0519, accuracy : 56.76\n",
            "iteration : 350, loss : 1.0558, accuracy : 56.62\n",
            "Epoch : 112, training loss : 1.0569, training accuracy : 56.61, test loss : 1.0811, test accuracy : 55.86\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 1.0631, accuracy : 56.94\n",
            "iteration : 100, loss : 1.0524, accuracy : 57.15\n",
            "iteration : 150, loss : 1.0538, accuracy : 56.93\n",
            "iteration : 200, loss : 1.0582, accuracy : 56.91\n",
            "iteration : 250, loss : 1.0603, accuracy : 56.78\n",
            "iteration : 300, loss : 1.0601, accuracy : 56.72\n",
            "iteration : 350, loss : 1.0597, accuracy : 56.75\n",
            "Epoch : 113, training loss : 1.0593, training accuracy : 56.73, test loss : 1.0612, test accuracy : 57.00\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 1.0572, accuracy : 56.28\n",
            "iteration : 100, loss : 1.0560, accuracy : 56.31\n",
            "iteration : 150, loss : 1.0536, accuracy : 56.70\n",
            "iteration : 200, loss : 1.0536, accuracy : 56.85\n",
            "iteration : 250, loss : 1.0559, accuracy : 56.74\n",
            "iteration : 300, loss : 1.0609, accuracy : 56.50\n",
            "iteration : 350, loss : 1.0607, accuracy : 56.46\n",
            "Epoch : 114, training loss : 1.0601, training accuracy : 56.48, test loss : 1.0560, test accuracy : 56.95\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 1.0466, accuracy : 57.09\n",
            "iteration : 100, loss : 1.0510, accuracy : 57.05\n",
            "iteration : 150, loss : 1.0462, accuracy : 57.05\n",
            "iteration : 200, loss : 1.0446, accuracy : 57.00\n",
            "iteration : 250, loss : 1.0476, accuracy : 57.10\n",
            "iteration : 300, loss : 1.0486, accuracy : 57.03\n",
            "iteration : 350, loss : 1.0486, accuracy : 57.08\n",
            "Epoch : 115, training loss : 1.0488, training accuracy : 57.09, test loss : 1.0650, test accuracy : 56.32\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 1.0579, accuracy : 57.14\n",
            "iteration : 100, loss : 1.0621, accuracy : 56.60\n",
            "iteration : 150, loss : 1.0593, accuracy : 56.73\n",
            "iteration : 200, loss : 1.0553, accuracy : 56.80\n",
            "iteration : 250, loss : 1.0528, accuracy : 56.92\n",
            "iteration : 300, loss : 1.0547, accuracy : 56.73\n",
            "iteration : 350, loss : 1.0537, accuracy : 56.70\n",
            "Epoch : 116, training loss : 1.0511, training accuracy : 56.77, test loss : 1.0634, test accuracy : 56.45\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 1.0657, accuracy : 56.61\n",
            "iteration : 100, loss : 1.0689, accuracy : 56.51\n",
            "iteration : 150, loss : 1.0648, accuracy : 56.46\n",
            "iteration : 200, loss : 1.0614, accuracy : 56.36\n",
            "iteration : 250, loss : 1.0616, accuracy : 56.28\n",
            "iteration : 300, loss : 1.0561, accuracy : 56.59\n",
            "iteration : 350, loss : 1.0552, accuracy : 56.73\n",
            "Epoch : 117, training loss : 1.0565, training accuracy : 56.71, test loss : 1.0576, test accuracy : 56.73\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 1.0444, accuracy : 57.94\n",
            "iteration : 100, loss : 1.0433, accuracy : 57.23\n",
            "iteration : 150, loss : 1.0448, accuracy : 57.41\n",
            "iteration : 200, loss : 1.0480, accuracy : 57.18\n",
            "iteration : 250, loss : 1.0482, accuracy : 57.15\n",
            "iteration : 300, loss : 1.0482, accuracy : 57.10\n",
            "iteration : 350, loss : 1.0488, accuracy : 57.06\n",
            "Epoch : 118, training loss : 1.0502, training accuracy : 57.00, test loss : 1.0696, test accuracy : 56.76\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 1.0633, accuracy : 56.92\n",
            "iteration : 100, loss : 1.0525, accuracy : 56.98\n",
            "iteration : 150, loss : 1.0561, accuracy : 56.81\n",
            "iteration : 200, loss : 1.0545, accuracy : 56.83\n",
            "iteration : 250, loss : 1.0556, accuracy : 56.75\n",
            "iteration : 300, loss : 1.0577, accuracy : 56.62\n",
            "iteration : 350, loss : 1.0598, accuracy : 56.57\n",
            "Epoch : 119, training loss : 1.0589, training accuracy : 56.59, test loss : 1.0672, test accuracy : 56.28\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 1.0633, accuracy : 56.19\n",
            "iteration : 100, loss : 1.0579, accuracy : 56.40\n",
            "iteration : 150, loss : 1.0466, accuracy : 56.96\n",
            "iteration : 200, loss : 1.0460, accuracy : 56.83\n",
            "iteration : 250, loss : 1.0445, accuracy : 56.73\n",
            "iteration : 300, loss : 1.0427, accuracy : 56.89\n",
            "iteration : 350, loss : 1.0441, accuracy : 56.89\n",
            "Epoch : 120, training loss : 1.0449, training accuracy : 56.87, test loss : 1.0621, test accuracy : 56.54\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 1.0680, accuracy : 56.73\n",
            "iteration : 100, loss : 1.0589, accuracy : 57.13\n",
            "iteration : 150, loss : 1.0570, accuracy : 57.03\n",
            "iteration : 200, loss : 1.0532, accuracy : 56.99\n",
            "iteration : 250, loss : 1.0508, accuracy : 57.03\n",
            "iteration : 300, loss : 1.0502, accuracy : 57.00\n",
            "iteration : 350, loss : 1.0540, accuracy : 56.89\n",
            "Epoch : 121, training loss : 1.0554, training accuracy : 56.88, test loss : 1.0701, test accuracy : 56.42\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 1.0409, accuracy : 56.77\n",
            "iteration : 100, loss : 1.0381, accuracy : 57.49\n",
            "iteration : 150, loss : 1.0423, accuracy : 57.51\n",
            "iteration : 200, loss : 1.0433, accuracy : 57.27\n",
            "iteration : 250, loss : 1.0456, accuracy : 57.07\n",
            "iteration : 300, loss : 1.0446, accuracy : 57.08\n",
            "iteration : 350, loss : 1.0493, accuracy : 57.01\n",
            "Epoch : 122, training loss : 1.0505, training accuracy : 56.92, test loss : 1.0637, test accuracy : 56.70\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 1.0459, accuracy : 57.25\n",
            "iteration : 100, loss : 1.0526, accuracy : 56.56\n",
            "iteration : 150, loss : 1.0579, accuracy : 56.48\n",
            "iteration : 200, loss : 1.0575, accuracy : 56.51\n",
            "iteration : 250, loss : 1.0553, accuracy : 56.55\n",
            "iteration : 300, loss : 1.0491, accuracy : 56.85\n",
            "iteration : 350, loss : 1.0502, accuracy : 56.80\n",
            "Epoch : 123, training loss : 1.0517, training accuracy : 56.77, test loss : 1.0750, test accuracy : 56.20\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 1.0410, accuracy : 56.59\n",
            "iteration : 100, loss : 1.0510, accuracy : 56.25\n",
            "iteration : 150, loss : 1.0521, accuracy : 56.36\n",
            "iteration : 200, loss : 1.0482, accuracy : 56.65\n",
            "iteration : 250, loss : 1.0491, accuracy : 56.59\n",
            "iteration : 300, loss : 1.0509, accuracy : 56.44\n",
            "iteration : 350, loss : 1.0535, accuracy : 56.42\n",
            "Epoch : 124, training loss : 1.0536, training accuracy : 56.36, test loss : 1.0670, test accuracy : 56.17\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 1.0539, accuracy : 56.42\n",
            "iteration : 100, loss : 1.0487, accuracy : 56.70\n",
            "iteration : 150, loss : 1.0489, accuracy : 56.71\n",
            "iteration : 200, loss : 1.0498, accuracy : 56.59\n",
            "iteration : 250, loss : 1.0492, accuracy : 56.67\n",
            "iteration : 300, loss : 1.0474, accuracy : 56.67\n",
            "iteration : 350, loss : 1.0487, accuracy : 56.71\n",
            "Epoch : 125, training loss : 1.0512, training accuracy : 56.63, test loss : 1.0683, test accuracy : 56.96\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 1.0454, accuracy : 56.56\n",
            "iteration : 100, loss : 1.0314, accuracy : 57.25\n",
            "iteration : 150, loss : 1.0380, accuracy : 57.21\n",
            "iteration : 200, loss : 1.0440, accuracy : 57.09\n",
            "iteration : 250, loss : 1.0440, accuracy : 57.07\n",
            "iteration : 300, loss : 1.0448, accuracy : 57.06\n",
            "iteration : 350, loss : 1.0445, accuracy : 57.05\n",
            "Epoch : 126, training loss : 1.0452, training accuracy : 57.06, test loss : 1.0818, test accuracy : 56.30\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 1.0208, accuracy : 58.05\n",
            "iteration : 100, loss : 1.0379, accuracy : 57.35\n",
            "iteration : 150, loss : 1.0383, accuracy : 57.28\n",
            "iteration : 200, loss : 1.0443, accuracy : 56.98\n",
            "iteration : 250, loss : 1.0416, accuracy : 57.22\n",
            "iteration : 300, loss : 1.0464, accuracy : 56.97\n",
            "iteration : 350, loss : 1.0457, accuracy : 57.02\n",
            "Epoch : 127, training loss : 1.0444, training accuracy : 57.06, test loss : 1.0417, test accuracy : 57.33\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 1.0361, accuracy : 56.98\n",
            "iteration : 100, loss : 1.0427, accuracy : 57.32\n",
            "iteration : 150, loss : 1.0554, accuracy : 56.73\n",
            "iteration : 200, loss : 1.0549, accuracy : 56.74\n",
            "iteration : 250, loss : 1.0541, accuracy : 56.83\n",
            "iteration : 300, loss : 1.0558, accuracy : 56.64\n",
            "iteration : 350, loss : 1.0543, accuracy : 56.67\n",
            "Epoch : 128, training loss : 1.0528, training accuracy : 56.71, test loss : 1.0600, test accuracy : 56.85\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 1.0322, accuracy : 57.02\n",
            "iteration : 100, loss : 1.0360, accuracy : 57.35\n",
            "iteration : 150, loss : 1.0394, accuracy : 57.24\n",
            "iteration : 200, loss : 1.0359, accuracy : 57.39\n",
            "iteration : 250, loss : 1.0366, accuracy : 57.46\n",
            "iteration : 300, loss : 1.0375, accuracy : 57.42\n",
            "iteration : 350, loss : 1.0398, accuracy : 57.24\n",
            "Epoch : 129, training loss : 1.0421, training accuracy : 57.24, test loss : 1.0714, test accuracy : 56.20\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 1.0504, accuracy : 56.69\n",
            "iteration : 100, loss : 1.0440, accuracy : 57.16\n",
            "iteration : 150, loss : 1.0434, accuracy : 57.27\n",
            "iteration : 200, loss : 1.0404, accuracy : 57.23\n",
            "iteration : 250, loss : 1.0397, accuracy : 57.42\n",
            "iteration : 300, loss : 1.0388, accuracy : 57.42\n",
            "iteration : 350, loss : 1.0398, accuracy : 57.35\n",
            "Epoch : 130, training loss : 1.0415, training accuracy : 57.26, test loss : 1.0662, test accuracy : 56.22\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 1.0789, accuracy : 56.17\n",
            "iteration : 100, loss : 1.0427, accuracy : 57.58\n",
            "iteration : 150, loss : 1.0382, accuracy : 57.48\n",
            "iteration : 200, loss : 1.0466, accuracy : 57.21\n",
            "iteration : 250, loss : 1.0436, accuracy : 57.23\n",
            "iteration : 300, loss : 1.0421, accuracy : 57.18\n",
            "iteration : 350, loss : 1.0432, accuracy : 57.08\n",
            "Epoch : 131, training loss : 1.0445, training accuracy : 57.09, test loss : 1.0650, test accuracy : 56.29\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 1.0550, accuracy : 56.95\n",
            "iteration : 100, loss : 1.0412, accuracy : 57.19\n",
            "iteration : 150, loss : 1.0473, accuracy : 56.95\n",
            "iteration : 200, loss : 1.0432, accuracy : 56.91\n",
            "iteration : 250, loss : 1.0449, accuracy : 56.81\n",
            "iteration : 300, loss : 1.0453, accuracy : 56.95\n",
            "iteration : 350, loss : 1.0465, accuracy : 56.87\n",
            "Epoch : 132, training loss : 1.0472, training accuracy : 56.84, test loss : 1.0572, test accuracy : 56.64\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 1.0319, accuracy : 58.06\n",
            "iteration : 100, loss : 1.0336, accuracy : 57.84\n",
            "iteration : 150, loss : 1.0302, accuracy : 57.89\n",
            "iteration : 200, loss : 1.0378, accuracy : 57.52\n",
            "iteration : 250, loss : 1.0376, accuracy : 57.48\n",
            "iteration : 300, loss : 1.0406, accuracy : 57.41\n",
            "iteration : 350, loss : 1.0388, accuracy : 57.52\n",
            "Epoch : 133, training loss : 1.0396, training accuracy : 57.45, test loss : 1.0797, test accuracy : 55.69\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 1.0502, accuracy : 56.12\n",
            "iteration : 100, loss : 1.0567, accuracy : 56.34\n",
            "iteration : 150, loss : 1.0526, accuracy : 56.43\n",
            "iteration : 200, loss : 1.0432, accuracy : 56.82\n",
            "iteration : 250, loss : 1.0398, accuracy : 57.01\n",
            "iteration : 300, loss : 1.0406, accuracy : 56.98\n",
            "iteration : 350, loss : 1.0425, accuracy : 56.89\n",
            "Epoch : 134, training loss : 1.0415, training accuracy : 56.95, test loss : 1.0579, test accuracy : 56.72\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 1.0352, accuracy : 57.12\n",
            "iteration : 100, loss : 1.0447, accuracy : 56.82\n",
            "iteration : 150, loss : 1.0389, accuracy : 57.08\n",
            "iteration : 200, loss : 1.0377, accuracy : 57.09\n",
            "iteration : 250, loss : 1.0404, accuracy : 56.97\n",
            "iteration : 300, loss : 1.0403, accuracy : 56.97\n",
            "iteration : 350, loss : 1.0386, accuracy : 57.07\n",
            "Epoch : 135, training loss : 1.0392, training accuracy : 57.04, test loss : 1.0717, test accuracy : 56.23\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 1.0210, accuracy : 57.55\n",
            "iteration : 100, loss : 1.0395, accuracy : 56.81\n",
            "iteration : 150, loss : 1.0391, accuracy : 56.69\n",
            "iteration : 200, loss : 1.0441, accuracy : 56.79\n",
            "iteration : 250, loss : 1.0432, accuracy : 56.93\n",
            "iteration : 300, loss : 1.0431, accuracy : 56.84\n",
            "iteration : 350, loss : 1.0444, accuracy : 56.85\n",
            "Epoch : 136, training loss : 1.0432, training accuracy : 56.90, test loss : 1.0618, test accuracy : 56.32\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 1.0407, accuracy : 57.19\n",
            "iteration : 100, loss : 1.0472, accuracy : 57.11\n",
            "iteration : 150, loss : 1.0513, accuracy : 56.94\n",
            "iteration : 200, loss : 1.0401, accuracy : 57.26\n",
            "iteration : 250, loss : 1.0389, accuracy : 57.30\n",
            "iteration : 300, loss : 1.0416, accuracy : 57.17\n",
            "iteration : 350, loss : 1.0440, accuracy : 57.06\n",
            "Epoch : 137, training loss : 1.0440, training accuracy : 57.00, test loss : 1.0778, test accuracy : 55.57\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 1.0436, accuracy : 57.03\n",
            "iteration : 100, loss : 1.0443, accuracy : 57.06\n",
            "iteration : 150, loss : 1.0460, accuracy : 56.87\n",
            "iteration : 200, loss : 1.0421, accuracy : 57.01\n",
            "iteration : 250, loss : 1.0425, accuracy : 56.97\n",
            "iteration : 300, loss : 1.0455, accuracy : 56.83\n",
            "iteration : 350, loss : 1.0428, accuracy : 57.00\n",
            "Epoch : 138, training loss : 1.0423, training accuracy : 57.01, test loss : 1.0589, test accuracy : 56.66\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 1.0329, accuracy : 56.67\n",
            "iteration : 100, loss : 1.0381, accuracy : 56.66\n",
            "iteration : 150, loss : 1.0400, accuracy : 56.74\n",
            "iteration : 200, loss : 1.0414, accuracy : 56.66\n",
            "iteration : 250, loss : 1.0418, accuracy : 56.63\n",
            "iteration : 300, loss : 1.0424, accuracy : 56.55\n",
            "iteration : 350, loss : 1.0430, accuracy : 56.66\n",
            "Epoch : 139, training loss : 1.0431, training accuracy : 56.65, test loss : 1.0528, test accuracy : 56.80\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 1.0649, accuracy : 55.52\n",
            "iteration : 100, loss : 1.0519, accuracy : 56.53\n",
            "iteration : 150, loss : 1.0521, accuracy : 56.49\n",
            "iteration : 200, loss : 1.0455, accuracy : 56.87\n",
            "iteration : 250, loss : 1.0407, accuracy : 57.06\n",
            "iteration : 300, loss : 1.0399, accuracy : 57.05\n",
            "iteration : 350, loss : 1.0342, accuracy : 57.27\n",
            "Epoch : 140, training loss : 1.0353, training accuracy : 57.29, test loss : 1.0593, test accuracy : 56.50\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 1.0420, accuracy : 55.86\n",
            "iteration : 100, loss : 1.0390, accuracy : 56.86\n",
            "iteration : 150, loss : 1.0431, accuracy : 56.66\n",
            "iteration : 200, loss : 1.0425, accuracy : 56.78\n",
            "iteration : 250, loss : 1.0476, accuracy : 56.66\n",
            "iteration : 300, loss : 1.0477, accuracy : 56.78\n",
            "iteration : 350, loss : 1.0477, accuracy : 56.73\n",
            "Epoch : 141, training loss : 1.0460, training accuracy : 56.82, test loss : 1.0444, test accuracy : 57.14\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 1.0190, accuracy : 58.19\n",
            "iteration : 100, loss : 1.0305, accuracy : 57.45\n",
            "iteration : 150, loss : 1.0345, accuracy : 57.30\n",
            "iteration : 200, loss : 1.0330, accuracy : 57.16\n",
            "iteration : 250, loss : 1.0314, accuracy : 57.12\n",
            "iteration : 300, loss : 1.0342, accuracy : 57.04\n",
            "iteration : 350, loss : 1.0356, accuracy : 56.98\n",
            "Epoch : 142, training loss : 1.0363, training accuracy : 56.97, test loss : 1.0536, test accuracy : 56.83\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 1.0034, accuracy : 58.83\n",
            "iteration : 100, loss : 1.0268, accuracy : 57.55\n",
            "iteration : 150, loss : 1.0328, accuracy : 57.23\n",
            "iteration : 200, loss : 1.0405, accuracy : 56.98\n",
            "iteration : 250, loss : 1.0441, accuracy : 56.78\n",
            "iteration : 300, loss : 1.0394, accuracy : 57.03\n",
            "iteration : 350, loss : 1.0395, accuracy : 56.97\n",
            "Epoch : 143, training loss : 1.0380, training accuracy : 57.01, test loss : 1.0512, test accuracy : 57.01\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 1.0156, accuracy : 58.02\n",
            "iteration : 100, loss : 1.0240, accuracy : 57.60\n",
            "iteration : 150, loss : 1.0317, accuracy : 57.47\n",
            "iteration : 200, loss : 1.0294, accuracy : 57.50\n",
            "iteration : 250, loss : 1.0369, accuracy : 57.10\n",
            "iteration : 300, loss : 1.0375, accuracy : 57.02\n",
            "iteration : 350, loss : 1.0368, accuracy : 57.06\n",
            "Epoch : 144, training loss : 1.0386, training accuracy : 56.99, test loss : 1.0718, test accuracy : 56.06\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 1.0337, accuracy : 57.62\n",
            "iteration : 100, loss : 1.0330, accuracy : 57.28\n",
            "iteration : 150, loss : 1.0379, accuracy : 57.18\n",
            "iteration : 200, loss : 1.0356, accuracy : 57.23\n",
            "iteration : 250, loss : 1.0349, accuracy : 57.21\n",
            "iteration : 300, loss : 1.0373, accuracy : 57.10\n",
            "iteration : 350, loss : 1.0345, accuracy : 57.23\n",
            "Epoch : 145, training loss : 1.0345, training accuracy : 57.18, test loss : 1.0503, test accuracy : 56.67\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 1.0362, accuracy : 57.14\n",
            "iteration : 100, loss : 1.0326, accuracy : 57.23\n",
            "iteration : 150, loss : 1.0395, accuracy : 56.97\n",
            "iteration : 200, loss : 1.0403, accuracy : 57.02\n",
            "iteration : 250, loss : 1.0399, accuracy : 56.95\n",
            "iteration : 300, loss : 1.0417, accuracy : 56.92\n",
            "iteration : 350, loss : 1.0433, accuracy : 56.83\n",
            "Epoch : 146, training loss : 1.0419, training accuracy : 56.85, test loss : 1.0445, test accuracy : 57.17\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 1.0165, accuracy : 57.38\n",
            "iteration : 100, loss : 1.0254, accuracy : 57.59\n",
            "iteration : 150, loss : 1.0343, accuracy : 57.30\n",
            "iteration : 200, loss : 1.0372, accuracy : 57.34\n",
            "iteration : 250, loss : 1.0340, accuracy : 57.34\n",
            "iteration : 300, loss : 1.0361, accuracy : 57.27\n",
            "iteration : 350, loss : 1.0344, accuracy : 57.34\n",
            "Epoch : 147, training loss : 1.0342, training accuracy : 57.31, test loss : 1.0544, test accuracy : 56.68\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 1.0504, accuracy : 56.89\n",
            "iteration : 100, loss : 1.0412, accuracy : 57.10\n",
            "iteration : 150, loss : 1.0379, accuracy : 57.06\n",
            "iteration : 200, loss : 1.0347, accuracy : 57.27\n",
            "iteration : 250, loss : 1.0339, accuracy : 57.25\n",
            "iteration : 300, loss : 1.0351, accuracy : 57.28\n",
            "iteration : 350, loss : 1.0331, accuracy : 57.33\n",
            "Epoch : 148, training loss : 1.0338, training accuracy : 57.30, test loss : 1.0401, test accuracy : 57.44\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 1.0157, accuracy : 57.48\n",
            "iteration : 100, loss : 1.0249, accuracy : 57.17\n",
            "iteration : 150, loss : 1.0220, accuracy : 57.31\n",
            "iteration : 200, loss : 1.0214, accuracy : 57.41\n",
            "iteration : 250, loss : 1.0244, accuracy : 57.36\n",
            "iteration : 300, loss : 1.0274, accuracy : 57.25\n",
            "iteration : 350, loss : 1.0245, accuracy : 57.37\n",
            "Epoch : 149, training loss : 1.0263, training accuracy : 57.31, test loss : 1.0644, test accuracy : 56.04\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 1.0336, accuracy : 57.25\n",
            "iteration : 100, loss : 1.0321, accuracy : 57.36\n",
            "iteration : 150, loss : 1.0314, accuracy : 57.21\n",
            "iteration : 200, loss : 1.0322, accuracy : 57.24\n",
            "iteration : 250, loss : 1.0314, accuracy : 57.24\n",
            "iteration : 300, loss : 1.0337, accuracy : 57.18\n",
            "iteration : 350, loss : 1.0346, accuracy : 57.17\n",
            "Epoch : 150, training loss : 1.0348, training accuracy : 57.19, test loss : 1.0525, test accuracy : 56.76\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 1.0317, accuracy : 56.61\n",
            "iteration : 100, loss : 1.0212, accuracy : 57.13\n",
            "iteration : 150, loss : 1.0313, accuracy : 56.96\n",
            "iteration : 200, loss : 1.0323, accuracy : 57.24\n",
            "iteration : 250, loss : 1.0357, accuracy : 57.12\n",
            "iteration : 300, loss : 1.0342, accuracy : 57.10\n",
            "iteration : 350, loss : 1.0350, accuracy : 57.08\n",
            "Epoch : 151, training loss : 1.0326, training accuracy : 57.17, test loss : 1.0492, test accuracy : 57.13\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 1.0582, accuracy : 56.20\n",
            "iteration : 100, loss : 1.0336, accuracy : 57.09\n",
            "iteration : 150, loss : 1.0399, accuracy : 57.08\n",
            "iteration : 200, loss : 1.0372, accuracy : 57.01\n",
            "iteration : 250, loss : 1.0339, accuracy : 57.18\n",
            "iteration : 300, loss : 1.0332, accuracy : 57.05\n",
            "iteration : 350, loss : 1.0353, accuracy : 56.96\n",
            "Epoch : 152, training loss : 1.0359, training accuracy : 56.99, test loss : 1.0453, test accuracy : 56.90\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 1.0242, accuracy : 57.56\n",
            "iteration : 100, loss : 1.0210, accuracy : 57.92\n",
            "iteration : 150, loss : 1.0187, accuracy : 57.78\n",
            "iteration : 200, loss : 1.0219, accuracy : 57.65\n",
            "iteration : 250, loss : 1.0241, accuracy : 57.52\n",
            "iteration : 300, loss : 1.0274, accuracy : 57.49\n",
            "iteration : 350, loss : 1.0278, accuracy : 57.54\n",
            "Epoch : 153, training loss : 1.0276, training accuracy : 57.52, test loss : 1.0562, test accuracy : 56.48\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 1.0168, accuracy : 57.78\n",
            "iteration : 100, loss : 1.0225, accuracy : 57.33\n",
            "iteration : 150, loss : 1.0345, accuracy : 56.95\n",
            "iteration : 200, loss : 1.0392, accuracy : 57.05\n",
            "iteration : 250, loss : 1.0343, accuracy : 57.08\n",
            "iteration : 300, loss : 1.0358, accuracy : 57.12\n",
            "iteration : 350, loss : 1.0335, accuracy : 57.22\n",
            "Epoch : 154, training loss : 1.0339, training accuracy : 57.19, test loss : 1.0448, test accuracy : 56.61\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 1.0296, accuracy : 56.89\n",
            "iteration : 100, loss : 1.0447, accuracy : 56.33\n",
            "iteration : 150, loss : 1.0386, accuracy : 56.76\n",
            "iteration : 200, loss : 1.0315, accuracy : 57.18\n",
            "iteration : 250, loss : 1.0322, accuracy : 57.11\n",
            "iteration : 300, loss : 1.0300, accuracy : 57.11\n",
            "iteration : 350, loss : 1.0304, accuracy : 57.17\n",
            "Epoch : 155, training loss : 1.0280, training accuracy : 57.30, test loss : 1.0538, test accuracy : 56.65\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 1.0199, accuracy : 57.84\n",
            "iteration : 100, loss : 1.0329, accuracy : 57.47\n",
            "iteration : 150, loss : 1.0326, accuracy : 57.26\n",
            "iteration : 200, loss : 1.0321, accuracy : 57.22\n",
            "iteration : 250, loss : 1.0308, accuracy : 57.36\n",
            "iteration : 300, loss : 1.0284, accuracy : 57.47\n",
            "iteration : 350, loss : 1.0275, accuracy : 57.46\n",
            "Epoch : 156, training loss : 1.0286, training accuracy : 57.43, test loss : 1.0566, test accuracy : 56.32\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 1.0273, accuracy : 57.78\n",
            "iteration : 100, loss : 1.0295, accuracy : 57.24\n",
            "iteration : 150, loss : 1.0285, accuracy : 57.61\n",
            "iteration : 200, loss : 1.0325, accuracy : 57.43\n",
            "iteration : 250, loss : 1.0288, accuracy : 57.54\n",
            "iteration : 300, loss : 1.0275, accuracy : 57.55\n",
            "iteration : 350, loss : 1.0283, accuracy : 57.50\n",
            "Epoch : 157, training loss : 1.0272, training accuracy : 57.55, test loss : 1.0591, test accuracy : 56.41\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 1.0324, accuracy : 57.25\n",
            "iteration : 100, loss : 1.0227, accuracy : 57.45\n",
            "iteration : 150, loss : 1.0277, accuracy : 57.40\n",
            "iteration : 200, loss : 1.0274, accuracy : 57.44\n",
            "iteration : 250, loss : 1.0302, accuracy : 57.27\n",
            "iteration : 300, loss : 1.0298, accuracy : 57.23\n",
            "iteration : 350, loss : 1.0271, accuracy : 57.31\n",
            "Epoch : 158, training loss : 1.0280, training accuracy : 57.26, test loss : 1.0559, test accuracy : 56.58\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 1.0361, accuracy : 57.02\n",
            "iteration : 100, loss : 1.0318, accuracy : 56.98\n",
            "iteration : 150, loss : 1.0313, accuracy : 57.10\n",
            "iteration : 200, loss : 1.0340, accuracy : 57.08\n",
            "iteration : 250, loss : 1.0306, accuracy : 57.09\n",
            "iteration : 300, loss : 1.0301, accuracy : 57.22\n",
            "iteration : 350, loss : 1.0302, accuracy : 57.16\n",
            "Epoch : 159, training loss : 1.0284, training accuracy : 57.26, test loss : 1.0333, test accuracy : 57.38\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 1.0143, accuracy : 58.67\n",
            "iteration : 100, loss : 1.0145, accuracy : 58.07\n",
            "iteration : 150, loss : 1.0132, accuracy : 58.07\n",
            "iteration : 200, loss : 1.0090, accuracy : 58.16\n",
            "iteration : 250, loss : 1.0162, accuracy : 57.73\n",
            "iteration : 300, loss : 1.0189, accuracy : 57.74\n",
            "iteration : 350, loss : 1.0185, accuracy : 57.77\n",
            "Epoch : 160, training loss : 1.0167, training accuracy : 57.83, test loss : 1.0375, test accuracy : 57.05\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 1.0085, accuracy : 57.58\n",
            "iteration : 100, loss : 1.0112, accuracy : 57.63\n",
            "iteration : 150, loss : 1.0221, accuracy : 57.34\n",
            "iteration : 200, loss : 1.0267, accuracy : 57.32\n",
            "iteration : 250, loss : 1.0270, accuracy : 57.25\n",
            "iteration : 300, loss : 1.0304, accuracy : 57.08\n",
            "iteration : 350, loss : 1.0279, accuracy : 57.18\n",
            "Epoch : 161, training loss : 1.0282, training accuracy : 57.22, test loss : 1.0367, test accuracy : 57.19\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 1.0359, accuracy : 56.72\n",
            "iteration : 100, loss : 1.0291, accuracy : 57.16\n",
            "iteration : 150, loss : 1.0323, accuracy : 57.31\n",
            "iteration : 200, loss : 1.0352, accuracy : 57.50\n",
            "iteration : 250, loss : 1.0340, accuracy : 57.53\n",
            "iteration : 300, loss : 1.0354, accuracy : 57.53\n",
            "iteration : 350, loss : 1.0322, accuracy : 57.54\n",
            "Epoch : 162, training loss : 1.0328, training accuracy : 57.48, test loss : 1.0470, test accuracy : 56.85\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 1.0263, accuracy : 57.66\n",
            "iteration : 100, loss : 1.0187, accuracy : 57.56\n",
            "iteration : 150, loss : 1.0190, accuracy : 57.68\n",
            "iteration : 200, loss : 1.0195, accuracy : 57.67\n",
            "iteration : 250, loss : 1.0190, accuracy : 57.83\n",
            "iteration : 300, loss : 1.0208, accuracy : 57.73\n",
            "iteration : 350, loss : 1.0251, accuracy : 57.55\n",
            "Epoch : 163, training loss : 1.0253, training accuracy : 57.53, test loss : 1.0502, test accuracy : 56.61\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 1.0316, accuracy : 57.34\n",
            "iteration : 100, loss : 1.0241, accuracy : 57.80\n",
            "iteration : 150, loss : 1.0233, accuracy : 57.63\n",
            "iteration : 200, loss : 1.0214, accuracy : 57.68\n",
            "iteration : 250, loss : 1.0224, accuracy : 57.58\n",
            "iteration : 300, loss : 1.0227, accuracy : 57.57\n",
            "iteration : 350, loss : 1.0234, accuracy : 57.63\n",
            "Epoch : 164, training loss : 1.0240, training accuracy : 57.66, test loss : 1.0362, test accuracy : 57.32\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.9957, accuracy : 59.12\n",
            "iteration : 100, loss : 1.0023, accuracy : 58.66\n",
            "iteration : 150, loss : 1.0155, accuracy : 58.04\n",
            "iteration : 200, loss : 1.0198, accuracy : 57.91\n",
            "iteration : 250, loss : 1.0203, accuracy : 57.82\n",
            "iteration : 300, loss : 1.0225, accuracy : 57.70\n",
            "iteration : 350, loss : 1.0244, accuracy : 57.72\n",
            "Epoch : 165, training loss : 1.0244, training accuracy : 57.71, test loss : 1.0670, test accuracy : 56.58\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 1.0026, accuracy : 58.08\n",
            "iteration : 100, loss : 1.0237, accuracy : 57.41\n",
            "iteration : 150, loss : 1.0217, accuracy : 57.57\n",
            "iteration : 200, loss : 1.0188, accuracy : 57.66\n",
            "iteration : 250, loss : 1.0194, accuracy : 57.59\n",
            "iteration : 300, loss : 1.0175, accuracy : 57.76\n",
            "iteration : 350, loss : 1.0165, accuracy : 57.76\n",
            "Epoch : 166, training loss : 1.0157, training accuracy : 57.80, test loss : 1.0487, test accuracy : 56.99\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 1.0206, accuracy : 57.38\n",
            "iteration : 100, loss : 1.0217, accuracy : 57.19\n",
            "iteration : 150, loss : 1.0159, accuracy : 57.31\n",
            "iteration : 200, loss : 1.0143, accuracy : 57.54\n",
            "iteration : 250, loss : 1.0166, accuracy : 57.48\n",
            "iteration : 300, loss : 1.0201, accuracy : 57.40\n",
            "iteration : 350, loss : 1.0171, accuracy : 57.58\n",
            "Epoch : 167, training loss : 1.0168, training accuracy : 57.62, test loss : 1.0453, test accuracy : 56.71\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 1.0287, accuracy : 57.19\n",
            "iteration : 100, loss : 1.0277, accuracy : 57.23\n",
            "iteration : 150, loss : 1.0292, accuracy : 57.30\n",
            "iteration : 200, loss : 1.0266, accuracy : 57.29\n",
            "iteration : 250, loss : 1.0284, accuracy : 57.17\n",
            "iteration : 300, loss : 1.0301, accuracy : 57.10\n",
            "iteration : 350, loss : 1.0298, accuracy : 57.13\n",
            "Epoch : 168, training loss : 1.0281, training accuracy : 57.23, test loss : 1.0442, test accuracy : 56.41\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.9967, accuracy : 58.89\n",
            "iteration : 100, loss : 1.0042, accuracy : 58.64\n",
            "iteration : 150, loss : 1.0094, accuracy : 58.24\n",
            "iteration : 200, loss : 1.0148, accuracy : 57.86\n",
            "iteration : 250, loss : 1.0110, accuracy : 57.90\n",
            "iteration : 300, loss : 1.0149, accuracy : 57.79\n",
            "iteration : 350, loss : 1.0165, accuracy : 57.69\n",
            "Epoch : 169, training loss : 1.0181, training accuracy : 57.64, test loss : 1.0317, test accuracy : 57.51\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.9992, accuracy : 58.45\n",
            "iteration : 100, loss : 1.0153, accuracy : 57.90\n",
            "iteration : 150, loss : 1.0139, accuracy : 57.78\n",
            "iteration : 200, loss : 1.0139, accuracy : 57.97\n",
            "iteration : 250, loss : 1.0166, accuracy : 57.81\n",
            "iteration : 300, loss : 1.0153, accuracy : 57.82\n",
            "iteration : 350, loss : 1.0148, accuracy : 57.83\n",
            "Epoch : 170, training loss : 1.0142, training accuracy : 57.85, test loss : 1.0379, test accuracy : 57.24\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 1.0332, accuracy : 56.11\n",
            "iteration : 100, loss : 1.0263, accuracy : 57.18\n",
            "iteration : 150, loss : 1.0206, accuracy : 57.44\n",
            "iteration : 200, loss : 1.0224, accuracy : 57.46\n",
            "iteration : 250, loss : 1.0204, accuracy : 57.58\n",
            "iteration : 300, loss : 1.0200, accuracy : 57.59\n",
            "iteration : 350, loss : 1.0190, accuracy : 57.60\n",
            "Epoch : 171, training loss : 1.0197, training accuracy : 57.59, test loss : 1.0531, test accuracy : 56.33\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 1.0036, accuracy : 57.92\n",
            "iteration : 100, loss : 1.0271, accuracy : 57.19\n",
            "iteration : 150, loss : 1.0146, accuracy : 57.67\n",
            "iteration : 200, loss : 1.0133, accuracy : 57.80\n",
            "iteration : 250, loss : 1.0157, accuracy : 57.72\n",
            "iteration : 300, loss : 1.0176, accuracy : 57.61\n",
            "iteration : 350, loss : 1.0191, accuracy : 57.60\n",
            "Epoch : 172, training loss : 1.0189, training accuracy : 57.58, test loss : 1.0378, test accuracy : 57.33\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.9976, accuracy : 58.73\n",
            "iteration : 100, loss : 1.0044, accuracy : 57.95\n",
            "iteration : 150, loss : 1.0192, accuracy : 57.44\n",
            "iteration : 200, loss : 1.0158, accuracy : 57.62\n",
            "iteration : 250, loss : 1.0181, accuracy : 57.52\n",
            "iteration : 300, loss : 1.0183, accuracy : 57.61\n",
            "iteration : 350, loss : 1.0201, accuracy : 57.52\n",
            "Epoch : 173, training loss : 1.0192, training accuracy : 57.55, test loss : 1.0435, test accuracy : 57.50\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 1.0085, accuracy : 58.36\n",
            "iteration : 100, loss : 1.0172, accuracy : 57.93\n",
            "iteration : 150, loss : 1.0102, accuracy : 57.97\n",
            "iteration : 200, loss : 1.0125, accuracy : 57.75\n",
            "iteration : 250, loss : 1.0116, accuracy : 57.62\n",
            "iteration : 300, loss : 1.0136, accuracy : 57.70\n",
            "iteration : 350, loss : 1.0123, accuracy : 57.73\n",
            "Epoch : 174, training loss : 1.0125, training accuracy : 57.71, test loss : 1.0363, test accuracy : 56.95\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.9822, accuracy : 58.92\n",
            "iteration : 100, loss : 0.9896, accuracy : 58.09\n",
            "iteration : 150, loss : 0.9980, accuracy : 58.10\n",
            "iteration : 200, loss : 1.0036, accuracy : 57.92\n",
            "iteration : 250, loss : 1.0028, accuracy : 58.10\n",
            "iteration : 300, loss : 1.0038, accuracy : 58.02\n",
            "iteration : 350, loss : 1.0063, accuracy : 57.91\n",
            "Epoch : 175, training loss : 1.0061, training accuracy : 57.95, test loss : 1.0321, test accuracy : 57.44\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 1.0154, accuracy : 58.05\n",
            "iteration : 100, loss : 1.0307, accuracy : 57.45\n",
            "iteration : 150, loss : 1.0246, accuracy : 57.58\n",
            "iteration : 200, loss : 1.0269, accuracy : 57.55\n",
            "iteration : 250, loss : 1.0193, accuracy : 57.80\n",
            "iteration : 300, loss : 1.0136, accuracy : 57.98\n",
            "iteration : 350, loss : 1.0137, accuracy : 58.04\n",
            "Epoch : 176, training loss : 1.0133, training accuracy : 58.01, test loss : 1.0365, test accuracy : 57.31\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 1.0201, accuracy : 57.53\n",
            "iteration : 100, loss : 1.0179, accuracy : 57.17\n",
            "iteration : 150, loss : 1.0175, accuracy : 57.32\n",
            "iteration : 200, loss : 1.0209, accuracy : 57.25\n",
            "iteration : 250, loss : 1.0193, accuracy : 57.31\n",
            "iteration : 300, loss : 1.0188, accuracy : 57.36\n",
            "iteration : 350, loss : 1.0172, accuracy : 57.50\n",
            "Epoch : 177, training loss : 1.0165, training accuracy : 57.52, test loss : 1.0201, test accuracy : 57.97\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 1.0116, accuracy : 57.80\n",
            "iteration : 100, loss : 1.0203, accuracy : 57.13\n",
            "iteration : 150, loss : 1.0210, accuracy : 57.23\n",
            "iteration : 200, loss : 1.0165, accuracy : 57.44\n",
            "iteration : 250, loss : 1.0180, accuracy : 57.30\n",
            "iteration : 300, loss : 1.0174, accuracy : 57.38\n",
            "iteration : 350, loss : 1.0151, accuracy : 57.49\n",
            "Epoch : 178, training loss : 1.0153, training accuracy : 57.56, test loss : 1.0416, test accuracy : 56.71\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.9933, accuracy : 58.59\n",
            "iteration : 100, loss : 1.0107, accuracy : 57.93\n",
            "iteration : 150, loss : 1.0121, accuracy : 57.94\n",
            "iteration : 200, loss : 1.0100, accuracy : 57.89\n",
            "iteration : 250, loss : 1.0124, accuracy : 57.83\n",
            "iteration : 300, loss : 1.0140, accuracy : 57.72\n",
            "iteration : 350, loss : 1.0151, accuracy : 57.64\n",
            "Epoch : 179, training loss : 1.0133, training accuracy : 57.72, test loss : 1.0387, test accuracy : 57.14\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 1.0339, accuracy : 57.11\n",
            "iteration : 100, loss : 1.0198, accuracy : 57.47\n",
            "iteration : 150, loss : 1.0172, accuracy : 57.54\n",
            "iteration : 200, loss : 1.0130, accuracy : 57.74\n",
            "iteration : 250, loss : 1.0118, accuracy : 57.82\n",
            "iteration : 300, loss : 1.0134, accuracy : 57.68\n",
            "iteration : 350, loss : 1.0161, accuracy : 57.60\n",
            "Epoch : 180, training loss : 1.0162, training accuracy : 57.59, test loss : 1.0305, test accuracy : 57.24\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 1.0248, accuracy : 57.66\n",
            "iteration : 100, loss : 1.0120, accuracy : 57.79\n",
            "iteration : 150, loss : 1.0023, accuracy : 58.29\n",
            "iteration : 200, loss : 1.0053, accuracy : 58.25\n",
            "iteration : 250, loss : 1.0026, accuracy : 58.37\n",
            "iteration : 300, loss : 1.0009, accuracy : 58.47\n",
            "iteration : 350, loss : 1.0037, accuracy : 58.26\n",
            "Epoch : 181, training loss : 1.0063, training accuracy : 58.13, test loss : 1.0416, test accuracy : 56.91\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 1.0092, accuracy : 58.02\n",
            "iteration : 100, loss : 1.0009, accuracy : 58.16\n",
            "iteration : 150, loss : 1.0058, accuracy : 58.04\n",
            "iteration : 200, loss : 1.0060, accuracy : 57.98\n",
            "iteration : 250, loss : 1.0055, accuracy : 58.02\n",
            "iteration : 300, loss : 1.0052, accuracy : 58.04\n",
            "iteration : 350, loss : 1.0026, accuracy : 58.14\n",
            "Epoch : 182, training loss : 1.0026, training accuracy : 58.11, test loss : 1.0248, test accuracy : 57.29\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 1.0185, accuracy : 57.88\n",
            "iteration : 100, loss : 1.0120, accuracy : 58.06\n",
            "iteration : 150, loss : 1.0027, accuracy : 58.23\n",
            "iteration : 200, loss : 1.0079, accuracy : 58.26\n",
            "iteration : 250, loss : 1.0098, accuracy : 58.03\n",
            "iteration : 300, loss : 1.0087, accuracy : 58.08\n",
            "iteration : 350, loss : 1.0097, accuracy : 58.12\n",
            "Epoch : 183, training loss : 1.0090, training accuracy : 58.04, test loss : 1.0315, test accuracy : 57.19\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 1.0020, accuracy : 59.00\n",
            "iteration : 100, loss : 1.0070, accuracy : 58.24\n",
            "iteration : 150, loss : 0.9980, accuracy : 58.38\n",
            "iteration : 200, loss : 1.0035, accuracy : 58.09\n",
            "iteration : 250, loss : 1.0052, accuracy : 58.03\n",
            "iteration : 300, loss : 1.0061, accuracy : 58.06\n",
            "iteration : 350, loss : 1.0057, accuracy : 58.08\n",
            "Epoch : 184, training loss : 1.0062, training accuracy : 57.98, test loss : 1.0360, test accuracy : 57.32\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 1.0168, accuracy : 57.75\n",
            "iteration : 100, loss : 1.0128, accuracy : 57.72\n",
            "iteration : 150, loss : 1.0109, accuracy : 57.72\n",
            "iteration : 200, loss : 1.0061, accuracy : 57.73\n",
            "iteration : 250, loss : 1.0083, accuracy : 57.68\n",
            "iteration : 300, loss : 1.0074, accuracy : 57.66\n",
            "iteration : 350, loss : 1.0094, accuracy : 57.63\n",
            "Epoch : 185, training loss : 1.0094, training accuracy : 57.73, test loss : 1.0307, test accuracy : 57.37\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.9997, accuracy : 58.62\n",
            "iteration : 100, loss : 1.0172, accuracy : 57.91\n",
            "iteration : 150, loss : 1.0074, accuracy : 58.22\n",
            "iteration : 200, loss : 1.0045, accuracy : 58.21\n",
            "iteration : 250, loss : 1.0046, accuracy : 58.12\n",
            "iteration : 300, loss : 1.0044, accuracy : 58.07\n",
            "iteration : 350, loss : 1.0073, accuracy : 57.81\n",
            "Epoch : 186, training loss : 1.0069, training accuracy : 57.83, test loss : 1.0238, test accuracy : 57.63\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.9877, accuracy : 58.00\n",
            "iteration : 100, loss : 1.0046, accuracy : 57.88\n",
            "iteration : 150, loss : 1.0051, accuracy : 57.82\n",
            "iteration : 200, loss : 1.0079, accuracy : 57.65\n",
            "iteration : 250, loss : 1.0084, accuracy : 57.64\n",
            "iteration : 300, loss : 1.0113, accuracy : 57.61\n",
            "iteration : 350, loss : 1.0108, accuracy : 57.64\n",
            "Epoch : 187, training loss : 1.0114, training accuracy : 57.60, test loss : 1.0265, test accuracy : 57.38\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 1.0133, accuracy : 57.17\n",
            "iteration : 100, loss : 1.0144, accuracy : 57.47\n",
            "iteration : 150, loss : 1.0187, accuracy : 57.17\n",
            "iteration : 200, loss : 1.0121, accuracy : 57.44\n",
            "iteration : 250, loss : 1.0138, accuracy : 57.50\n",
            "iteration : 300, loss : 1.0131, accuracy : 57.60\n",
            "iteration : 350, loss : 1.0124, accuracy : 57.59\n",
            "Epoch : 188, training loss : 1.0143, training accuracy : 57.50, test loss : 1.0306, test accuracy : 57.33\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 1.0100, accuracy : 57.03\n",
            "iteration : 100, loss : 1.0083, accuracy : 57.29\n",
            "iteration : 150, loss : 1.0101, accuracy : 57.14\n",
            "iteration : 200, loss : 1.0105, accuracy : 57.17\n",
            "iteration : 250, loss : 1.0069, accuracy : 57.48\n",
            "iteration : 300, loss : 1.0099, accuracy : 57.58\n",
            "iteration : 350, loss : 1.0116, accuracy : 57.52\n",
            "Epoch : 189, training loss : 1.0131, training accuracy : 57.52, test loss : 1.0347, test accuracy : 56.93\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.9727, accuracy : 59.09\n",
            "iteration : 100, loss : 0.9837, accuracy : 58.41\n",
            "iteration : 150, loss : 0.9897, accuracy : 58.23\n",
            "iteration : 200, loss : 0.9963, accuracy : 58.07\n",
            "iteration : 250, loss : 0.9991, accuracy : 57.95\n",
            "iteration : 300, loss : 1.0000, accuracy : 58.03\n",
            "iteration : 350, loss : 1.0015, accuracy : 58.02\n",
            "Epoch : 190, training loss : 0.9990, training accuracy : 58.16, test loss : 1.0293, test accuracy : 57.30\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.9883, accuracy : 57.94\n",
            "iteration : 100, loss : 0.9936, accuracy : 58.07\n",
            "iteration : 150, loss : 1.0052, accuracy : 57.59\n",
            "iteration : 200, loss : 1.0020, accuracy : 57.76\n",
            "iteration : 250, loss : 1.0029, accuracy : 57.77\n",
            "iteration : 300, loss : 1.0029, accuracy : 57.86\n",
            "iteration : 350, loss : 1.0025, accuracy : 57.88\n",
            "Epoch : 191, training loss : 1.0039, training accuracy : 57.84, test loss : 1.0313, test accuracy : 57.02\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 1.0071, accuracy : 57.75\n",
            "iteration : 100, loss : 0.9985, accuracy : 58.44\n",
            "iteration : 150, loss : 0.9928, accuracy : 58.33\n",
            "iteration : 200, loss : 0.9968, accuracy : 57.93\n",
            "iteration : 250, loss : 0.9924, accuracy : 58.17\n",
            "iteration : 300, loss : 0.9934, accuracy : 58.15\n",
            "iteration : 350, loss : 0.9951, accuracy : 58.11\n",
            "Epoch : 192, training loss : 0.9953, training accuracy : 58.11, test loss : 1.0293, test accuracy : 57.44\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 1.0037, accuracy : 58.17\n",
            "iteration : 100, loss : 0.9917, accuracy : 58.23\n",
            "iteration : 150, loss : 0.9991, accuracy : 58.15\n",
            "iteration : 200, loss : 1.0002, accuracy : 58.10\n",
            "iteration : 250, loss : 1.0022, accuracy : 57.97\n",
            "iteration : 300, loss : 1.0023, accuracy : 58.00\n",
            "iteration : 350, loss : 1.0020, accuracy : 57.91\n",
            "Epoch : 193, training loss : 1.0010, training accuracy : 57.94, test loss : 1.0334, test accuracy : 57.15\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.9969, accuracy : 57.28\n",
            "iteration : 100, loss : 0.9936, accuracy : 57.31\n",
            "iteration : 150, loss : 0.9967, accuracy : 57.69\n",
            "iteration : 200, loss : 1.0001, accuracy : 57.61\n",
            "iteration : 250, loss : 0.9979, accuracy : 57.69\n",
            "iteration : 300, loss : 0.9968, accuracy : 57.78\n",
            "iteration : 350, loss : 0.9987, accuracy : 57.81\n",
            "Epoch : 194, training loss : 0.9991, training accuracy : 57.82, test loss : 1.0316, test accuracy : 57.46\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 1.0131, accuracy : 57.08\n",
            "iteration : 100, loss : 1.0172, accuracy : 57.22\n",
            "iteration : 150, loss : 1.0077, accuracy : 57.62\n",
            "iteration : 200, loss : 1.0072, accuracy : 57.69\n",
            "iteration : 250, loss : 1.0045, accuracy : 57.75\n",
            "iteration : 300, loss : 1.0071, accuracy : 57.71\n",
            "iteration : 350, loss : 1.0027, accuracy : 57.96\n",
            "Epoch : 195, training loss : 1.0022, training accuracy : 57.91, test loss : 1.0270, test accuracy : 57.39\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 1.0100, accuracy : 57.33\n",
            "iteration : 100, loss : 1.0072, accuracy : 57.54\n",
            "iteration : 150, loss : 1.0013, accuracy : 57.90\n",
            "iteration : 200, loss : 0.9994, accuracy : 57.78\n",
            "iteration : 250, loss : 1.0017, accuracy : 57.95\n",
            "iteration : 300, loss : 1.0022, accuracy : 57.87\n",
            "iteration : 350, loss : 0.9987, accuracy : 58.00\n",
            "Epoch : 196, training loss : 0.9992, training accuracy : 58.03, test loss : 1.0262, test accuracy : 57.32\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 1.0060, accuracy : 58.11\n",
            "iteration : 100, loss : 0.9946, accuracy : 58.39\n",
            "iteration : 150, loss : 0.9989, accuracy : 58.11\n",
            "iteration : 200, loss : 0.9928, accuracy : 58.33\n",
            "iteration : 250, loss : 0.9962, accuracy : 58.19\n",
            "iteration : 300, loss : 0.9965, accuracy : 58.11\n",
            "iteration : 350, loss : 0.9969, accuracy : 58.03\n",
            "Epoch : 197, training loss : 0.9970, training accuracy : 57.97, test loss : 1.0312, test accuracy : 57.38\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 1.0125, accuracy : 58.03\n",
            "iteration : 100, loss : 1.0073, accuracy : 58.00\n",
            "iteration : 150, loss : 1.0057, accuracy : 57.95\n",
            "iteration : 200, loss : 0.9959, accuracy : 58.41\n",
            "iteration : 250, loss : 0.9995, accuracy : 58.12\n",
            "iteration : 300, loss : 1.0016, accuracy : 58.07\n",
            "iteration : 350, loss : 0.9999, accuracy : 58.17\n",
            "Epoch : 198, training loss : 1.0011, training accuracy : 58.08, test loss : 1.0282, test accuracy : 57.00\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 1.0082, accuracy : 57.89\n",
            "iteration : 100, loss : 1.0064, accuracy : 57.69\n",
            "iteration : 150, loss : 1.0095, accuracy : 57.68\n",
            "iteration : 200, loss : 1.0072, accuracy : 57.77\n",
            "iteration : 250, loss : 1.0065, accuracy : 57.84\n",
            "iteration : 300, loss : 1.0039, accuracy : 57.91\n",
            "iteration : 350, loss : 1.0020, accuracy : 57.89\n",
            "Epoch : 199, training loss : 1.0011, training accuracy : 57.91, test loss : 1.0192, test accuracy : 57.43\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.9865, accuracy : 58.42\n",
            "iteration : 100, loss : 0.9971, accuracy : 57.82\n",
            "iteration : 150, loss : 0.9943, accuracy : 58.04\n",
            "iteration : 200, loss : 0.9979, accuracy : 57.93\n",
            "iteration : 250, loss : 0.9932, accuracy : 58.07\n",
            "iteration : 300, loss : 0.9944, accuracy : 58.04\n",
            "iteration : 350, loss : 0.9951, accuracy : 58.04\n",
            "Epoch : 200, training loss : 0.9956, training accuracy : 58.05, test loss : 1.0326, test accuracy : 56.95\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.9888, accuracy : 58.94\n",
            "iteration : 100, loss : 1.0003, accuracy : 58.09\n",
            "iteration : 150, loss : 0.9993, accuracy : 58.19\n",
            "iteration : 200, loss : 0.9954, accuracy : 58.16\n",
            "iteration : 250, loss : 1.0010, accuracy : 57.92\n",
            "iteration : 300, loss : 0.9997, accuracy : 57.96\n",
            "iteration : 350, loss : 0.9986, accuracy : 58.02\n",
            "Epoch : 201, training loss : 0.9977, training accuracy : 58.02, test loss : 1.0266, test accuracy : 57.13\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.9958, accuracy : 57.83\n",
            "iteration : 100, loss : 1.0032, accuracy : 57.37\n",
            "iteration : 150, loss : 0.9929, accuracy : 57.99\n",
            "iteration : 200, loss : 0.9953, accuracy : 57.89\n",
            "iteration : 250, loss : 0.9855, accuracy : 58.36\n",
            "iteration : 300, loss : 0.9885, accuracy : 58.24\n",
            "iteration : 350, loss : 0.9911, accuracy : 58.20\n",
            "Epoch : 202, training loss : 0.9920, training accuracy : 58.14, test loss : 1.0266, test accuracy : 57.40\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.9765, accuracy : 58.80\n",
            "iteration : 100, loss : 0.9873, accuracy : 58.59\n",
            "iteration : 150, loss : 0.9922, accuracy : 58.41\n",
            "iteration : 200, loss : 0.9971, accuracy : 58.05\n",
            "iteration : 250, loss : 1.0003, accuracy : 58.05\n",
            "iteration : 300, loss : 0.9999, accuracy : 57.96\n",
            "iteration : 350, loss : 0.9937, accuracy : 58.19\n",
            "Epoch : 203, training loss : 0.9937, training accuracy : 58.21, test loss : 1.0255, test accuracy : 57.32\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 1.0007, accuracy : 56.72\n",
            "iteration : 100, loss : 0.9875, accuracy : 57.84\n",
            "iteration : 150, loss : 0.9862, accuracy : 58.12\n",
            "iteration : 200, loss : 0.9916, accuracy : 58.00\n",
            "iteration : 250, loss : 0.9975, accuracy : 57.73\n",
            "iteration : 300, loss : 0.9944, accuracy : 57.92\n",
            "iteration : 350, loss : 0.9928, accuracy : 57.98\n",
            "Epoch : 204, training loss : 0.9931, training accuracy : 57.96, test loss : 1.0193, test accuracy : 57.79\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.9871, accuracy : 57.89\n",
            "iteration : 100, loss : 0.9932, accuracy : 57.90\n",
            "iteration : 150, loss : 0.9953, accuracy : 58.02\n",
            "iteration : 200, loss : 0.9930, accuracy : 58.23\n",
            "iteration : 250, loss : 0.9912, accuracy : 58.32\n",
            "iteration : 300, loss : 0.9931, accuracy : 58.10\n",
            "iteration : 350, loss : 0.9943, accuracy : 58.08\n",
            "Epoch : 205, training loss : 0.9952, training accuracy : 58.05, test loss : 1.0266, test accuracy : 57.21\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 1.0196, accuracy : 56.66\n",
            "iteration : 100, loss : 1.0061, accuracy : 57.20\n",
            "iteration : 150, loss : 1.0018, accuracy : 57.26\n",
            "iteration : 200, loss : 0.9987, accuracy : 57.49\n",
            "iteration : 250, loss : 0.9967, accuracy : 57.61\n",
            "iteration : 300, loss : 0.9915, accuracy : 57.95\n",
            "iteration : 350, loss : 0.9946, accuracy : 57.90\n",
            "Epoch : 206, training loss : 0.9932, training accuracy : 57.97, test loss : 1.0331, test accuracy : 56.83\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.9733, accuracy : 58.42\n",
            "iteration : 100, loss : 0.9840, accuracy : 58.27\n",
            "iteration : 150, loss : 0.9812, accuracy : 58.40\n",
            "iteration : 200, loss : 0.9795, accuracy : 58.43\n",
            "iteration : 250, loss : 0.9820, accuracy : 58.48\n",
            "iteration : 300, loss : 0.9815, accuracy : 58.51\n",
            "iteration : 350, loss : 0.9846, accuracy : 58.43\n",
            "Epoch : 207, training loss : 0.9841, training accuracy : 58.47, test loss : 1.0300, test accuracy : 57.44\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.9994, accuracy : 57.39\n",
            "iteration : 100, loss : 1.0077, accuracy : 57.45\n",
            "iteration : 150, loss : 1.0003, accuracy : 57.69\n",
            "iteration : 200, loss : 0.9996, accuracy : 57.80\n",
            "iteration : 250, loss : 0.9953, accuracy : 58.06\n",
            "iteration : 300, loss : 0.9936, accuracy : 58.11\n",
            "iteration : 350, loss : 0.9925, accuracy : 58.13\n",
            "Epoch : 208, training loss : 0.9912, training accuracy : 58.17, test loss : 1.0329, test accuracy : 56.92\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.9638, accuracy : 59.03\n",
            "iteration : 100, loss : 0.9781, accuracy : 58.23\n",
            "iteration : 150, loss : 0.9782, accuracy : 58.57\n",
            "iteration : 200, loss : 0.9790, accuracy : 58.68\n",
            "iteration : 250, loss : 0.9783, accuracy : 58.65\n",
            "iteration : 300, loss : 0.9787, accuracy : 58.56\n",
            "iteration : 350, loss : 0.9791, accuracy : 58.52\n",
            "Epoch : 209, training loss : 0.9783, training accuracy : 58.52, test loss : 1.0143, test accuracy : 57.46\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.9882, accuracy : 58.66\n",
            "iteration : 100, loss : 0.9942, accuracy : 58.53\n",
            "iteration : 150, loss : 0.9998, accuracy : 58.23\n",
            "iteration : 200, loss : 1.0003, accuracy : 57.93\n",
            "iteration : 250, loss : 0.9966, accuracy : 57.98\n",
            "iteration : 300, loss : 0.9945, accuracy : 57.99\n",
            "iteration : 350, loss : 0.9950, accuracy : 58.01\n",
            "Epoch : 210, training loss : 0.9944, training accuracy : 58.02, test loss : 1.0359, test accuracy : 56.96\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 1.0051, accuracy : 57.33\n",
            "iteration : 100, loss : 1.0055, accuracy : 57.79\n",
            "iteration : 150, loss : 0.9979, accuracy : 58.31\n",
            "iteration : 200, loss : 0.9853, accuracy : 58.68\n",
            "iteration : 250, loss : 0.9863, accuracy : 58.47\n",
            "iteration : 300, loss : 0.9862, accuracy : 58.38\n",
            "iteration : 350, loss : 0.9865, accuracy : 58.40\n",
            "Epoch : 211, training loss : 0.9869, training accuracy : 58.38, test loss : 1.0153, test accuracy : 57.48\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.9710, accuracy : 58.17\n",
            "iteration : 100, loss : 0.9835, accuracy : 57.83\n",
            "iteration : 150, loss : 0.9873, accuracy : 57.65\n",
            "iteration : 200, loss : 0.9926, accuracy : 57.39\n",
            "iteration : 250, loss : 0.9911, accuracy : 57.68\n",
            "iteration : 300, loss : 0.9882, accuracy : 57.89\n",
            "iteration : 350, loss : 0.9898, accuracy : 57.83\n",
            "Epoch : 212, training loss : 0.9873, training accuracy : 57.90, test loss : 1.0162, test accuracy : 57.63\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.9813, accuracy : 58.53\n",
            "iteration : 100, loss : 0.9742, accuracy : 58.87\n",
            "iteration : 150, loss : 0.9788, accuracy : 58.71\n",
            "iteration : 200, loss : 0.9835, accuracy : 58.61\n",
            "iteration : 250, loss : 0.9849, accuracy : 58.41\n",
            "iteration : 300, loss : 0.9811, accuracy : 58.61\n",
            "iteration : 350, loss : 0.9810, accuracy : 58.58\n",
            "Epoch : 213, training loss : 0.9813, training accuracy : 58.49, test loss : 1.0231, test accuracy : 57.31\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.9897, accuracy : 58.67\n",
            "iteration : 100, loss : 0.9793, accuracy : 58.98\n",
            "iteration : 150, loss : 0.9875, accuracy : 58.53\n",
            "iteration : 200, loss : 0.9858, accuracy : 58.22\n",
            "iteration : 250, loss : 0.9874, accuracy : 58.14\n",
            "iteration : 300, loss : 0.9889, accuracy : 58.20\n",
            "iteration : 350, loss : 0.9884, accuracy : 58.13\n",
            "Epoch : 214, training loss : 0.9884, training accuracy : 58.19, test loss : 1.0195, test accuracy : 57.43\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.9874, accuracy : 58.17\n",
            "iteration : 100, loss : 0.9855, accuracy : 58.23\n",
            "iteration : 150, loss : 0.9827, accuracy : 58.27\n",
            "iteration : 200, loss : 0.9845, accuracy : 58.30\n",
            "iteration : 250, loss : 0.9918, accuracy : 57.91\n",
            "iteration : 300, loss : 0.9894, accuracy : 57.92\n",
            "iteration : 350, loss : 0.9880, accuracy : 58.06\n",
            "Epoch : 215, training loss : 0.9876, training accuracy : 58.11, test loss : 1.0171, test accuracy : 57.30\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.9852, accuracy : 57.45\n",
            "iteration : 100, loss : 0.9894, accuracy : 57.98\n",
            "iteration : 150, loss : 0.9829, accuracy : 58.35\n",
            "iteration : 200, loss : 0.9845, accuracy : 58.50\n",
            "iteration : 250, loss : 0.9841, accuracy : 58.36\n",
            "iteration : 300, loss : 0.9835, accuracy : 58.40\n",
            "iteration : 350, loss : 0.9846, accuracy : 58.31\n",
            "Epoch : 216, training loss : 0.9866, training accuracy : 58.21, test loss : 1.0134, test accuracy : 57.74\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.9605, accuracy : 59.19\n",
            "iteration : 100, loss : 0.9618, accuracy : 59.39\n",
            "iteration : 150, loss : 0.9681, accuracy : 59.06\n",
            "iteration : 200, loss : 0.9702, accuracy : 58.85\n",
            "iteration : 250, loss : 0.9734, accuracy : 58.89\n",
            "iteration : 300, loss : 0.9801, accuracy : 58.61\n",
            "iteration : 350, loss : 0.9801, accuracy : 58.62\n",
            "Epoch : 217, training loss : 0.9805, training accuracy : 58.56, test loss : 1.0169, test accuracy : 57.64\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.9780, accuracy : 57.91\n",
            "iteration : 100, loss : 0.9758, accuracy : 58.54\n",
            "iteration : 150, loss : 0.9740, accuracy : 58.52\n",
            "iteration : 200, loss : 0.9738, accuracy : 58.48\n",
            "iteration : 250, loss : 0.9746, accuracy : 58.50\n",
            "iteration : 300, loss : 0.9793, accuracy : 58.44\n",
            "iteration : 350, loss : 0.9824, accuracy : 58.42\n",
            "Epoch : 218, training loss : 0.9823, training accuracy : 58.42, test loss : 1.0123, test accuracy : 57.72\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.9829, accuracy : 57.62\n",
            "iteration : 100, loss : 0.9816, accuracy : 57.74\n",
            "iteration : 150, loss : 0.9847, accuracy : 58.02\n",
            "iteration : 200, loss : 0.9824, accuracy : 58.27\n",
            "iteration : 250, loss : 0.9854, accuracy : 58.11\n",
            "iteration : 300, loss : 0.9860, accuracy : 57.97\n",
            "iteration : 350, loss : 0.9840, accuracy : 58.21\n",
            "Epoch : 219, training loss : 0.9848, training accuracy : 58.23, test loss : 1.0089, test accuracy : 57.64\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.9705, accuracy : 59.78\n",
            "iteration : 100, loss : 0.9801, accuracy : 59.04\n",
            "iteration : 150, loss : 0.9776, accuracy : 59.12\n",
            "iteration : 200, loss : 0.9800, accuracy : 58.83\n",
            "iteration : 250, loss : 0.9774, accuracy : 58.74\n",
            "iteration : 300, loss : 0.9797, accuracy : 58.61\n",
            "iteration : 350, loss : 0.9789, accuracy : 58.67\n",
            "Epoch : 220, training loss : 0.9798, training accuracy : 58.66, test loss : 1.0165, test accuracy : 57.67\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.9675, accuracy : 59.50\n",
            "iteration : 100, loss : 0.9774, accuracy : 58.81\n",
            "iteration : 150, loss : 0.9738, accuracy : 58.68\n",
            "iteration : 200, loss : 0.9698, accuracy : 58.79\n",
            "iteration : 250, loss : 0.9799, accuracy : 58.46\n",
            "iteration : 300, loss : 0.9809, accuracy : 58.48\n",
            "iteration : 350, loss : 0.9808, accuracy : 58.52\n",
            "Epoch : 221, training loss : 0.9801, training accuracy : 58.49, test loss : 1.0186, test accuracy : 57.63\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.9815, accuracy : 58.62\n",
            "iteration : 100, loss : 0.9932, accuracy : 57.87\n",
            "iteration : 150, loss : 0.9945, accuracy : 57.99\n",
            "iteration : 200, loss : 0.9917, accuracy : 58.15\n",
            "iteration : 250, loss : 0.9883, accuracy : 58.17\n",
            "iteration : 300, loss : 0.9879, accuracy : 58.24\n",
            "iteration : 350, loss : 0.9895, accuracy : 58.17\n",
            "Epoch : 222, training loss : 0.9883, training accuracy : 58.16, test loss : 1.0158, test accuracy : 57.64\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.9734, accuracy : 58.38\n",
            "iteration : 100, loss : 0.9708, accuracy : 58.55\n",
            "iteration : 150, loss : 0.9718, accuracy : 58.59\n",
            "iteration : 200, loss : 0.9741, accuracy : 58.62\n",
            "iteration : 250, loss : 0.9749, accuracy : 58.65\n",
            "iteration : 300, loss : 0.9745, accuracy : 58.61\n",
            "iteration : 350, loss : 0.9758, accuracy : 58.58\n",
            "Epoch : 223, training loss : 0.9745, training accuracy : 58.64, test loss : 1.0230, test accuracy : 57.46\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.9826, accuracy : 58.36\n",
            "iteration : 100, loss : 0.9840, accuracy : 58.45\n",
            "iteration : 150, loss : 0.9762, accuracy : 58.77\n",
            "iteration : 200, loss : 0.9749, accuracy : 58.69\n",
            "iteration : 250, loss : 0.9744, accuracy : 58.78\n",
            "iteration : 300, loss : 0.9762, accuracy : 58.68\n",
            "iteration : 350, loss : 0.9770, accuracy : 58.65\n",
            "Epoch : 224, training loss : 0.9759, training accuracy : 58.67, test loss : 1.0123, test accuracy : 57.61\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.9956, accuracy : 57.19\n",
            "iteration : 100, loss : 0.9827, accuracy : 58.41\n",
            "iteration : 150, loss : 0.9815, accuracy : 58.22\n",
            "iteration : 200, loss : 0.9775, accuracy : 58.52\n",
            "iteration : 250, loss : 0.9764, accuracy : 58.53\n",
            "iteration : 300, loss : 0.9756, accuracy : 58.56\n",
            "iteration : 350, loss : 0.9744, accuracy : 58.51\n",
            "Epoch : 225, training loss : 0.9759, training accuracy : 58.48, test loss : 1.0161, test accuracy : 57.69\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.9665, accuracy : 58.78\n",
            "iteration : 100, loss : 0.9707, accuracy : 58.56\n",
            "iteration : 150, loss : 0.9727, accuracy : 58.58\n",
            "iteration : 200, loss : 0.9676, accuracy : 58.69\n",
            "iteration : 250, loss : 0.9682, accuracy : 58.67\n",
            "iteration : 300, loss : 0.9737, accuracy : 58.67\n",
            "iteration : 350, loss : 0.9732, accuracy : 58.64\n",
            "Epoch : 226, training loss : 0.9744, training accuracy : 58.54, test loss : 1.0134, test accuracy : 57.25\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.9824, accuracy : 57.58\n",
            "iteration : 100, loss : 0.9793, accuracy : 58.03\n",
            "iteration : 150, loss : 0.9776, accuracy : 58.23\n",
            "iteration : 200, loss : 0.9698, accuracy : 58.55\n",
            "iteration : 250, loss : 0.9713, accuracy : 58.42\n",
            "iteration : 300, loss : 0.9739, accuracy : 58.41\n",
            "iteration : 350, loss : 0.9746, accuracy : 58.49\n",
            "Epoch : 227, training loss : 0.9746, training accuracy : 58.51, test loss : 1.0110, test accuracy : 57.40\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.9812, accuracy : 58.44\n",
            "iteration : 100, loss : 0.9761, accuracy : 58.34\n",
            "iteration : 150, loss : 0.9722, accuracy : 58.63\n",
            "iteration : 200, loss : 0.9751, accuracy : 58.58\n",
            "iteration : 250, loss : 0.9724, accuracy : 58.56\n",
            "iteration : 300, loss : 0.9724, accuracy : 58.43\n",
            "iteration : 350, loss : 0.9732, accuracy : 58.52\n",
            "Epoch : 228, training loss : 0.9734, training accuracy : 58.55, test loss : 1.0045, test accuracy : 57.99\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.9865, accuracy : 57.64\n",
            "iteration : 100, loss : 0.9819, accuracy : 57.72\n",
            "iteration : 150, loss : 0.9809, accuracy : 57.65\n",
            "iteration : 200, loss : 0.9779, accuracy : 57.93\n",
            "iteration : 250, loss : 0.9757, accuracy : 58.10\n",
            "iteration : 300, loss : 0.9745, accuracy : 58.18\n",
            "iteration : 350, loss : 0.9748, accuracy : 58.19\n",
            "Epoch : 229, training loss : 0.9743, training accuracy : 58.22, test loss : 1.0162, test accuracy : 57.53\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.9807, accuracy : 58.06\n",
            "iteration : 100, loss : 0.9747, accuracy : 58.46\n",
            "iteration : 150, loss : 0.9741, accuracy : 58.29\n",
            "iteration : 200, loss : 0.9767, accuracy : 58.13\n",
            "iteration : 250, loss : 0.9796, accuracy : 58.08\n",
            "iteration : 300, loss : 0.9786, accuracy : 58.19\n",
            "iteration : 350, loss : 0.9750, accuracy : 58.26\n",
            "Epoch : 230, training loss : 0.9751, training accuracy : 58.26, test loss : 1.0112, test accuracy : 57.89\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.9544, accuracy : 59.11\n",
            "iteration : 100, loss : 0.9562, accuracy : 59.35\n",
            "iteration : 150, loss : 0.9699, accuracy : 58.57\n",
            "iteration : 200, loss : 0.9675, accuracy : 58.83\n",
            "iteration : 250, loss : 0.9697, accuracy : 58.72\n",
            "iteration : 300, loss : 0.9731, accuracy : 58.58\n",
            "iteration : 350, loss : 0.9693, accuracy : 58.81\n",
            "Epoch : 231, training loss : 0.9707, training accuracy : 58.75, test loss : 1.0098, test accuracy : 57.89\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.9550, accuracy : 58.48\n",
            "iteration : 100, loss : 0.9578, accuracy : 58.70\n",
            "iteration : 150, loss : 0.9589, accuracy : 58.62\n",
            "iteration : 200, loss : 0.9633, accuracy : 58.75\n",
            "iteration : 250, loss : 0.9656, accuracy : 58.66\n",
            "iteration : 300, loss : 0.9683, accuracy : 58.55\n",
            "iteration : 350, loss : 0.9688, accuracy : 58.57\n",
            "Epoch : 232, training loss : 0.9699, training accuracy : 58.52, test loss : 1.0131, test accuracy : 57.26\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.9800, accuracy : 58.75\n",
            "iteration : 100, loss : 0.9744, accuracy : 58.44\n",
            "iteration : 150, loss : 0.9788, accuracy : 58.30\n",
            "iteration : 200, loss : 0.9724, accuracy : 58.56\n",
            "iteration : 250, loss : 0.9678, accuracy : 58.69\n",
            "iteration : 300, loss : 0.9694, accuracy : 58.59\n",
            "iteration : 350, loss : 0.9676, accuracy : 58.65\n",
            "Epoch : 233, training loss : 0.9677, training accuracy : 58.64, test loss : 1.0069, test accuracy : 57.72\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.9689, accuracy : 58.33\n",
            "iteration : 100, loss : 0.9655, accuracy : 59.03\n",
            "iteration : 150, loss : 0.9652, accuracy : 59.11\n",
            "iteration : 200, loss : 0.9602, accuracy : 59.34\n",
            "iteration : 250, loss : 0.9644, accuracy : 59.23\n",
            "iteration : 300, loss : 0.9670, accuracy : 59.08\n",
            "iteration : 350, loss : 0.9653, accuracy : 59.08\n",
            "Epoch : 234, training loss : 0.9655, training accuracy : 59.05, test loss : 1.0013, test accuracy : 58.02\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.9605, accuracy : 59.14\n",
            "iteration : 100, loss : 0.9740, accuracy : 58.55\n",
            "iteration : 150, loss : 0.9751, accuracy : 58.53\n",
            "iteration : 200, loss : 0.9664, accuracy : 58.70\n",
            "iteration : 250, loss : 0.9703, accuracy : 58.62\n",
            "iteration : 300, loss : 0.9697, accuracy : 58.69\n",
            "iteration : 350, loss : 0.9666, accuracy : 58.81\n",
            "Epoch : 235, training loss : 0.9678, training accuracy : 58.83, test loss : 1.0148, test accuracy : 57.71\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.9573, accuracy : 58.73\n",
            "iteration : 100, loss : 0.9683, accuracy : 58.56\n",
            "iteration : 150, loss : 0.9642, accuracy : 58.81\n",
            "iteration : 200, loss : 0.9632, accuracy : 58.67\n",
            "iteration : 250, loss : 0.9609, accuracy : 58.77\n",
            "iteration : 300, loss : 0.9628, accuracy : 58.70\n",
            "iteration : 350, loss : 0.9644, accuracy : 58.72\n",
            "Epoch : 236, training loss : 0.9647, training accuracy : 58.72, test loss : 1.0092, test accuracy : 57.83\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.9723, accuracy : 58.44\n",
            "iteration : 100, loss : 0.9692, accuracy : 58.48\n",
            "iteration : 150, loss : 0.9672, accuracy : 58.86\n",
            "iteration : 200, loss : 0.9677, accuracy : 58.77\n",
            "iteration : 250, loss : 0.9678, accuracy : 58.73\n",
            "iteration : 300, loss : 0.9707, accuracy : 58.75\n",
            "iteration : 350, loss : 0.9702, accuracy : 58.68\n",
            "Epoch : 237, training loss : 0.9683, training accuracy : 58.73, test loss : 1.0221, test accuracy : 57.25\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.9766, accuracy : 57.66\n",
            "iteration : 100, loss : 0.9821, accuracy : 57.70\n",
            "iteration : 150, loss : 0.9759, accuracy : 58.14\n",
            "iteration : 200, loss : 0.9692, accuracy : 58.32\n",
            "iteration : 250, loss : 0.9702, accuracy : 58.43\n",
            "iteration : 300, loss : 0.9718, accuracy : 58.40\n",
            "iteration : 350, loss : 0.9710, accuracy : 58.43\n",
            "Epoch : 238, training loss : 0.9719, training accuracy : 58.39, test loss : 1.0056, test accuracy : 58.16\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.9527, accuracy : 59.39\n",
            "iteration : 100, loss : 0.9524, accuracy : 59.06\n",
            "iteration : 150, loss : 0.9492, accuracy : 59.17\n",
            "iteration : 200, loss : 0.9540, accuracy : 58.89\n",
            "iteration : 250, loss : 0.9550, accuracy : 58.90\n",
            "iteration : 300, loss : 0.9563, accuracy : 58.88\n",
            "iteration : 350, loss : 0.9563, accuracy : 59.08\n",
            "Epoch : 239, training loss : 0.9585, training accuracy : 59.00, test loss : 1.0202, test accuracy : 57.38\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.9693, accuracy : 58.62\n",
            "iteration : 100, loss : 0.9687, accuracy : 58.63\n",
            "iteration : 150, loss : 0.9744, accuracy : 58.53\n",
            "iteration : 200, loss : 0.9709, accuracy : 58.55\n",
            "iteration : 250, loss : 0.9668, accuracy : 58.59\n",
            "iteration : 300, loss : 0.9685, accuracy : 58.60\n",
            "iteration : 350, loss : 0.9685, accuracy : 58.59\n",
            "Epoch : 240, training loss : 0.9675, training accuracy : 58.64, test loss : 1.0140, test accuracy : 57.80\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.9620, accuracy : 58.20\n",
            "iteration : 100, loss : 0.9668, accuracy : 58.46\n",
            "iteration : 150, loss : 0.9653, accuracy : 58.75\n",
            "iteration : 200, loss : 0.9581, accuracy : 58.96\n",
            "iteration : 250, loss : 0.9608, accuracy : 58.97\n",
            "iteration : 300, loss : 0.9617, accuracy : 58.91\n",
            "iteration : 350, loss : 0.9615, accuracy : 58.90\n",
            "Epoch : 241, training loss : 0.9613, training accuracy : 58.91, test loss : 1.0104, test accuracy : 57.58\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.9532, accuracy : 59.50\n",
            "iteration : 100, loss : 0.9567, accuracy : 59.56\n",
            "iteration : 150, loss : 0.9537, accuracy : 59.33\n",
            "iteration : 200, loss : 0.9529, accuracy : 59.45\n",
            "iteration : 250, loss : 0.9538, accuracy : 59.41\n",
            "iteration : 300, loss : 0.9546, accuracy : 59.36\n",
            "iteration : 350, loss : 0.9564, accuracy : 59.21\n",
            "Epoch : 242, training loss : 0.9551, training accuracy : 59.24, test loss : 1.0219, test accuracy : 56.98\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.9631, accuracy : 58.30\n",
            "iteration : 100, loss : 0.9634, accuracy : 58.59\n",
            "iteration : 150, loss : 0.9710, accuracy : 58.38\n",
            "iteration : 200, loss : 0.9641, accuracy : 58.57\n",
            "iteration : 250, loss : 0.9583, accuracy : 58.81\n",
            "iteration : 300, loss : 0.9599, accuracy : 58.73\n",
            "iteration : 350, loss : 0.9607, accuracy : 58.80\n",
            "Epoch : 243, training loss : 0.9609, training accuracy : 58.80, test loss : 0.9993, test accuracy : 58.28\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.9569, accuracy : 59.14\n",
            "iteration : 100, loss : 0.9603, accuracy : 58.72\n",
            "iteration : 150, loss : 0.9649, accuracy : 58.64\n",
            "iteration : 200, loss : 0.9680, accuracy : 58.65\n",
            "iteration : 250, loss : 0.9649, accuracy : 58.72\n",
            "iteration : 300, loss : 0.9633, accuracy : 58.67\n",
            "iteration : 350, loss : 0.9627, accuracy : 58.87\n",
            "Epoch : 244, training loss : 0.9634, training accuracy : 58.88, test loss : 1.0109, test accuracy : 57.31\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.9679, accuracy : 58.56\n",
            "iteration : 100, loss : 0.9634, accuracy : 58.87\n",
            "iteration : 150, loss : 0.9620, accuracy : 59.00\n",
            "iteration : 200, loss : 0.9616, accuracy : 58.90\n",
            "iteration : 250, loss : 0.9616, accuracy : 58.91\n",
            "iteration : 300, loss : 0.9597, accuracy : 58.99\n",
            "iteration : 350, loss : 0.9598, accuracy : 59.07\n",
            "Epoch : 245, training loss : 0.9593, training accuracy : 59.11, test loss : 1.0068, test accuracy : 57.78\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.9762, accuracy : 57.95\n",
            "iteration : 100, loss : 0.9584, accuracy : 58.80\n",
            "iteration : 150, loss : 0.9669, accuracy : 58.61\n",
            "iteration : 200, loss : 0.9632, accuracy : 58.83\n",
            "iteration : 250, loss : 0.9578, accuracy : 59.05\n",
            "iteration : 300, loss : 0.9576, accuracy : 59.13\n",
            "iteration : 350, loss : 0.9549, accuracy : 59.17\n",
            "Epoch : 246, training loss : 0.9548, training accuracy : 59.16, test loss : 1.0149, test accuracy : 57.59\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.9679, accuracy : 58.64\n",
            "iteration : 100, loss : 0.9549, accuracy : 58.85\n",
            "iteration : 150, loss : 0.9525, accuracy : 59.26\n",
            "iteration : 200, loss : 0.9495, accuracy : 59.35\n",
            "iteration : 250, loss : 0.9474, accuracy : 59.48\n",
            "iteration : 300, loss : 0.9502, accuracy : 59.35\n",
            "iteration : 350, loss : 0.9508, accuracy : 59.30\n",
            "Epoch : 247, training loss : 0.9505, training accuracy : 59.30, test loss : 1.0148, test accuracy : 57.56\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.9852, accuracy : 58.03\n",
            "iteration : 100, loss : 0.9778, accuracy : 58.33\n",
            "iteration : 150, loss : 0.9731, accuracy : 58.53\n",
            "iteration : 200, loss : 0.9648, accuracy : 58.73\n",
            "iteration : 250, loss : 0.9616, accuracy : 58.93\n",
            "iteration : 300, loss : 0.9650, accuracy : 58.76\n",
            "iteration : 350, loss : 0.9651, accuracy : 58.74\n",
            "Epoch : 248, training loss : 0.9660, training accuracy : 58.67, test loss : 1.0152, test accuracy : 57.55\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.9641, accuracy : 58.52\n",
            "iteration : 100, loss : 0.9640, accuracy : 58.79\n",
            "iteration : 150, loss : 0.9652, accuracy : 58.53\n",
            "iteration : 200, loss : 0.9595, accuracy : 58.89\n",
            "iteration : 250, loss : 0.9608, accuracy : 58.77\n",
            "iteration : 300, loss : 0.9596, accuracy : 58.76\n",
            "iteration : 350, loss : 0.9577, accuracy : 58.77\n",
            "Epoch : 249, training loss : 0.9591, training accuracy : 58.70, test loss : 1.0071, test accuracy : 58.24\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.9690, accuracy : 57.59\n",
            "iteration : 100, loss : 0.9746, accuracy : 57.47\n",
            "iteration : 150, loss : 0.9703, accuracy : 57.86\n",
            "iteration : 200, loss : 0.9655, accuracy : 58.34\n",
            "iteration : 250, loss : 0.9630, accuracy : 58.64\n",
            "iteration : 300, loss : 0.9620, accuracy : 58.74\n",
            "iteration : 350, loss : 0.9613, accuracy : 58.70\n",
            "Epoch : 250, training loss : 0.9611, training accuracy : 58.69, test loss : 1.0104, test accuracy : 57.54\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.9353, accuracy : 59.89\n",
            "iteration : 100, loss : 0.9454, accuracy : 59.30\n",
            "iteration : 150, loss : 0.9442, accuracy : 59.30\n",
            "iteration : 200, loss : 0.9525, accuracy : 58.98\n",
            "iteration : 250, loss : 0.9527, accuracy : 59.00\n",
            "iteration : 300, loss : 0.9522, accuracy : 59.08\n",
            "iteration : 350, loss : 0.9551, accuracy : 58.96\n",
            "Epoch : 251, training loss : 0.9559, training accuracy : 58.87, test loss : 1.0065, test accuracy : 57.94\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.9352, accuracy : 59.72\n",
            "iteration : 100, loss : 0.9431, accuracy : 59.27\n",
            "iteration : 150, loss : 0.9513, accuracy : 59.09\n",
            "iteration : 200, loss : 0.9546, accuracy : 59.11\n",
            "iteration : 250, loss : 0.9565, accuracy : 59.03\n",
            "iteration : 300, loss : 0.9568, accuracy : 59.01\n",
            "iteration : 350, loss : 0.9548, accuracy : 58.96\n",
            "Epoch : 252, training loss : 0.9597, training accuracy : 58.90, test loss : 1.0153, test accuracy : 57.72\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.9482, accuracy : 59.12\n",
            "iteration : 100, loss : 0.9562, accuracy : 58.70\n",
            "iteration : 150, loss : 0.9571, accuracy : 58.74\n",
            "iteration : 200, loss : 0.9579, accuracy : 58.71\n",
            "iteration : 250, loss : 0.9583, accuracy : 58.81\n",
            "iteration : 300, loss : 0.9570, accuracy : 58.90\n",
            "iteration : 350, loss : 0.9548, accuracy : 58.93\n",
            "Epoch : 253, training loss : 0.9540, training accuracy : 58.96, test loss : 1.0001, test accuracy : 58.19\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.9442, accuracy : 59.09\n",
            "iteration : 100, loss : 0.9590, accuracy : 58.60\n",
            "iteration : 150, loss : 0.9558, accuracy : 58.72\n",
            "iteration : 200, loss : 0.9571, accuracy : 58.66\n",
            "iteration : 250, loss : 0.9623, accuracy : 58.48\n",
            "iteration : 300, loss : 0.9621, accuracy : 58.62\n",
            "iteration : 350, loss : 0.9604, accuracy : 58.77\n",
            "Epoch : 254, training loss : 0.9624, training accuracy : 58.72, test loss : 1.0139, test accuracy : 57.46\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.9381, accuracy : 60.03\n",
            "iteration : 100, loss : 0.9458, accuracy : 59.54\n",
            "iteration : 150, loss : 0.9576, accuracy : 58.91\n",
            "iteration : 200, loss : 0.9542, accuracy : 58.84\n",
            "iteration : 250, loss : 0.9571, accuracy : 58.77\n",
            "iteration : 300, loss : 0.9581, accuracy : 58.62\n",
            "iteration : 350, loss : 0.9573, accuracy : 58.78\n",
            "Epoch : 255, training loss : 0.9561, training accuracy : 58.81, test loss : 1.0014, test accuracy : 58.06\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.9466, accuracy : 59.06\n",
            "iteration : 100, loss : 0.9507, accuracy : 59.12\n",
            "iteration : 150, loss : 0.9559, accuracy : 59.09\n",
            "iteration : 200, loss : 0.9588, accuracy : 58.93\n",
            "iteration : 250, loss : 0.9597, accuracy : 58.92\n",
            "iteration : 300, loss : 0.9608, accuracy : 58.82\n",
            "iteration : 350, loss : 0.9613, accuracy : 58.83\n",
            "Epoch : 256, training loss : 0.9605, training accuracy : 58.86, test loss : 1.0104, test accuracy : 57.60\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.9678, accuracy : 58.91\n",
            "iteration : 100, loss : 0.9564, accuracy : 59.27\n",
            "iteration : 150, loss : 0.9502, accuracy : 59.08\n",
            "iteration : 200, loss : 0.9448, accuracy : 59.14\n",
            "iteration : 250, loss : 0.9516, accuracy : 58.89\n",
            "iteration : 300, loss : 0.9477, accuracy : 59.11\n",
            "iteration : 350, loss : 0.9454, accuracy : 59.16\n",
            "Epoch : 257, training loss : 0.9459, training accuracy : 59.13, test loss : 0.9973, test accuracy : 57.96\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.9789, accuracy : 58.45\n",
            "iteration : 100, loss : 0.9650, accuracy : 59.02\n",
            "iteration : 150, loss : 0.9607, accuracy : 58.95\n",
            "iteration : 200, loss : 0.9610, accuracy : 58.98\n",
            "iteration : 250, loss : 0.9531, accuracy : 59.19\n",
            "iteration : 300, loss : 0.9512, accuracy : 59.16\n",
            "iteration : 350, loss : 0.9494, accuracy : 59.17\n",
            "Epoch : 258, training loss : 0.9506, training accuracy : 59.07, test loss : 1.0038, test accuracy : 57.61\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.9366, accuracy : 59.45\n",
            "iteration : 100, loss : 0.9416, accuracy : 59.24\n",
            "iteration : 150, loss : 0.9522, accuracy : 58.92\n",
            "iteration : 200, loss : 0.9556, accuracy : 58.66\n",
            "iteration : 250, loss : 0.9524, accuracy : 58.85\n",
            "iteration : 300, loss : 0.9503, accuracy : 58.95\n",
            "iteration : 350, loss : 0.9499, accuracy : 58.94\n",
            "Epoch : 259, training loss : 0.9481, training accuracy : 59.06, test loss : 1.0064, test accuracy : 57.69\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.9600, accuracy : 58.59\n",
            "iteration : 100, loss : 0.9448, accuracy : 59.38\n",
            "iteration : 150, loss : 0.9450, accuracy : 59.26\n",
            "iteration : 200, loss : 0.9452, accuracy : 59.23\n",
            "iteration : 250, loss : 0.9463, accuracy : 59.22\n",
            "iteration : 300, loss : 0.9474, accuracy : 59.17\n",
            "iteration : 350, loss : 0.9470, accuracy : 59.14\n",
            "Epoch : 260, training loss : 0.9480, training accuracy : 59.11, test loss : 1.0098, test accuracy : 57.82\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.9663, accuracy : 58.56\n",
            "iteration : 100, loss : 0.9664, accuracy : 58.30\n",
            "iteration : 150, loss : 0.9600, accuracy : 58.74\n",
            "iteration : 200, loss : 0.9568, accuracy : 58.89\n",
            "iteration : 250, loss : 0.9565, accuracy : 58.87\n",
            "iteration : 300, loss : 0.9559, accuracy : 58.88\n",
            "iteration : 350, loss : 0.9574, accuracy : 58.79\n",
            "Epoch : 261, training loss : 0.9562, training accuracy : 58.84, test loss : 1.0136, test accuracy : 57.26\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.9433, accuracy : 59.25\n",
            "iteration : 100, loss : 0.9508, accuracy : 58.88\n",
            "iteration : 150, loss : 0.9528, accuracy : 58.67\n",
            "iteration : 200, loss : 0.9489, accuracy : 58.93\n",
            "iteration : 250, loss : 0.9468, accuracy : 59.05\n",
            "iteration : 300, loss : 0.9475, accuracy : 59.06\n",
            "iteration : 350, loss : 0.9482, accuracy : 59.05\n",
            "Epoch : 262, training loss : 0.9491, training accuracy : 59.03, test loss : 0.9950, test accuracy : 58.11\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.9688, accuracy : 58.22\n",
            "iteration : 100, loss : 0.9459, accuracy : 59.12\n",
            "iteration : 150, loss : 0.9495, accuracy : 58.93\n",
            "iteration : 200, loss : 0.9530, accuracy : 58.74\n",
            "iteration : 250, loss : 0.9535, accuracy : 58.83\n",
            "iteration : 300, loss : 0.9522, accuracy : 58.88\n",
            "iteration : 350, loss : 0.9508, accuracy : 58.96\n",
            "Epoch : 263, training loss : 0.9500, training accuracy : 59.00, test loss : 1.0010, test accuracy : 57.87\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.9435, accuracy : 58.83\n",
            "iteration : 100, loss : 0.9495, accuracy : 58.88\n",
            "iteration : 150, loss : 0.9540, accuracy : 58.79\n",
            "iteration : 200, loss : 0.9528, accuracy : 58.80\n",
            "iteration : 250, loss : 0.9510, accuracy : 58.93\n",
            "iteration : 300, loss : 0.9505, accuracy : 59.14\n",
            "iteration : 350, loss : 0.9489, accuracy : 59.17\n",
            "Epoch : 264, training loss : 0.9470, training accuracy : 59.26, test loss : 1.0014, test accuracy : 57.74\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.9605, accuracy : 58.77\n",
            "iteration : 100, loss : 0.9559, accuracy : 58.46\n",
            "iteration : 150, loss : 0.9489, accuracy : 58.91\n",
            "iteration : 200, loss : 0.9480, accuracy : 59.04\n",
            "iteration : 250, loss : 0.9509, accuracy : 58.93\n",
            "iteration : 300, loss : 0.9498, accuracy : 58.99\n",
            "iteration : 350, loss : 0.9472, accuracy : 59.11\n",
            "Epoch : 265, training loss : 0.9478, training accuracy : 59.03, test loss : 1.0021, test accuracy : 58.14\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.9595, accuracy : 59.64\n",
            "iteration : 100, loss : 0.9516, accuracy : 59.42\n",
            "iteration : 150, loss : 0.9562, accuracy : 58.97\n",
            "iteration : 200, loss : 0.9553, accuracy : 58.89\n",
            "iteration : 250, loss : 0.9568, accuracy : 58.88\n",
            "iteration : 300, loss : 0.9543, accuracy : 58.83\n",
            "iteration : 350, loss : 0.9539, accuracy : 58.76\n",
            "Epoch : 266, training loss : 0.9534, training accuracy : 58.76, test loss : 1.0156, test accuracy : 57.31\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.9573, accuracy : 59.27\n",
            "iteration : 100, loss : 0.9537, accuracy : 58.94\n",
            "iteration : 150, loss : 0.9433, accuracy : 59.46\n",
            "iteration : 200, loss : 0.9511, accuracy : 59.04\n",
            "iteration : 250, loss : 0.9509, accuracy : 59.05\n",
            "iteration : 300, loss : 0.9484, accuracy : 59.10\n",
            "iteration : 350, loss : 0.9471, accuracy : 59.16\n",
            "Epoch : 267, training loss : 0.9472, training accuracy : 59.20, test loss : 1.0040, test accuracy : 57.54\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.9309, accuracy : 60.20\n",
            "iteration : 100, loss : 0.9406, accuracy : 59.51\n",
            "iteration : 150, loss : 0.9341, accuracy : 59.82\n",
            "iteration : 200, loss : 0.9385, accuracy : 59.63\n",
            "iteration : 250, loss : 0.9439, accuracy : 59.48\n",
            "iteration : 300, loss : 0.9432, accuracy : 59.40\n",
            "iteration : 350, loss : 0.9432, accuracy : 59.39\n",
            "Epoch : 268, training loss : 0.9428, training accuracy : 59.42, test loss : 1.0013, test accuracy : 57.86\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.9526, accuracy : 58.84\n",
            "iteration : 100, loss : 0.9443, accuracy : 59.44\n",
            "iteration : 150, loss : 0.9421, accuracy : 59.39\n",
            "iteration : 200, loss : 0.9452, accuracy : 59.25\n",
            "iteration : 250, loss : 0.9476, accuracy : 59.13\n",
            "iteration : 300, loss : 0.9474, accuracy : 59.08\n",
            "iteration : 350, loss : 0.9457, accuracy : 59.16\n",
            "Epoch : 269, training loss : 0.9441, training accuracy : 59.17, test loss : 0.9972, test accuracy : 58.23\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.9486, accuracy : 59.55\n",
            "iteration : 100, loss : 0.9420, accuracy : 59.33\n",
            "iteration : 150, loss : 0.9344, accuracy : 59.75\n",
            "iteration : 200, loss : 0.9361, accuracy : 59.62\n",
            "iteration : 250, loss : 0.9375, accuracy : 59.57\n",
            "iteration : 300, loss : 0.9409, accuracy : 59.32\n",
            "iteration : 350, loss : 0.9397, accuracy : 59.28\n",
            "Epoch : 270, training loss : 0.9398, training accuracy : 59.26, test loss : 1.0015, test accuracy : 58.33\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.9320, accuracy : 59.91\n",
            "iteration : 100, loss : 0.9342, accuracy : 59.75\n",
            "iteration : 150, loss : 0.9415, accuracy : 59.20\n",
            "iteration : 200, loss : 0.9425, accuracy : 59.14\n",
            "iteration : 250, loss : 0.9409, accuracy : 59.05\n",
            "iteration : 300, loss : 0.9403, accuracy : 59.18\n",
            "iteration : 350, loss : 0.9418, accuracy : 59.18\n",
            "Epoch : 271, training loss : 0.9412, training accuracy : 59.22, test loss : 1.0007, test accuracy : 58.08\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.9460, accuracy : 59.39\n",
            "iteration : 100, loss : 0.9447, accuracy : 59.22\n",
            "iteration : 150, loss : 0.9458, accuracy : 59.05\n",
            "iteration : 200, loss : 0.9462, accuracy : 58.95\n",
            "iteration : 250, loss : 0.9463, accuracy : 58.98\n",
            "iteration : 300, loss : 0.9491, accuracy : 58.90\n",
            "iteration : 350, loss : 0.9503, accuracy : 58.89\n",
            "Epoch : 272, training loss : 0.9503, training accuracy : 58.91, test loss : 1.0047, test accuracy : 57.71\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.9413, accuracy : 58.70\n",
            "iteration : 100, loss : 0.9404, accuracy : 58.62\n",
            "iteration : 150, loss : 0.9424, accuracy : 58.94\n",
            "iteration : 200, loss : 0.9405, accuracy : 59.04\n",
            "iteration : 250, loss : 0.9436, accuracy : 58.94\n",
            "iteration : 300, loss : 0.9420, accuracy : 59.12\n",
            "iteration : 350, loss : 0.9399, accuracy : 59.21\n",
            "Epoch : 273, training loss : 0.9397, training accuracy : 59.20, test loss : 0.9968, test accuracy : 58.19\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.9326, accuracy : 59.42\n",
            "iteration : 100, loss : 0.9347, accuracy : 59.46\n",
            "iteration : 150, loss : 0.9417, accuracy : 59.01\n",
            "iteration : 200, loss : 0.9393, accuracy : 59.14\n",
            "iteration : 250, loss : 0.9450, accuracy : 58.97\n",
            "iteration : 300, loss : 0.9476, accuracy : 58.91\n",
            "iteration : 350, loss : 0.9457, accuracy : 59.03\n",
            "Epoch : 274, training loss : 0.9444, training accuracy : 59.10, test loss : 1.0000, test accuracy : 57.63\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.9423, accuracy : 59.34\n",
            "iteration : 100, loss : 0.9505, accuracy : 58.90\n",
            "iteration : 150, loss : 0.9499, accuracy : 58.80\n",
            "iteration : 200, loss : 0.9463, accuracy : 58.93\n",
            "iteration : 250, loss : 0.9461, accuracy : 59.00\n",
            "iteration : 300, loss : 0.9438, accuracy : 59.04\n",
            "iteration : 350, loss : 0.9447, accuracy : 59.08\n",
            "Epoch : 275, training loss : 0.9479, training accuracy : 58.97, test loss : 1.0075, test accuracy : 57.98\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.9631, accuracy : 57.89\n",
            "iteration : 100, loss : 0.9514, accuracy : 58.66\n",
            "iteration : 150, loss : 0.9560, accuracy : 58.31\n",
            "iteration : 200, loss : 0.9572, accuracy : 58.38\n",
            "iteration : 250, loss : 0.9534, accuracy : 58.56\n",
            "iteration : 300, loss : 0.9531, accuracy : 58.51\n",
            "iteration : 350, loss : 0.9500, accuracy : 58.73\n",
            "Epoch : 276, training loss : 0.9491, training accuracy : 58.85, test loss : 1.0021, test accuracy : 57.85\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.9609, accuracy : 58.05\n",
            "iteration : 100, loss : 0.9451, accuracy : 58.98\n",
            "iteration : 150, loss : 0.9439, accuracy : 58.90\n",
            "iteration : 200, loss : 0.9425, accuracy : 58.93\n",
            "iteration : 250, loss : 0.9453, accuracy : 58.71\n",
            "iteration : 300, loss : 0.9424, accuracy : 58.95\n",
            "iteration : 350, loss : 0.9437, accuracy : 58.91\n",
            "Epoch : 277, training loss : 0.9432, training accuracy : 58.96, test loss : 1.0005, test accuracy : 58.10\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.9311, accuracy : 59.48\n",
            "iteration : 100, loss : 0.9322, accuracy : 59.26\n",
            "iteration : 150, loss : 0.9312, accuracy : 59.44\n",
            "iteration : 200, loss : 0.9342, accuracy : 59.26\n",
            "iteration : 250, loss : 0.9377, accuracy : 59.18\n",
            "iteration : 300, loss : 0.9394, accuracy : 59.23\n",
            "iteration : 350, loss : 0.9389, accuracy : 59.29\n",
            "Epoch : 278, training loss : 0.9388, training accuracy : 59.25, test loss : 1.0024, test accuracy : 57.96\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.9309, accuracy : 59.39\n",
            "iteration : 100, loss : 0.9322, accuracy : 59.25\n",
            "iteration : 150, loss : 0.9357, accuracy : 59.14\n",
            "iteration : 200, loss : 0.9376, accuracy : 59.07\n",
            "iteration : 250, loss : 0.9349, accuracy : 59.24\n",
            "iteration : 300, loss : 0.9337, accuracy : 59.24\n",
            "iteration : 350, loss : 0.9350, accuracy : 59.29\n",
            "Epoch : 279, training loss : 0.9347, training accuracy : 59.35, test loss : 0.9978, test accuracy : 58.19\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.9292, accuracy : 59.14\n",
            "iteration : 100, loss : 0.9260, accuracy : 59.42\n",
            "iteration : 150, loss : 0.9354, accuracy : 59.34\n",
            "iteration : 200, loss : 0.9409, accuracy : 59.18\n",
            "iteration : 250, loss : 0.9432, accuracy : 59.15\n",
            "iteration : 300, loss : 0.9438, accuracy : 59.09\n",
            "iteration : 350, loss : 0.9444, accuracy : 59.04\n",
            "Epoch : 280, training loss : 0.9445, training accuracy : 59.03, test loss : 1.0001, test accuracy : 58.12\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.9378, accuracy : 59.02\n",
            "iteration : 100, loss : 0.9392, accuracy : 59.20\n",
            "iteration : 150, loss : 0.9402, accuracy : 59.20\n",
            "iteration : 200, loss : 0.9377, accuracy : 59.30\n",
            "iteration : 250, loss : 0.9409, accuracy : 59.18\n",
            "iteration : 300, loss : 0.9403, accuracy : 59.29\n",
            "iteration : 350, loss : 0.9400, accuracy : 59.32\n",
            "Epoch : 281, training loss : 0.9407, training accuracy : 59.30, test loss : 1.0025, test accuracy : 57.76\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.9445, accuracy : 60.00\n",
            "iteration : 100, loss : 0.9400, accuracy : 59.91\n",
            "iteration : 150, loss : 0.9379, accuracy : 59.94\n",
            "iteration : 200, loss : 0.9404, accuracy : 59.68\n",
            "iteration : 250, loss : 0.9385, accuracy : 59.67\n",
            "iteration : 300, loss : 0.9378, accuracy : 59.77\n",
            "iteration : 350, loss : 0.9373, accuracy : 59.58\n",
            "Epoch : 282, training loss : 0.9383, training accuracy : 59.48, test loss : 1.0001, test accuracy : 58.00\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.9470, accuracy : 59.55\n",
            "iteration : 100, loss : 0.9404, accuracy : 59.56\n",
            "iteration : 150, loss : 0.9428, accuracy : 59.52\n",
            "iteration : 200, loss : 0.9442, accuracy : 59.56\n",
            "iteration : 250, loss : 0.9410, accuracy : 59.55\n",
            "iteration : 300, loss : 0.9444, accuracy : 59.32\n",
            "iteration : 350, loss : 0.9442, accuracy : 59.30\n",
            "iteration :  50, loss : 0.9343, accuracy : 59.50\n",
            "iteration : 100, loss : 0.9408, accuracy : 59.32\n",
            "iteration : 150, loss : 0.9447, accuracy : 59.21\n",
            "iteration : 200, loss : 0.9450, accuracy : 59.08\n",
            "iteration : 250, loss : 0.9453, accuracy : 59.09\n",
            "iteration : 300, loss : 0.9418, accuracy : 59.24\n",
            "iteration : 350, loss : 0.9406, accuracy : 59.22\n",
            "Epoch : 284, training loss : 0.9405, training accuracy : 59.22, test loss : 1.0005, test accuracy : 57.97\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.9451, accuracy : 58.77\n",
            "iteration : 100, loss : 0.9579, accuracy : 58.58\n",
            "iteration : 150, loss : 0.9538, accuracy : 58.83\n",
            "iteration : 200, loss : 0.9426, accuracy : 59.23\n",
            "iteration : 250, loss : 0.9422, accuracy : 59.19\n",
            "iteration : 300, loss : 0.9419, accuracy : 59.25\n",
            "iteration : 350, loss : 0.9433, accuracy : 59.24\n",
            "Epoch : 285, training loss : 0.9463, training accuracy : 59.13, test loss : 1.0185, test accuracy : 57.51\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.9524, accuracy : 58.86\n",
            "iteration : 100, loss : 0.9550, accuracy : 58.73\n",
            "iteration : 150, loss : 0.9464, accuracy : 59.15\n",
            "iteration : 200, loss : 0.9454, accuracy : 59.15\n",
            "iteration : 250, loss : 0.9434, accuracy : 59.22\n",
            "iteration : 300, loss : 0.9424, accuracy : 59.21\n",
            "iteration : 350, loss : 0.9432, accuracy : 59.06\n",
            "Epoch : 286, training loss : 0.9431, training accuracy : 59.07, test loss : 0.9983, test accuracy : 58.18\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.9299, accuracy : 59.83\n",
            "iteration : 100, loss : 0.9497, accuracy : 58.96\n",
            "iteration : 150, loss : 0.9523, accuracy : 58.64\n",
            "iteration : 200, loss : 0.9420, accuracy : 58.98\n",
            "iteration : 250, loss : 0.9418, accuracy : 59.11\n",
            "iteration : 300, loss : 0.9407, accuracy : 59.08\n",
            "iteration : 350, loss : 0.9416, accuracy : 59.08\n",
            "Epoch : 287, training loss : 0.9406, training accuracy : 59.10, test loss : 0.9949, test accuracy : 58.31\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.9360, accuracy : 59.94\n",
            "iteration : 100, loss : 0.9428, accuracy : 59.48\n",
            "iteration : 150, loss : 0.9416, accuracy : 59.44\n",
            "iteration : 200, loss : 0.9392, accuracy : 59.50\n",
            "iteration : 250, loss : 0.9382, accuracy : 59.58\n",
            "iteration : 300, loss : 0.9388, accuracy : 59.41\n",
            "iteration : 350, loss : 0.9355, accuracy : 59.55\n",
            "Epoch : 288, training loss : 0.9376, training accuracy : 59.44, test loss : 1.0064, test accuracy : 57.93\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.9326, accuracy : 60.14\n",
            "iteration : 100, loss : 0.9396, accuracy : 59.86\n",
            "iteration : 150, loss : 0.9362, accuracy : 59.77\n",
            "iteration : 200, loss : 0.9378, accuracy : 59.49\n",
            "iteration : 250, loss : 0.9430, accuracy : 59.30\n",
            "iteration : 300, loss : 0.9415, accuracy : 59.32\n",
            "iteration : 350, loss : 0.9400, accuracy : 59.33\n",
            "Epoch : 289, training loss : 0.9383, training accuracy : 59.37, test loss : 0.9990, test accuracy : 58.11\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.9358, accuracy : 59.62\n",
            "iteration : 100, loss : 0.9431, accuracy : 59.05\n",
            "iteration : 150, loss : 0.9396, accuracy : 59.51\n",
            "iteration : 200, loss : 0.9390, accuracy : 59.40\n",
            "iteration : 250, loss : 0.9409, accuracy : 59.27\n",
            "iteration : 300, loss : 0.9407, accuracy : 59.18\n",
            "iteration : 350, loss : 0.9420, accuracy : 59.15\n",
            "Epoch : 290, training loss : 0.9426, training accuracy : 59.14, test loss : 0.9955, test accuracy : 58.07\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.9384, accuracy : 59.50\n",
            "iteration : 100, loss : 0.9422, accuracy : 59.29\n",
            "iteration : 150, loss : 0.9384, accuracy : 59.48\n",
            "iteration : 200, loss : 0.9328, accuracy : 59.69\n",
            "iteration : 250, loss : 0.9373, accuracy : 59.46\n",
            "iteration : 300, loss : 0.9395, accuracy : 59.39\n",
            "iteration : 350, loss : 0.9404, accuracy : 59.25\n",
            "Epoch : 291, training loss : 0.9387, training accuracy : 59.27, test loss : 1.0028, test accuracy : 58.14\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.9417, accuracy : 58.84\n",
            "iteration : 100, loss : 0.9308, accuracy : 59.47\n",
            "iteration : 150, loss : 0.9329, accuracy : 59.30\n",
            "iteration : 200, loss : 0.9345, accuracy : 59.30\n",
            "iteration : 250, loss : 0.9397, accuracy : 58.98\n",
            "iteration : 300, loss : 0.9370, accuracy : 59.13\n",
            "iteration : 350, loss : 0.9381, accuracy : 59.05\n",
            "Epoch : 292, training loss : 0.9386, training accuracy : 59.03, test loss : 0.9957, test accuracy : 58.02\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.9359, accuracy : 59.72\n",
            "iteration : 100, loss : 0.9408, accuracy : 59.29\n",
            "iteration : 150, loss : 0.9422, accuracy : 59.62\n",
            "iteration : 200, loss : 0.9379, accuracy : 59.59\n",
            "iteration : 250, loss : 0.9393, accuracy : 59.52\n",
            "iteration : 300, loss : 0.9372, accuracy : 59.61\n",
            "iteration : 350, loss : 0.9372, accuracy : 59.54\n",
            "Epoch : 293, training loss : 0.9368, training accuracy : 59.55, test loss : 0.9898, test accuracy : 58.37\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.9391, accuracy : 59.70\n",
            "iteration : 100, loss : 0.9400, accuracy : 59.34\n",
            "iteration : 150, loss : 0.9444, accuracy : 58.89\n",
            "iteration : 200, loss : 0.9451, accuracy : 58.81\n",
            "iteration : 250, loss : 0.9394, accuracy : 59.12\n",
            "iteration : 300, loss : 0.9418, accuracy : 59.06\n",
            "iteration : 350, loss : 0.9430, accuracy : 59.03\n",
            "Epoch : 294, training loss : 0.9416, training accuracy : 59.07, test loss : 1.0070, test accuracy : 57.65\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.9497, accuracy : 58.59\n",
            "iteration : 100, loss : 0.9523, accuracy : 58.74\n",
            "iteration : 150, loss : 0.9481, accuracy : 59.08\n",
            "iteration : 200, loss : 0.9454, accuracy : 59.27\n",
            "iteration : 250, loss : 0.9453, accuracy : 59.14\n",
            "iteration : 300, loss : 0.9425, accuracy : 59.16\n",
            "iteration : 350, loss : 0.9397, accuracy : 59.24\n",
            "Epoch : 295, training loss : 0.9388, training accuracy : 59.21, test loss : 0.9906, test accuracy : 58.33\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.9555, accuracy : 58.70\n",
            "iteration : 100, loss : 0.9406, accuracy : 59.17\n",
            "iteration : 150, loss : 0.9359, accuracy : 59.45\n",
            "iteration : 200, loss : 0.9376, accuracy : 59.37\n",
            "iteration : 250, loss : 0.9366, accuracy : 59.27\n",
            "iteration : 300, loss : 0.9354, accuracy : 59.31\n",
            "iteration : 350, loss : 0.9378, accuracy : 59.13\n",
            "Epoch : 296, training loss : 0.9374, training accuracy : 59.13, test loss : 1.0092, test accuracy : 57.67\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.9409, accuracy : 59.45\n",
            "iteration : 100, loss : 0.9352, accuracy : 59.45\n",
            "iteration : 150, loss : 0.9356, accuracy : 59.40\n",
            "iteration : 200, loss : 0.9394, accuracy : 59.20\n",
            "iteration : 250, loss : 0.9380, accuracy : 59.34\n",
            "iteration : 300, loss : 0.9380, accuracy : 59.22\n",
            "iteration : 350, loss : 0.9390, accuracy : 59.21\n",
            "Epoch : 297, training loss : 0.9390, training accuracy : 59.25, test loss : 1.0044, test accuracy : 57.94\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.9299, accuracy : 60.11\n",
            "iteration : 100, loss : 0.9324, accuracy : 59.75\n",
            "iteration : 150, loss : 0.9414, accuracy : 59.39\n",
            "iteration : 200, loss : 0.9404, accuracy : 59.37\n",
            "iteration : 250, loss : 0.9414, accuracy : 59.31\n",
            "iteration : 300, loss : 0.9416, accuracy : 59.19\n",
            "iteration : 350, loss : 0.9408, accuracy : 59.24\n",
            "Epoch : 298, training loss : 0.9425, training accuracy : 59.19, test loss : 1.0068, test accuracy : 57.89\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.9260, accuracy : 59.70\n",
            "iteration : 100, loss : 0.9229, accuracy : 60.08\n",
            "iteration : 150, loss : 0.9289, accuracy : 59.84\n",
            "iteration : 200, loss : 0.9313, accuracy : 59.73\n",
            "iteration : 250, loss : 0.9363, accuracy : 59.43\n",
            "iteration : 300, loss : 0.9364, accuracy : 59.40\n",
            "iteration : 350, loss : 0.9351, accuracy : 59.45\n",
            "Epoch : 299, training loss : 0.9347, training accuracy : 59.48, test loss : 1.0027, test accuracy : 58.00\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.9518, accuracy : 59.02\n",
            "iteration : 100, loss : 0.9350, accuracy : 59.55\n",
            "iteration : 150, loss : 0.9362, accuracy : 59.60\n",
            "iteration : 200, loss : 0.9333, accuracy : 59.77\n",
            "iteration : 250, loss : 0.9412, accuracy : 59.50\n",
            "iteration : 300, loss : 0.9388, accuracy : 59.45\n",
            "iteration : 350, loss : 0.9384, accuracy : 59.41\n",
            "Epoch : 300, training loss : 0.9392, training accuracy : 59.34, test loss : 0.9997, test accuracy : 58.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "id": "iUQVIKR-X3v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5648974-8fc8-4492-9520-8e5ea2cabeec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9476416005807764, 59.47295636140135)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : Dropout\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7CNz1iabSB21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2cda1da3-91b6-4436-9c2c-67534755a597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5hU5fn/8fe9hY7SlaICVtREFMSKsQvYwIJdoyboV2M0PzVqEo0ae4tiLyj2hhobKkRRYwMWREWpIsoCwoosAsK2+fz+eM7szDZYYIfdnblf1zXXzJwy5zkzu5/zzH3KmCScc85ljqz6boBzzrmNy4PfOecyjAe/c85lGA9+55zLMB78zjmXYTz4nXMuw3jwO+dchvHgdzUys/fNbKmZNa3vtrgEM5trZqvMbLmZFZrZJ2Z2rpk1uP9nM+tuZjKznPpui0tocH8ormEws+5Af0DAURt52Y0qJOqpvUdKag1sBdwEXAaMqGliM8veWA1zDZ8Hv6vJ6cBnwEjgjOQRZraFmb1sZgVmtsTM7kka90czmxb1Rr8xs92i4TKzbZKmG2lm10WP9zezfDO7zMx+BB4zs7Zm9ka0jKXR425J87czs8fMbEE0/j/R8KlmdmTSdLlm9pOZ7VrdSprZ0WY2xcx+MbNvzWxANHyumR2cNN3VZvZU9Djeiz3bzH4A3jOzt8zsT5Ve+wszOyZ6vIOZjTWzn81shpkNXZcPoyaSlkl6DTgBOMPMdk56f+83s9FmthI4wMx6Rd/iCs3sazMr36BH0z8QtXG5mX1gZlsljd/bzCaa2bLofu+kcTW+V8CH0X2hma0ws73qYr3dhvHgdzU5HXg6uh1mZptBec/xDeB7oDvQFXguGnc8cHU07yaEbwpLarm8zYF2hB7sMMLf5mPR8y2BVcA9SdM/CbQAdgI6Af+Ohj8BnJo03SBgoaTPKy/QzPpF018KtAH2A+bWsr0AvwN6AYcBzwInJb32jlHb3zSzlsBY4JmorScC90XTVGFml5vZG+vQDiRNAPIJ39LiTgauB1oD44HXgTFRGy4Anjaz7ZOmPwX4F9ABmEL47DGzdsCbwHCgPXBHtF7ta9G0/aL7NpJaSfp0XdbLpYYHv6vCzPYlhNYLkiYB3xJCBKAf0AW4VNJKSaslfRSN+wNwi6SJCmZL+r6Wi40B/5RUJGmVpCWSXpL0q6TlhAD7XdS+zsBA4FxJSyWVSPogep2ngEFmtkn0/DTCRqI6ZwOPShorKSZpvqTptWwvwNXRe7AKeAXondRLPgV4WVIRcAQwV9JjkkqjjdBLwPHVvaikmyQdsQ7tiFtA2HjGvSrpY0kxoDfQCrhJUrGk9wgb8JOSpn9T0odRm/8O7GVmWwCHA7MkPRm1/1lgOnAkrlHy4HfVOQMYI+mn6PkzJMo9WwDfSyqtZr4tCBuJ9VEgaXX8iZm1MLMHzex7M/uFUDJoE33j2AL4WdLSyi8iaQHwMXCsmbUhbCCermGZG9JegHlJy11O6BWfGA06KWm5WwF7RCWWQjMrJGwYNt+AZVenK/Bzde0jbKznRRuBuO+jeapML2lF9FpdolvlDXjleV0j0qh2ornUM7PmwFAgO6q3AzQlhO4uhHDY0sxyqgn/ecDWNbz0r4TSTNzmhNJEXOXLxF4MbA/sIelHM+sNfA5YtJx2ZtZGUmE1y3qc8O0jB/hU0vwa2rSm9q6spr2VVW7zs8A/zexDoBkwLmk5H0g6pIZlbTAz250QxB8lDU5u3wJgCzPLSgr/LYGZSdNskfR6rQjfHhZEt62oaEvg7ejxmt4rv/xvA+Q9flfZYKAM2JFQHuhNqGP/j1C7nwAsBG4ys5Zm1szM9onmfQS4xMz6WLBNUuljCnCymWVHO1B/t5Z2tCbU9QujGvM/4yMkLQTeItTJ20Y7cPdLmvc/wG7AhYQafk1GAGea2UFmlmVmXc1sh6T2nhi9dl/guLW0F2A0ISCvBZ5PCtg3gO3M7LTo9XLNbHcz61WL11wjM9vEzI4g7Gd5StJXNUw6nrDx/Wu0/P0JpZrnkqYZZGb7mlkTQq3/M0nzovXazsxONrMcMzuB8PcR3w+xpveqgFDG67mh6+rqkCS/+a38RujF3V7N8KHAj4Re9JaEcF0C/AQMT5ruXGAGsAKYCuwaDe8LfA0sJ9TcnwWui8btD+RXWl4X4P3odWYC5xB6jznR+HaEnv0iYCmhnp48/yOEnmirtazvEODLqF2zgcOi4T0JYbmCxI7Np6Jx3ZPbUun1RkTjdq80fPvodQqi9+09oHcNbfob8NYa2jyXsFFcDiwDPgXOB7KTphkZf3+Thu0EfBDN8w0wpNL0DxB2Qq8glNZ6JI3fF5gUzTsJ2DdpXI3vVTT+2mi9C4E96/tv3G/Cog/GubRiZlcB20k6da0TO8xsJGHj+4/6botLPa/xu7QTlYbOJhzR45yrxGv8Lq2Y2R8JO1PfkvTh2qZ3LhN5qcc55zKM9/idcy7DNIoaf4cOHdS9e/f6boZzzjUqkyZN+klSx8rDG0Xwd+/enby8vPpuhnPONSpmVu0lU7zU45xzGcaD3znnMowHv3POZZiUBr+ZtTGzUWY23cKPc+xl4Qc0xprZrOi+bSrb4JxzrqJU9/jvAt6WtAOwCzANuBx4V9K2wLvRc+eccxtJyoLfzDYl/PrOCACFH38oBI4mXFyL6H5wqtrgnHOuqlT2+HsQrsj3mJl9bmaPRD9Bt5nCZXUhXO1xs+pmNrNhZpZnZnkFBQUpbKZzzmWWVAZ/DuGa6PdL2pVwidwKZR2F60VUe80ISQ9J6iupb8eOVc4/cM65BmnVKli2rHbTzp8PT1bzw6ASfF/bHy1dD6kM/nzCZV7HR89HETYEi6LfTI3/duriFLbBOZdmVqyo+LywEF5/PYTlhpg3D4qLw+OFC2GffeCKK2DOHJgxA2Kx6ucrLIQ77oBnnoFJk6BXL9hpp/AapUm/UffXv8Imm8CZZ4ZxU6fCxRfD6afDh9HlBEtL4dtv4bLLoHt3OPbYsHGoaym9SJuZ/Q/4g6QZZnY10DIatUTSTWZ2OdBO0l/X9Dp9+/aVn7nrXPpbuRK++w523jkxTIKiImjWDG6+Ga65BsaPh003hauugjffhJ9+gpdfhiFDan7tFSvg73+Hzp1h881h2rQQwrm5kJcHp50Wwv7VV2HPPauG/bHHhpAvKoIrr4RDDgk9+732gunTISsLWrcO4V5QEKbbZBMYOhQ22wyuuw523z0sC8As3Mdi0K4dlJSE+3hPf7/9QhsnTYIttmC9mNkkSX2rjEjlr7wQfrYvj/ALR/8B2gLtCUfzzAL+Swj+Nb5Onz595JxLf0OGSGbSPfdIZWVh2EMPSa1bS08/LeXkSCDtu6/Us6fUsqV07LFSp07S0UeH6Zcska68Mgz/7W+lV16R3n9fGjAgzJt869gxLA+kLbcMj7faKjz/z3+ka6+VbrpJuuSSxDxduoT7PfeUdtkltOmVV6Rtt5U6dJC+/VZ6/XXpz3+WTjpJatUqTL/PPtKvv0rPPCOdcEJ43qSJdPnlUvPmYdiBB0r//rd0ww1SUVGYfkMAeaoum6sb2NBuHvzONXy//CKtWhUev/CCNHKkFItJH38s/fGP0p13SiUl0urVIdDi0954Ywjohx8OidS5c7hv1y6E4w47JEJ3q62kv/41EdqffRZe4+KLQwDvs4+0+eZSVpa09dbSNtsk5s3Kku67L2wYZs0K7WrZUjrmGOn558Pwe+4J0+63X2h7XCwmXXihNHx4COQHHpC6dw/LeOWVMM3PP0s//lj1fSkulpYtqzp89Wpp7tzwuKSkTj6CKjz4nXPrrKQkhPZzz0l9+kgPPihNmiQtXSr93/+F51IIth49Qm/4gQekpk1Duvz+96E33rx5eB4fDiE0v/suhG982Lbbhg3IM89IZ52V6OEPHChtt530zTfhm8CkSSFQ42bPlvbYQzrgAOnww6Xx48PwpUvDRuGFF6Sffqq6fitXVgx4SXr7bWn+/KQBP/8c0n5DxGLVp/uKFdLEiWF8YWHV8fn5G7RYD37n0sjKldK4cSHwKgdXXGlpKHGUlVUKsiTJWVRWJo0eLf2//yd9/XUY9txzqlAaadNGys5O9MKzsqR77w295qwsaccdEwF/9tmJ+T74QHrkEem880L55LLLwuu0bx/GP/VU2GCsXFmxff/+51Idu8kYLV2aNHDCBGm33UIgJ/vxR+mUU6TFi8PzoqLQhV++fM1v5pIlibrS55+rfGHxbn5OjrTXXiGkR48Oy49btSrc3ngj1GgWLKj6+l9/Hdrbo0do8/ffh3pQXl7ijbvyyrAFTP7K8MQTYYs5ceKa278GHvzONQKVQ/yrr0KP+8bTvlbsyafKh//lL4lQ7dZNOv/8ROkk7t57w/i99w73Z58tzZsXSir9+0uHHirl5konnhgyqVOnxGt27BhCvF27MK5bt1BDT94ING8e8iy+Abj++tCGSy8NIV5cHEomAwZUs6Jvv61xxwzXPv2K9dyFn4SGSaHmcsopiZX529/CAubMScx71VVh2MsvV3zNG28Mw2+6SRo1Srr11vD80kvD14sxY0L9qaAgMU9hYQj2fv2kL74Ijy+4IIyLb/UOPzys4MCBUrNm4avH44+HoN9++1DYz8pS+Y6C++8Pda28vNDGQw8NW0wzaZNNKr6JyXUoCFvFJ54Ib2Dz5tLvfrdBdSAPfufqQSwmDRoUOnRx8+eH/+1jjw0Zs3Jl6GQefHAI0iVLwnSTJ4fgzc2V3mCQYllZmjrxVz34YMiPQYNCxhxzTPhP7tVL2mHbUk2aFObfY49Ervz2txVzplevUFYZODA832MP6eSTQ4nliwmr9Z/Wp+r/+o7XTjuFDC0rC+ty9dXSq69KHdsU66wzYyorC2WVxXc9I40YEfbAjhoVeqlffKGyuT+oZMXqMPPkydL06eHxttuGIOzTJzSgQ4dQsI/XgnbcUdp9d2nXXcPzU06Rjjwy9JaHDAnDhgwJW7PZs8MKx6fNza24svG9t/G9rAMGhK8fO+5Y8WtJkyYqr0H9+GPYEvbpE746XXNN9YHdvHnYsXDEEdJbbyWWBdKmm4avNVlZYQN2wQVhA3DzzWGrfPfdYQu+3XbVv3bv3tLChRv09+fB71yktDR0xtbbnDkVC8wKAfzll+HxrFmh/v3SSyHL4hnw7rvSOeeETmPyESUHHSRduPvHGpL9qpo0CdOe1ONTHZ3zhrbtvFxznhuvErIl0F45E8pz4ZNPEssfPlz6V4sbtNTa6LDm7+uMM8I0//yn9Nhj0qpZ8/TVlFI9depb+mjUwgrfLH6YsFBlpdGA4uKwBxRCmMXXt6golEEKC6UXX1RZm7YqOfX3off8+ech4KoLr3gvOP61o2PHcMhL8vj4lqtdO2mzzaRzz02EcOVb9+7h60fl8P3Nb8Ljnj3D/c47h2Udckh4Hu9pxw/JSe5lt2olvfiitNNO4XAhCF9zmjULX7mksP577hm+frVvH/ZAv/Za1T+kW2+Vhg0L65yTk/iwp04NG7zq9hVceWVoy0UXhWlHjAjvaU01vHXgwe8yz/jx0qmnqmh5kR5/PHTyiopCSEMI4spWrQoliwo1ZYWjUIp+LdUVp/6gkqxcPbTb/XriidA7nzAhkW9nnZXIwCZNpI/aHK7buFjNWakcitWypTSy3736/l+Pq7Q0UY5ZTAcJ9MVVozTigCdVSpaKs5qoqP9BFUJu5L4Pa/Tds/Xiee8pNn1GKHs8+GCoQYPKmjVXUVZTvZ09SAO2/FrLL7gilCSgfBqZhd5u+/bSddeF0BkxImxJWrYMgZWVFW55eSG8Tjop9KQPOSSM32yziuHZokWoc3/2WSjov/pqKJXcd18I6w4dEmWbVq3C1u3uu6Uzzwwbm/ihPMOHhze8uDgR4r17h/sLLki8F61bh/szzwx1rqOPDj35L76QjjoqbKxisVBfz84OJaQHHgg9+QcekN57L/G+xDdwUthqx5cxYkTFP4L4157x4xMbhJp8801Yxk03hW8ma7JyZXifYzHphx/WPO068uB3jcLLL4eAXrEiaeDChdInn2j0f4p0111hUGFh2D9WrbIyfT/1F330m3Ml0IsDR5T/L++1V6Lssc8+4Rjx/PzQCX3nnbDsrszTd1321g8j39V7fxurG/5Vqp7Zc7Uiu7XeY38J9HTLP6gly/UOh+iyVveoRYuQL02bhuO3p06VDtr2+/IQWZ7VWh+1PEQ/568Mwde5s/TTTyot+Fn77lGcCJsttgh1ly23TAzr2zcEXKtWoXcbD9v4AecQerzXXBNKIRdcEKZJPlwmvjU64QTpX/+SDjtM6to1Mb5r1xDOPXqE+s9zz4UNROXedTzsZ80K5Zdzzw3BeffdNX+oq1Yl9tqeckp4zbffrjjNNdeEYy+Td1QMHx567bNnh41bLBZqVPE6/lFHJepia1LTNEuXhrY89VTF4X/6k/Too2t/3UbAg9+lRGlpxeeLFtX+G+rKlaGm/MEH4fmTTyay5be/TYR/2YBBEuijpgcIpHvvKtGxPSdrhxbf6+67Q4Bvs03IhLfflmK336FfctpoJuEg7hlsq2OOLtVzzyUqCL16SbkUVcizTVmqPzR5XDdweYWgO5MRur/phRWGxfbaS3MGJ/awPnvYY8rLq3j0Xdk9Uclkt91UsvMuKq8vx1+nfXupWzcVvzo6PN8/bFRkFg57Oeig0NtetCi8YPyQmf79Q1kCQuDGD91Jds45Yfyhh4be9dixYa9u8mEzY8eG3vsJJ4Rpt9lGmjkzMf7881X+VSa+1czNlYYOrd0HXJ14r7k6lYdXdwjkXXeFjVgd94zTlQe/U35++AYuhf+/+IEUccmHEi9fHkK5uDh8Oz777PC/dvXVYf+cJE2ZErLryitD6XTffUNmXXFFxdddvTp0ru65s0Tv9/mLpt09VqtWhfDtwbe6iqt1yyFj1Lp1OApk9K1T1cumqWdPafDRMS21NuVheXDbPL3LARKogPbqyjwdvtVXGrXD3/Vls75qkb1a87dM7NX8rmUIyC/ufE+S9N7wrzRm89O05KnRKs1tqsmPTdEVV0gfPpuvn5tuVj7f9CY7a/5mYWfhst8doVjzFlrQcutEMDdvHgLorLO0avd9FevUKZxZtP/+4Y2LxUKvumfP8DgWC3tzIXFMJITgje90fOutxEbh9dfDGx4/Q0kKR5dAeOO//TYEeU0hGj8YvqRkzUeFLF0aXuOdd6oeR15YGGrzr7wS9kZ/+204gL42vexUKSuruHFya+TB34gUFm7g+SKVwmDFilDqPPTQ8InPmhWOcMvODqWOf/wjVAiuPfgDzWA7zfh0ic48YaUgpttuCzsqIXHsdvMmpZp2x5vq3/fXkIOU6c/NH1Tfzvn67Xar9AH99fUp12vxophmz5aOazNWnfixvCc93vbQFbuP1TsckghotlKOlWrahF+kjh21ulU7nX5Qvg7u+a0E+mfu9SoiV2XtwoHf3w0JO8JWZiWVM0DXNL1eZSSOrPjl8ZdV3Ly1YmeeFcoIyV8pINRfP/00vBGgj7c6KQwfOTK8eccdl5hn4kTpllvCOfbxYV9+GWq+8XJK586h3j0ofEupcDjP6tVhJ8Btt4XXPfroEM4QShqxWDjtFKoP1wULpP/+dwP+MFym8eBvoD7/PHTO4lauDOXWP/xhzfOtWiV99JE0Y0aoS9/xjyWaP36evht6qeZ366drr02UYY4/vkI26rTTKp5BGb+9SOiRjuwYLkyywLpo36YTys+ehHCwxM3N/ymBXuA4jTlyuP6+zbOKlzRWjEr0Wr9jKw3LelhlmKbtMFix7GyVtQ87MYvIVUGLLaRrr1XpXeE8+ceGvBrqzxBKHIMHh51yoJnPTVLBoSeHnXr33BNWLL6j8aKLwllHu+2mMgv175WHHh1eY+lS6YwzQomiRYuqKx2/bb99OKqktDRsCOIbz+HDExuK+LD33lN5aSQ+7PXXQ2E/Pz/Uytu2DXXoyiWYZPF5b7st3KSwld1nn9r++Ti3Rh78DcAvc5eopDhW/v/+yy9Ss9xSzeq4p76/5jH9/HMoYUII5kcfDdnxyCNh/9bzF36sl66fplWrEoc/x29vc6ims52+IpQ2ujNHQw/6SSOvzw+l5R2+0809H9B5e+Tpd4xTs2bS6/f9oKf2vkdTz79X43c7R2W5oQC+ghCQJZ06a1luO520/wLdcF2ZBvGGRl02QTEzzWLrxMJzchLn5HfpopiZ/nfiPVravmfVgH3hBQkU22QTFc+LzlIsKQlbu0MPDT3eQYPC15CsrHB2UZMm4StQUVHFGvXPP4ejOOLy8kINfejQMG38pJ+vvgo97AsvDD3m+NEtv/99qKXH21bdmUZffRXGxfcqS6HmbhZKO9UpLAy9+/VRWpq6C7e4jOPBvzGVlGjxYumEoTHNvWqExo9dpgeuK9AKWujPTe9X06ahPDt6tLQ7oUzwMkO03XahZp58Yan9eU/bMV3X2D8l0DfsoAHbzNJbHKYZ/U7VVVfGNPwvc6oE7MIt+6mEbJWSpVtbXa2iU8+qML7gzEvDESTxYdFJL6XNQ+nk1932Dj1YkG65RSUvvhICu317xVq10t/P+1mLHnwlsQPwvPPChVIgnEgjhZCNNgaCcBRKLBZOWnn++YrvWfxQPwjj5iStU//+dfv5xMM+3obBg8Pzq66qfvqJE6vuxX7vvYpf1ZxrgDz4N5LY51NU2rS5rttntHbhcwl0EXdoEG9IoLkd+2iHHUIW/vnP0tVZ4YzAeTndy88GnzEjVC9eGzxCAhVv3q28hCHQ9023UUlOVKu5/fawNzU5+KOzB0sGHanCA45WzCwssEuXcDZPfCdh587hiJApU0JJ5JNPEjXtSy4JK9SvXwjy009PvP6f/5xY4ZUrw+Fvc+aEk2AgND7uxRfDCrVtG2rjNZk+PczbvHnicJ5jjw1HrySfrl8XJk0KpZv4qfsjR4Zlv/lm3S7HuXrmwV/HSkpCKfhvf5NmTixUwQXXaEiXzzSmdTgLcTy76/bdngqPuw/VD2deVR6cnzw2vTxDv2i9d/nw4q23V1nXbmHP67Jl4ZTx+EkszZqFU0HjMz70UOipZmWFnYrHH584qWbYsLAFWbYslD/i81x/fWh8WVnYw1tdSeH668O08WvN3nlnYv7DDgsn8lQ+HCiuuDgcQx6/wleyRYvWvsd60KBwGGJcaWmdnL24VsXF4VIDlXv1zjVyHvzrqKAglGOSs3HRonAi4muvJe8njOn5rBMq9Lh/2jwcbx2LTv+ObbllqB936yY1aaLYnntqTP9rdRhvhZ78nnsm5u/fP/TY44f+/fvfIQxvuSU0ZpNNwok5v/wSesb77Rd65MuWhcDv2zc0Nh5iZWWJq2+9//7aV3zatBDw8UP7li1LHD9e+aJYzrkGzYN/HcVL18ccE3asLvpupS7a6mVBTFlZUp9tCvXmBW9p3rP/k0D/4u8aecDIcAr8woXVX2vk7LNDeCZdRCrWtGkotUA4a7K0NHE1QAgn2SS7/vrEESBS6BHHQ764uPqf7Pn978MyK1/ztrZWrw71+o3R+3bO1RkP/tr45BP9+oc/afSbsfIzw7OypLYs0dW510mgWw5+Rzs1TbqeR3Qm5qkDfyr/NR1J4azL5B2bID37bBi3dGk4YSY7O3EJ2BNOSIyPH9IIG3x1PknhNeKnxzrnMkZNwZ/SH1uvKxvtx9aHDYOHH2ZrZjOHrbn9djhrxXBaX3sJK3LbsunqxXDUUZTusDM5t9yQmK9HD5gzp+Jr3Xor/PWv4da0KfTpA0cdlfiFZQi/qty1K+TkVJx37Fg49FBo3z78anPyPM45V0s1/dh6TnUTZ6KSEsj+fApZwDG8zE90YNBBJ9Nm4E1QVsKmZYthu+3gjTfImTwZ9t8fli2Dzz+HvlV/xJ5Bg+Cyy6BfPzj22OoXutVW1Q/v1y+E/c47e+g75+pcVn03oCG44QZo3qSUoryvALjFLuMxzmL7f50CCxfC3/4G++0Ho0dD27aQnw9HHhmGQfXBv9NOMH06DBmy7g3adNOwsVifeZ1zbi0yvtSzejVssQXs1+EbXpq+EzGMLJLek4ED4c03Ez3vF1+EP/wBvvgi3AYPhg8+SGwEnHOugaip1JPxPf7HH4effoJrhkwBIOvII8KIG28MJZdHH61Ybjn+eCgshO7dQ83+ww+hf/+N33DnnFtPGdnjHzsWFjz0Br/T++w4+jYO3Hkxr/96IJafDzNnwtSpcOCBdbY855yrD75zN1JaGg7e+W7ukQD8dov/xws734A9Mxveegs6dfLQd86ltYwr9YwaBXPnQnFuCwDGXf42Ld59HQYMgAMOqN/GOefcRpBxwf/kk+Gw+9z2mwDQ/J7b4Icf4PDD67llzjm3cWRU8JeUhH2xRxxShP34Yxg4bVq4HzSo/hrmnHMbUUbV+CdOhNNX3MtVHzwTBgwfHo7nzM4OZ9A651wGyKjgf/e/4hJuo8OMuWHAb34TzsB1zrkMkjGlHgmmPjGZHsxNDKzpkgnOOZfGMib4J06EXb59iVhWdmJgt2711yDnnKsnGRP8TzwBvbO+QjvtDEOHQuvWkJtb381yzrmNLmOCf+ZM6NlsAdlbdIVnnw2XXXDOuQyUMcG/aBF0KlsAXbpAVla4OedcBkrpUT1mNhdYDpQBpZL6mlk74HmgOzAXGCppaSrbAVCwsJQ2RYtC8DvnXAbbGN3eAyT1TrpQ0OXAu5K2Bd6NnqdUaSlkFSwKl1v24HfOZbj6qHccDTwePX4cGJzqBRYUQGcWhCce/M65DJfq4BcwxswmmdmwaNhmkhZGj38ENqtuRjMbZmZ5ZpZXUFCwQY1YtAi6ePA75xyQ+jN395U038w6AWPNbHrySEkys2p/EEDSQ8BDEK7HvyGN+HFBjO7xE7c8+J1zGS6lPX5J86P7xcArQD9gkZl1BojuF6eyDQDdb/sTd3FReNKpU6oX55xzDVrKgt/MWppZ6/hj4FBgKvAacEY02RnAq6lqQ1zXvP8knmRn1zyhc85lgFSWejYDXrHwe7U5wDOS3jazicALZnY28D0wNAw1cSIAABfVSURBVIVtACC/3S70Wr4Q7rkn1YtyzrkGL2XBL2kOsEs1w5cAB6VqudUpXV3CpGZ70+f88zfmYp1zrkHKiNNXrbSEWHaT+m6Gc841CBkR/NllxZRl+wXZnHMOMib4S4hlefA75xxkSPBnxUq8x++cc5GMCP6cWLHX+J1zLpIRwZ8dKyHmPX7nnAM8+J1zLuNkRPDnqIRYjpd6nHMOMiT4c2PFKMd7/M45BxkS/Nkq8eB3zrlIRgR/Lh78zjkXlxnBr2KU6zV+55yDTAh+iRzKvMfvnHOR9A/+kpJwn+vB75xzkEHB76Ue55wL0j/4i4vDfRPv8TvnHGRA8JetDj1+81KPc84BGRD8Jb9GNf6mXupxzjnIgOAv/TWUerzH75xzQfoH/6qo1OM1fuecAzz4nXMu46R98Jetiko9XuN3zjkgA4I/3uPPauo9fuecgwwI/vLDOT34nXMOyKDgz27mpR7nnINMCP6oxu+lHuecC9I/+Fd7jd8555KlffDHiuKlHg9+55yDTAj+1VGpx2v8zjkHZELwe4/fOecqyJjgz2nuwe+cc1CL4DezI82s0W4gYkWh1JPd3Es9zjkHtevxnwDMMrNbzGyHVDeorsl7/M45V8Fag1/SqcCuwLfASDP71MyGmVnrlLeuDqjYg98555LVqoQj6RdgFPAc0BkYAkw2swtS2LY6UR78LbzU45xzULsa/1Fm9grwPpAL9JM0ENgFuLgW82eb2edm9kb0vIeZjTez2Wb2vJmlNpGj39z1Hr9zzgW16fEfC/xb0m8k3SppMYCkX4GzazH/hcC0pOc3R6+3DbC0lq+x3uI9/twWHvzOOQe1C/6rgQnxJ2bW3My6A0h6d00zmlk34HDgkei5AQcSykYAjwOD17HN66akhFKyyW1iKV2Mc841FrUJ/heBWNLzsmhYbdwJ/DVp/vZAoaTS6Hk+0LW6GaMdyHlmlldQUFDLxVWjuJhimuA/ueucc0Ftgj9HUnH8SfR4rXV5MzsCWCxp0vo0TNJDkvpK6tuxY8f1eYmgpIQScj34nXMuUpvgLzCzo+JPzOxo4KdazLcPcJSZzSUcDXQgcBfQxsxyomm6AfPXqcXrKgr+Jn5Qj3POAbUL/nOBv5nZD2Y2D7gMOGdtM0m6QlI3Sd2BE4H3JJ0CjAOOiyY7A3h1vVpeW6UlFNOE7OyULsU55xqNnLVNIOlbYE8zaxU9X7GBy7wMeM7MrgM+B0Zs4OutUVZJMSV4ncc55+LWGvwAZnY4sBPQLByYA5Kure1CJL1POA8ASXOAfuvYzvVmpSWUmge/c87F1eYErgcI1+u5ADDgeGCrFLerznjwO+dcRbWp8e8t6XRgqaRrgL2A7VLbrLpjpcWUefA751y52gT/6uj+VzPrApQQrtfTKFhZKaVZHvzOORdXmxr/62bWBrgVmAwIeDilrapDVlZKzGq1K8M55zLCGhMx+gGWdyUVAi9FF1prJmnZRmldHbCyMsqyPPidcy5ujaUeSTHg3qTnRY0p9AEsVkrMg98558rVpsb/rpkda/HjOBuZrDIPfuecS1ab4D+HcFG2IjP7xcyWm9kvKW5XncmKeY3fOeeS1ebM3UbxE4s1yVKp1/idcy7JWhPRzParbrikD+u+OXUvK1ZKLNuD3znn4mqTiJcmPW5GuNzCJMLVNhu87FgpsVwPfueci6tNqefI5OdmtgXhB1YaBS/1OOdcRbXZuVtZPtCrrhuSKtl+OKdzzlVQmxr/3YSzdSFsKHoTzuBtFLLkwe+cc8lqk4h5SY9LgWclfZyi9tS5bA9+55yroDaJOApYLakMwMyyzayFpF9T27S6ka1S5MHvnHPlanXmLtA86Xlz4L+paU7d8x6/c85VVJvgb5b8c4vR4xapa1Ld8uB3zrmKahP8K81st/gTM+sDrEpdk+pWtkqRn8DlnHPlapOIFwEvmtkCwk8vbk74KcZGwXv8zjlXUW1O4JpoZjsA20eDZkgqSW2z6o73+J1zrqLa/Nj6+UBLSVMlTQVamdl5qW9aHZDIxXv8zjmXrDY1/j9Gv8AFgKSlwB9T16Q6FIsBeI/fOeeS1Cb4s5N/hMXMsoEmqWtSHSotBTz4nXMuWW0S8W3geTN7MHp+DvBW6ppUh6Lg91KPc84l1CYRLwOGAedGz78kHNnT8MV7/Dke/M45F7fWUk/0g+vjgbmEa/EfCExLbbPqSBT8eKnHOefK1ZiIZrYdcFJ0+wl4HkDSARunaXUgXurx4HfOuXJrSsTpwP+AIyTNBjCzv2yUVtWV8h5/dv22wznnGpA1lXqOARYC48zsYTM7iHDmbuMRr/H7zl3nnCtXY/BL+o+kE4EdgHGESzd0MrP7zezQjdXADRLv8fvOXeecK1ebnbsrJT0T/fZuN+BzwpE+DV9ZWbj34HfOuXLr9Ju7kpZKekjSQalqUJ3yE7icc66K9fmx9cbDj+N3zrkqMiL4vdTjnHMJKQt+M2tmZhPM7Asz+9rMromG9zCz8WY228yeN7PUXffHT+ByzrkqUtnjLwIOlLQL0BsYYGZ7AjcD/5a0DbAUODtVDVCJ9/idc66ylAW/gvhv9eZGNxEu+TAqGv44MDhlbSjxnbvOOVdZSmv8ZpZtZlOAxcBY4FugUFLUFScf6FrDvMPMLM/M8goKCtZr+bHisBjL9eB3zrm4lAa/pDJJvQnH//cjnAxW23kfktRXUt+OHTuu1/Ljwe+lHuecS9goR/VEv+A1DtgLaGNm8STuBsxP1XI9+J1zrqpUHtXT0czaRI+bA4cQLuc8DjgumuwM4NVUtSHmO3edc66KVCZiZ+Dx6Kcas4AXJL1hZt8Az5nZdYTLP4xIVQPkNX7nnKsiZYko6Utg12qGzyHU+1POD+d0zrmq0vrMXT+qxznnqkrr4C/v8XvwO+dcubQO/vjOXfNSj3POlUvr4KfESz3OOVdZWge/H87pnHNVpXXw++GczjlXVVoHf/yyzFlNPPidcy4urYPfj+N3zrmqMiL4vcfvnHMJGRH8XuN3zrmE9A7+0lJiGFk5ab2azjm3TtI7EUtKKSOb7Oz6bohzzjUc6R38paWUkkNWeq+lc86tk7SORJWE4Pcev3POJaR18FNW5sHvnHOVpHXwy0s9zjlXRXpHYqmXepxzrjIPfuecyzAZEfxe6nHOuYS0jkTzHr9zzlWR1sHvPX7nnKsqvSPRe/zOOVdFegd/mQe/c85VltbBb17qcc65KtI6EmcceQlXca33+J1zLklaB/+iHQ/gbQZ68DvnXJK0Dv6ysnDvpR7nnEtI60iMxcK99/idcy4hrYPfe/zOOVdVWkdiPPi9x++ccwlpHfxe6nHOuarSOvi91OOcc1WldSR6qcc556pK6+D3Uo9zzlWVU98NSCUv9TiXuUpKSsjPz2f16tX13ZSUa9asGd26dSM3N7dW06cs+M1sC+AJYDNAwEOS7jKzdsDzQHdgLjBU0tJUtMF7/M5lrvz8fFq3bk337t0xs/puTspIYsmSJeTn59OjR49azZPKvnApcLGkHYE9gfPNbEfgcuBdSdsC70bPU8J7/M5lrtWrV9O+ffu0Dn0AM6N9+/br9M0mZZEoaaGkydHj5cA0oCtwNPB4NNnjwOBUtcF37jqX2dI99OPWdT03Sl/YzLoDuwLjgc0kLYxG/UgoBVU3zzAzyzOzvIKCgvVarpd6nHOuqpQHv5m1Al4CLpL0S/I4SSLU/6uQ9JCkvpL6duzYcb2W7aUe51x9KSws5L777lvn+QYNGkRhYWEKWpSQ0kg0s1xC6D8t6eVo8CIz6xyN7wwsTtXyvdTjnKsvNQV/aWnpGucbPXo0bdq0SVWzgNQe1WPACGCapDuSRr0GnAHcFN2/mqo2xEs93uN3LrNddBFMmVK3r9m7N9x5Z83jL7/8cr799lt69+5Nbm4uzZo1o23btkyfPp2ZM2cyePBg5s2bx+rVq7nwwgsZNmwYAN27dycvL48VK1YwcOBA9t13Xz755BO6du3Kq6++SvPmzTe47amMxH2A04ADzWxKdBtECPxDzGwWcHD0PCXKysAs3JxzbmO66aab2HrrrZkyZQq33norkydP5q677mLmzJkAPProo0yaNIm8vDyGDx/OkiVLqrzGrFmzOP/88/n6669p06YNL730Up20LWU9fkkfATVF7kGpWm6yWMzLPM65NffMN5Z+/fpVOM5++PDhvPLKKwDMmzePWbNm0b59+wrz9OjRg969ewPQp08f5s6dWydtSfszd73M45xrCFq2bFn++P333+e///0vn376KS1atGD//fev9jj8pk2blj/Ozs5m1apVddKWtI7FsjLv8Tvn6kfr1q1Zvnx5teOWLVtG27ZtadGiBdOnT+ezzz7bqG1L6x6/l3qcc/Wlffv27LPPPuy88840b96czTZLnLI0YMAAHnjgAXr16sX222/PnnvuuVHbltbB76Ue51x9euaZZ6od3rRpU956661qx8Xr+B06dGDq1Knlwy+55JI6a1dax6KXepxzrqq0Dn4v9TjnXFVpHfxe6nHOuarSOha91OOcc1WldfDHYt7jd865ytI6Fr3H75xzVaV18PvOXedcfVnfyzID3Hnnnfz666913KKEtA5+37nrnKsvDTn40/4ELu/xO+fq47rMyZdlPuSQQ+jUqRMvvPACRUVFDBkyhGuuuYaVK1cydOhQ8vPzKSsr48orr2TRokUsWLCAAw44gA4dOjBu3Li6bTdpHvxe6nHO1ZebbrqJqVOnMmXKFMaMGcOoUaOYMGECkjjqqKP48MMPKSgooEuXLrz55ptAuIbPpptuyh133MG4cePo0KFDStqW1sHvpR7nHFDv12UeM2YMY8aMYddddwVgxYoVzJo1i/79+3PxxRdz2WWXccQRR9C/f/+N0p60D37v8Tvn6pskrrjiCs4555wq4yZPnszo0aP5xz/+wUEHHcRVV12V8vakdX/Yj+N3ztWX5MsyH3bYYTz66KOsWLECgPnz57N48WIWLFhAixYtOPXUU7n00kuZPHlylXlTwXv8zjmXAsmXZR44cCAnn3wye+21FwCtWrXiqaeeYvbs2Vx66aVkZWWRm5vL/fffD8CwYcMYMGAAXbp0ScnOXZNU5y9a1/r27au8vLx1nu/GG+GXX8K9cy6zTJs2jV69etV3Mzaa6tbXzCZJ6lt52rTu8V9xRX23wDnnGh6vgDvnXIbx4HfOpa3GUMquC+u6nh78zrm01KxZM5YsWZL24S+JJUuW0KxZs1rPk9Y1fudc5urWrRv5+fkUFBTUd1NSrlmzZnTr1q3W03vwO+fSUm5uLj169KjvZjRIXupxzrkM48HvnHMZxoPfOecyTKM4c9fMCoDv13P2DsBPddic+uTr0jD5ujRM6bIuG7IeW0nqWHlgowj+DWFmedWdstwY+bo0TL4uDVO6rEsq1sNLPc45l2E8+J1zLsNkQvA/VN8NqEO+Lg2Tr0vDlC7rUufrkfY1fueccxVlQo/fOedcEg9+55zLMGkd/GY2wMxmmNlsM7u8vtuzLsxsrpl9ZWZTzCwvGtbOzMaa2azovm19t7MmZvaomS02s6lJw6ptvwXDo8/pSzPbrf5aXlEN63G1mc2PPpspZjYoadwV0XrMMLPD6qfV1TOzLcxsnJl9Y2Zfm9mF0fDG+LnUtC6N7rMxs2ZmNsHMvojW5ZpoeA8zGx+1+XkzaxINbxo9nx2N777OC5WUljcgG/gW6Ak0Ab4Adqzvdq1D++cCHSoNuwW4PHp8OXBzfbdzDe3fD9gNmLq29gODgLcAA/YExtd3+9eyHlcDl1Qz7Y7R31lToEf095dd3+uQ1L7OwG7R49bAzKjNjfFzqWldGt1nE72/raLHucD46P1+ATgxGv4A8H/R4/OAB6LHJwLPr+sy07nH3w+YLWmOpGLgOeDoem7ThjoaeDx6/DgwuB7bskaSPgR+rjS4pvYfDTyh4DOgjZl13jgtXbMa1qMmRwPPSSqS9B0wm/B32CBIWihpcvR4OTAN6Erj/FxqWpeaNNjPJnp/V0RPc6ObgAOBUdHwyp9L/PMaBRxkZrYuy0zn4O8KzEt6ns+a/zAaGgFjzGySmQ2Lhm0maWH0+Edgs/pp2nqrqf2N8bP6U1T+eDSp5NZo1iMqD+xK6F026s+l0rpAI/xszCzbzKYAi4GxhG8khZJKo0mS21u+LtH4ZUD7dVleOgd/Y7evpN2AgcD5ZrZf8kiF73mN9ljcRt7++4Gtgd7AQuD2+m3OujGzVsBLwEWSfkke19g+l2rWpVF+NpLKJPUGuhG+ieyQyuWlc/DPB7ZIet4tGtYoSJof3S8GXiH8MSyKf9WO7hfXXwvXS03tb1SflaRF0T9qDHiYRMmgwa+HmeUSgvJpSS9Hgxvl51LdujTmzwZAUiEwDtiLUFqL/1hWcnvL1yUavymwZF2Wk87BPxHYNtoz3oSwE+S1em5TrZhZSzNrHX8MHApMJbT/jGiyM4BX66eF662m9r8GnB4dRbInsCyp9NDgVKpzDyF8NhDW48ToqIsewLbAhI3dvppEdeARwDRJdySNanSfS03r0hg/GzPraGZtosfNgUMI+yzGAcdFk1X+XOKf13HAe9E3tdqr7z3aqbwRjkqYSaiX/b2+27MO7e5JOALhC+DreNsJdbx3gVnAf4F29d3WNazDs4Sv2iWE+uTZNbWfcFTDvdHn9BXQt77bv5b1eDJq55fRP2HnpOn/Hq3HDGBgfbe/0rrsSyjjfAlMiW6DGunnUtO6NLrPBvgt8HnU5qnAVdHwnoSN02zgRaBpNLxZ9Hx2NL7nui7TL9ngnHMZJp1LPc4556rhwe+ccxnGg9855zKMB79zzmUYD37nnMswHvyuQTEzmdntSc8vMbOr6+i1R5rZcWufcoOXc7yZTTOzcaleVqXl/t7M7tmYy3SNkwe/a2iKgGPMrEN9NyRZ0hmUtXE28EdJB6SqPc5tCA9+19CUEn5j9C+VR1TusZvZiuh+fzP7wMxeNbM5ZnaTmZ0SXeP8KzPbOullDjazPDObaWZHRPNnm9mtZjYxurjXOUmv+z8zew34ppr2nBS9/lQzuzkadhXh5KIRZnZrNfNcmrSc+HXXu5vZdDN7OvqmMMrMWkTjDjKzz6PlPGpmTaPhu5vZJxau4T4hfqY30MXM3rZwbf1bktZvZNTOr8ysynvrMsu69GKc21juBb6MB1ct7QL0IlxCeQ7wiKR+Fn6g4wLgomi67oTrt2wNjDOzbYDTCZcj2D0K1o/NbEw0/W7AzgqX8i1nZl2Am4E+wFLClVQHS7rWzA4kXBM+r9I8hxIuFdCPcFbsa9HF934AtgfOlvSxmT0KnBeVbUYCB0maaWZPAP9nZvcBzwMnSJpoZpsAq6LF9CZcqbIImGFmdwOdgK6Sdo7a0WYd3leXhrzH7xochassPgH8eR1mm6hwjfYiwmn58eD+ihD2cS9IikmaRdhA7EC4FtLpFi6LO55wCYNto+knVA79yO7A+5IKFC6N+zThR1vW5NDo9jkwOVp2fDnzJH0cPX6K8K1he+A7STOj4Y9Hy9geWChpIoT3S4nL974raZmk1YRvKVtF69nTzO42swFAhStyuszjPX7XUN1JCMfHkoaVEnVWzCyL8MtqcUVJj2NJz2NU/DuvfI0SEXrfF0h6J3mEme0PrFy/5lfLgBslPVhpOd1raNf6SH4fyoAcSUvNbBfgMOBcYChw1nq+vksD3uN3DZKknwk/PXd20uC5hNIKwFGEXypaV8ebWVZU9+9JuGDXO4QSSi6AmW0XXRV1TSYAvzOzDmaWDZwEfLCWed4BzrJwDXnMrKuZdYrGbWlme0WPTwY+itrWPSpHAZwWLWMG0NnMdo9ep/Wadj5HO8qzJL0E/INQvnIZzHv8riG7HfhT0vOHgVfN7AvgbdavN/4DIbQ3Ac6VtNrMHiGUgyZHl/stYC0/aylpoZldTrh0rgFvSlrjZbIljTGzXsCnYTGsAE4l9MxnEH5w51FCieb+qG1nAi9GwT6R8FurxWZ2AnB3dBnfVcDBa1h0V+Cx6FsSwBVraqdLf351TufqWVTqeSO+89W5VPNSj3POZRjv8TvnXIbxHr9zzmUYD37nnMswHvzOOZdhPPidcy7DePA751yG+f8BTBQor9v7zwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : Dropout\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ed06b070-b8db-44f1-971e-acce1a35f54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_drop = {train_loss_list}\") \n",
        "print(f\"train_acc_list_drop = {train_acc_list}\")\n",
        "print(f\"test_loss_list_drop = {test_loss_list}\")\n",
        "print(f\"test_acc_list_drop = {test_acc_list}\")"
      ],
      "metadata": {
        "id": "3eiY3bTlWipW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7a7b8b-3a55-43c3-e7a5-14875901b11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_drop = [2.3269979856847747, 1.7653623900116298, 1.3874595969673094, 1.2879569950465588, 1.2635592367590927, 1.2367184840566743, 1.2146497220204775, 1.2130844773961922, 1.199504979903782, 1.1933544463904568, 1.1808399522530677, 1.1757891002063183, 1.1778574581068706, 1.1606368593730254, 1.1600934472187425, 1.1438661821812472, 1.1432536811363407, 1.1429211870441591, 1.1426363309547507, 1.133141807747404, 1.1345912316627296, 1.1346861663549572, 1.1272323819679944, 1.1205709327850237, 1.1248636617247005, 1.1250518688018405, 1.1194298835950816, 1.114780056444287, 1.1212061199392407, 1.1095433983053296, 1.1143530374620019, 1.1052800034119832, 1.1108045892986824, 1.1065942677378977, 1.1010452583230286, 1.0994206405267484, 1.099108748804263, 1.0993594152494497, 1.1091623603489986, 1.1096312763891245, 1.0985218368894685, 1.1004681642139507, 1.096200450326046, 1.0976620978779263, 1.094955347417816, 1.0958598713241618, 1.0961977980001185, 1.0916708507511996, 1.0923026947148124, 1.0862725456232625, 1.094909934332054, 1.093874169235953, 1.0920930133279423, 1.0931878089904785, 1.086355970642431, 1.0918006407536143, 1.0867836842045875, 1.0882600700952174, 1.0883855926312083, 1.087175705407047, 1.0858582045004619, 1.0828832702908089, 1.08583508645939, 1.0874728814050112, 1.084138987510185, 1.0823427366047371, 1.0800837197600988, 1.0770875377060598, 1.0864950287632826, 1.084827565726872, 1.073665852953748, 1.0765001387777045, 1.0789985916801907, 1.0776529564120905, 1.076349344195389, 1.0786382840254765, 1.077901720677611, 1.0782686996912245, 1.0696677211823502, 1.0752064093018612, 1.0767290782799244, 1.0666720652321813, 1.0750372480893846, 1.0693097597538295, 1.0701931870080592, 1.076136302172653, 1.0733054117135563, 1.0641420357595615, 1.065630742527928, 1.0657350051047678, 1.067523883771767, 1.063597192447683, 1.0660317696853059, 1.0706347740762603, 1.0645221793231603, 1.0684885780016582, 1.064461873314245, 1.0615039595420444, 1.0653091063344382, 1.0617163404216612, 1.0675504254454842, 1.0631515193114758, 1.0612066819415829, 1.058221712829621, 1.0584084538586418, 1.0670307564541577, 1.0533388339407075, 1.0498751852570511, 1.05395630559301, 1.0629614714361464, 1.0591821676993434, 1.0568597993553492, 1.05931279620504, 1.0601192338033743, 1.0487628386918768, 1.0510774044809625, 1.0565306873825508, 1.0501588705755507, 1.058949898090466, 1.044939726510345, 1.0554435949338483, 1.0505332924163115, 1.051683856865901, 1.0536485832881153, 1.0511912296457988, 1.0452298652835008, 1.0443854107443233, 1.052779132273139, 1.0420751316437553, 1.0414705583396642, 1.0444812233532024, 1.0471862510937016, 1.0396204216047353, 1.0414724818413175, 1.03923720379832, 1.0432482720390568, 1.0439860240876837, 1.0423357339732369, 1.0430644690828919, 1.0352552113171192, 1.0460254693742044, 1.0362712912120147, 1.03799982510285, 1.0385781335959912, 1.0344820675165027, 1.0418778047329043, 1.034199855191921, 1.033808571213306, 1.0262756110206852, 1.034790510892222, 1.032612440708853, 1.0359039529552305, 1.0275757677832915, 1.033923864364624, 1.0279648560818617, 1.0285659957384352, 1.0272157080774384, 1.0279640723373185, 1.0284397287097404, 1.0167389515292677, 1.028240316282443, 1.0327637229508502, 1.0252947663550132, 1.0240331461759118, 1.024374627647038, 1.0157274292090397, 1.0168284508917067, 1.028080809568648, 1.0181243873547086, 1.0142114248056076, 1.0197146307162153, 1.0189350626979095, 1.0192417449098292, 1.0124707494970906, 1.006117636433785, 1.0133408476020584, 1.0164885971604325, 1.0152640806304083, 1.0132970919777062, 1.0161991471520608, 1.0063403848063979, 1.0026034696638422, 1.0090475037169004, 1.0062155059682645, 1.0093863816442206, 1.0068934393445974, 1.0114482490997003, 1.0143260293536716, 1.0130569900923627, 0.9989717342666171, 1.0038538991920347, 0.9952844082501523, 1.0010124690487456, 0.999101043878209, 1.0022100877309557, 0.9991934141169395, 0.9969953040120402, 1.0010999458915173, 1.0011287208818163, 0.9955835604086155, 0.9976842358183409, 0.9920098645900323, 0.9937127245473991, 0.993120821349343, 0.9951552117097022, 0.993177265171113, 0.984136873629035, 0.9912217191564359, 0.9782676732313988, 0.9944041190431693, 0.9868646126775561, 0.98730035913669, 0.9813357396177483, 0.9883824942234732, 0.9876484284556009, 0.9866081552130743, 0.9804793106831187, 0.982329702958828, 0.9847826557107734, 0.979827643248447, 0.9801138112861613, 0.9883290366105594, 0.9744695815936659, 0.9759068254855913, 0.9758978694435058, 0.9743595796871961, 0.9745533957390927, 0.973437243360814, 0.9742537187690011, 0.9750634103932678, 0.970686482866282, 0.9699480381761463, 0.9677449755552339, 0.9655040294174256, 0.967782849864908, 0.9647368157136085, 0.9682623627709179, 0.9718506287753097, 0.9585308182207226, 0.9675361998682099, 0.9612520716700773, 0.9551117786870094, 0.9608719082382636, 0.9633615604907193, 0.9592795095792631, 0.9548227001981038, 0.9505302474750736, 0.9660026182327167, 0.9591187398608138, 0.9611322548654344, 0.9559221098093482, 0.959710897468939, 0.9540097088348575, 0.9624342855399217, 0.9561364148367388, 0.9604551173484099, 0.9458873228328984, 0.9505534968402004, 0.9481128198990654, 0.9479895839846231, 0.9562433150402576, 0.9491052908626029, 0.9499763810860755, 0.9469542624504586, 0.9478379463761802, 0.9533831753381868, 0.9471941099257327, 0.942795716165527, 0.944058593049605, 0.9398068259078959, 0.9412317177467553, 0.9502919862263894, 0.9397357658642095, 0.9444108484237175, 0.9479240702419747, 0.9491396483367052, 0.9432178651737327, 0.9387787769480449, 0.9346725168589977, 0.9445450221943016, 0.9406514087020543, 0.9382828810027621, 0.9466914734866237, 0.9405026495618226, 0.9463330999945561, 0.9430602538553357, 0.9406261730000256, 0.9375631096886425, 0.9383391232348393, 0.9425545480193162, 0.9386809471499952, 0.9386361192881576, 0.9367613829571381, 0.9416064718551429, 0.93884884745771, 0.9373711018058343, 0.9390345194798498, 0.9424526292457167, 0.9347226997701134, 0.9392069563633059]\n",
            "train_acc_list_drop = [14.517734250926416, 37.158284806776074, 48.72419269454738, 51.22710428798306, 51.817893065113815, 52.71148755955532, 53.21334039174166, 53.033350979354154, 53.164637374272104, 53.44203282159873, 53.97988353626257, 53.948120698782425, 53.86553732133404, 54.51773425092642, 54.18316569613552, 54.99841185812599, 54.76971942826893, 54.69137109581789, 54.591847538380094, 54.74854420328216, 54.99629433562732, 54.7358390682901, 55.1064055055585, 55.12122816304923, 55.24827951296982, 55.10428798305982, 54.80359978824775, 55.32027527792483, 55.24827951296982, 55.396506087877185, 55.381683430386445, 55.74166225516146, 55.40497617787189, 55.63366860772896, 55.612493382742194, 55.824245632609845, 55.7564849126522, 55.7924827951297, 55.32239280042351, 55.167813658020115, 56.09317098994177, 55.591318157755424, 55.86659608258338, 55.6209634727369, 55.80307040762308, 56.283748014822656, 55.716251985177344, 55.90471148755955, 55.8920063525675, 55.86871360508206, 55.71201694017999, 55.942826892535734, 55.709899417681314, 55.73742721016411, 56.046585494970884, 56.08681842244574, 55.97035468501853, 56.169401799894125, 55.99152991000529, 56.07411328745368, 56.08046585494971, 56.167284277395446, 55.95129698253044, 55.86024351508735, 55.86659608258338, 56.31339332980413, 56.14187400741133, 56.5780836421387, 55.98517734250927, 56.08470089994706, 56.52938062466914, 56.19269454737957, 56.383271572260455, 56.21810481736368, 56.30915828480678, 56.51032292218105, 55.921651667548964, 56.26892535733192, 56.58231868713605, 56.188459502382216, 56.18422445738486, 56.63313922710429, 56.02329274748544, 56.609846479618845, 56.45738485971413, 55.97035468501853, 56.34727368978295, 56.715722604552674, 56.376919004764424, 56.700899947061934, 56.5865537321334, 56.741132874536795, 56.393859184753836, 56.2435150873478, 56.271042879830595, 56.18634197988354, 56.497617787188986, 56.46373742721016, 56.59290629962943, 56.599258867125464, 56.389624139756485, 56.32609846479619, 56.713605082053995, 56.80889359449444, 56.6860772895712, 56.364213869772364, 56.80889359449444, 57.052408681842245, 56.961355214399155, 56.67125463208047, 56.62678665960826, 56.607728957120166, 56.728427739544735, 56.4785600847009, 57.08628904182107, 56.77077818951826, 56.707252514557965, 56.99947061937533, 56.59290629962943, 56.87241926945474, 56.87877183695077, 56.91900476442562, 56.77289571201694, 56.364213869772364, 56.62678665960826, 57.058761249338275, 57.060878771836954, 56.70513499205929, 57.23875066172578, 57.2641609317099, 57.08628904182107, 56.84065643197459, 57.45262043409211, 56.952885124404446, 57.03758602435151, 56.90206458443621, 56.99947061937533, 57.00582318687136, 56.64584436209635, 57.287453679195345, 56.823716251985175, 56.96559025939651, 57.01429327686607, 56.99100052938063, 57.18369507676019, 56.851244044467975, 57.31074642668079, 57.298041291688726, 57.31286394917946, 57.187930121757546, 57.166754896770776, 56.98676548438327, 57.52249867654844, 57.19004764425622, 57.298041291688726, 57.427210164108, 57.54579142403388, 57.259925886712544, 57.2641609317099, 57.833774483853894, 57.21757543673902, 57.47803070407623, 57.53308628904182, 57.66437268395977, 57.71095817893065, 57.79989412387506, 57.615669666490206, 57.22604552673372, 57.63896241397565, 57.84647961884595, 57.592376919004764, 57.58390682901006, 57.54579142403388, 57.706723133933295, 57.950238221281104, 58.00952885124404, 57.52461619904712, 57.56273160402329, 57.723663313922714, 57.58602435150873, 58.13234515616728, 58.10693488618317, 58.04129168872419, 57.98411858125993, 57.725780836421386, 57.825304393859184, 57.60084700899947, 57.50344097406035, 57.518263631551086, 58.155637903652725, 57.84012705134992, 58.11116993118052, 57.935415563790365, 57.823186871360505, 57.90788777130757, 58.026469031233454, 57.973530968766546, 58.08152461619905, 57.90577024880889, 58.053996823716254, 58.02435150873478, 58.13658020116464, 58.20645844362096, 57.96294335627316, 58.045526733721545, 57.973530968766546, 58.47326627845421, 58.166225516146106, 58.519851773425096, 58.01799894123875, 58.382212811011115, 57.89941768131286, 58.485971413446265, 58.187400741132876, 58.10693488618317, 58.21281101111699, 58.56008470089995, 58.4203282159873, 58.23398623610376, 58.6638433033351, 58.492323980942295, 58.157755426151404, 58.64478560084701, 58.6723133933298, 58.481736368448914, 58.53679195341451, 58.513499205929065, 58.54526204340921, 58.21704605611435, 58.259396506087874, 58.75066172578084, 58.51773425092642, 58.636315510852306, 59.051349920592905, 58.829010058231866, 58.723133933298044, 58.729486500794074, 58.394917946003176, 59.00052938062467, 58.64055055584966, 58.90524086818422, 59.24192694547379, 58.80359978824775, 58.87559555320275, 59.1148755955532, 59.163578613022764, 59.299100052938066, 58.67443091582848, 58.6998411858126, 58.69348861831657, 58.8713605082054, 58.89677077818952, 58.95817893065114, 58.721016410799365, 58.80995235574378, 58.856537850714666, 59.12758073054526, 59.06829010058232, 59.055584965590256, 59.11275807305453, 58.835362625727896, 59.03440974060349, 59.00264690312335, 59.260984647961884, 59.02593965060879, 58.76124933827422, 59.19745897300159, 59.41979883536263, 59.167813658020115, 59.260984647961884, 59.22286924298571, 58.9073583906829, 59.19957649550027, 59.09581789306512, 58.96876654314452, 58.85230280571731, 58.96453149814717, 59.24616199047115, 59.34568554790895, 59.03017469560614, 59.30121757543674, 59.47697194282689, 59.174166225516146, 59.22075172048703, 59.12758073054526, 59.06829010058232, 59.10428798305982, 59.43885653785072, 59.37321334039174, 59.14452091053467, 59.273689782953944, 59.03017469560614, 59.5489677077819, 59.072525145579675, 59.214399152991, 59.12969825304394, 59.25251455796718, 59.19110640550556, 59.481206987824244, 59.34356802541027]\n",
            "test_loss_list_drop = [2.1144830187161765, 1.770705868800481, 1.354537381845362, 1.374193787574768, 1.269173956969205, 1.2487657707111508, 1.3196489787569232, 1.2091097261975794, 1.2207342497273987, 1.2468897922366273, 1.2101589245539086, 1.1896818405857272, 1.1665738452883327, 1.1898313062448127, 1.167635775956453, 1.1819146123586917, 1.1768447099363102, 1.1463913251371944, 1.1678617459301854, 1.1286959490355324, 1.142105237526052, 1.1536669786654266, 1.1455377363691144, 1.1280341978166617, 1.1673410149181591, 1.1291693218198477, 1.1233090556719725, 1.1226905750877716, 1.1175417350787742, 1.101983927044214, 1.114773612980749, 1.1577605964506374, 1.1099497921326582, 1.1291023641824722, 1.140588772647521, 1.1034612421895944, 1.1242947169378692, 1.1394709813244202, 1.119138209258809, 1.12652507073739, 1.1245381195171207, 1.094589379488253, 1.119369665781657, 1.1165281671519374, 1.1264003445120419, 1.1268001143254487, 1.1461899677912395, 1.1140161150810766, 1.1107007910807927, 1.1074825688904406, 1.108308892624051, 1.094352656135372, 1.1150785672898387, 1.0991073133898717, 1.0917623297256582, 1.0943028500267105, 1.1043529005027284, 1.1154845877020967, 1.0956647778842963, 1.1110237877742917, 1.0882367012547511, 1.1033515564951242, 1.1030048958811105, 1.0978559787366904, 1.1059362508502661, 1.1258834977360332, 1.0856871990596546, 1.0888648661328297, 1.1024866194701661, 1.0961040994700264, 1.0940976408766765, 1.091312126786101, 1.0945143512651032, 1.0899952042336558, 1.0977078103551678, 1.0727955243166756, 1.1112479231521195, 1.083779447218951, 1.0883180721133363, 1.081604456200319, 1.1069675511589236, 1.1113862325163448, 1.08329469286928, 1.1153728567502077, 1.0817266495204438, 1.0864259752572751, 1.0797949421639537, 1.0766122563212526, 1.085480871738172, 1.0766295869560802, 1.0755425443251927, 1.1023936102203293, 1.074645878053179, 1.0792558263329899, 1.0751844930882548, 1.084583418918591, 1.0630807026344187, 1.102461565358966, 1.0763371431944417, 1.0680023650912678, 1.0833880670514762, 1.0685595116194557, 1.0700204325657265, 1.0781285526121365, 1.0630561113357544, 1.088389485782268, 1.0853935868716706, 1.0706258831070918, 1.0689591449849747, 1.076810972363341, 1.0848006270095414, 1.0811175312481673, 1.0611938655960793, 1.0559648900639778, 1.0650489786091972, 1.0634413119040282, 1.0575937546935736, 1.0695800287466424, 1.067198390177652, 1.062145631687314, 1.0700830107226091, 1.0637324770876007, 1.0750169882587357, 1.06703304280253, 1.068343691966113, 1.0817719399929047, 1.0417305917716493, 1.0599698961949815, 1.071448670590625, 1.0661738950247859, 1.0650349817439622, 1.0572247689261156, 1.0796616322853987, 1.057907622234494, 1.0717077564959432, 1.0618412661201813, 1.0777777959318722, 1.058915902002185, 1.05283157322921, 1.0593377679002052, 1.0443851752024071, 1.0535672099566926, 1.051210990431262, 1.0717861929944916, 1.0503237460758172, 1.0444728504208958, 1.054432161006273, 1.0400924381672167, 1.064410779113863, 1.0525216352121503, 1.0491984489501691, 1.045340265713486, 1.0562218100416894, 1.0447716704186272, 1.0538054950097029, 1.056641648797428, 1.0590826348931182, 1.0559423729485156, 1.033347575395715, 1.0375388472103606, 1.0366858439702613, 1.0469530628008001, 1.0502466273074056, 1.036207514358502, 1.067028955209489, 1.0487351458446652, 1.0452506594798143, 1.0442006576294993, 1.0317357740565842, 1.0378813851697772, 1.0531027676428066, 1.0377793466927958, 1.0434763656527388, 1.0363010626797582, 1.032071295614336, 1.0364813147222294, 1.0200531605411978, 1.041566525312031, 1.038747032191239, 1.0305369207087685, 1.0415644949557734, 1.0248307495140563, 1.031455700303994, 1.0360358483066745, 1.0307040877786338, 1.023797578963579, 1.0264523488049413, 1.030638714923578, 1.0347188845569013, 1.0293481130810345, 1.031280215756566, 1.0292902369709576, 1.0334156865582746, 1.031577334684484, 1.0269628357069165, 1.0262205425430746, 1.0311612942639519, 1.0282401597382975, 1.01918892854569, 1.0326045102932875, 1.0266494137399338, 1.0266031354081397, 1.0255042562882106, 1.0192927028618606, 1.0265945044218325, 1.033088362684437, 1.0299573669246598, 1.0328738254074956, 1.0142579215998744, 1.0358880571290559, 1.0153153670184754, 1.016160689440428, 1.023149904959342, 1.019488983294543, 1.0170754033560847, 1.0134304808635337, 1.016931656236742, 1.012284860307095, 1.0089451068756627, 1.016482996005638, 1.0185778997108048, 1.0158315102259319, 1.023031898573333, 1.0122722004558526, 1.0160985562731237, 1.0133712399823993, 1.01100500395485, 1.0044899582862854, 1.0161759789083518, 1.0111533143356735, 1.0097735492037792, 1.0130605466809928, 1.00693079040331, 1.0013298807190913, 1.014782198211726, 1.009178220933559, 1.0220976109598197, 1.0055661785836314, 1.0202039015643738, 1.0140373724348404, 1.0104320841092689, 1.021941000924391, 0.9992682054346683, 1.0108502372807147, 1.0068416262374205, 1.014936113182236, 1.014849027874423, 1.0151805611802083, 1.0071095827163434, 1.0103699877565981, 1.0065367975655723, 1.015264817312652, 1.0000949396806604, 1.0138824883044935, 1.0014201008221681, 1.0104307102806427, 0.9973147453046313, 1.003813914516393, 1.0063805366848029, 1.0097704421655804, 1.013560435059024, 0.994986776043387, 1.0009713961797602, 1.0014018556650948, 1.0021180960477567, 1.0156038704456067, 1.0040169892942203, 1.0012714500520743, 0.9971654491097319, 1.0015128380527683, 1.000653420008865, 1.0046796012742847, 0.9967991408764147, 1.0000053013072294, 1.0075030046350815, 1.002073511481285, 1.0004754829056122, 1.002359372143652, 0.9977667007376166, 1.0001270879133075, 1.0024964450036777, 1.0000760324445426, 1.0075691716343749, 1.0004645562639423, 1.0185351894766677, 0.998321448471032, 0.994868344243835, 1.0064027674642264, 0.9990221223994797, 0.9954676566755071, 1.0027908709119349, 0.9956688986105078, 0.9898490306793475, 1.006957445951069, 0.9906078004369548, 1.0091955571197997, 1.004394779310507, 1.0068047683612973, 1.0027133614993562, 0.9996551611259872]\n",
            "test_acc_list_drop = [23.244468346650276, 38.249078057775044, 49.435310387215736, 49.13952059004303, 51.413644744929314, 52.12046711739398, 51.32529194837124, 53.234480639213274, 52.41241548862938, 52.339428395820526, 53.130762138905965, 53.35740626920713, 54.594345421020286, 53.53027043638598, 54.08343577135833, 53.63783036263061, 53.96819299323909, 54.46757836508912, 54.110325752919486, 54.7480024585126, 54.19483712354026, 53.84142593730793, 54.60586969883221, 54.74031960663798, 53.77228027043638, 54.91702519975415, 55.235863552550704, 55.132145052243395, 55.09757221880762, 55.57775046097111, 55.09373079287031, 54.24861708666257, 55.67762753534112, 54.61739397664413, 54.72879532882606, 55.82744314689613, 55.178242163491085, 54.95159803318992, 55.001536570374924, 54.701905347264905, 55.185925015365704, 56.01183159188691, 54.79409956976029, 55.33574062692071, 55.28580208973571, 55.16287645974186, 54.091118623232944, 55.43945912722803, 55.666103257529194, 55.251229256299936, 55.72372464658881, 55.88890596189305, 55.03226797787339, 55.62768899815612, 55.77366318377382, 55.45482483097726, 55.20129071911494, 55.477873386601104, 56.08097725875845, 55.185925015365704, 56.26152427781192, 55.431776275353414, 55.7659803318992, 55.84280885064536, 55.70067609096497, 54.8939766441303, 55.908113091579594, 55.796711739397665, 55.87354025814383, 55.62384757221881, 55.67762753534112, 55.904271665642284, 55.84280885064536, 55.94652735095267, 55.82360172095882, 56.465119852489245, 55.255070682237246, 56.26536570374923, 56.223110018438845, 55.977258758451136, 55.316533497234175, 55.132145052243395, 55.87738168408113, 55.50476336816226, 56.034880147510755, 56.338352796558084, 56.038721573448065, 56.13091579594345, 55.74677320221266, 56.37292562999385, 56.07329440688383, 55.51244622003688, 56.457437000614625, 56.26152427781192, 56.19237861094038, 55.792870313460355, 56.342194222495394, 55.62768899815612, 56.3460356484327, 56.645666871542716, 55.95421020282729, 56.338352796558084, 56.42286416717886, 56.48048555623848, 56.61493546404425, 56.06561155500922, 55.43945912722803, 56.12707437000615, 56.56883835279656, 56.24615857406269, 55.65457897971727, 55.858174554394594, 56.999078057775044, 56.945298094652735, 56.32298709280885, 56.45359557467732, 56.7340196681008, 56.757068223724644, 56.276889981561155, 56.5381069452981, 56.42286416717886, 56.699446834665025, 56.2039028887523, 56.17317148125384, 56.96066379840197, 56.299938537185, 57.33328211432084, 56.84542102028273, 56.19622003687769, 56.223110018438845, 56.28841425937308, 56.641825445605406, 55.68915181315304, 56.72249539028888, 56.226951444376155, 56.31530424093423, 55.5700676090965, 56.66487400122926, 56.80316533497234, 56.50353411186232, 57.137369391518135, 56.833896742470806, 57.01444376152428, 56.05792870313461, 56.66871542716657, 57.1719422249539, 56.68023970497849, 57.44084204056546, 56.038721573448065, 56.760909649661954, 57.129686539643515, 56.90304240934235, 56.48048555623848, 56.61493546404425, 56.653349723417335, 56.32298709280885, 56.407498463429626, 56.584204056545786, 57.375537799631225, 57.04901659496005, 57.187307928703135, 56.84926244622004, 56.61493546404425, 57.321757836508915, 56.576521204671174, 56.98755377996312, 56.707129686539645, 56.411339889366936, 57.506146281499696, 57.244929317762754, 56.334511370620774, 57.32559926244622, 57.502304855562386, 56.949139520590045, 57.44468346650277, 57.30639213275968, 57.96711739397664, 56.71481253841426, 57.137369391518135, 57.244929317762754, 56.91456668715427, 57.29102642901045, 57.187307928703135, 57.317916410571605, 57.371696373693915, 57.62523048555624, 57.375537799631225, 57.32944068838353, 56.9299323909035, 57.30255070682237, 57.022126613398896, 57.43700061462815, 57.15273509526736, 57.46389059618931, 57.39090350338046, 57.321757836508915, 57.37937922556853, 56.999078057775044, 57.425476336816224, 56.949139520590045, 57.129686539643515, 57.39858635525507, 57.317916410571605, 57.78657037492317, 57.20651505838968, 56.833896742470806, 57.44084204056546, 56.92224953902889, 57.460049170252, 56.96066379840197, 57.47541487400123, 57.63291333743086, 57.30639213275968, 57.433159188690844, 57.30255070682237, 57.744314689612786, 57.64059618930547, 57.71742470805163, 57.64443761524278, 57.667486170866624, 57.63291333743086, 57.64059618930547, 57.46389059618931, 57.6060233558697, 57.690534726490476, 57.252612169637366, 57.39858635525507, 57.99016594960049, 57.52919483712354, 57.89028887523049, 57.89028887523049, 57.256453595574676, 57.721266133988934, 58.02473878303626, 57.70974185617701, 57.82882606023356, 57.248770743700064, 58.163030116779346, 57.375537799631225, 57.798094652735095, 57.57529194837124, 56.98371235402581, 58.278272894898585, 57.314074984634296, 57.77504609711125, 57.59065765212047, 57.563767670559315, 57.54840196681008, 58.2360172095882, 57.54456054087277, 57.94022741241549, 57.71742470805163, 58.1860786724032, 57.460049170252, 58.05931161647204, 57.59834050399508, 57.95559311616472, 57.609864781807005, 57.690534726490476, 57.82498463429625, 57.260295021511986, 58.11309157959435, 57.87492317148126, 57.736631837738166, 58.14382298709281, 57.314074984634296, 57.54456054087277, 57.86339889366933, 58.23217578365089, 58.332052858020894, 58.08236017209588, 57.71358328211432, 58.1860786724032, 57.62523048555624, 57.982483097725876, 57.851874615857405, 58.097725875845114, 57.96327596803933, 58.19376152427781, 58.12077443146896, 57.76352181929932, 57.99784880147511, 57.613706207744315, 57.96711739397664, 57.51382913337431, 58.17839582052858, 58.31284572833436, 57.928703134603566, 58.10540872771973, 58.07083589428396, 58.13614013521819, 58.02089735709895, 58.37046711739398, 57.64827904118009, 58.32821143208359, 57.667486170866624, 57.94022741241549, 57.89413030116779, 57.99784880147511, 58.29748002458513]\n"
          ]
        }
      ]
    }
  ]
}