{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9ac32ba27af442eda4b86e08522a3856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f82873d8cdd043c5b89df840ab759afd",
              "IPY_MODEL_ddc8ca832fca48f0b8490fde0e8777d4",
              "IPY_MODEL_062d2faa611549a4bdd0ad3e2a098cae"
            ],
            "layout": "IPY_MODEL_3d1c8a13d2254f2a9212eaec8d5480c2"
          }
        },
        "f82873d8cdd043c5b89df840ab759afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bb8df656f374f6084d4f95add809d47",
            "placeholder": "​",
            "style": "IPY_MODEL_3d2e400df59640788a63ad30638ec7b8",
            "value": "100%"
          }
        },
        "ddc8ca832fca48f0b8490fde0e8777d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95b428dc72f4451da1616b6dd520993e",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd1e9650c32c40efaf3ec2c6e5b1918e",
            "value": 182040794
          }
        },
        "062d2faa611549a4bdd0ad3e2a098cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48ea38c2e2414f138eeb3a16bfe5e853",
            "placeholder": "​",
            "style": "IPY_MODEL_8e7eaffc3b3d44b392bfeaf5003f12f0",
            "value": " 182040794/182040794 [00:04&lt;00:00, 55345966.17it/s]"
          }
        },
        "3d1c8a13d2254f2a9212eaec8d5480c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bb8df656f374f6084d4f95add809d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d2e400df59640788a63ad30638ec7b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95b428dc72f4451da1616b6dd520993e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd1e9650c32c40efaf3ec2c6e5b1918e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48ea38c2e2414f138eeb3a16bfe5e853": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e7eaffc3b3d44b392bfeaf5003f12f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f1549eeef8c43d48faf9d91985a82be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c148f49813d34530ab199f112d885f74",
              "IPY_MODEL_13b2fc47ef274941b3b74b89b31ea42c",
              "IPY_MODEL_d2437ae8e22c4d558175e3d4fb55d690"
            ],
            "layout": "IPY_MODEL_7f15a63030134f15b345b81b7f26be93"
          }
        },
        "c148f49813d34530ab199f112d885f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18e83a887ff34ffda067af1869bcc039",
            "placeholder": "​",
            "style": "IPY_MODEL_9c12734234f2460ca9f9da29a62f1451",
            "value": "100%"
          }
        },
        "13b2fc47ef274941b3b74b89b31ea42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8dfed461fc8460f9b5b0b9c2bc6b263",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65bc5cc737a34251904bd6e29dddd6cd",
            "value": 64275384
          }
        },
        "d2437ae8e22c4d558175e3d4fb55d690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_786836ac7cc544e4a4a5ffb51c974532",
            "placeholder": "​",
            "style": "IPY_MODEL_bca313b528284e9598891c07c341be27",
            "value": " 64275384/64275384 [00:02&lt;00:00, 53946946.96it/s]"
          }
        },
        "7f15a63030134f15b345b81b7f26be93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18e83a887ff34ffda067af1869bcc039": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c12734234f2460ca9f9da29a62f1451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8dfed461fc8460f9b5b0b9c2bc6b263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65bc5cc737a34251904bd6e29dddd6cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "786836ac7cc544e4a4a5ffb51c974532": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bca313b528284e9598891c07c341be27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh40Mux0nJr8"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.SVHN),\n",
        "    transforms.ToTensor(),\n",
        "    \n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "nAJA8iggnSJl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "9ac32ba27af442eda4b86e08522a3856",
            "f82873d8cdd043c5b89df840ab759afd",
            "ddc8ca832fca48f0b8490fde0e8777d4",
            "062d2faa611549a4bdd0ad3e2a098cae",
            "3d1c8a13d2254f2a9212eaec8d5480c2",
            "0bb8df656f374f6084d4f95add809d47",
            "3d2e400df59640788a63ad30638ec7b8",
            "95b428dc72f4451da1616b6dd520993e",
            "cd1e9650c32c40efaf3ec2c6e5b1918e",
            "48ea38c2e2414f138eeb3a16bfe5e853",
            "8e7eaffc3b3d44b392bfeaf5003f12f0",
            "9f1549eeef8c43d48faf9d91985a82be",
            "c148f49813d34530ab199f112d885f74",
            "13b2fc47ef274941b3b74b89b31ea42c",
            "d2437ae8e22c4d558175e3d4fb55d690",
            "7f15a63030134f15b345b81b7f26be93",
            "18e83a887ff34ffda067af1869bcc039",
            "9c12734234f2460ca9f9da29a62f1451",
            "f8dfed461fc8460f9b5b0b9c2bc6b263",
            "65bc5cc737a34251904bd6e29dddd6cd",
            "786836ac7cc544e4a4a5ffb51c974532",
            "bca313b528284e9598891c07c341be27"
          ]
        },
        "outputId": "2d088611-4405-4d32-bb55-576de10f5584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ac32ba27af442eda4b86e08522a3856"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f1549eeef8c43d48faf9d91985a82be"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))\n",
        "print(trainset[4][1])"
      ],
      "metadata": {
        "id": "hGZZIgPPnSr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b0836f-1aed-41d4-f2a5-35d0381256f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "metadata": {
        "id": "10WNPH8DnS1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "metadata": {
        "id": "Dr_sTY--nTPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wkILG8tUnTag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        #out = F.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        #out = F.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        #out = F.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "metadata": {
        "id": "C3Xw_r4uncvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OBUsUSuInc4t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "7a22feca-98e8-482c-e240-1a59e95717f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "id": "U1H0AggZnc_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa68f202-d9e6-452e-fd95-63f41e838826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.001).to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "metadata": {
        "id": "wsPOwpqBnh-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07aae83b-b0b2-4e4b-cf64-2762661e17ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3539, accuracy : 16.36\n",
            "iteration : 100, loss : 2.2830, accuracy : 18.62\n",
            "iteration : 150, loss : 2.1056, accuracy : 25.45\n",
            "iteration : 200, loss : 1.8469, accuracy : 35.11\n",
            "iteration : 250, loss : 1.6241, accuracy : 43.36\n",
            "iteration : 300, loss : 1.4514, accuracy : 49.76\n",
            "iteration : 350, loss : 1.3215, accuracy : 54.56\n",
            "Epoch :   1, training loss : 1.2821, training accuracy : 55.97, test loss : 0.6926, test accuracy : 77.97\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.4933, accuracy : 84.52\n",
            "iteration : 100, loss : 0.4805, accuracy : 85.16\n",
            "iteration : 150, loss : 0.4774, accuracy : 85.30\n",
            "iteration : 200, loss : 0.4731, accuracy : 85.46\n",
            "iteration : 250, loss : 0.4703, accuracy : 85.57\n",
            "iteration : 300, loss : 0.4646, accuracy : 85.80\n",
            "iteration : 350, loss : 0.4612, accuracy : 85.85\n",
            "Epoch :   2, training loss : 0.4607, training accuracy : 85.88, test loss : 0.6267, test accuracy : 81.02\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.4152, accuracy : 87.19\n",
            "iteration : 100, loss : 0.4235, accuracy : 87.08\n",
            "iteration : 150, loss : 0.4251, accuracy : 87.08\n",
            "iteration : 200, loss : 0.4159, accuracy : 87.36\n",
            "iteration : 250, loss : 0.4097, accuracy : 87.51\n",
            "iteration : 300, loss : 0.4122, accuracy : 87.48\n",
            "iteration : 350, loss : 0.4088, accuracy : 87.66\n",
            "Epoch :   3, training loss : 0.4084, training accuracy : 87.67, test loss : 0.4128, test accuracy : 87.73\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.3844, accuracy : 88.52\n",
            "iteration : 100, loss : 0.3842, accuracy : 88.68\n",
            "iteration : 150, loss : 0.3838, accuracy : 88.54\n",
            "iteration : 200, loss : 0.3814, accuracy : 88.58\n",
            "iteration : 250, loss : 0.3810, accuracy : 88.59\n",
            "iteration : 300, loss : 0.3804, accuracy : 88.60\n",
            "iteration : 350, loss : 0.3794, accuracy : 88.66\n",
            "Epoch :   4, training loss : 0.3793, training accuracy : 88.69, test loss : 0.4427, test accuracy : 86.48\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.3607, accuracy : 89.34\n",
            "iteration : 100, loss : 0.3586, accuracy : 89.48\n",
            "iteration : 150, loss : 0.3611, accuracy : 89.28\n",
            "iteration : 200, loss : 0.3673, accuracy : 89.09\n",
            "iteration : 250, loss : 0.3651, accuracy : 89.17\n",
            "iteration : 300, loss : 0.3671, accuracy : 89.15\n",
            "iteration : 350, loss : 0.3643, accuracy : 89.20\n",
            "Epoch :   5, training loss : 0.3636, training accuracy : 89.22, test loss : 0.4414, test accuracy : 86.74\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.3354, accuracy : 89.86\n",
            "iteration : 100, loss : 0.3443, accuracy : 89.88\n",
            "iteration : 150, loss : 0.3444, accuracy : 89.87\n",
            "iteration : 200, loss : 0.3457, accuracy : 89.77\n",
            "iteration : 250, loss : 0.3485, accuracy : 89.68\n",
            "iteration : 300, loss : 0.3487, accuracy : 89.76\n",
            "iteration : 350, loss : 0.3460, accuracy : 89.85\n",
            "Epoch :   6, training loss : 0.3458, training accuracy : 89.85, test loss : 0.4257, test accuracy : 87.40\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.3163, accuracy : 90.55\n",
            "iteration : 100, loss : 0.3324, accuracy : 90.13\n",
            "iteration : 150, loss : 0.3282, accuracy : 90.39\n",
            "iteration : 200, loss : 0.3330, accuracy : 90.26\n",
            "iteration : 250, loss : 0.3307, accuracy : 90.28\n",
            "iteration : 300, loss : 0.3274, accuracy : 90.39\n",
            "iteration : 350, loss : 0.3280, accuracy : 90.38\n",
            "Epoch :   7, training loss : 0.3270, training accuracy : 90.38, test loss : 0.4052, test accuracy : 87.90\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.3263, accuracy : 90.28\n",
            "iteration : 100, loss : 0.3252, accuracy : 90.51\n",
            "iteration : 150, loss : 0.3240, accuracy : 90.59\n",
            "iteration : 200, loss : 0.3177, accuracy : 90.71\n",
            "iteration : 250, loss : 0.3199, accuracy : 90.66\n",
            "iteration : 300, loss : 0.3202, accuracy : 90.74\n",
            "iteration : 350, loss : 0.3168, accuracy : 90.80\n",
            "Epoch :   8, training loss : 0.3168, training accuracy : 90.83, test loss : 0.3939, test accuracy : 88.25\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.3212, accuracy : 90.97\n",
            "iteration : 100, loss : 0.3171, accuracy : 91.02\n",
            "iteration : 150, loss : 0.3134, accuracy : 91.16\n",
            "iteration : 200, loss : 0.3119, accuracy : 91.20\n",
            "iteration : 250, loss : 0.3103, accuracy : 91.24\n",
            "iteration : 300, loss : 0.3100, accuracy : 91.19\n",
            "iteration : 350, loss : 0.3086, accuracy : 91.14\n",
            "Epoch :   9, training loss : 0.3083, training accuracy : 91.11, test loss : 0.3466, test accuracy : 89.79\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.2612, accuracy : 92.12\n",
            "iteration : 100, loss : 0.2899, accuracy : 91.45\n",
            "iteration : 150, loss : 0.2917, accuracy : 91.33\n",
            "iteration : 200, loss : 0.2905, accuracy : 91.43\n",
            "iteration : 250, loss : 0.2928, accuracy : 91.39\n",
            "iteration : 300, loss : 0.2932, accuracy : 91.39\n",
            "iteration : 350, loss : 0.2953, accuracy : 91.35\n",
            "Epoch :  10, training loss : 0.2959, training accuracy : 91.33, test loss : 0.3816, test accuracy : 88.79\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.2963, accuracy : 91.92\n",
            "iteration : 100, loss : 0.2858, accuracy : 91.86\n",
            "iteration : 150, loss : 0.2827, accuracy : 91.94\n",
            "iteration : 200, loss : 0.2867, accuracy : 91.77\n",
            "iteration : 250, loss : 0.2857, accuracy : 91.78\n",
            "iteration : 300, loss : 0.2856, accuracy : 91.79\n",
            "iteration : 350, loss : 0.2844, accuracy : 91.82\n",
            "Epoch :  11, training loss : 0.2840, training accuracy : 91.86, test loss : 0.3193, test accuracy : 90.86\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.2739, accuracy : 92.17\n",
            "iteration : 100, loss : 0.2801, accuracy : 92.04\n",
            "iteration : 150, loss : 0.2805, accuracy : 91.98\n",
            "iteration : 200, loss : 0.2789, accuracy : 92.06\n",
            "iteration : 250, loss : 0.2786, accuracy : 92.05\n",
            "iteration : 300, loss : 0.2774, accuracy : 92.11\n",
            "iteration : 350, loss : 0.2758, accuracy : 92.13\n",
            "Epoch :  12, training loss : 0.2756, training accuracy : 92.14, test loss : 0.2958, test accuracy : 91.47\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.2656, accuracy : 92.45\n",
            "iteration : 100, loss : 0.2542, accuracy : 92.80\n",
            "iteration : 150, loss : 0.2578, accuracy : 92.64\n",
            "iteration : 200, loss : 0.2622, accuracy : 92.56\n",
            "iteration : 250, loss : 0.2650, accuracy : 92.53\n",
            "iteration : 300, loss : 0.2658, accuracy : 92.52\n",
            "iteration : 350, loss : 0.2672, accuracy : 92.46\n",
            "Epoch :  13, training loss : 0.2685, training accuracy : 92.44, test loss : 0.3228, test accuracy : 90.52\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.2520, accuracy : 92.48\n",
            "iteration : 100, loss : 0.2483, accuracy : 92.75\n",
            "iteration : 150, loss : 0.2487, accuracy : 92.76\n",
            "iteration : 200, loss : 0.2514, accuracy : 92.74\n",
            "iteration : 250, loss : 0.2542, accuracy : 92.66\n",
            "iteration : 300, loss : 0.2580, accuracy : 92.57\n",
            "iteration : 350, loss : 0.2553, accuracy : 92.65\n",
            "Epoch :  14, training loss : 0.2556, training accuracy : 92.68, test loss : 0.2959, test accuracy : 91.61\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.2511, accuracy : 92.73\n",
            "iteration : 100, loss : 0.2484, accuracy : 92.71\n",
            "iteration : 150, loss : 0.2582, accuracy : 92.45\n",
            "iteration : 200, loss : 0.2551, accuracy : 92.63\n",
            "iteration : 250, loss : 0.2528, accuracy : 92.72\n",
            "iteration : 300, loss : 0.2527, accuracy : 92.77\n",
            "iteration : 350, loss : 0.2537, accuracy : 92.71\n",
            "Epoch :  15, training loss : 0.2560, training accuracy : 92.66, test loss : 0.2872, test accuracy : 91.89\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.2670, accuracy : 92.28\n",
            "iteration : 100, loss : 0.2511, accuracy : 92.78\n",
            "iteration : 150, loss : 0.2506, accuracy : 92.79\n",
            "iteration : 200, loss : 0.2560, accuracy : 92.66\n",
            "iteration : 250, loss : 0.2553, accuracy : 92.71\n",
            "iteration : 300, loss : 0.2543, accuracy : 92.74\n",
            "iteration : 350, loss : 0.2516, accuracy : 92.84\n",
            "Epoch :  16, training loss : 0.2513, training accuracy : 92.85, test loss : 0.3020, test accuracy : 91.38\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.2253, accuracy : 93.69\n",
            "iteration : 100, loss : 0.2240, accuracy : 93.62\n",
            "iteration : 150, loss : 0.2328, accuracy : 93.32\n",
            "iteration : 200, loss : 0.2368, accuracy : 93.19\n",
            "iteration : 250, loss : 0.2385, accuracy : 93.20\n",
            "iteration : 300, loss : 0.2443, accuracy : 93.10\n",
            "iteration : 350, loss : 0.2434, accuracy : 93.12\n",
            "Epoch :  17, training loss : 0.2438, training accuracy : 93.12, test loss : 0.2696, test accuracy : 92.51\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.2353, accuracy : 93.45\n",
            "iteration : 100, loss : 0.2311, accuracy : 93.54\n",
            "iteration : 150, loss : 0.2369, accuracy : 93.31\n",
            "iteration : 200, loss : 0.2391, accuracy : 93.23\n",
            "iteration : 250, loss : 0.2358, accuracy : 93.30\n",
            "iteration : 300, loss : 0.2354, accuracy : 93.29\n",
            "iteration : 350, loss : 0.2374, accuracy : 93.28\n",
            "Epoch :  18, training loss : 0.2382, training accuracy : 93.31, test loss : 0.3072, test accuracy : 91.30\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.2026, accuracy : 93.84\n",
            "iteration : 100, loss : 0.2101, accuracy : 93.97\n",
            "iteration : 150, loss : 0.2201, accuracy : 93.81\n",
            "iteration : 200, loss : 0.2248, accuracy : 93.67\n",
            "iteration : 250, loss : 0.2310, accuracy : 93.50\n",
            "iteration : 300, loss : 0.2329, accuracy : 93.42\n",
            "iteration : 350, loss : 0.2339, accuracy : 93.40\n",
            "Epoch :  19, training loss : 0.2338, training accuracy : 93.40, test loss : 0.3117, test accuracy : 91.11\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.2332, accuracy : 93.39\n",
            "iteration : 100, loss : 0.2265, accuracy : 93.63\n",
            "iteration : 150, loss : 0.2275, accuracy : 93.62\n",
            "iteration : 200, loss : 0.2310, accuracy : 93.51\n",
            "iteration : 250, loss : 0.2275, accuracy : 93.62\n",
            "iteration : 300, loss : 0.2251, accuracy : 93.66\n",
            "iteration : 350, loss : 0.2279, accuracy : 93.54\n",
            "Epoch :  20, training loss : 0.2294, training accuracy : 93.52, test loss : 0.2872, test accuracy : 91.83\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.2083, accuracy : 94.00\n",
            "iteration : 100, loss : 0.2117, accuracy : 94.07\n",
            "iteration : 150, loss : 0.2179, accuracy : 93.87\n",
            "iteration : 200, loss : 0.2242, accuracy : 93.77\n",
            "iteration : 250, loss : 0.2291, accuracy : 93.63\n",
            "iteration : 300, loss : 0.2273, accuracy : 93.70\n",
            "iteration : 350, loss : 0.2260, accuracy : 93.73\n",
            "Epoch :  21, training loss : 0.2245, training accuracy : 93.75, test loss : 0.2940, test accuracy : 91.64\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.2198, accuracy : 93.52\n",
            "iteration : 100, loss : 0.2228, accuracy : 93.71\n",
            "iteration : 150, loss : 0.2184, accuracy : 93.82\n",
            "iteration : 200, loss : 0.2209, accuracy : 93.72\n",
            "iteration : 250, loss : 0.2231, accuracy : 93.73\n",
            "iteration : 300, loss : 0.2255, accuracy : 93.67\n",
            "iteration : 350, loss : 0.2257, accuracy : 93.67\n",
            "Epoch :  22, training loss : 0.2262, training accuracy : 93.67, test loss : 0.2766, test accuracy : 92.15\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.2275, accuracy : 93.47\n",
            "iteration : 100, loss : 0.2207, accuracy : 93.67\n",
            "iteration : 150, loss : 0.2199, accuracy : 93.70\n",
            "iteration : 200, loss : 0.2201, accuracy : 93.74\n",
            "iteration : 250, loss : 0.2209, accuracy : 93.74\n",
            "iteration : 300, loss : 0.2207, accuracy : 93.72\n",
            "iteration : 350, loss : 0.2203, accuracy : 93.73\n",
            "Epoch :  23, training loss : 0.2198, training accuracy : 93.73, test loss : 0.2964, test accuracy : 91.62\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.2183, accuracy : 93.66\n",
            "iteration : 100, loss : 0.2158, accuracy : 93.78\n",
            "iteration : 150, loss : 0.2093, accuracy : 94.00\n",
            "iteration : 200, loss : 0.2114, accuracy : 94.00\n",
            "iteration : 250, loss : 0.2155, accuracy : 93.89\n",
            "iteration : 300, loss : 0.2154, accuracy : 93.91\n",
            "iteration : 350, loss : 0.2159, accuracy : 93.88\n",
            "Epoch :  24, training loss : 0.2171, training accuracy : 93.85, test loss : 0.2681, test accuracy : 92.43\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.2093, accuracy : 94.05\n",
            "iteration : 100, loss : 0.2091, accuracy : 94.09\n",
            "iteration : 150, loss : 0.2071, accuracy : 94.18\n",
            "iteration : 200, loss : 0.2087, accuracy : 94.14\n",
            "iteration : 250, loss : 0.2113, accuracy : 94.03\n",
            "iteration : 300, loss : 0.2107, accuracy : 94.05\n",
            "iteration : 350, loss : 0.2114, accuracy : 94.04\n",
            "Epoch :  25, training loss : 0.2135, training accuracy : 93.98, test loss : 0.2787, test accuracy : 92.12\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.2070, accuracy : 94.16\n",
            "iteration : 100, loss : 0.2081, accuracy : 94.13\n",
            "iteration : 150, loss : 0.2071, accuracy : 94.12\n",
            "iteration : 200, loss : 0.2093, accuracy : 94.13\n",
            "iteration : 250, loss : 0.2137, accuracy : 93.97\n",
            "iteration : 300, loss : 0.2124, accuracy : 93.94\n",
            "iteration : 350, loss : 0.2124, accuracy : 93.94\n",
            "Epoch :  26, training loss : 0.2129, training accuracy : 93.91, test loss : 0.2665, test accuracy : 92.44\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.1922, accuracy : 94.34\n",
            "iteration : 100, loss : 0.2004, accuracy : 94.07\n",
            "iteration : 150, loss : 0.2006, accuracy : 94.25\n",
            "iteration : 200, loss : 0.2041, accuracy : 94.25\n",
            "iteration : 250, loss : 0.2048, accuracy : 94.24\n",
            "iteration : 300, loss : 0.2058, accuracy : 94.20\n",
            "iteration : 350, loss : 0.2053, accuracy : 94.25\n",
            "Epoch :  27, training loss : 0.2060, training accuracy : 94.24, test loss : 0.2505, test accuracy : 93.00\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.1836, accuracy : 94.92\n",
            "iteration : 100, loss : 0.1917, accuracy : 94.66\n",
            "iteration : 150, loss : 0.1966, accuracy : 94.43\n",
            "iteration : 200, loss : 0.2037, accuracy : 94.25\n",
            "iteration : 250, loss : 0.2031, accuracy : 94.27\n",
            "iteration : 300, loss : 0.2030, accuracy : 94.31\n",
            "iteration : 350, loss : 0.2037, accuracy : 94.23\n",
            "Epoch :  28, training loss : 0.2044, training accuracy : 94.22, test loss : 0.2650, test accuracy : 92.66\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.1933, accuracy : 94.78\n",
            "iteration : 100, loss : 0.1938, accuracy : 94.73\n",
            "iteration : 150, loss : 0.1970, accuracy : 94.61\n",
            "iteration : 200, loss : 0.1961, accuracy : 94.55\n",
            "iteration : 250, loss : 0.2005, accuracy : 94.41\n",
            "iteration : 300, loss : 0.2017, accuracy : 94.35\n",
            "iteration : 350, loss : 0.2035, accuracy : 94.29\n",
            "Epoch :  29, training loss : 0.2044, training accuracy : 94.26, test loss : 0.2763, test accuracy : 92.34\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.1944, accuracy : 94.53\n",
            "iteration : 100, loss : 0.1972, accuracy : 94.48\n",
            "iteration : 150, loss : 0.2003, accuracy : 94.29\n",
            "iteration : 200, loss : 0.2012, accuracy : 94.25\n",
            "iteration : 250, loss : 0.2037, accuracy : 94.17\n",
            "iteration : 300, loss : 0.2060, accuracy : 94.15\n",
            "iteration : 350, loss : 0.2067, accuracy : 94.13\n",
            "Epoch :  30, training loss : 0.2063, training accuracy : 94.16, test loss : 0.2596, test accuracy : 92.92\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.1994, accuracy : 94.33\n",
            "iteration : 100, loss : 0.1963, accuracy : 94.36\n",
            "iteration : 150, loss : 0.1966, accuracy : 94.35\n",
            "iteration : 200, loss : 0.1971, accuracy : 94.38\n",
            "iteration : 250, loss : 0.2010, accuracy : 94.27\n",
            "iteration : 300, loss : 0.2009, accuracy : 94.27\n",
            "iteration : 350, loss : 0.2009, accuracy : 94.30\n",
            "Epoch :  31, training loss : 0.2005, training accuracy : 94.31, test loss : 0.2567, test accuracy : 92.81\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.1862, accuracy : 94.83\n",
            "iteration : 100, loss : 0.1891, accuracy : 94.76\n",
            "iteration : 150, loss : 0.1961, accuracy : 94.62\n",
            "iteration : 200, loss : 0.2002, accuracy : 94.52\n",
            "iteration : 250, loss : 0.1992, accuracy : 94.51\n",
            "iteration : 300, loss : 0.2027, accuracy : 94.38\n",
            "iteration : 350, loss : 0.2008, accuracy : 94.40\n",
            "Epoch :  32, training loss : 0.2020, training accuracy : 94.37, test loss : 0.2692, test accuracy : 92.57\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.1886, accuracy : 94.86\n",
            "iteration : 100, loss : 0.1877, accuracy : 94.80\n",
            "iteration : 150, loss : 0.1922, accuracy : 94.66\n",
            "iteration : 200, loss : 0.1928, accuracy : 94.70\n",
            "iteration : 250, loss : 0.1964, accuracy : 94.56\n",
            "iteration : 300, loss : 0.1985, accuracy : 94.52\n",
            "iteration : 350, loss : 0.1984, accuracy : 94.47\n",
            "Epoch :  33, training loss : 0.1985, training accuracy : 94.46, test loss : 0.2511, test accuracy : 92.91\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.1914, accuracy : 94.42\n",
            "iteration : 100, loss : 0.1819, accuracy : 94.76\n",
            "iteration : 150, loss : 0.1858, accuracy : 94.67\n",
            "iteration : 200, loss : 0.1886, accuracy : 94.59\n",
            "iteration : 250, loss : 0.1908, accuracy : 94.59\n",
            "iteration : 300, loss : 0.1927, accuracy : 94.54\n",
            "iteration : 350, loss : 0.1950, accuracy : 94.46\n",
            "Epoch :  34, training loss : 0.1955, training accuracy : 94.46, test loss : 0.2459, test accuracy : 93.22\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.1716, accuracy : 95.28\n",
            "iteration : 100, loss : 0.1754, accuracy : 95.17\n",
            "iteration : 150, loss : 0.1821, accuracy : 95.01\n",
            "iteration : 200, loss : 0.1829, accuracy : 94.98\n",
            "iteration : 250, loss : 0.1857, accuracy : 94.92\n",
            "iteration : 300, loss : 0.1884, accuracy : 94.85\n",
            "iteration : 350, loss : 0.1904, accuracy : 94.77\n",
            "Epoch :  35, training loss : 0.1903, training accuracy : 94.77, test loss : 0.2451, test accuracy : 93.14\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.1945, accuracy : 94.55\n",
            "iteration : 100, loss : 0.1919, accuracy : 94.62\n",
            "iteration : 150, loss : 0.1919, accuracy : 94.67\n",
            "iteration : 200, loss : 0.1889, accuracy : 94.72\n",
            "iteration : 250, loss : 0.1913, accuracy : 94.67\n",
            "iteration : 300, loss : 0.1930, accuracy : 94.59\n",
            "iteration : 350, loss : 0.1949, accuracy : 94.57\n",
            "Epoch :  36, training loss : 0.1944, training accuracy : 94.57, test loss : 0.2672, test accuracy : 92.42\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.1779, accuracy : 94.81\n",
            "iteration : 100, loss : 0.1771, accuracy : 95.01\n",
            "iteration : 150, loss : 0.1806, accuracy : 94.92\n",
            "iteration : 200, loss : 0.1824, accuracy : 94.88\n",
            "iteration : 250, loss : 0.1823, accuracy : 94.89\n",
            "iteration : 300, loss : 0.1844, accuracy : 94.82\n",
            "iteration : 350, loss : 0.1867, accuracy : 94.78\n",
            "Epoch :  37, training loss : 0.1866, training accuracy : 94.79, test loss : 0.2601, test accuracy : 92.80\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.1821, accuracy : 94.92\n",
            "iteration : 100, loss : 0.1812, accuracy : 94.88\n",
            "iteration : 150, loss : 0.1824, accuracy : 94.89\n",
            "iteration : 200, loss : 0.1840, accuracy : 94.86\n",
            "iteration : 250, loss : 0.1867, accuracy : 94.75\n",
            "iteration : 300, loss : 0.1891, accuracy : 94.72\n",
            "iteration : 350, loss : 0.1881, accuracy : 94.73\n",
            "Epoch :  38, training loss : 0.1880, training accuracy : 94.74, test loss : 0.2582, test accuracy : 92.91\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.1782, accuracy : 95.19\n",
            "iteration : 100, loss : 0.1768, accuracy : 95.13\n",
            "iteration : 150, loss : 0.1795, accuracy : 95.09\n",
            "iteration : 200, loss : 0.1793, accuracy : 95.07\n",
            "iteration : 250, loss : 0.1804, accuracy : 95.04\n",
            "iteration : 300, loss : 0.1845, accuracy : 94.90\n",
            "iteration : 350, loss : 0.1866, accuracy : 94.83\n",
            "Epoch :  39, training loss : 0.1877, training accuracy : 94.78, test loss : 0.2512, test accuracy : 93.11\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.1834, accuracy : 94.92\n",
            "iteration : 100, loss : 0.1847, accuracy : 94.80\n",
            "iteration : 150, loss : 0.1858, accuracy : 94.78\n",
            "iteration : 200, loss : 0.1877, accuracy : 94.72\n",
            "iteration : 250, loss : 0.1842, accuracy : 94.81\n",
            "iteration : 300, loss : 0.1861, accuracy : 94.78\n",
            "iteration : 350, loss : 0.1852, accuracy : 94.77\n",
            "Epoch :  40, training loss : 0.1849, training accuracy : 94.80, test loss : 0.2573, test accuracy : 93.00\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.1571, accuracy : 95.55\n",
            "iteration : 100, loss : 0.1732, accuracy : 95.24\n",
            "iteration : 150, loss : 0.1813, accuracy : 94.95\n",
            "iteration : 200, loss : 0.1833, accuracy : 94.89\n",
            "iteration : 250, loss : 0.1845, accuracy : 94.82\n",
            "iteration : 300, loss : 0.1869, accuracy : 94.74\n",
            "iteration : 350, loss : 0.1866, accuracy : 94.77\n",
            "Epoch :  41, training loss : 0.1858, training accuracy : 94.78, test loss : 0.2472, test accuracy : 93.26\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.1809, accuracy : 94.81\n",
            "iteration : 100, loss : 0.1742, accuracy : 95.20\n",
            "iteration : 150, loss : 0.1759, accuracy : 95.12\n",
            "iteration : 200, loss : 0.1786, accuracy : 95.07\n",
            "iteration : 250, loss : 0.1819, accuracy : 94.93\n",
            "iteration : 300, loss : 0.1821, accuracy : 94.93\n",
            "iteration : 350, loss : 0.1815, accuracy : 94.95\n",
            "Epoch :  42, training loss : 0.1810, training accuracy : 94.95, test loss : 0.2428, test accuracy : 93.55\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.1673, accuracy : 95.62\n",
            "iteration : 100, loss : 0.1757, accuracy : 95.19\n",
            "iteration : 150, loss : 0.1759, accuracy : 95.09\n",
            "iteration : 200, loss : 0.1794, accuracy : 95.02\n",
            "iteration : 250, loss : 0.1795, accuracy : 94.97\n",
            "iteration : 300, loss : 0.1820, accuracy : 94.91\n",
            "iteration : 350, loss : 0.1798, accuracy : 94.94\n",
            "Epoch :  43, training loss : 0.1806, training accuracy : 94.96, test loss : 0.2649, test accuracy : 92.70\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.1588, accuracy : 95.64\n",
            "iteration : 100, loss : 0.1618, accuracy : 95.45\n",
            "iteration : 150, loss : 0.1689, accuracy : 95.30\n",
            "iteration : 200, loss : 0.1707, accuracy : 95.29\n",
            "iteration : 250, loss : 0.1728, accuracy : 95.29\n",
            "iteration : 300, loss : 0.1786, accuracy : 95.12\n",
            "iteration : 350, loss : 0.1808, accuracy : 95.04\n",
            "Epoch :  44, training loss : 0.1819, training accuracy : 95.00, test loss : 0.2700, test accuracy : 92.42\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.1840, accuracy : 94.89\n",
            "iteration : 100, loss : 0.1838, accuracy : 94.88\n",
            "iteration : 150, loss : 0.1854, accuracy : 94.83\n",
            "iteration : 200, loss : 0.1849, accuracy : 94.87\n",
            "iteration : 250, loss : 0.1828, accuracy : 94.85\n",
            "iteration : 300, loss : 0.1815, accuracy : 94.88\n",
            "iteration : 350, loss : 0.1828, accuracy : 94.83\n",
            "Epoch :  45, training loss : 0.1823, training accuracy : 94.85, test loss : 0.2539, test accuracy : 92.85\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.1778, accuracy : 95.11\n",
            "iteration : 100, loss : 0.1807, accuracy : 95.11\n",
            "iteration : 150, loss : 0.1791, accuracy : 95.16\n",
            "iteration : 200, loss : 0.1762, accuracy : 95.19\n",
            "iteration : 250, loss : 0.1771, accuracy : 95.14\n",
            "iteration : 300, loss : 0.1774, accuracy : 95.13\n",
            "iteration : 350, loss : 0.1781, accuracy : 95.08\n",
            "Epoch :  46, training loss : 0.1792, training accuracy : 95.05, test loss : 0.2734, test accuracy : 92.36\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.1753, accuracy : 95.16\n",
            "iteration : 100, loss : 0.1703, accuracy : 95.23\n",
            "iteration : 150, loss : 0.1738, accuracy : 95.18\n",
            "iteration : 200, loss : 0.1760, accuracy : 95.05\n",
            "iteration : 250, loss : 0.1758, accuracy : 95.04\n",
            "iteration : 300, loss : 0.1748, accuracy : 95.13\n",
            "iteration : 350, loss : 0.1767, accuracy : 95.06\n",
            "Epoch :  47, training loss : 0.1782, training accuracy : 95.03, test loss : 0.2496, test accuracy : 93.12\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.1816, accuracy : 94.97\n",
            "iteration : 100, loss : 0.1763, accuracy : 95.05\n",
            "iteration : 150, loss : 0.1740, accuracy : 95.12\n",
            "iteration : 200, loss : 0.1732, accuracy : 95.10\n",
            "iteration : 250, loss : 0.1747, accuracy : 95.10\n",
            "iteration : 300, loss : 0.1766, accuracy : 95.08\n",
            "iteration : 350, loss : 0.1783, accuracy : 95.01\n",
            "Epoch :  48, training loss : 0.1782, training accuracy : 95.01, test loss : 0.2567, test accuracy : 92.97\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.1580, accuracy : 95.69\n",
            "iteration : 100, loss : 0.1643, accuracy : 95.52\n",
            "iteration : 150, loss : 0.1663, accuracy : 95.43\n",
            "iteration : 200, loss : 0.1697, accuracy : 95.29\n",
            "iteration : 250, loss : 0.1733, accuracy : 95.21\n",
            "iteration : 300, loss : 0.1720, accuracy : 95.22\n",
            "iteration : 350, loss : 0.1744, accuracy : 95.11\n",
            "Epoch :  49, training loss : 0.1747, training accuracy : 95.12, test loss : 0.2481, test accuracy : 93.17\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.1570, accuracy : 95.70\n",
            "iteration : 100, loss : 0.1605, accuracy : 95.45\n",
            "iteration : 150, loss : 0.1670, accuracy : 95.22\n",
            "iteration : 200, loss : 0.1713, accuracy : 95.14\n",
            "iteration : 250, loss : 0.1737, accuracy : 95.09\n",
            "iteration : 300, loss : 0.1740, accuracy : 95.09\n",
            "iteration : 350, loss : 0.1763, accuracy : 95.03\n",
            "Epoch :  50, training loss : 0.1762, training accuracy : 95.03, test loss : 0.2468, test accuracy : 93.23\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.1710, accuracy : 95.33\n",
            "iteration : 100, loss : 0.1643, accuracy : 95.45\n",
            "iteration : 150, loss : 0.1668, accuracy : 95.39\n",
            "iteration : 200, loss : 0.1656, accuracy : 95.39\n",
            "iteration : 250, loss : 0.1680, accuracy : 95.31\n",
            "iteration : 300, loss : 0.1739, accuracy : 95.14\n",
            "iteration : 350, loss : 0.1733, accuracy : 95.16\n",
            "Epoch :  51, training loss : 0.1739, training accuracy : 95.14, test loss : 0.2422, test accuracy : 93.47\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.1639, accuracy : 95.55\n",
            "iteration : 100, loss : 0.1603, accuracy : 95.59\n",
            "iteration : 150, loss : 0.1613, accuracy : 95.58\n",
            "iteration : 200, loss : 0.1648, accuracy : 95.45\n",
            "iteration : 250, loss : 0.1689, accuracy : 95.34\n",
            "iteration : 300, loss : 0.1708, accuracy : 95.31\n",
            "iteration : 350, loss : 0.1750, accuracy : 95.20\n",
            "Epoch :  52, training loss : 0.1750, training accuracy : 95.18, test loss : 0.2708, test accuracy : 92.51\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.1670, accuracy : 95.30\n",
            "iteration : 100, loss : 0.1688, accuracy : 95.32\n",
            "iteration : 150, loss : 0.1643, accuracy : 95.49\n",
            "iteration : 200, loss : 0.1661, accuracy : 95.45\n",
            "iteration : 250, loss : 0.1719, accuracy : 95.23\n",
            "iteration : 300, loss : 0.1738, accuracy : 95.16\n",
            "iteration : 350, loss : 0.1729, accuracy : 95.16\n",
            "Epoch :  53, training loss : 0.1723, training accuracy : 95.15, test loss : 0.2514, test accuracy : 93.05\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.1782, accuracy : 94.94\n",
            "iteration : 100, loss : 0.1755, accuracy : 95.05\n",
            "iteration : 150, loss : 0.1712, accuracy : 95.09\n",
            "iteration : 200, loss : 0.1681, accuracy : 95.24\n",
            "iteration : 250, loss : 0.1679, accuracy : 95.25\n",
            "iteration : 300, loss : 0.1703, accuracy : 95.16\n",
            "iteration : 350, loss : 0.1718, accuracy : 95.16\n",
            "Epoch :  54, training loss : 0.1732, training accuracy : 95.13, test loss : 0.2479, test accuracy : 93.34\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.1547, accuracy : 95.88\n",
            "iteration : 100, loss : 0.1604, accuracy : 95.78\n",
            "iteration : 150, loss : 0.1708, accuracy : 95.47\n",
            "iteration : 200, loss : 0.1690, accuracy : 95.45\n",
            "iteration : 250, loss : 0.1686, accuracy : 95.46\n",
            "iteration : 300, loss : 0.1696, accuracy : 95.40\n",
            "iteration : 350, loss : 0.1679, accuracy : 95.43\n",
            "Epoch :  55, training loss : 0.1672, training accuracy : 95.44, test loss : 0.2438, test accuracy : 93.41\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.1507, accuracy : 95.91\n",
            "iteration : 100, loss : 0.1450, accuracy : 96.11\n",
            "iteration : 150, loss : 0.1563, accuracy : 95.74\n",
            "iteration : 200, loss : 0.1630, accuracy : 95.52\n",
            "iteration : 250, loss : 0.1659, accuracy : 95.51\n",
            "iteration : 300, loss : 0.1670, accuracy : 95.43\n",
            "iteration : 350, loss : 0.1677, accuracy : 95.44\n",
            "Epoch :  56, training loss : 0.1677, training accuracy : 95.42, test loss : 0.2445, test accuracy : 93.25\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.1575, accuracy : 95.66\n",
            "iteration : 100, loss : 0.1592, accuracy : 95.45\n",
            "iteration : 150, loss : 0.1625, accuracy : 95.40\n",
            "iteration : 200, loss : 0.1642, accuracy : 95.39\n",
            "iteration : 250, loss : 0.1668, accuracy : 95.34\n",
            "iteration : 300, loss : 0.1671, accuracy : 95.33\n",
            "iteration : 350, loss : 0.1662, accuracy : 95.41\n",
            "Epoch :  57, training loss : 0.1658, training accuracy : 95.44, test loss : 0.2620, test accuracy : 92.79\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.1671, accuracy : 95.30\n",
            "iteration : 100, loss : 0.1625, accuracy : 95.39\n",
            "iteration : 150, loss : 0.1617, accuracy : 95.51\n",
            "iteration : 200, loss : 0.1652, accuracy : 95.37\n",
            "iteration : 250, loss : 0.1680, accuracy : 95.34\n",
            "iteration : 300, loss : 0.1684, accuracy : 95.32\n",
            "iteration : 350, loss : 0.1696, accuracy : 95.28\n",
            "Epoch :  58, training loss : 0.1698, training accuracy : 95.27, test loss : 0.2556, test accuracy : 92.98\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.1558, accuracy : 95.78\n",
            "iteration : 100, loss : 0.1499, accuracy : 95.73\n",
            "iteration : 150, loss : 0.1569, accuracy : 95.55\n",
            "iteration : 200, loss : 0.1598, accuracy : 95.50\n",
            "iteration : 250, loss : 0.1648, accuracy : 95.37\n",
            "iteration : 300, loss : 0.1660, accuracy : 95.38\n",
            "iteration : 350, loss : 0.1687, accuracy : 95.30\n",
            "Epoch :  59, training loss : 0.1689, training accuracy : 95.29, test loss : 0.2409, test accuracy : 93.45\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.1615, accuracy : 95.83\n",
            "iteration : 100, loss : 0.1643, accuracy : 95.55\n",
            "iteration : 150, loss : 0.1665, accuracy : 95.45\n",
            "iteration : 200, loss : 0.1667, accuracy : 95.45\n",
            "iteration : 250, loss : 0.1677, accuracy : 95.37\n",
            "iteration : 300, loss : 0.1683, accuracy : 95.33\n",
            "iteration : 350, loss : 0.1699, accuracy : 95.26\n",
            "Epoch :  60, training loss : 0.1710, training accuracy : 95.24, test loss : 0.2450, test accuracy : 93.40\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.1554, accuracy : 95.95\n",
            "iteration : 100, loss : 0.1612, accuracy : 95.55\n",
            "iteration : 150, loss : 0.1632, accuracy : 95.55\n",
            "iteration : 200, loss : 0.1642, accuracy : 95.51\n",
            "iteration : 250, loss : 0.1657, accuracy : 95.41\n",
            "iteration : 300, loss : 0.1674, accuracy : 95.37\n",
            "iteration : 350, loss : 0.1663, accuracy : 95.39\n",
            "Epoch :  61, training loss : 0.1665, training accuracy : 95.39, test loss : 0.2567, test accuracy : 92.96\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.1549, accuracy : 95.73\n",
            "iteration : 100, loss : 0.1513, accuracy : 95.67\n",
            "iteration : 150, loss : 0.1491, accuracy : 95.84\n",
            "iteration : 200, loss : 0.1545, accuracy : 95.73\n",
            "iteration : 250, loss : 0.1594, accuracy : 95.65\n",
            "iteration : 300, loss : 0.1606, accuracy : 95.62\n",
            "iteration : 350, loss : 0.1636, accuracy : 95.55\n",
            "Epoch :  62, training loss : 0.1623, training accuracy : 95.60, test loss : 0.2362, test accuracy : 93.65\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.1512, accuracy : 95.77\n",
            "iteration : 100, loss : 0.1588, accuracy : 95.56\n",
            "iteration : 150, loss : 0.1587, accuracy : 95.58\n",
            "iteration : 200, loss : 0.1610, accuracy : 95.39\n",
            "iteration : 250, loss : 0.1600, accuracy : 95.47\n",
            "iteration : 300, loss : 0.1616, accuracy : 95.43\n",
            "iteration : 350, loss : 0.1632, accuracy : 95.38\n",
            "Epoch :  63, training loss : 0.1631, training accuracy : 95.37, test loss : 0.2429, test accuracy : 93.46\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.1528, accuracy : 95.86\n",
            "iteration : 100, loss : 0.1547, accuracy : 95.82\n",
            "iteration : 150, loss : 0.1585, accuracy : 95.69\n",
            "iteration : 200, loss : 0.1621, accuracy : 95.52\n",
            "iteration : 250, loss : 0.1674, accuracy : 95.35\n",
            "iteration : 300, loss : 0.1649, accuracy : 95.41\n",
            "iteration : 350, loss : 0.1647, accuracy : 95.45\n",
            "Epoch :  64, training loss : 0.1648, training accuracy : 95.47, test loss : 0.2503, test accuracy : 93.32\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.1594, accuracy : 95.72\n",
            "iteration : 100, loss : 0.1634, accuracy : 95.58\n",
            "iteration : 150, loss : 0.1566, accuracy : 95.68\n",
            "iteration : 200, loss : 0.1574, accuracy : 95.66\n",
            "iteration : 250, loss : 0.1614, accuracy : 95.52\n",
            "iteration : 300, loss : 0.1640, accuracy : 95.46\n",
            "iteration : 350, loss : 0.1652, accuracy : 95.41\n",
            "Epoch :  65, training loss : 0.1659, training accuracy : 95.40, test loss : 0.2365, test accuracy : 93.65\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.1541, accuracy : 95.86\n",
            "iteration : 100, loss : 0.1564, accuracy : 95.75\n",
            "iteration : 150, loss : 0.1531, accuracy : 95.72\n",
            "iteration : 200, loss : 0.1561, accuracy : 95.62\n",
            "iteration : 250, loss : 0.1606, accuracy : 95.50\n",
            "iteration : 300, loss : 0.1611, accuracy : 95.48\n",
            "iteration : 350, loss : 0.1617, accuracy : 95.47\n",
            "Epoch :  66, training loss : 0.1623, training accuracy : 95.46, test loss : 0.2608, test accuracy : 92.59\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.1512, accuracy : 95.73\n",
            "iteration : 100, loss : 0.1567, accuracy : 95.58\n",
            "iteration : 150, loss : 0.1543, accuracy : 95.66\n",
            "iteration : 200, loss : 0.1541, accuracy : 95.64\n",
            "iteration : 250, loss : 0.1602, accuracy : 95.46\n",
            "iteration : 300, loss : 0.1602, accuracy : 95.48\n",
            "iteration : 350, loss : 0.1625, accuracy : 95.41\n",
            "Epoch :  67, training loss : 0.1632, training accuracy : 95.39, test loss : 0.2374, test accuracy : 93.42\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.1600, accuracy : 95.59\n",
            "iteration : 100, loss : 0.1588, accuracy : 95.57\n",
            "iteration : 150, loss : 0.1604, accuracy : 95.60\n",
            "iteration : 200, loss : 0.1591, accuracy : 95.61\n",
            "iteration : 250, loss : 0.1616, accuracy : 95.53\n",
            "iteration : 300, loss : 0.1634, accuracy : 95.45\n",
            "iteration : 350, loss : 0.1635, accuracy : 95.45\n",
            "Epoch :  68, training loss : 0.1637, training accuracy : 95.44, test loss : 0.2390, test accuracy : 93.51\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.1661, accuracy : 95.48\n",
            "iteration : 100, loss : 0.1550, accuracy : 95.73\n",
            "iteration : 150, loss : 0.1565, accuracy : 95.69\n",
            "iteration : 200, loss : 0.1583, accuracy : 95.64\n",
            "iteration : 250, loss : 0.1605, accuracy : 95.59\n",
            "iteration : 300, loss : 0.1601, accuracy : 95.59\n",
            "iteration : 350, loss : 0.1613, accuracy : 95.50\n",
            "Epoch :  69, training loss : 0.1614, training accuracy : 95.49, test loss : 0.2412, test accuracy : 93.45\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.1485, accuracy : 96.00\n",
            "iteration : 100, loss : 0.1530, accuracy : 95.84\n",
            "iteration : 150, loss : 0.1563, accuracy : 95.74\n",
            "iteration : 200, loss : 0.1567, accuracy : 95.73\n",
            "iteration : 250, loss : 0.1574, accuracy : 95.65\n",
            "iteration : 300, loss : 0.1573, accuracy : 95.63\n",
            "iteration : 350, loss : 0.1583, accuracy : 95.64\n",
            "Epoch :  70, training loss : 0.1589, training accuracy : 95.64, test loss : 0.2431, test accuracy : 93.42\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.1489, accuracy : 95.94\n",
            "iteration : 100, loss : 0.1508, accuracy : 95.87\n",
            "iteration : 150, loss : 0.1502, accuracy : 95.89\n",
            "iteration : 200, loss : 0.1518, accuracy : 95.78\n",
            "iteration : 250, loss : 0.1570, accuracy : 95.60\n",
            "iteration : 300, loss : 0.1580, accuracy : 95.58\n",
            "iteration : 350, loss : 0.1598, accuracy : 95.52\n",
            "Epoch :  71, training loss : 0.1598, training accuracy : 95.51, test loss : 0.2333, test accuracy : 93.72\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.1491, accuracy : 95.88\n",
            "iteration : 100, loss : 0.1510, accuracy : 95.89\n",
            "iteration : 150, loss : 0.1509, accuracy : 95.82\n",
            "iteration : 200, loss : 0.1537, accuracy : 95.75\n",
            "iteration : 250, loss : 0.1514, accuracy : 95.71\n",
            "iteration : 300, loss : 0.1539, accuracy : 95.71\n",
            "iteration : 350, loss : 0.1546, accuracy : 95.67\n",
            "Epoch :  72, training loss : 0.1548, training accuracy : 95.66, test loss : 0.2425, test accuracy : 93.38\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.1408, accuracy : 96.06\n",
            "iteration : 100, loss : 0.1499, accuracy : 95.78\n",
            "iteration : 150, loss : 0.1555, accuracy : 95.69\n",
            "iteration : 200, loss : 0.1557, accuracy : 95.57\n",
            "iteration : 250, loss : 0.1598, accuracy : 95.49\n",
            "iteration : 300, loss : 0.1593, accuracy : 95.54\n",
            "iteration : 350, loss : 0.1585, accuracy : 95.59\n",
            "Epoch :  73, training loss : 0.1582, training accuracy : 95.62, test loss : 0.2404, test accuracy : 93.43\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.1471, accuracy : 96.11\n",
            "iteration : 100, loss : 0.1539, accuracy : 95.72\n",
            "iteration : 150, loss : 0.1549, accuracy : 95.76\n",
            "iteration : 200, loss : 0.1542, accuracy : 95.68\n",
            "iteration : 250, loss : 0.1509, accuracy : 95.78\n",
            "iteration : 300, loss : 0.1554, accuracy : 95.67\n",
            "iteration : 350, loss : 0.1556, accuracy : 95.67\n",
            "Epoch :  74, training loss : 0.1553, training accuracy : 95.71, test loss : 0.2464, test accuracy : 93.37\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.1546, accuracy : 95.70\n",
            "iteration : 100, loss : 0.1595, accuracy : 95.70\n",
            "iteration : 150, loss : 0.1595, accuracy : 95.58\n",
            "iteration : 200, loss : 0.1565, accuracy : 95.71\n",
            "iteration : 250, loss : 0.1576, accuracy : 95.62\n",
            "iteration : 300, loss : 0.1586, accuracy : 95.58\n",
            "iteration : 350, loss : 0.1592, accuracy : 95.55\n",
            "Epoch :  75, training loss : 0.1579, training accuracy : 95.59, test loss : 0.2489, test accuracy : 93.24\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.1368, accuracy : 96.22\n",
            "iteration : 100, loss : 0.1427, accuracy : 96.02\n",
            "iteration : 150, loss : 0.1505, accuracy : 95.79\n",
            "iteration : 200, loss : 0.1512, accuracy : 95.82\n",
            "iteration : 250, loss : 0.1502, accuracy : 95.82\n",
            "iteration : 300, loss : 0.1509, accuracy : 95.82\n",
            "iteration : 350, loss : 0.1532, accuracy : 95.77\n",
            "Epoch :  76, training loss : 0.1540, training accuracy : 95.76, test loss : 0.2453, test accuracy : 93.25\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.1318, accuracy : 96.34\n",
            "iteration : 100, loss : 0.1419, accuracy : 96.16\n",
            "iteration : 150, loss : 0.1478, accuracy : 95.94\n",
            "iteration : 200, loss : 0.1508, accuracy : 95.87\n",
            "iteration : 250, loss : 0.1519, accuracy : 95.85\n",
            "iteration : 300, loss : 0.1528, accuracy : 95.78\n",
            "iteration : 350, loss : 0.1532, accuracy : 95.76\n",
            "Epoch :  77, training loss : 0.1543, training accuracy : 95.72, test loss : 0.2512, test accuracy : 93.28\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.1534, accuracy : 95.86\n",
            "iteration : 100, loss : 0.1578, accuracy : 95.67\n",
            "iteration : 150, loss : 0.1548, accuracy : 95.68\n",
            "iteration : 200, loss : 0.1558, accuracy : 95.67\n",
            "iteration : 250, loss : 0.1549, accuracy : 95.67\n",
            "iteration : 300, loss : 0.1562, accuracy : 95.62\n",
            "iteration : 350, loss : 0.1570, accuracy : 95.62\n",
            "Epoch :  78, training loss : 0.1575, training accuracy : 95.59, test loss : 0.2411, test accuracy : 93.40\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.1498, accuracy : 95.73\n",
            "iteration : 100, loss : 0.1469, accuracy : 95.94\n",
            "iteration : 150, loss : 0.1478, accuracy : 95.86\n",
            "iteration : 200, loss : 0.1493, accuracy : 95.77\n",
            "iteration : 250, loss : 0.1491, accuracy : 95.79\n",
            "iteration : 300, loss : 0.1529, accuracy : 95.72\n",
            "iteration : 350, loss : 0.1527, accuracy : 95.75\n",
            "Epoch :  79, training loss : 0.1522, training accuracy : 95.76, test loss : 0.2442, test accuracy : 93.62\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.1323, accuracy : 96.39\n",
            "iteration : 100, loss : 0.1347, accuracy : 96.40\n",
            "iteration : 150, loss : 0.1408, accuracy : 96.19\n",
            "iteration : 200, loss : 0.1452, accuracy : 96.01\n",
            "iteration : 250, loss : 0.1478, accuracy : 95.91\n",
            "iteration : 300, loss : 0.1483, accuracy : 95.91\n",
            "iteration : 350, loss : 0.1493, accuracy : 95.87\n",
            "Epoch :  80, training loss : 0.1519, training accuracy : 95.82, test loss : 0.2443, test accuracy : 93.41\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.1408, accuracy : 96.02\n",
            "iteration : 100, loss : 0.1391, accuracy : 96.15\n",
            "iteration : 150, loss : 0.1418, accuracy : 96.01\n",
            "iteration : 200, loss : 0.1466, accuracy : 95.91\n",
            "iteration : 250, loss : 0.1466, accuracy : 95.90\n",
            "iteration : 300, loss : 0.1476, accuracy : 95.86\n",
            "iteration : 350, loss : 0.1492, accuracy : 95.80\n",
            "Epoch :  81, training loss : 0.1493, training accuracy : 95.79, test loss : 0.2573, test accuracy : 93.05\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.1369, accuracy : 96.16\n",
            "iteration : 100, loss : 0.1399, accuracy : 96.06\n",
            "iteration : 150, loss : 0.1405, accuracy : 96.09\n",
            "iteration : 200, loss : 0.1405, accuracy : 96.12\n",
            "iteration : 250, loss : 0.1444, accuracy : 95.97\n",
            "iteration : 300, loss : 0.1464, accuracy : 95.90\n",
            "iteration : 350, loss : 0.1485, accuracy : 95.82\n",
            "Epoch :  82, training loss : 0.1476, training accuracy : 95.86, test loss : 0.2409, test accuracy : 93.51\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.1454, accuracy : 96.02\n",
            "iteration : 100, loss : 0.1378, accuracy : 96.16\n",
            "iteration : 150, loss : 0.1409, accuracy : 96.03\n",
            "iteration : 200, loss : 0.1455, accuracy : 95.89\n",
            "iteration : 250, loss : 0.1476, accuracy : 95.86\n",
            "iteration : 300, loss : 0.1482, accuracy : 95.87\n",
            "iteration : 350, loss : 0.1482, accuracy : 95.89\n",
            "Epoch :  83, training loss : 0.1484, training accuracy : 95.88, test loss : 0.2395, test accuracy : 93.58\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.1208, accuracy : 96.81\n",
            "iteration : 100, loss : 0.1346, accuracy : 96.36\n",
            "iteration : 150, loss : 0.1432, accuracy : 96.06\n",
            "iteration : 200, loss : 0.1434, accuracy : 96.00\n",
            "iteration : 250, loss : 0.1457, accuracy : 95.92\n",
            "iteration : 300, loss : 0.1469, accuracy : 95.88\n",
            "iteration : 350, loss : 0.1479, accuracy : 95.86\n",
            "Epoch :  84, training loss : 0.1495, training accuracy : 95.82, test loss : 0.2658, test accuracy : 92.79\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.1554, accuracy : 95.80\n",
            "iteration : 100, loss : 0.1447, accuracy : 96.05\n",
            "iteration : 150, loss : 0.1427, accuracy : 96.07\n",
            "iteration : 200, loss : 0.1466, accuracy : 95.91\n",
            "iteration : 250, loss : 0.1500, accuracy : 95.79\n",
            "iteration : 300, loss : 0.1505, accuracy : 95.76\n",
            "iteration : 350, loss : 0.1513, accuracy : 95.75\n",
            "Epoch :  85, training loss : 0.1508, training accuracy : 95.77, test loss : 0.2451, test accuracy : 93.50\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.1514, accuracy : 95.91\n",
            "iteration : 100, loss : 0.1465, accuracy : 95.95\n",
            "iteration : 150, loss : 0.1413, accuracy : 96.11\n",
            "iteration : 200, loss : 0.1427, accuracy : 96.08\n",
            "iteration : 250, loss : 0.1434, accuracy : 96.06\n",
            "iteration : 300, loss : 0.1428, accuracy : 96.02\n",
            "iteration : 350, loss : 0.1445, accuracy : 95.96\n",
            "Epoch :  86, training loss : 0.1450, training accuracy : 95.93, test loss : 0.2591, test accuracy : 93.14\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.1345, accuracy : 96.25\n",
            "iteration : 100, loss : 0.1428, accuracy : 95.96\n",
            "iteration : 150, loss : 0.1436, accuracy : 96.04\n",
            "iteration : 200, loss : 0.1477, accuracy : 95.92\n",
            "iteration : 250, loss : 0.1467, accuracy : 95.98\n",
            "iteration : 300, loss : 0.1450, accuracy : 96.03\n",
            "iteration : 350, loss : 0.1460, accuracy : 96.01\n",
            "Epoch :  87, training loss : 0.1460, training accuracy : 96.00, test loss : 0.2445, test accuracy : 93.34\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.1375, accuracy : 96.28\n",
            "iteration : 100, loss : 0.1391, accuracy : 96.19\n",
            "iteration : 150, loss : 0.1389, accuracy : 96.15\n",
            "iteration : 200, loss : 0.1427, accuracy : 96.11\n",
            "iteration : 250, loss : 0.1445, accuracy : 96.07\n",
            "iteration : 300, loss : 0.1448, accuracy : 96.03\n",
            "iteration : 350, loss : 0.1451, accuracy : 96.02\n",
            "Epoch :  88, training loss : 0.1459, training accuracy : 95.99, test loss : 0.2440, test accuracy : 93.60\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.1385, accuracy : 95.98\n",
            "iteration : 100, loss : 0.1384, accuracy : 96.17\n",
            "iteration : 150, loss : 0.1423, accuracy : 96.09\n",
            "iteration : 200, loss : 0.1438, accuracy : 96.03\n",
            "iteration : 250, loss : 0.1464, accuracy : 96.01\n",
            "iteration : 300, loss : 0.1478, accuracy : 95.91\n",
            "iteration : 350, loss : 0.1511, accuracy : 95.82\n",
            "Epoch :  89, training loss : 0.1514, training accuracy : 95.80, test loss : 0.2417, test accuracy : 93.62\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.1310, accuracy : 96.75\n",
            "iteration : 100, loss : 0.1327, accuracy : 96.57\n",
            "iteration : 150, loss : 0.1378, accuracy : 96.33\n",
            "iteration : 200, loss : 0.1402, accuracy : 96.27\n",
            "iteration : 250, loss : 0.1419, accuracy : 96.20\n",
            "iteration : 300, loss : 0.1456, accuracy : 96.08\n",
            "iteration : 350, loss : 0.1453, accuracy : 96.06\n",
            "Epoch :  90, training loss : 0.1449, training accuracy : 96.07, test loss : 0.2442, test accuracy : 93.47\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.1361, accuracy : 96.22\n",
            "iteration : 100, loss : 0.1353, accuracy : 96.20\n",
            "iteration : 150, loss : 0.1365, accuracy : 96.15\n",
            "iteration : 200, loss : 0.1384, accuracy : 96.18\n",
            "iteration : 250, loss : 0.1415, accuracy : 96.10\n",
            "iteration : 300, loss : 0.1422, accuracy : 96.08\n",
            "iteration : 350, loss : 0.1449, accuracy : 96.02\n",
            "Epoch :  91, training loss : 0.1447, training accuracy : 96.02, test loss : 0.2481, test accuracy : 93.52\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.1253, accuracy : 96.70\n",
            "iteration : 100, loss : 0.1317, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1355, accuracy : 96.29\n",
            "iteration : 200, loss : 0.1364, accuracy : 96.24\n",
            "iteration : 250, loss : 0.1364, accuracy : 96.23\n",
            "iteration : 300, loss : 0.1400, accuracy : 96.13\n",
            "iteration : 350, loss : 0.1407, accuracy : 96.10\n",
            "Epoch :  92, training loss : 0.1421, training accuracy : 96.07, test loss : 0.2391, test accuracy : 93.70\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.1252, accuracy : 96.69\n",
            "iteration : 100, loss : 0.1288, accuracy : 96.54\n",
            "iteration : 150, loss : 0.1336, accuracy : 96.40\n",
            "iteration : 200, loss : 0.1392, accuracy : 96.25\n",
            "iteration : 250, loss : 0.1408, accuracy : 96.19\n",
            "iteration : 300, loss : 0.1409, accuracy : 96.17\n",
            "iteration : 350, loss : 0.1417, accuracy : 96.12\n",
            "Epoch :  93, training loss : 0.1429, training accuracy : 96.08, test loss : 0.2443, test accuracy : 93.49\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.1321, accuracy : 96.33\n",
            "iteration : 100, loss : 0.1314, accuracy : 96.48\n",
            "iteration : 150, loss : 0.1315, accuracy : 96.48\n",
            "iteration : 200, loss : 0.1325, accuracy : 96.36\n",
            "iteration : 250, loss : 0.1344, accuracy : 96.30\n",
            "iteration : 300, loss : 0.1367, accuracy : 96.20\n",
            "iteration : 350, loss : 0.1390, accuracy : 96.12\n",
            "Epoch :  94, training loss : 0.1397, training accuracy : 96.11, test loss : 0.2525, test accuracy : 93.16\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.1369, accuracy : 96.23\n",
            "iteration : 100, loss : 0.1317, accuracy : 96.33\n",
            "iteration : 150, loss : 0.1328, accuracy : 96.33\n",
            "iteration : 200, loss : 0.1321, accuracy : 96.37\n",
            "iteration : 250, loss : 0.1344, accuracy : 96.32\n",
            "iteration : 300, loss : 0.1378, accuracy : 96.21\n",
            "iteration : 350, loss : 0.1417, accuracy : 96.09\n",
            "Epoch :  95, training loss : 0.1415, training accuracy : 96.11, test loss : 0.2491, test accuracy : 93.42\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.1311, accuracy : 96.28\n",
            "iteration : 100, loss : 0.1336, accuracy : 96.27\n",
            "iteration : 150, loss : 0.1385, accuracy : 96.06\n",
            "iteration : 200, loss : 0.1442, accuracy : 96.00\n",
            "iteration : 250, loss : 0.1427, accuracy : 96.08\n",
            "iteration : 300, loss : 0.1413, accuracy : 96.11\n",
            "iteration : 350, loss : 0.1414, accuracy : 96.11\n",
            "Epoch :  96, training loss : 0.1437, training accuracy : 96.06, test loss : 0.2458, test accuracy : 93.49\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.1391, accuracy : 96.16\n",
            "iteration : 100, loss : 0.1384, accuracy : 96.12\n",
            "iteration : 150, loss : 0.1397, accuracy : 96.10\n",
            "iteration : 200, loss : 0.1398, accuracy : 96.10\n",
            "iteration : 250, loss : 0.1403, accuracy : 96.07\n",
            "iteration : 300, loss : 0.1426, accuracy : 95.99\n",
            "iteration : 350, loss : 0.1425, accuracy : 95.99\n",
            "Epoch :  97, training loss : 0.1431, training accuracy : 95.99, test loss : 0.2578, test accuracy : 93.05\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.1377, accuracy : 96.30\n",
            "iteration : 100, loss : 0.1382, accuracy : 96.27\n",
            "iteration : 150, loss : 0.1382, accuracy : 96.21\n",
            "iteration : 200, loss : 0.1360, accuracy : 96.21\n",
            "iteration : 250, loss : 0.1374, accuracy : 96.19\n",
            "iteration : 300, loss : 0.1388, accuracy : 96.11\n",
            "iteration : 350, loss : 0.1400, accuracy : 96.08\n",
            "Epoch :  98, training loss : 0.1401, training accuracy : 96.07, test loss : 0.2586, test accuracy : 93.11\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.1386, accuracy : 96.52\n",
            "iteration : 100, loss : 0.1343, accuracy : 96.54\n",
            "iteration : 150, loss : 0.1340, accuracy : 96.43\n",
            "iteration : 200, loss : 0.1346, accuracy : 96.42\n",
            "iteration : 250, loss : 0.1365, accuracy : 96.33\n",
            "iteration : 300, loss : 0.1373, accuracy : 96.31\n",
            "iteration : 350, loss : 0.1367, accuracy : 96.33\n",
            "Epoch :  99, training loss : 0.1368, training accuracy : 96.31, test loss : 0.2587, test accuracy : 93.03\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.1340, accuracy : 96.45\n",
            "iteration : 100, loss : 0.1410, accuracy : 96.23\n",
            "iteration : 150, loss : 0.1373, accuracy : 96.30\n",
            "iteration : 200, loss : 0.1406, accuracy : 96.19\n",
            "iteration : 250, loss : 0.1390, accuracy : 96.18\n",
            "iteration : 300, loss : 0.1391, accuracy : 96.15\n",
            "iteration : 350, loss : 0.1392, accuracy : 96.13\n",
            "Epoch : 100, training loss : 0.1379, training accuracy : 96.17, test loss : 0.2371, test accuracy : 93.69\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.1247, accuracy : 96.61\n",
            "iteration : 100, loss : 0.1237, accuracy : 96.68\n",
            "iteration : 150, loss : 0.1311, accuracy : 96.41\n",
            "iteration : 200, loss : 0.1336, accuracy : 96.37\n",
            "iteration : 250, loss : 0.1339, accuracy : 96.36\n",
            "iteration : 300, loss : 0.1359, accuracy : 96.32\n",
            "iteration : 350, loss : 0.1350, accuracy : 96.33\n",
            "Epoch : 101, training loss : 0.1366, training accuracy : 96.29, test loss : 0.2503, test accuracy : 93.43\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.1305, accuracy : 96.45\n",
            "iteration : 100, loss : 0.1277, accuracy : 96.55\n",
            "iteration : 150, loss : 0.1307, accuracy : 96.45\n",
            "iteration : 200, loss : 0.1326, accuracy : 96.32\n",
            "iteration : 250, loss : 0.1375, accuracy : 96.25\n",
            "iteration : 300, loss : 0.1391, accuracy : 96.19\n",
            "iteration : 350, loss : 0.1386, accuracy : 96.19\n",
            "Epoch : 102, training loss : 0.1395, training accuracy : 96.14, test loss : 0.2501, test accuracy : 93.20\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.1315, accuracy : 96.45\n",
            "iteration : 100, loss : 0.1274, accuracy : 96.62\n",
            "iteration : 150, loss : 0.1266, accuracy : 96.61\n",
            "iteration : 200, loss : 0.1298, accuracy : 96.53\n",
            "iteration : 250, loss : 0.1327, accuracy : 96.41\n",
            "iteration : 300, loss : 0.1343, accuracy : 96.36\n",
            "iteration : 350, loss : 0.1358, accuracy : 96.29\n",
            "Epoch : 103, training loss : 0.1357, training accuracy : 96.30, test loss : 0.2483, test accuracy : 93.48\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.1385, accuracy : 96.17\n",
            "iteration : 100, loss : 0.1362, accuracy : 96.25\n",
            "iteration : 150, loss : 0.1339, accuracy : 96.28\n",
            "iteration : 200, loss : 0.1308, accuracy : 96.37\n",
            "iteration : 250, loss : 0.1332, accuracy : 96.27\n",
            "iteration : 300, loss : 0.1331, accuracy : 96.28\n",
            "iteration : 350, loss : 0.1347, accuracy : 96.23\n",
            "Epoch : 104, training loss : 0.1354, training accuracy : 96.19, test loss : 0.2372, test accuracy : 93.68\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.1322, accuracy : 96.44\n",
            "iteration : 100, loss : 0.1264, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1276, accuracy : 96.54\n",
            "iteration : 200, loss : 0.1318, accuracy : 96.37\n",
            "iteration : 250, loss : 0.1310, accuracy : 96.39\n",
            "iteration : 300, loss : 0.1342, accuracy : 96.27\n",
            "iteration : 350, loss : 0.1361, accuracy : 96.22\n",
            "Epoch : 105, training loss : 0.1370, training accuracy : 96.19, test loss : 0.2444, test accuracy : 93.49\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.1199, accuracy : 96.73\n",
            "iteration : 100, loss : 0.1183, accuracy : 96.80\n",
            "iteration : 150, loss : 0.1221, accuracy : 96.68\n",
            "iteration : 200, loss : 0.1243, accuracy : 96.59\n",
            "iteration : 250, loss : 0.1281, accuracy : 96.50\n",
            "iteration : 300, loss : 0.1313, accuracy : 96.40\n",
            "iteration : 350, loss : 0.1333, accuracy : 96.33\n",
            "Epoch : 106, training loss : 0.1341, training accuracy : 96.29, test loss : 0.2349, test accuracy : 93.70\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.1193, accuracy : 96.89\n",
            "iteration : 100, loss : 0.1205, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1226, accuracy : 96.60\n",
            "iteration : 200, loss : 0.1266, accuracy : 96.54\n",
            "iteration : 250, loss : 0.1250, accuracy : 96.61\n",
            "iteration : 300, loss : 0.1301, accuracy : 96.45\n",
            "iteration : 350, loss : 0.1334, accuracy : 96.36\n",
            "Epoch : 107, training loss : 0.1341, training accuracy : 96.35, test loss : 0.2384, test accuracy : 93.71\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.1206, accuracy : 96.75\n",
            "iteration : 100, loss : 0.1184, accuracy : 96.79\n",
            "iteration : 150, loss : 0.1240, accuracy : 96.59\n",
            "iteration : 200, loss : 0.1250, accuracy : 96.57\n",
            "iteration : 250, loss : 0.1273, accuracy : 96.51\n",
            "iteration : 300, loss : 0.1286, accuracy : 96.49\n",
            "iteration : 350, loss : 0.1299, accuracy : 96.46\n",
            "Epoch : 108, training loss : 0.1302, training accuracy : 96.46, test loss : 0.2348, test accuracy : 93.86\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.1182, accuracy : 96.44\n",
            "iteration : 100, loss : 0.1278, accuracy : 96.20\n",
            "iteration : 150, loss : 0.1241, accuracy : 96.40\n",
            "iteration : 200, loss : 0.1254, accuracy : 96.43\n",
            "iteration : 250, loss : 0.1301, accuracy : 96.32\n",
            "iteration : 300, loss : 0.1326, accuracy : 96.26\n",
            "iteration : 350, loss : 0.1335, accuracy : 96.30\n",
            "Epoch : 109, training loss : 0.1336, training accuracy : 96.30, test loss : 0.2478, test accuracy : 93.44\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.1207, accuracy : 96.66\n",
            "iteration : 100, loss : 0.1247, accuracy : 96.57\n",
            "iteration : 150, loss : 0.1288, accuracy : 96.45\n",
            "iteration : 200, loss : 0.1322, accuracy : 96.34\n",
            "iteration : 250, loss : 0.1328, accuracy : 96.32\n",
            "iteration : 300, loss : 0.1321, accuracy : 96.33\n",
            "iteration : 350, loss : 0.1320, accuracy : 96.35\n",
            "Epoch : 110, training loss : 0.1327, training accuracy : 96.32, test loss : 0.2476, test accuracy : 93.54\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.1221, accuracy : 96.64\n",
            "iteration : 100, loss : 0.1222, accuracy : 96.53\n",
            "iteration : 150, loss : 0.1252, accuracy : 96.49\n",
            "iteration : 200, loss : 0.1268, accuracy : 96.43\n",
            "iteration : 250, loss : 0.1275, accuracy : 96.45\n",
            "iteration : 300, loss : 0.1284, accuracy : 96.46\n",
            "iteration : 350, loss : 0.1298, accuracy : 96.43\n",
            "Epoch : 111, training loss : 0.1302, training accuracy : 96.42, test loss : 0.2478, test accuracy : 93.48\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.1239, accuracy : 96.59\n",
            "iteration : 100, loss : 0.1244, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1230, accuracy : 96.59\n",
            "iteration : 200, loss : 0.1237, accuracy : 96.54\n",
            "iteration : 250, loss : 0.1263, accuracy : 96.47\n",
            "iteration : 300, loss : 0.1268, accuracy : 96.49\n",
            "iteration : 350, loss : 0.1270, accuracy : 96.51\n",
            "Epoch : 112, training loss : 0.1286, training accuracy : 96.48, test loss : 0.2361, test accuracy : 93.83\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.1173, accuracy : 96.81\n",
            "iteration : 100, loss : 0.1164, accuracy : 96.77\n",
            "iteration : 150, loss : 0.1195, accuracy : 96.74\n",
            "iteration : 200, loss : 0.1235, accuracy : 96.57\n",
            "iteration : 250, loss : 0.1252, accuracy : 96.54\n",
            "iteration : 300, loss : 0.1257, accuracy : 96.55\n",
            "iteration : 350, loss : 0.1250, accuracy : 96.57\n",
            "Epoch : 113, training loss : 0.1262, training accuracy : 96.56, test loss : 0.2488, test accuracy : 93.54\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.1203, accuracy : 96.72\n",
            "iteration : 100, loss : 0.1188, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1238, accuracy : 96.68\n",
            "iteration : 200, loss : 0.1244, accuracy : 96.62\n",
            "iteration : 250, loss : 0.1269, accuracy : 96.55\n",
            "iteration : 300, loss : 0.1294, accuracy : 96.46\n",
            "iteration : 350, loss : 0.1309, accuracy : 96.39\n",
            "Epoch : 114, training loss : 0.1299, training accuracy : 96.40, test loss : 0.2430, test accuracy : 93.70\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.1107, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1124, accuracy : 96.98\n",
            "iteration : 150, loss : 0.1177, accuracy : 96.76\n",
            "iteration : 200, loss : 0.1162, accuracy : 96.80\n",
            "iteration : 250, loss : 0.1210, accuracy : 96.66\n",
            "iteration : 300, loss : 0.1223, accuracy : 96.66\n",
            "iteration : 350, loss : 0.1246, accuracy : 96.61\n",
            "Epoch : 115, training loss : 0.1254, training accuracy : 96.58, test loss : 0.2419, test accuracy : 93.65\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.1141, accuracy : 96.88\n",
            "iteration : 100, loss : 0.1218, accuracy : 96.77\n",
            "iteration : 150, loss : 0.1238, accuracy : 96.63\n",
            "iteration : 200, loss : 0.1299, accuracy : 96.48\n",
            "iteration : 250, loss : 0.1296, accuracy : 96.52\n",
            "iteration : 300, loss : 0.1290, accuracy : 96.48\n",
            "iteration : 350, loss : 0.1299, accuracy : 96.46\n",
            "Epoch : 116, training loss : 0.1296, training accuracy : 96.45, test loss : 0.2436, test accuracy : 93.74\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.1283, accuracy : 96.44\n",
            "iteration : 100, loss : 0.1272, accuracy : 96.41\n",
            "iteration : 150, loss : 0.1320, accuracy : 96.28\n",
            "iteration : 200, loss : 0.1321, accuracy : 96.28\n",
            "iteration : 250, loss : 0.1312, accuracy : 96.35\n",
            "iteration : 300, loss : 0.1301, accuracy : 96.39\n",
            "iteration : 350, loss : 0.1286, accuracy : 96.43\n",
            "Epoch : 117, training loss : 0.1285, training accuracy : 96.42, test loss : 0.2399, test accuracy : 93.78\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.1241, accuracy : 96.42\n",
            "iteration : 100, loss : 0.1221, accuracy : 96.47\n",
            "iteration : 150, loss : 0.1225, accuracy : 96.57\n",
            "iteration : 200, loss : 0.1239, accuracy : 96.57\n",
            "iteration : 250, loss : 0.1235, accuracy : 96.58\n",
            "iteration : 300, loss : 0.1248, accuracy : 96.52\n",
            "iteration : 350, loss : 0.1263, accuracy : 96.48\n",
            "Epoch : 118, training loss : 0.1269, training accuracy : 96.48, test loss : 0.2514, test accuracy : 93.37\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.1169, accuracy : 96.77\n",
            "iteration : 100, loss : 0.1126, accuracy : 96.84\n",
            "iteration : 150, loss : 0.1200, accuracy : 96.57\n",
            "iteration : 200, loss : 0.1208, accuracy : 96.59\n",
            "iteration : 250, loss : 0.1237, accuracy : 96.54\n",
            "iteration : 300, loss : 0.1261, accuracy : 96.45\n",
            "iteration : 350, loss : 0.1272, accuracy : 96.47\n",
            "Epoch : 119, training loss : 0.1271, training accuracy : 96.47, test loss : 0.2409, test accuracy : 93.73\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.1166, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1223, accuracy : 96.67\n",
            "iteration : 150, loss : 0.1223, accuracy : 96.66\n",
            "iteration : 200, loss : 0.1206, accuracy : 96.73\n",
            "iteration : 250, loss : 0.1203, accuracy : 96.73\n",
            "iteration : 300, loss : 0.1210, accuracy : 96.71\n",
            "iteration : 350, loss : 0.1223, accuracy : 96.65\n",
            "Epoch : 120, training loss : 0.1225, training accuracy : 96.63, test loss : 0.2460, test accuracy : 93.83\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.1256, accuracy : 96.45\n",
            "iteration : 100, loss : 0.1218, accuracy : 96.70\n",
            "iteration : 150, loss : 0.1199, accuracy : 96.77\n",
            "iteration : 200, loss : 0.1197, accuracy : 96.73\n",
            "iteration : 250, loss : 0.1190, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1197, accuracy : 96.66\n",
            "iteration : 350, loss : 0.1218, accuracy : 96.63\n",
            "Epoch : 121, training loss : 0.1209, training accuracy : 96.67, test loss : 0.2383, test accuracy : 93.75\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.1107, accuracy : 96.84\n",
            "iteration : 100, loss : 0.1171, accuracy : 96.82\n",
            "iteration : 150, loss : 0.1175, accuracy : 96.86\n",
            "iteration : 200, loss : 0.1165, accuracy : 96.87\n",
            "iteration : 250, loss : 0.1151, accuracy : 96.92\n",
            "iteration : 300, loss : 0.1180, accuracy : 96.84\n",
            "iteration : 350, loss : 0.1213, accuracy : 96.74\n",
            "Epoch : 122, training loss : 0.1220, training accuracy : 96.72, test loss : 0.2453, test accuracy : 93.69\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.1230, accuracy : 96.61\n",
            "iteration : 100, loss : 0.1198, accuracy : 96.67\n",
            "iteration : 150, loss : 0.1212, accuracy : 96.64\n",
            "iteration : 200, loss : 0.1210, accuracy : 96.66\n",
            "iteration : 250, loss : 0.1228, accuracy : 96.61\n",
            "iteration : 300, loss : 0.1207, accuracy : 96.63\n",
            "iteration : 350, loss : 0.1230, accuracy : 96.54\n",
            "Epoch : 123, training loss : 0.1233, training accuracy : 96.55, test loss : 0.2565, test accuracy : 93.37\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.1194, accuracy : 96.80\n",
            "iteration : 100, loss : 0.1178, accuracy : 96.82\n",
            "iteration : 150, loss : 0.1169, accuracy : 96.78\n",
            "iteration : 200, loss : 0.1166, accuracy : 96.77\n",
            "iteration : 250, loss : 0.1218, accuracy : 96.59\n",
            "iteration : 300, loss : 0.1256, accuracy : 96.52\n",
            "iteration : 350, loss : 0.1250, accuracy : 96.56\n",
            "Epoch : 124, training loss : 0.1253, training accuracy : 96.55, test loss : 0.2297, test accuracy : 94.07\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.1186, accuracy : 96.72\n",
            "iteration : 100, loss : 0.1207, accuracy : 96.65\n",
            "iteration : 150, loss : 0.1212, accuracy : 96.60\n",
            "iteration : 200, loss : 0.1205, accuracy : 96.64\n",
            "iteration : 250, loss : 0.1213, accuracy : 96.64\n",
            "iteration : 300, loss : 0.1196, accuracy : 96.70\n",
            "iteration : 350, loss : 0.1210, accuracy : 96.68\n",
            "Epoch : 125, training loss : 0.1213, training accuracy : 96.66, test loss : 0.2511, test accuracy : 93.39\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.1168, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1132, accuracy : 96.93\n",
            "iteration : 150, loss : 0.1161, accuracy : 96.80\n",
            "iteration : 200, loss : 0.1203, accuracy : 96.68\n",
            "iteration : 250, loss : 0.1183, accuracy : 96.75\n",
            "iteration : 300, loss : 0.1196, accuracy : 96.72\n",
            "iteration : 350, loss : 0.1198, accuracy : 96.70\n",
            "Epoch : 126, training loss : 0.1193, training accuracy : 96.68, test loss : 0.2471, test accuracy : 93.66\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.1115, accuracy : 97.06\n",
            "iteration : 100, loss : 0.1142, accuracy : 97.02\n",
            "iteration : 150, loss : 0.1147, accuracy : 96.98\n",
            "iteration : 200, loss : 0.1159, accuracy : 96.91\n",
            "iteration : 250, loss : 0.1168, accuracy : 96.87\n",
            "iteration : 300, loss : 0.1185, accuracy : 96.78\n",
            "iteration : 350, loss : 0.1207, accuracy : 96.72\n",
            "Epoch : 127, training loss : 0.1199, training accuracy : 96.75, test loss : 0.2428, test accuracy : 93.69\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.0947, accuracy : 97.19\n",
            "iteration : 100, loss : 0.1060, accuracy : 96.93\n",
            "iteration : 150, loss : 0.1159, accuracy : 96.73\n",
            "iteration : 200, loss : 0.1159, accuracy : 96.73\n",
            "iteration : 250, loss : 0.1188, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1187, accuracy : 96.69\n",
            "iteration : 350, loss : 0.1187, accuracy : 96.66\n",
            "Epoch : 128, training loss : 0.1200, training accuracy : 96.63, test loss : 0.2490, test accuracy : 93.27\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.1090, accuracy : 97.12\n",
            "iteration : 100, loss : 0.1132, accuracy : 97.15\n",
            "iteration : 150, loss : 0.1112, accuracy : 97.12\n",
            "iteration : 200, loss : 0.1114, accuracy : 97.06\n",
            "iteration : 250, loss : 0.1143, accuracy : 96.93\n",
            "iteration : 300, loss : 0.1163, accuracy : 96.83\n",
            "iteration : 350, loss : 0.1190, accuracy : 96.78\n",
            "Epoch : 129, training loss : 0.1205, training accuracy : 96.75, test loss : 0.2346, test accuracy : 93.81\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.1044, accuracy : 97.14\n",
            "iteration : 100, loss : 0.1048, accuracy : 97.18\n",
            "iteration : 150, loss : 0.1063, accuracy : 97.11\n",
            "iteration : 200, loss : 0.1076, accuracy : 97.16\n",
            "iteration : 250, loss : 0.1089, accuracy : 97.12\n",
            "iteration : 300, loss : 0.1110, accuracy : 97.06\n",
            "iteration : 350, loss : 0.1129, accuracy : 96.96\n",
            "Epoch : 130, training loss : 0.1131, training accuracy : 96.95, test loss : 0.2539, test accuracy : 93.48\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.1206, accuracy : 96.69\n",
            "iteration : 100, loss : 0.1164, accuracy : 96.79\n",
            "iteration : 150, loss : 0.1146, accuracy : 96.87\n",
            "iteration : 200, loss : 0.1164, accuracy : 96.77\n",
            "iteration : 250, loss : 0.1169, accuracy : 96.77\n",
            "iteration : 300, loss : 0.1164, accuracy : 96.81\n",
            "iteration : 350, loss : 0.1161, accuracy : 96.82\n",
            "Epoch : 131, training loss : 0.1177, training accuracy : 96.79, test loss : 0.2666, test accuracy : 93.08\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.1108, accuracy : 96.86\n",
            "iteration : 100, loss : 0.1111, accuracy : 96.91\n",
            "iteration : 150, loss : 0.1103, accuracy : 96.95\n",
            "iteration : 200, loss : 0.1080, accuracy : 97.05\n",
            "iteration : 250, loss : 0.1103, accuracy : 96.95\n",
            "iteration : 300, loss : 0.1124, accuracy : 96.86\n",
            "iteration : 350, loss : 0.1155, accuracy : 96.76\n",
            "Epoch : 132, training loss : 0.1162, training accuracy : 96.75, test loss : 0.2463, test accuracy : 93.73\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.1056, accuracy : 97.27\n",
            "iteration : 100, loss : 0.1042, accuracy : 97.26\n",
            "iteration : 150, loss : 0.1065, accuracy : 97.19\n",
            "iteration : 200, loss : 0.1095, accuracy : 97.09\n",
            "iteration : 250, loss : 0.1120, accuracy : 96.96\n",
            "iteration : 300, loss : 0.1142, accuracy : 96.84\n",
            "iteration : 350, loss : 0.1148, accuracy : 96.83\n",
            "Epoch : 133, training loss : 0.1153, training accuracy : 96.81, test loss : 0.2383, test accuracy : 93.71\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.0944, accuracy : 97.50\n",
            "iteration : 100, loss : 0.1061, accuracy : 97.18\n",
            "iteration : 150, loss : 0.1051, accuracy : 97.22\n",
            "iteration : 200, loss : 0.1070, accuracy : 97.14\n",
            "iteration : 250, loss : 0.1067, accuracy : 97.14\n",
            "iteration : 300, loss : 0.1087, accuracy : 97.09\n",
            "iteration : 350, loss : 0.1104, accuracy : 97.04\n",
            "Epoch : 134, training loss : 0.1106, training accuracy : 97.03, test loss : 0.2447, test accuracy : 93.60\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.1097, accuracy : 97.00\n",
            "iteration : 100, loss : 0.1138, accuracy : 96.92\n",
            "iteration : 150, loss : 0.1165, accuracy : 96.82\n",
            "iteration : 200, loss : 0.1179, accuracy : 96.80\n",
            "iteration : 250, loss : 0.1157, accuracy : 96.86\n",
            "iteration : 300, loss : 0.1162, accuracy : 96.82\n",
            "iteration : 350, loss : 0.1147, accuracy : 96.84\n",
            "Epoch : 135, training loss : 0.1151, training accuracy : 96.83, test loss : 0.2361, test accuracy : 93.92\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.0920, accuracy : 97.64\n",
            "iteration : 100, loss : 0.1006, accuracy : 97.31\n",
            "iteration : 150, loss : 0.1053, accuracy : 97.14\n",
            "iteration : 200, loss : 0.1111, accuracy : 96.99\n",
            "iteration : 250, loss : 0.1127, accuracy : 96.97\n",
            "iteration : 300, loss : 0.1151, accuracy : 96.90\n",
            "iteration : 350, loss : 0.1177, accuracy : 96.84\n",
            "Epoch : 136, training loss : 0.1169, training accuracy : 96.83, test loss : 0.2502, test accuracy : 93.50\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.1105, accuracy : 96.94\n",
            "iteration : 100, loss : 0.1067, accuracy : 97.18\n",
            "iteration : 150, loss : 0.1103, accuracy : 97.09\n",
            "iteration : 200, loss : 0.1119, accuracy : 97.07\n",
            "iteration : 250, loss : 0.1119, accuracy : 97.04\n",
            "iteration : 300, loss : 0.1098, accuracy : 97.08\n",
            "iteration : 350, loss : 0.1097, accuracy : 97.07\n",
            "Epoch : 137, training loss : 0.1092, training accuracy : 97.08, test loss : 0.2553, test accuracy : 93.47\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.1088, accuracy : 96.84\n",
            "iteration : 100, loss : 0.1079, accuracy : 97.00\n",
            "iteration : 150, loss : 0.1074, accuracy : 96.98\n",
            "iteration : 200, loss : 0.1072, accuracy : 97.08\n",
            "iteration : 250, loss : 0.1071, accuracy : 97.12\n",
            "iteration : 300, loss : 0.1099, accuracy : 97.06\n",
            "iteration : 350, loss : 0.1100, accuracy : 97.02\n",
            "Epoch : 138, training loss : 0.1102, training accuracy : 97.01, test loss : 0.2480, test accuracy : 93.69\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.1097, accuracy : 97.03\n",
            "iteration : 100, loss : 0.1057, accuracy : 97.09\n",
            "iteration : 150, loss : 0.1075, accuracy : 97.04\n",
            "iteration : 200, loss : 0.1092, accuracy : 97.01\n",
            "iteration : 250, loss : 0.1096, accuracy : 96.96\n",
            "iteration : 300, loss : 0.1095, accuracy : 96.95\n",
            "iteration : 350, loss : 0.1112, accuracy : 96.91\n",
            "Epoch : 139, training loss : 0.1116, training accuracy : 96.90, test loss : 0.2464, test accuracy : 93.71\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.1085, accuracy : 96.98\n",
            "iteration : 100, loss : 0.1037, accuracy : 97.16\n",
            "iteration : 150, loss : 0.1037, accuracy : 97.18\n",
            "iteration : 200, loss : 0.1051, accuracy : 97.12\n",
            "iteration : 250, loss : 0.1088, accuracy : 97.04\n",
            "iteration : 300, loss : 0.1088, accuracy : 97.04\n",
            "iteration : 350, loss : 0.1081, accuracy : 97.05\n",
            "Epoch : 140, training loss : 0.1089, training accuracy : 97.02, test loss : 0.2476, test accuracy : 93.70\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.0965, accuracy : 97.30\n",
            "iteration : 100, loss : 0.1007, accuracy : 97.25\n",
            "iteration : 150, loss : 0.1008, accuracy : 97.26\n",
            "iteration : 200, loss : 0.0998, accuracy : 97.28\n",
            "iteration : 250, loss : 0.1026, accuracy : 97.22\n",
            "iteration : 300, loss : 0.1053, accuracy : 97.13\n",
            "iteration : 350, loss : 0.1075, accuracy : 97.08\n",
            "Epoch : 141, training loss : 0.1074, training accuracy : 97.09, test loss : 0.2505, test accuracy : 93.56\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.1062, accuracy : 97.33\n",
            "iteration : 100, loss : 0.1017, accuracy : 97.26\n",
            "iteration : 150, loss : 0.1031, accuracy : 97.20\n",
            "iteration : 200, loss : 0.1016, accuracy : 97.22\n",
            "iteration : 250, loss : 0.1011, accuracy : 97.24\n",
            "iteration : 300, loss : 0.1033, accuracy : 97.21\n",
            "iteration : 350, loss : 0.1048, accuracy : 97.12\n",
            "Epoch : 142, training loss : 0.1061, training accuracy : 97.10, test loss : 0.2642, test accuracy : 93.29\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.1004, accuracy : 97.22\n",
            "iteration : 100, loss : 0.1078, accuracy : 96.95\n",
            "iteration : 150, loss : 0.1094, accuracy : 96.97\n",
            "iteration : 200, loss : 0.1098, accuracy : 97.04\n",
            "iteration : 250, loss : 0.1113, accuracy : 96.99\n",
            "iteration : 300, loss : 0.1094, accuracy : 97.03\n",
            "iteration : 350, loss : 0.1101, accuracy : 96.98\n",
            "Epoch : 143, training loss : 0.1103, training accuracy : 96.96, test loss : 0.2428, test accuracy : 93.81\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.1051, accuracy : 97.08\n",
            "iteration : 100, loss : 0.1011, accuracy : 97.26\n",
            "iteration : 150, loss : 0.1008, accuracy : 97.27\n",
            "iteration : 200, loss : 0.1027, accuracy : 97.21\n",
            "iteration : 250, loss : 0.1045, accuracy : 97.14\n",
            "iteration : 300, loss : 0.1054, accuracy : 97.14\n",
            "iteration : 350, loss : 0.1056, accuracy : 97.12\n",
            "Epoch : 144, training loss : 0.1069, training accuracy : 97.11, test loss : 0.2472, test accuracy : 93.59\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.1022, accuracy : 97.27\n",
            "iteration : 100, loss : 0.1020, accuracy : 97.23\n",
            "iteration : 150, loss : 0.1009, accuracy : 97.28\n",
            "iteration : 200, loss : 0.1051, accuracy : 97.14\n",
            "iteration : 250, loss : 0.1063, accuracy : 97.09\n",
            "iteration : 300, loss : 0.1063, accuracy : 97.10\n",
            "iteration : 350, loss : 0.1076, accuracy : 97.03\n",
            "Epoch : 145, training loss : 0.1074, training accuracy : 97.04, test loss : 0.2425, test accuracy : 93.86\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.0973, accuracy : 97.41\n",
            "iteration : 100, loss : 0.0947, accuracy : 97.31\n",
            "iteration : 150, loss : 0.0950, accuracy : 97.31\n",
            "iteration : 200, loss : 0.0988, accuracy : 97.22\n",
            "iteration : 250, loss : 0.0998, accuracy : 97.22\n",
            "iteration : 300, loss : 0.1027, accuracy : 97.13\n",
            "iteration : 350, loss : 0.1038, accuracy : 97.14\n",
            "Epoch : 146, training loss : 0.1036, training accuracy : 97.16, test loss : 0.2424, test accuracy : 93.86\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.0956, accuracy : 97.38\n",
            "iteration : 100, loss : 0.0976, accuracy : 97.38\n",
            "iteration : 150, loss : 0.1019, accuracy : 97.30\n",
            "iteration : 200, loss : 0.1012, accuracy : 97.28\n",
            "iteration : 250, loss : 0.1025, accuracy : 97.24\n",
            "iteration : 300, loss : 0.1048, accuracy : 97.18\n",
            "iteration : 350, loss : 0.1034, accuracy : 97.20\n",
            "Epoch : 147, training loss : 0.1026, training accuracy : 97.22, test loss : 0.2411, test accuracy : 93.86\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.1085, accuracy : 97.19\n",
            "iteration : 100, loss : 0.1014, accuracy : 97.34\n",
            "iteration : 150, loss : 0.1001, accuracy : 97.40\n",
            "iteration : 200, loss : 0.1007, accuracy : 97.36\n",
            "iteration : 250, loss : 0.1017, accuracy : 97.31\n",
            "iteration : 300, loss : 0.1032, accuracy : 97.24\n",
            "iteration : 350, loss : 0.1035, accuracy : 97.20\n",
            "Epoch : 148, training loss : 0.1036, training accuracy : 97.20, test loss : 0.2405, test accuracy : 93.80\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.0953, accuracy : 97.38\n",
            "iteration : 100, loss : 0.0975, accuracy : 97.30\n",
            "iteration : 150, loss : 0.0978, accuracy : 97.30\n",
            "iteration : 200, loss : 0.1012, accuracy : 97.20\n",
            "iteration : 250, loss : 0.1033, accuracy : 97.14\n",
            "iteration : 300, loss : 0.1042, accuracy : 97.17\n",
            "iteration : 350, loss : 0.1032, accuracy : 97.20\n",
            "Epoch : 149, training loss : 0.1031, training accuracy : 97.21, test loss : 0.2458, test accuracy : 93.65\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.0994, accuracy : 97.31\n",
            "iteration : 100, loss : 0.1041, accuracy : 97.17\n",
            "iteration : 150, loss : 0.1004, accuracy : 97.30\n",
            "iteration : 200, loss : 0.0994, accuracy : 97.27\n",
            "iteration : 250, loss : 0.0999, accuracy : 97.27\n",
            "iteration : 300, loss : 0.1004, accuracy : 97.22\n",
            "iteration : 350, loss : 0.1041, accuracy : 97.14\n",
            "Epoch : 150, training loss : 0.1047, training accuracy : 97.11, test loss : 0.2375, test accuracy : 93.93\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.0867, accuracy : 97.78\n",
            "iteration : 100, loss : 0.0917, accuracy : 97.54\n",
            "iteration : 150, loss : 0.0968, accuracy : 97.41\n",
            "iteration : 200, loss : 0.0990, accuracy : 97.36\n",
            "iteration : 250, loss : 0.0984, accuracy : 97.36\n",
            "iteration : 300, loss : 0.1005, accuracy : 97.28\n",
            "iteration : 350, loss : 0.1021, accuracy : 97.26\n",
            "Epoch : 151, training loss : 0.1019, training accuracy : 97.27, test loss : 0.2475, test accuracy : 93.75\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.1089, accuracy : 96.97\n",
            "iteration : 100, loss : 0.1031, accuracy : 97.17\n",
            "iteration : 150, loss : 0.1047, accuracy : 97.18\n",
            "iteration : 200, loss : 0.1038, accuracy : 97.20\n",
            "iteration : 250, loss : 0.1039, accuracy : 97.20\n",
            "iteration : 300, loss : 0.1028, accuracy : 97.21\n",
            "iteration : 350, loss : 0.1031, accuracy : 97.21\n",
            "Epoch : 152, training loss : 0.1027, training accuracy : 97.22, test loss : 0.2451, test accuracy : 93.92\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.0860, accuracy : 97.62\n",
            "iteration : 100, loss : 0.0936, accuracy : 97.45\n",
            "iteration : 150, loss : 0.0947, accuracy : 97.45\n",
            "iteration : 200, loss : 0.0966, accuracy : 97.41\n",
            "iteration : 250, loss : 0.0975, accuracy : 97.35\n",
            "iteration : 300, loss : 0.0999, accuracy : 97.23\n",
            "iteration : 350, loss : 0.0999, accuracy : 97.22\n",
            "Epoch : 153, training loss : 0.0997, training accuracy : 97.23, test loss : 0.2388, test accuracy : 93.92\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.0881, accuracy : 97.56\n",
            "iteration : 100, loss : 0.0871, accuracy : 97.63\n",
            "iteration : 150, loss : 0.0921, accuracy : 97.53\n",
            "iteration : 200, loss : 0.0940, accuracy : 97.46\n",
            "iteration : 250, loss : 0.0929, accuracy : 97.45\n",
            "iteration : 300, loss : 0.0931, accuracy : 97.48\n",
            "iteration : 350, loss : 0.0948, accuracy : 97.46\n",
            "Epoch : 154, training loss : 0.0957, training accuracy : 97.43, test loss : 0.2464, test accuracy : 93.80\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.1092, accuracy : 97.06\n",
            "iteration : 100, loss : 0.1052, accuracy : 97.09\n",
            "iteration : 150, loss : 0.1045, accuracy : 97.14\n",
            "iteration : 200, loss : 0.1049, accuracy : 97.15\n",
            "iteration : 250, loss : 0.1053, accuracy : 97.14\n",
            "iteration : 300, loss : 0.1017, accuracy : 97.24\n",
            "iteration : 350, loss : 0.1030, accuracy : 97.23\n",
            "Epoch : 155, training loss : 0.1035, training accuracy : 97.21, test loss : 0.2422, test accuracy : 94.00\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.0926, accuracy : 97.42\n",
            "iteration : 100, loss : 0.0903, accuracy : 97.59\n",
            "iteration : 150, loss : 0.0923, accuracy : 97.47\n",
            "iteration : 200, loss : 0.0904, accuracy : 97.51\n",
            "iteration : 250, loss : 0.0934, accuracy : 97.42\n",
            "iteration : 300, loss : 0.0962, accuracy : 97.37\n",
            "iteration : 350, loss : 0.0973, accuracy : 97.35\n",
            "Epoch : 156, training loss : 0.0976, training accuracy : 97.35, test loss : 0.2466, test accuracy : 93.77\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.0993, accuracy : 97.53\n",
            "iteration : 100, loss : 0.0936, accuracy : 97.46\n",
            "iteration : 150, loss : 0.0971, accuracy : 97.36\n",
            "iteration : 200, loss : 0.0982, accuracy : 97.34\n",
            "iteration : 250, loss : 0.0987, accuracy : 97.34\n",
            "iteration : 300, loss : 0.0969, accuracy : 97.39\n",
            "iteration : 350, loss : 0.0975, accuracy : 97.37\n",
            "Epoch : 157, training loss : 0.0975, training accuracy : 97.38, test loss : 0.2506, test accuracy : 93.87\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.1021, accuracy : 97.44\n",
            "iteration : 100, loss : 0.0939, accuracy : 97.54\n",
            "iteration : 150, loss : 0.0988, accuracy : 97.32\n",
            "iteration : 200, loss : 0.0970, accuracy : 97.31\n",
            "iteration : 250, loss : 0.0995, accuracy : 97.26\n",
            "iteration : 300, loss : 0.0974, accuracy : 97.33\n",
            "iteration : 350, loss : 0.0977, accuracy : 97.33\n",
            "Epoch : 158, training loss : 0.0977, training accuracy : 97.31, test loss : 0.2419, test accuracy : 93.95\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.0957, accuracy : 97.27\n",
            "iteration : 100, loss : 0.0963, accuracy : 97.37\n",
            "iteration : 150, loss : 0.0936, accuracy : 97.46\n",
            "iteration : 200, loss : 0.0963, accuracy : 97.38\n",
            "iteration : 250, loss : 0.0958, accuracy : 97.39\n",
            "iteration : 300, loss : 0.0959, accuracy : 97.37\n",
            "iteration : 350, loss : 0.0966, accuracy : 97.38\n",
            "Epoch : 159, training loss : 0.0962, training accuracy : 97.38, test loss : 0.2453, test accuracy : 93.95\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.0862, accuracy : 97.66\n",
            "iteration : 100, loss : 0.0896, accuracy : 97.56\n",
            "iteration : 150, loss : 0.0935, accuracy : 97.52\n",
            "iteration : 200, loss : 0.0923, accuracy : 97.52\n",
            "iteration : 250, loss : 0.0929, accuracy : 97.48\n",
            "iteration : 300, loss : 0.0948, accuracy : 97.48\n",
            "iteration : 350, loss : 0.0932, accuracy : 97.51\n",
            "Epoch : 160, training loss : 0.0935, training accuracy : 97.50, test loss : 0.2582, test accuracy : 93.54\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.0882, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0926, accuracy : 97.53\n",
            "iteration : 150, loss : 0.0943, accuracy : 97.43\n",
            "iteration : 200, loss : 0.0951, accuracy : 97.43\n",
            "iteration : 250, loss : 0.0943, accuracy : 97.46\n",
            "iteration : 300, loss : 0.0957, accuracy : 97.44\n",
            "iteration : 350, loss : 0.0947, accuracy : 97.44\n",
            "Epoch : 161, training loss : 0.0945, training accuracy : 97.46, test loss : 0.2321, test accuracy : 94.25\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.0820, accuracy : 97.97\n",
            "iteration : 100, loss : 0.0893, accuracy : 97.70\n",
            "iteration : 150, loss : 0.0934, accuracy : 97.49\n",
            "iteration : 200, loss : 0.0952, accuracy : 97.41\n",
            "iteration : 250, loss : 0.0963, accuracy : 97.44\n",
            "iteration : 300, loss : 0.0961, accuracy : 97.47\n",
            "iteration : 350, loss : 0.0971, accuracy : 97.44\n",
            "Epoch : 162, training loss : 0.0976, training accuracy : 97.44, test loss : 0.2482, test accuracy : 93.78\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.0880, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0865, accuracy : 97.71\n",
            "iteration : 150, loss : 0.0845, accuracy : 97.78\n",
            "iteration : 200, loss : 0.0856, accuracy : 97.76\n",
            "iteration : 250, loss : 0.0876, accuracy : 97.70\n",
            "iteration : 300, loss : 0.0905, accuracy : 97.61\n",
            "iteration : 350, loss : 0.0919, accuracy : 97.55\n",
            "Epoch : 163, training loss : 0.0921, training accuracy : 97.55, test loss : 0.2390, test accuracy : 94.02\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.0828, accuracy : 97.83\n",
            "iteration : 100, loss : 0.0793, accuracy : 97.92\n",
            "iteration : 150, loss : 0.0817, accuracy : 97.81\n",
            "iteration : 200, loss : 0.0824, accuracy : 97.81\n",
            "iteration : 250, loss : 0.0856, accuracy : 97.75\n",
            "iteration : 300, loss : 0.0864, accuracy : 97.71\n",
            "iteration : 350, loss : 0.0908, accuracy : 97.57\n",
            "Epoch : 164, training loss : 0.0911, training accuracy : 97.57, test loss : 0.2504, test accuracy : 93.63\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.0870, accuracy : 97.67\n",
            "iteration : 100, loss : 0.0897, accuracy : 97.55\n",
            "iteration : 150, loss : 0.0892, accuracy : 97.61\n",
            "iteration : 200, loss : 0.0887, accuracy : 97.58\n",
            "iteration : 250, loss : 0.0914, accuracy : 97.52\n",
            "iteration : 300, loss : 0.0921, accuracy : 97.51\n",
            "iteration : 350, loss : 0.0909, accuracy : 97.55\n",
            "Epoch : 165, training loss : 0.0906, training accuracy : 97.56, test loss : 0.2439, test accuracy : 94.06\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.0708, accuracy : 98.14\n",
            "iteration : 100, loss : 0.0857, accuracy : 97.76\n",
            "iteration : 150, loss : 0.0894, accuracy : 97.64\n",
            "iteration : 200, loss : 0.0918, accuracy : 97.59\n",
            "iteration : 250, loss : 0.0906, accuracy : 97.58\n",
            "iteration : 300, loss : 0.0903, accuracy : 97.58\n",
            "iteration : 350, loss : 0.0916, accuracy : 97.54\n",
            "Epoch : 166, training loss : 0.0920, training accuracy : 97.52, test loss : 0.2471, test accuracy : 93.87\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.0939, accuracy : 97.52\n",
            "iteration : 100, loss : 0.0934, accuracy : 97.46\n",
            "iteration : 150, loss : 0.0873, accuracy : 97.68\n",
            "iteration : 200, loss : 0.0881, accuracy : 97.66\n",
            "iteration : 250, loss : 0.0872, accuracy : 97.67\n",
            "iteration : 300, loss : 0.0893, accuracy : 97.63\n",
            "iteration : 350, loss : 0.0899, accuracy : 97.62\n",
            "Epoch : 167, training loss : 0.0897, training accuracy : 97.63, test loss : 0.2467, test accuracy : 93.84\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.0858, accuracy : 97.72\n",
            "iteration : 100, loss : 0.0814, accuracy : 97.80\n",
            "iteration : 150, loss : 0.0825, accuracy : 97.78\n",
            "iteration : 200, loss : 0.0814, accuracy : 97.79\n",
            "iteration : 250, loss : 0.0818, accuracy : 97.76\n",
            "iteration : 300, loss : 0.0834, accuracy : 97.72\n",
            "iteration : 350, loss : 0.0866, accuracy : 97.65\n",
            "Epoch : 168, training loss : 0.0871, training accuracy : 97.64, test loss : 0.2481, test accuracy : 93.95\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.0817, accuracy : 97.97\n",
            "iteration : 100, loss : 0.0816, accuracy : 97.90\n",
            "iteration : 150, loss : 0.0817, accuracy : 97.88\n",
            "iteration : 200, loss : 0.0857, accuracy : 97.83\n",
            "iteration : 250, loss : 0.0865, accuracy : 97.76\n",
            "iteration : 300, loss : 0.0863, accuracy : 97.72\n",
            "iteration : 350, loss : 0.0882, accuracy : 97.65\n",
            "Epoch : 169, training loss : 0.0889, training accuracy : 97.63, test loss : 0.2445, test accuracy : 93.98\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.0818, accuracy : 97.86\n",
            "iteration : 100, loss : 0.0828, accuracy : 97.90\n",
            "iteration : 150, loss : 0.0829, accuracy : 97.85\n",
            "iteration : 200, loss : 0.0834, accuracy : 97.85\n",
            "iteration : 250, loss : 0.0836, accuracy : 97.83\n",
            "iteration : 300, loss : 0.0837, accuracy : 97.83\n",
            "iteration : 350, loss : 0.0841, accuracy : 97.82\n",
            "Epoch : 170, training loss : 0.0843, training accuracy : 97.80, test loss : 0.2475, test accuracy : 93.92\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.0791, accuracy : 98.08\n",
            "iteration : 100, loss : 0.0761, accuracy : 97.96\n",
            "iteration : 150, loss : 0.0754, accuracy : 97.94\n",
            "iteration : 200, loss : 0.0782, accuracy : 97.93\n",
            "iteration : 250, loss : 0.0796, accuracy : 97.85\n",
            "iteration : 300, loss : 0.0811, accuracy : 97.80\n",
            "iteration : 350, loss : 0.0831, accuracy : 97.72\n",
            "Epoch : 171, training loss : 0.0833, training accuracy : 97.71, test loss : 0.2481, test accuracy : 93.82\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.0742, accuracy : 98.11\n",
            "iteration : 100, loss : 0.0824, accuracy : 97.88\n",
            "iteration : 150, loss : 0.0845, accuracy : 97.79\n",
            "iteration : 200, loss : 0.0847, accuracy : 97.75\n",
            "iteration : 250, loss : 0.0862, accuracy : 97.67\n",
            "iteration : 300, loss : 0.0874, accuracy : 97.65\n",
            "iteration : 350, loss : 0.0877, accuracy : 97.65\n",
            "Epoch : 172, training loss : 0.0870, training accuracy : 97.67, test loss : 0.2384, test accuracy : 94.22\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.0765, accuracy : 98.09\n",
            "iteration : 100, loss : 0.0759, accuracy : 97.95\n",
            "iteration : 150, loss : 0.0798, accuracy : 97.89\n",
            "iteration : 200, loss : 0.0825, accuracy : 97.79\n",
            "iteration : 250, loss : 0.0849, accuracy : 97.72\n",
            "iteration : 300, loss : 0.0843, accuracy : 97.75\n",
            "iteration : 350, loss : 0.0847, accuracy : 97.74\n",
            "Epoch : 173, training loss : 0.0850, training accuracy : 97.73, test loss : 0.2466, test accuracy : 93.85\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.0852, accuracy : 97.70\n",
            "iteration : 100, loss : 0.0836, accuracy : 97.75\n",
            "iteration : 150, loss : 0.0804, accuracy : 97.90\n",
            "iteration : 200, loss : 0.0813, accuracy : 97.88\n",
            "iteration : 250, loss : 0.0826, accuracy : 97.78\n",
            "iteration : 300, loss : 0.0839, accuracy : 97.77\n",
            "iteration : 350, loss : 0.0843, accuracy : 97.75\n",
            "Epoch : 174, training loss : 0.0839, training accuracy : 97.76, test loss : 0.2390, test accuracy : 94.18\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.0716, accuracy : 98.17\n",
            "iteration : 100, loss : 0.0767, accuracy : 97.94\n",
            "iteration : 150, loss : 0.0799, accuracy : 97.80\n",
            "iteration : 200, loss : 0.0815, accuracy : 97.78\n",
            "iteration : 250, loss : 0.0800, accuracy : 97.83\n",
            "iteration : 300, loss : 0.0816, accuracy : 97.78\n",
            "iteration : 350, loss : 0.0828, accuracy : 97.77\n",
            "Epoch : 175, training loss : 0.0823, training accuracy : 97.78, test loss : 0.2513, test accuracy : 93.90\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.0712, accuracy : 98.22\n",
            "iteration : 100, loss : 0.0768, accuracy : 97.98\n",
            "iteration : 150, loss : 0.0781, accuracy : 97.93\n",
            "iteration : 200, loss : 0.0792, accuracy : 97.86\n",
            "iteration : 250, loss : 0.0802, accuracy : 97.82\n",
            "iteration : 300, loss : 0.0811, accuracy : 97.79\n",
            "iteration : 350, loss : 0.0828, accuracy : 97.77\n",
            "Epoch : 176, training loss : 0.0830, training accuracy : 97.76, test loss : 0.2538, test accuracy : 93.78\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.0922, accuracy : 97.39\n",
            "iteration : 100, loss : 0.0853, accuracy : 97.66\n",
            "iteration : 150, loss : 0.0844, accuracy : 97.69\n",
            "iteration : 200, loss : 0.0858, accuracy : 97.69\n",
            "iteration : 250, loss : 0.0854, accuracy : 97.67\n",
            "iteration : 300, loss : 0.0854, accuracy : 97.65\n",
            "iteration : 350, loss : 0.0853, accuracy : 97.66\n",
            "Epoch : 177, training loss : 0.0851, training accuracy : 97.67, test loss : 0.2489, test accuracy : 93.84\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.0678, accuracy : 98.11\n",
            "iteration : 100, loss : 0.0753, accuracy : 97.95\n",
            "iteration : 150, loss : 0.0792, accuracy : 97.88\n",
            "iteration : 200, loss : 0.0804, accuracy : 97.83\n",
            "iteration : 250, loss : 0.0800, accuracy : 97.83\n",
            "iteration : 300, loss : 0.0804, accuracy : 97.82\n",
            "iteration : 350, loss : 0.0810, accuracy : 97.81\n",
            "Epoch : 178, training loss : 0.0814, training accuracy : 97.81, test loss : 0.2460, test accuracy : 94.05\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.0760, accuracy : 97.91\n",
            "iteration : 100, loss : 0.0772, accuracy : 97.93\n",
            "iteration : 150, loss : 0.0780, accuracy : 97.97\n",
            "iteration : 200, loss : 0.0781, accuracy : 97.92\n",
            "iteration : 250, loss : 0.0786, accuracy : 97.92\n",
            "iteration : 300, loss : 0.0800, accuracy : 97.86\n",
            "iteration : 350, loss : 0.0807, accuracy : 97.83\n",
            "Epoch : 179, training loss : 0.0814, training accuracy : 97.83, test loss : 0.2427, test accuracy : 94.06\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.0808, accuracy : 98.06\n",
            "iteration : 100, loss : 0.0759, accuracy : 98.09\n",
            "iteration : 150, loss : 0.0763, accuracy : 98.02\n",
            "iteration : 200, loss : 0.0770, accuracy : 98.00\n",
            "iteration : 250, loss : 0.0778, accuracy : 98.01\n",
            "iteration : 300, loss : 0.0791, accuracy : 97.96\n",
            "iteration : 350, loss : 0.0797, accuracy : 97.96\n",
            "Epoch : 180, training loss : 0.0803, training accuracy : 97.93, test loss : 0.2421, test accuracy : 94.08\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.0869, accuracy : 97.59\n",
            "iteration : 100, loss : 0.0813, accuracy : 97.84\n",
            "iteration : 150, loss : 0.0819, accuracy : 97.82\n",
            "iteration : 200, loss : 0.0832, accuracy : 97.80\n",
            "iteration : 250, loss : 0.0823, accuracy : 97.85\n",
            "iteration : 300, loss : 0.0813, accuracy : 97.84\n",
            "iteration : 350, loss : 0.0815, accuracy : 97.85\n",
            "Epoch : 181, training loss : 0.0822, training accuracy : 97.82, test loss : 0.2509, test accuracy : 93.94\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.0708, accuracy : 98.28\n",
            "iteration : 100, loss : 0.0675, accuracy : 98.30\n",
            "iteration : 150, loss : 0.0721, accuracy : 98.09\n",
            "iteration : 200, loss : 0.0746, accuracy : 98.00\n",
            "iteration : 250, loss : 0.0753, accuracy : 97.95\n",
            "iteration : 300, loss : 0.0770, accuracy : 97.89\n",
            "iteration : 350, loss : 0.0768, accuracy : 97.90\n",
            "Epoch : 182, training loss : 0.0770, training accuracy : 97.90, test loss : 0.2507, test accuracy : 93.91\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.0744, accuracy : 97.91\n",
            "iteration : 100, loss : 0.0759, accuracy : 97.95\n",
            "iteration : 150, loss : 0.0736, accuracy : 98.04\n",
            "iteration : 200, loss : 0.0755, accuracy : 97.96\n",
            "iteration : 250, loss : 0.0759, accuracy : 97.94\n",
            "iteration : 300, loss : 0.0758, accuracy : 97.98\n",
            "iteration : 350, loss : 0.0778, accuracy : 97.93\n",
            "Epoch : 183, training loss : 0.0774, training accuracy : 97.94, test loss : 0.2518, test accuracy : 94.00\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.0761, accuracy : 98.11\n",
            "iteration : 100, loss : 0.0746, accuracy : 98.06\n",
            "iteration : 150, loss : 0.0759, accuracy : 98.04\n",
            "iteration : 200, loss : 0.0780, accuracy : 97.97\n",
            "iteration : 250, loss : 0.0792, accuracy : 97.89\n",
            "iteration : 300, loss : 0.0797, accuracy : 97.92\n",
            "iteration : 350, loss : 0.0798, accuracy : 97.94\n",
            "Epoch : 184, training loss : 0.0797, training accuracy : 97.93, test loss : 0.2455, test accuracy : 94.08\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.0743, accuracy : 98.03\n",
            "iteration : 100, loss : 0.0730, accuracy : 98.12\n",
            "iteration : 150, loss : 0.0768, accuracy : 98.00\n",
            "iteration : 200, loss : 0.0752, accuracy : 98.04\n",
            "iteration : 250, loss : 0.0739, accuracy : 98.10\n",
            "iteration : 300, loss : 0.0748, accuracy : 98.08\n",
            "iteration : 350, loss : 0.0756, accuracy : 98.00\n",
            "Epoch : 185, training loss : 0.0756, training accuracy : 98.00, test loss : 0.2465, test accuracy : 94.05\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.0872, accuracy : 97.69\n",
            "iteration : 100, loss : 0.0864, accuracy : 97.70\n",
            "iteration : 150, loss : 0.0842, accuracy : 97.77\n",
            "iteration : 200, loss : 0.0828, accuracy : 97.85\n",
            "iteration : 250, loss : 0.0799, accuracy : 97.90\n",
            "iteration : 300, loss : 0.0791, accuracy : 97.92\n",
            "iteration : 350, loss : 0.0787, accuracy : 97.92\n",
            "Epoch : 186, training loss : 0.0791, training accuracy : 97.91, test loss : 0.2429, test accuracy : 93.96\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.0683, accuracy : 98.31\n",
            "iteration : 100, loss : 0.0756, accuracy : 98.06\n",
            "iteration : 150, loss : 0.0742, accuracy : 98.12\n",
            "iteration : 200, loss : 0.0738, accuracy : 98.12\n",
            "iteration : 250, loss : 0.0732, accuracy : 98.10\n",
            "iteration : 300, loss : 0.0732, accuracy : 98.09\n",
            "iteration : 350, loss : 0.0740, accuracy : 98.08\n",
            "Epoch : 187, training loss : 0.0740, training accuracy : 98.06, test loss : 0.2533, test accuracy : 93.83\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.0668, accuracy : 98.20\n",
            "iteration : 100, loss : 0.0668, accuracy : 98.26\n",
            "iteration : 150, loss : 0.0691, accuracy : 98.17\n",
            "iteration : 200, loss : 0.0677, accuracy : 98.19\n",
            "iteration : 250, loss : 0.0683, accuracy : 98.19\n",
            "iteration : 300, loss : 0.0719, accuracy : 98.09\n",
            "iteration : 350, loss : 0.0727, accuracy : 98.06\n",
            "Epoch : 188, training loss : 0.0735, training accuracy : 98.03, test loss : 0.2491, test accuracy : 94.05\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.0772, accuracy : 97.92\n",
            "iteration : 100, loss : 0.0735, accuracy : 98.05\n",
            "iteration : 150, loss : 0.0696, accuracy : 98.17\n",
            "iteration : 200, loss : 0.0708, accuracy : 98.11\n",
            "iteration : 250, loss : 0.0717, accuracy : 98.09\n",
            "iteration : 300, loss : 0.0734, accuracy : 98.05\n",
            "iteration : 350, loss : 0.0734, accuracy : 98.06\n",
            "Epoch : 189, training loss : 0.0736, training accuracy : 98.05, test loss : 0.2507, test accuracy : 94.08\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.0623, accuracy : 98.39\n",
            "iteration : 100, loss : 0.0648, accuracy : 98.29\n",
            "iteration : 150, loss : 0.0655, accuracy : 98.27\n",
            "iteration : 200, loss : 0.0656, accuracy : 98.28\n",
            "iteration : 250, loss : 0.0657, accuracy : 98.28\n",
            "iteration : 300, loss : 0.0697, accuracy : 98.16\n",
            "iteration : 350, loss : 0.0694, accuracy : 98.16\n",
            "Epoch : 190, training loss : 0.0698, training accuracy : 98.14, test loss : 0.2462, test accuracy : 94.08\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.0761, accuracy : 98.12\n",
            "iteration : 100, loss : 0.0731, accuracy : 98.10\n",
            "iteration : 150, loss : 0.0721, accuracy : 98.20\n",
            "iteration : 200, loss : 0.0733, accuracy : 98.17\n",
            "iteration : 250, loss : 0.0734, accuracy : 98.19\n",
            "iteration : 300, loss : 0.0730, accuracy : 98.21\n",
            "iteration : 350, loss : 0.0745, accuracy : 98.14\n",
            "Epoch : 191, training loss : 0.0746, training accuracy : 98.13, test loss : 0.2490, test accuracy : 93.98\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.0714, accuracy : 98.22\n",
            "iteration : 100, loss : 0.0680, accuracy : 98.27\n",
            "iteration : 150, loss : 0.0675, accuracy : 98.32\n",
            "iteration : 200, loss : 0.0687, accuracy : 98.27\n",
            "iteration : 250, loss : 0.0692, accuracy : 98.22\n",
            "iteration : 300, loss : 0.0690, accuracy : 98.21\n",
            "iteration : 350, loss : 0.0691, accuracy : 98.21\n",
            "Epoch : 192, training loss : 0.0695, training accuracy : 98.20, test loss : 0.2491, test accuracy : 94.03\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.0724, accuracy : 98.06\n",
            "iteration : 100, loss : 0.0692, accuracy : 98.13\n",
            "iteration : 150, loss : 0.0698, accuracy : 98.19\n",
            "iteration : 200, loss : 0.0696, accuracy : 98.20\n",
            "iteration : 250, loss : 0.0685, accuracy : 98.23\n",
            "iteration : 300, loss : 0.0698, accuracy : 98.17\n",
            "iteration : 350, loss : 0.0693, accuracy : 98.17\n",
            "Epoch : 193, training loss : 0.0697, training accuracy : 98.15, test loss : 0.2463, test accuracy : 94.22\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.0641, accuracy : 98.39\n",
            "iteration : 100, loss : 0.0646, accuracy : 98.37\n",
            "iteration : 150, loss : 0.0630, accuracy : 98.45\n",
            "iteration : 200, loss : 0.0648, accuracy : 98.38\n",
            "iteration : 250, loss : 0.0657, accuracy : 98.37\n",
            "iteration : 300, loss : 0.0653, accuracy : 98.36\n",
            "iteration : 350, loss : 0.0666, accuracy : 98.31\n",
            "Epoch : 194, training loss : 0.0668, training accuracy : 98.30, test loss : 0.2462, test accuracy : 94.23\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.0634, accuracy : 98.20\n",
            "iteration : 100, loss : 0.0669, accuracy : 98.17\n",
            "iteration : 150, loss : 0.0676, accuracy : 98.16\n",
            "iteration : 200, loss : 0.0687, accuracy : 98.11\n",
            "iteration : 250, loss : 0.0684, accuracy : 98.13\n",
            "iteration : 300, loss : 0.0680, accuracy : 98.16\n",
            "iteration : 350, loss : 0.0689, accuracy : 98.15\n",
            "Epoch : 195, training loss : 0.0693, training accuracy : 98.13, test loss : 0.2518, test accuracy : 93.93\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.0670, accuracy : 98.30\n",
            "iteration : 100, loss : 0.0682, accuracy : 98.20\n",
            "iteration : 150, loss : 0.0669, accuracy : 98.25\n",
            "iteration : 200, loss : 0.0677, accuracy : 98.22\n",
            "iteration : 250, loss : 0.0685, accuracy : 98.18\n",
            "iteration : 300, loss : 0.0687, accuracy : 98.20\n",
            "iteration : 350, loss : 0.0683, accuracy : 98.21\n",
            "Epoch : 196, training loss : 0.0692, training accuracy : 98.18, test loss : 0.2479, test accuracy : 94.18\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.0680, accuracy : 98.36\n",
            "iteration : 100, loss : 0.0664, accuracy : 98.37\n",
            "iteration : 150, loss : 0.0649, accuracy : 98.41\n",
            "iteration : 200, loss : 0.0655, accuracy : 98.36\n",
            "iteration : 250, loss : 0.0659, accuracy : 98.38\n",
            "iteration : 300, loss : 0.0667, accuracy : 98.34\n",
            "iteration : 350, loss : 0.0670, accuracy : 98.34\n",
            "Epoch : 197, training loss : 0.0675, training accuracy : 98.33, test loss : 0.2504, test accuracy : 94.08\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.0705, accuracy : 98.17\n",
            "iteration : 100, loss : 0.0644, accuracy : 98.34\n",
            "iteration : 150, loss : 0.0648, accuracy : 98.32\n",
            "iteration : 200, loss : 0.0627, accuracy : 98.38\n",
            "iteration : 250, loss : 0.0637, accuracy : 98.35\n",
            "iteration : 300, loss : 0.0640, accuracy : 98.33\n",
            "iteration : 350, loss : 0.0641, accuracy : 98.31\n",
            "Epoch : 198, training loss : 0.0639, training accuracy : 98.33, test loss : 0.2459, test accuracy : 94.18\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.0578, accuracy : 98.53\n",
            "iteration : 100, loss : 0.0584, accuracy : 98.48\n",
            "iteration : 150, loss : 0.0620, accuracy : 98.41\n",
            "iteration : 200, loss : 0.0624, accuracy : 98.34\n",
            "iteration : 250, loss : 0.0652, accuracy : 98.31\n",
            "iteration : 300, loss : 0.0656, accuracy : 98.33\n",
            "iteration : 350, loss : 0.0651, accuracy : 98.35\n",
            "Epoch : 199, training loss : 0.0661, training accuracy : 98.32, test loss : 0.2514, test accuracy : 94.06\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.0679, accuracy : 98.19\n",
            "iteration : 100, loss : 0.0639, accuracy : 98.33\n",
            "iteration : 150, loss : 0.0644, accuracy : 98.32\n",
            "iteration : 200, loss : 0.0674, accuracy : 98.23\n",
            "iteration : 250, loss : 0.0659, accuracy : 98.25\n",
            "iteration : 300, loss : 0.0659, accuracy : 98.24\n",
            "iteration : 350, loss : 0.0661, accuracy : 98.24\n",
            "Epoch : 200, training loss : 0.0667, training accuracy : 98.21, test loss : 0.2532, test accuracy : 94.07\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.0686, accuracy : 98.20\n",
            "iteration : 100, loss : 0.0669, accuracy : 98.26\n",
            "iteration : 150, loss : 0.0646, accuracy : 98.33\n",
            "iteration : 200, loss : 0.0639, accuracy : 98.37\n",
            "iteration : 250, loss : 0.0646, accuracy : 98.33\n",
            "iteration : 300, loss : 0.0654, accuracy : 98.32\n",
            "iteration : 350, loss : 0.0650, accuracy : 98.32\n",
            "Epoch : 201, training loss : 0.0648, training accuracy : 98.34, test loss : 0.2512, test accuracy : 94.05\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.0665, accuracy : 98.36\n",
            "iteration : 100, loss : 0.0635, accuracy : 98.42\n",
            "iteration : 150, loss : 0.0617, accuracy : 98.44\n",
            "iteration : 200, loss : 0.0607, accuracy : 98.46\n",
            "iteration : 250, loss : 0.0611, accuracy : 98.44\n",
            "iteration : 300, loss : 0.0624, accuracy : 98.39\n",
            "iteration : 350, loss : 0.0623, accuracy : 98.40\n",
            "Epoch : 202, training loss : 0.0629, training accuracy : 98.37, test loss : 0.2434, test accuracy : 94.16\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.0517, accuracy : 98.66\n",
            "iteration : 100, loss : 0.0589, accuracy : 98.45\n",
            "iteration : 150, loss : 0.0595, accuracy : 98.44\n",
            "iteration : 200, loss : 0.0623, accuracy : 98.37\n",
            "iteration : 250, loss : 0.0624, accuracy : 98.38\n",
            "iteration : 300, loss : 0.0640, accuracy : 98.35\n",
            "iteration : 350, loss : 0.0637, accuracy : 98.36\n",
            "Epoch : 203, training loss : 0.0637, training accuracy : 98.36, test loss : 0.2444, test accuracy : 94.36\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.0564, accuracy : 98.59\n",
            "iteration : 100, loss : 0.0595, accuracy : 98.52\n",
            "iteration : 150, loss : 0.0591, accuracy : 98.59\n",
            "iteration : 200, loss : 0.0590, accuracy : 98.54\n",
            "iteration : 250, loss : 0.0600, accuracy : 98.54\n",
            "iteration : 300, loss : 0.0612, accuracy : 98.50\n",
            "iteration : 350, loss : 0.0619, accuracy : 98.47\n",
            "Epoch : 204, training loss : 0.0626, training accuracy : 98.45, test loss : 0.2479, test accuracy : 94.19\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.0577, accuracy : 98.61\n",
            "iteration : 100, loss : 0.0575, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0613, accuracy : 98.51\n",
            "iteration : 200, loss : 0.0610, accuracy : 98.48\n",
            "iteration : 250, loss : 0.0602, accuracy : 98.50\n",
            "iteration : 300, loss : 0.0602, accuracy : 98.50\n",
            "iteration : 350, loss : 0.0609, accuracy : 98.48\n",
            "Epoch : 205, training loss : 0.0607, training accuracy : 98.49, test loss : 0.2489, test accuracy : 94.35\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.0555, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0593, accuracy : 98.55\n",
            "iteration : 150, loss : 0.0591, accuracy : 98.55\n",
            "iteration : 200, loss : 0.0596, accuracy : 98.53\n",
            "iteration : 250, loss : 0.0589, accuracy : 98.56\n",
            "iteration : 300, loss : 0.0593, accuracy : 98.52\n",
            "iteration : 350, loss : 0.0604, accuracy : 98.49\n",
            "Epoch : 206, training loss : 0.0613, training accuracy : 98.46, test loss : 0.2505, test accuracy : 94.22\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.0530, accuracy : 98.64\n",
            "iteration : 100, loss : 0.0538, accuracy : 98.57\n",
            "iteration : 150, loss : 0.0542, accuracy : 98.57\n",
            "iteration : 200, loss : 0.0567, accuracy : 98.48\n",
            "iteration : 250, loss : 0.0587, accuracy : 98.44\n",
            "iteration : 300, loss : 0.0595, accuracy : 98.43\n",
            "iteration : 350, loss : 0.0603, accuracy : 98.42\n",
            "Epoch : 207, training loss : 0.0605, training accuracy : 98.41, test loss : 0.2566, test accuracy : 94.07\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.0563, accuracy : 98.53\n",
            "iteration : 100, loss : 0.0605, accuracy : 98.39\n",
            "iteration : 150, loss : 0.0588, accuracy : 98.50\n",
            "iteration : 200, loss : 0.0589, accuracy : 98.52\n",
            "iteration : 250, loss : 0.0599, accuracy : 98.50\n",
            "iteration : 300, loss : 0.0593, accuracy : 98.52\n",
            "iteration : 350, loss : 0.0596, accuracy : 98.50\n",
            "Epoch : 208, training loss : 0.0595, training accuracy : 98.51, test loss : 0.2451, test accuracy : 94.22\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.0515, accuracy : 98.62\n",
            "iteration : 100, loss : 0.0520, accuracy : 98.69\n",
            "iteration : 150, loss : 0.0537, accuracy : 98.69\n",
            "iteration : 200, loss : 0.0561, accuracy : 98.63\n",
            "iteration : 250, loss : 0.0578, accuracy : 98.58\n",
            "iteration : 300, loss : 0.0599, accuracy : 98.52\n",
            "iteration : 350, loss : 0.0611, accuracy : 98.48\n",
            "Epoch : 209, training loss : 0.0611, training accuracy : 98.48, test loss : 0.2457, test accuracy : 94.26\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.0516, accuracy : 98.83\n",
            "iteration : 100, loss : 0.0568, accuracy : 98.65\n",
            "iteration : 150, loss : 0.0572, accuracy : 98.64\n",
            "iteration : 200, loss : 0.0554, accuracy : 98.67\n",
            "iteration : 250, loss : 0.0558, accuracy : 98.63\n",
            "iteration : 300, loss : 0.0562, accuracy : 98.58\n",
            "iteration : 350, loss : 0.0587, accuracy : 98.51\n",
            "Epoch : 210, training loss : 0.0593, training accuracy : 98.49, test loss : 0.2534, test accuracy : 94.01\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.0549, accuracy : 98.70\n",
            "iteration : 100, loss : 0.0583, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0592, accuracy : 98.55\n",
            "iteration : 200, loss : 0.0586, accuracy : 98.58\n",
            "iteration : 250, loss : 0.0587, accuracy : 98.55\n",
            "iteration : 300, loss : 0.0585, accuracy : 98.54\n",
            "iteration : 350, loss : 0.0582, accuracy : 98.55\n",
            "Epoch : 211, training loss : 0.0587, training accuracy : 98.53, test loss : 0.2508, test accuracy : 94.11\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.0503, accuracy : 98.78\n",
            "iteration : 100, loss : 0.0526, accuracy : 98.67\n",
            "iteration : 150, loss : 0.0536, accuracy : 98.66\n",
            "iteration : 200, loss : 0.0547, accuracy : 98.64\n",
            "iteration : 250, loss : 0.0567, accuracy : 98.60\n",
            "iteration : 300, loss : 0.0566, accuracy : 98.59\n",
            "iteration : 350, loss : 0.0583, accuracy : 98.54\n",
            "Epoch : 212, training loss : 0.0582, training accuracy : 98.54, test loss : 0.2437, test accuracy : 94.29\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.0572, accuracy : 98.45\n",
            "iteration : 100, loss : 0.0540, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0538, accuracy : 98.64\n",
            "iteration : 200, loss : 0.0544, accuracy : 98.63\n",
            "iteration : 250, loss : 0.0555, accuracy : 98.59\n",
            "iteration : 300, loss : 0.0555, accuracy : 98.60\n",
            "iteration : 350, loss : 0.0559, accuracy : 98.60\n",
            "Epoch : 213, training loss : 0.0565, training accuracy : 98.56, test loss : 0.2483, test accuracy : 94.30\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.0547, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0557, accuracy : 98.62\n",
            "iteration : 150, loss : 0.0520, accuracy : 98.74\n",
            "iteration : 200, loss : 0.0533, accuracy : 98.72\n",
            "iteration : 250, loss : 0.0543, accuracy : 98.68\n",
            "iteration : 300, loss : 0.0550, accuracy : 98.66\n",
            "iteration : 350, loss : 0.0547, accuracy : 98.68\n",
            "Epoch : 214, training loss : 0.0553, training accuracy : 98.66, test loss : 0.2469, test accuracy : 94.35\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.0534, accuracy : 98.67\n",
            "iteration : 100, loss : 0.0560, accuracy : 98.59\n",
            "iteration : 150, loss : 0.0553, accuracy : 98.60\n",
            "iteration : 200, loss : 0.0550, accuracy : 98.62\n",
            "iteration : 250, loss : 0.0549, accuracy : 98.61\n",
            "iteration : 300, loss : 0.0561, accuracy : 98.58\n",
            "iteration : 350, loss : 0.0560, accuracy : 98.56\n",
            "Epoch : 215, training loss : 0.0564, training accuracy : 98.55, test loss : 0.2577, test accuracy : 94.19\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.0544, accuracy : 98.59\n",
            "iteration : 100, loss : 0.0575, accuracy : 98.51\n",
            "iteration : 150, loss : 0.0569, accuracy : 98.57\n",
            "iteration : 200, loss : 0.0562, accuracy : 98.62\n",
            "iteration : 250, loss : 0.0571, accuracy : 98.64\n",
            "iteration : 300, loss : 0.0569, accuracy : 98.62\n",
            "iteration : 350, loss : 0.0567, accuracy : 98.63\n",
            "Epoch : 216, training loss : 0.0567, training accuracy : 98.64, test loss : 0.2477, test accuracy : 94.45\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.0507, accuracy : 98.83\n",
            "iteration : 100, loss : 0.0523, accuracy : 98.80\n",
            "iteration : 150, loss : 0.0518, accuracy : 98.78\n",
            "iteration : 200, loss : 0.0518, accuracy : 98.79\n",
            "iteration : 250, loss : 0.0520, accuracy : 98.79\n",
            "iteration : 300, loss : 0.0527, accuracy : 98.72\n",
            "iteration : 350, loss : 0.0543, accuracy : 98.67\n",
            "Epoch : 217, training loss : 0.0548, training accuracy : 98.66, test loss : 0.2509, test accuracy : 94.21\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.0495, accuracy : 98.84\n",
            "iteration : 100, loss : 0.0519, accuracy : 98.74\n",
            "iteration : 150, loss : 0.0536, accuracy : 98.68\n",
            "iteration : 200, loss : 0.0534, accuracy : 98.66\n",
            "iteration : 250, loss : 0.0535, accuracy : 98.67\n",
            "iteration : 300, loss : 0.0549, accuracy : 98.64\n",
            "iteration : 350, loss : 0.0551, accuracy : 98.62\n",
            "Epoch : 218, training loss : 0.0548, training accuracy : 98.64, test loss : 0.2509, test accuracy : 94.36\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.0521, accuracy : 98.75\n",
            "iteration : 100, loss : 0.0509, accuracy : 98.72\n",
            "iteration : 150, loss : 0.0535, accuracy : 98.65\n",
            "iteration : 200, loss : 0.0521, accuracy : 98.68\n",
            "iteration : 250, loss : 0.0520, accuracy : 98.69\n",
            "iteration : 300, loss : 0.0519, accuracy : 98.69\n",
            "iteration : 350, loss : 0.0523, accuracy : 98.68\n",
            "Epoch : 219, training loss : 0.0527, training accuracy : 98.69, test loss : 0.2495, test accuracy : 94.26\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.0523, accuracy : 98.77\n",
            "iteration : 100, loss : 0.0514, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0526, accuracy : 98.74\n",
            "iteration : 200, loss : 0.0527, accuracy : 98.73\n",
            "iteration : 250, loss : 0.0539, accuracy : 98.70\n",
            "iteration : 300, loss : 0.0533, accuracy : 98.71\n",
            "iteration : 350, loss : 0.0528, accuracy : 98.71\n",
            "Epoch : 220, training loss : 0.0527, training accuracy : 98.70, test loss : 0.2427, test accuracy : 94.40\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.0520, accuracy : 98.83\n",
            "iteration : 100, loss : 0.0537, accuracy : 98.67\n",
            "iteration : 150, loss : 0.0526, accuracy : 98.70\n",
            "iteration : 200, loss : 0.0529, accuracy : 98.72\n",
            "iteration : 250, loss : 0.0519, accuracy : 98.73\n",
            "iteration : 300, loss : 0.0519, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0522, accuracy : 98.74\n",
            "Epoch : 221, training loss : 0.0519, training accuracy : 98.74, test loss : 0.2486, test accuracy : 94.33\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.0435, accuracy : 98.95\n",
            "iteration : 100, loss : 0.0479, accuracy : 98.84\n",
            "iteration : 150, loss : 0.0471, accuracy : 98.88\n",
            "iteration : 200, loss : 0.0474, accuracy : 98.87\n",
            "iteration : 250, loss : 0.0485, accuracy : 98.84\n",
            "iteration : 300, loss : 0.0488, accuracy : 98.82\n",
            "iteration : 350, loss : 0.0491, accuracy : 98.81\n",
            "Epoch : 222, training loss : 0.0491, training accuracy : 98.81, test loss : 0.2488, test accuracy : 94.38\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.0472, accuracy : 98.81\n",
            "iteration : 100, loss : 0.0447, accuracy : 98.90\n",
            "iteration : 150, loss : 0.0453, accuracy : 98.85\n",
            "iteration : 200, loss : 0.0479, accuracy : 98.80\n",
            "iteration : 250, loss : 0.0479, accuracy : 98.81\n",
            "iteration : 300, loss : 0.0496, accuracy : 98.77\n",
            "iteration : 350, loss : 0.0505, accuracy : 98.74\n",
            "Epoch : 223, training loss : 0.0503, training accuracy : 98.74, test loss : 0.2553, test accuracy : 94.15\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.0561, accuracy : 98.47\n",
            "iteration : 100, loss : 0.0516, accuracy : 98.67\n",
            "iteration : 150, loss : 0.0515, accuracy : 98.68\n",
            "iteration : 200, loss : 0.0518, accuracy : 98.72\n",
            "iteration : 250, loss : 0.0505, accuracy : 98.76\n",
            "iteration : 300, loss : 0.0511, accuracy : 98.73\n",
            "iteration : 350, loss : 0.0516, accuracy : 98.72\n",
            "Epoch : 224, training loss : 0.0516, training accuracy : 98.72, test loss : 0.2473, test accuracy : 94.41\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.0538, accuracy : 98.70\n",
            "iteration : 100, loss : 0.0515, accuracy : 98.77\n",
            "iteration : 150, loss : 0.0517, accuracy : 98.74\n",
            "iteration : 200, loss : 0.0510, accuracy : 98.77\n",
            "iteration : 250, loss : 0.0501, accuracy : 98.78\n",
            "iteration : 300, loss : 0.0499, accuracy : 98.78\n",
            "iteration : 350, loss : 0.0493, accuracy : 98.80\n",
            "Epoch : 225, training loss : 0.0496, training accuracy : 98.79, test loss : 0.2526, test accuracy : 94.31\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.0462, accuracy : 99.02\n",
            "iteration : 100, loss : 0.0457, accuracy : 99.02\n",
            "iteration : 150, loss : 0.0459, accuracy : 99.00\n",
            "iteration : 200, loss : 0.0465, accuracy : 98.96\n",
            "iteration : 250, loss : 0.0465, accuracy : 98.93\n",
            "iteration : 300, loss : 0.0472, accuracy : 98.92\n",
            "iteration : 350, loss : 0.0468, accuracy : 98.94\n",
            "Epoch : 226, training loss : 0.0470, training accuracy : 98.93, test loss : 0.2549, test accuracy : 94.34\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.0480, accuracy : 98.78\n",
            "iteration : 100, loss : 0.0454, accuracy : 98.95\n",
            "iteration : 150, loss : 0.0453, accuracy : 98.94\n",
            "iteration : 200, loss : 0.0457, accuracy : 98.90\n",
            "iteration : 250, loss : 0.0463, accuracy : 98.89\n",
            "iteration : 300, loss : 0.0458, accuracy : 98.89\n",
            "iteration : 350, loss : 0.0476, accuracy : 98.85\n",
            "Epoch : 227, training loss : 0.0476, training accuracy : 98.84, test loss : 0.2562, test accuracy : 94.22\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.0430, accuracy : 98.98\n",
            "iteration : 100, loss : 0.0465, accuracy : 98.90\n",
            "iteration : 150, loss : 0.0482, accuracy : 98.81\n",
            "iteration : 200, loss : 0.0482, accuracy : 98.79\n",
            "iteration : 250, loss : 0.0470, accuracy : 98.84\n",
            "iteration : 300, loss : 0.0478, accuracy : 98.85\n",
            "iteration : 350, loss : 0.0473, accuracy : 98.86\n",
            "Epoch : 228, training loss : 0.0476, training accuracy : 98.87, test loss : 0.2525, test accuracy : 94.27\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.0399, accuracy : 99.09\n",
            "iteration : 100, loss : 0.0409, accuracy : 99.08\n",
            "iteration : 150, loss : 0.0450, accuracy : 98.95\n",
            "iteration : 200, loss : 0.0451, accuracy : 98.89\n",
            "iteration : 250, loss : 0.0451, accuracy : 98.91\n",
            "iteration : 300, loss : 0.0463, accuracy : 98.88\n",
            "iteration : 350, loss : 0.0460, accuracy : 98.89\n",
            "Epoch : 229, training loss : 0.0459, training accuracy : 98.89, test loss : 0.2507, test accuracy : 94.33\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.0422, accuracy : 99.17\n",
            "iteration : 100, loss : 0.0448, accuracy : 99.06\n",
            "iteration : 150, loss : 0.0436, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0443, accuracy : 99.07\n",
            "iteration : 250, loss : 0.0452, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0458, accuracy : 99.00\n",
            "iteration : 350, loss : 0.0460, accuracy : 98.97\n",
            "Epoch : 230, training loss : 0.0460, training accuracy : 98.96, test loss : 0.2457, test accuracy : 94.37\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.0513, accuracy : 98.61\n",
            "iteration : 100, loss : 0.0465, accuracy : 98.76\n",
            "iteration : 150, loss : 0.0459, accuracy : 98.82\n",
            "iteration : 200, loss : 0.0460, accuracy : 98.81\n",
            "iteration : 250, loss : 0.0454, accuracy : 98.85\n",
            "iteration : 300, loss : 0.0455, accuracy : 98.85\n",
            "iteration : 350, loss : 0.0455, accuracy : 98.85\n",
            "Epoch : 231, training loss : 0.0452, training accuracy : 98.86, test loss : 0.2469, test accuracy : 94.50\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.0476, accuracy : 98.78\n",
            "iteration : 100, loss : 0.0469, accuracy : 98.83\n",
            "iteration : 150, loss : 0.0438, accuracy : 98.93\n",
            "iteration : 200, loss : 0.0453, accuracy : 98.88\n",
            "iteration : 250, loss : 0.0453, accuracy : 98.90\n",
            "iteration : 300, loss : 0.0453, accuracy : 98.90\n",
            "iteration : 350, loss : 0.0455, accuracy : 98.91\n",
            "Epoch : 232, training loss : 0.0454, training accuracy : 98.91, test loss : 0.2487, test accuracy : 94.43\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.0421, accuracy : 98.91\n",
            "iteration : 100, loss : 0.0431, accuracy : 98.94\n",
            "iteration : 150, loss : 0.0436, accuracy : 98.97\n",
            "iteration : 200, loss : 0.0435, accuracy : 98.97\n",
            "iteration : 250, loss : 0.0445, accuracy : 98.95\n",
            "iteration : 300, loss : 0.0444, accuracy : 98.95\n",
            "iteration : 350, loss : 0.0434, accuracy : 98.98\n",
            "Epoch : 233, training loss : 0.0437, training accuracy : 98.98, test loss : 0.2533, test accuracy : 94.53\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.0463, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0443, accuracy : 98.95\n",
            "iteration : 150, loss : 0.0427, accuracy : 99.03\n",
            "iteration : 200, loss : 0.0431, accuracy : 99.00\n",
            "iteration : 250, loss : 0.0441, accuracy : 98.97\n",
            "iteration : 300, loss : 0.0436, accuracy : 98.97\n",
            "iteration : 350, loss : 0.0436, accuracy : 98.98\n",
            "Epoch : 234, training loss : 0.0434, training accuracy : 98.99, test loss : 0.2523, test accuracy : 94.46\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.0429, accuracy : 99.00\n",
            "iteration : 100, loss : 0.0415, accuracy : 98.97\n",
            "iteration : 150, loss : 0.0438, accuracy : 98.95\n",
            "iteration : 200, loss : 0.0414, accuracy : 99.06\n",
            "iteration : 250, loss : 0.0405, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0409, accuracy : 99.09\n",
            "iteration : 350, loss : 0.0419, accuracy : 99.06\n",
            "Epoch : 235, training loss : 0.0421, training accuracy : 99.04, test loss : 0.2480, test accuracy : 94.30\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.0439, accuracy : 98.86\n",
            "iteration : 100, loss : 0.0431, accuracy : 99.00\n",
            "iteration : 150, loss : 0.0420, accuracy : 99.06\n",
            "iteration : 200, loss : 0.0423, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0426, accuracy : 99.03\n",
            "iteration : 300, loss : 0.0434, accuracy : 98.99\n",
            "iteration : 350, loss : 0.0432, accuracy : 99.00\n",
            "Epoch : 236, training loss : 0.0441, training accuracy : 98.97, test loss : 0.2464, test accuracy : 94.48\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.0446, accuracy : 98.98\n",
            "iteration : 100, loss : 0.0417, accuracy : 99.03\n",
            "iteration : 150, loss : 0.0412, accuracy : 99.05\n",
            "iteration : 200, loss : 0.0412, accuracy : 99.04\n",
            "iteration : 250, loss : 0.0408, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0411, accuracy : 99.06\n",
            "iteration : 350, loss : 0.0416, accuracy : 99.03\n",
            "Epoch : 237, training loss : 0.0416, training accuracy : 99.03, test loss : 0.2439, test accuracy : 94.52\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.0410, accuracy : 99.16\n",
            "iteration : 100, loss : 0.0426, accuracy : 99.09\n",
            "iteration : 150, loss : 0.0428, accuracy : 99.06\n",
            "iteration : 200, loss : 0.0423, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0422, accuracy : 99.07\n",
            "iteration : 300, loss : 0.0427, accuracy : 99.04\n",
            "iteration : 350, loss : 0.0428, accuracy : 99.03\n",
            "Epoch : 238, training loss : 0.0425, training accuracy : 99.04, test loss : 0.2542, test accuracy : 94.31\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.0431, accuracy : 98.97\n",
            "iteration : 100, loss : 0.0417, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0410, accuracy : 99.08\n",
            "iteration : 200, loss : 0.0414, accuracy : 99.08\n",
            "iteration : 250, loss : 0.0414, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0412, accuracy : 99.08\n",
            "iteration : 350, loss : 0.0404, accuracy : 99.10\n",
            "Epoch : 239, training loss : 0.0405, training accuracy : 99.10, test loss : 0.2507, test accuracy : 94.48\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.0399, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0400, accuracy : 99.04\n",
            "iteration : 150, loss : 0.0404, accuracy : 99.09\n",
            "iteration : 200, loss : 0.0407, accuracy : 99.08\n",
            "iteration : 250, loss : 0.0399, accuracy : 99.11\n",
            "iteration : 300, loss : 0.0402, accuracy : 99.11\n",
            "iteration : 350, loss : 0.0394, accuracy : 99.11\n",
            "Epoch : 240, training loss : 0.0394, training accuracy : 99.12, test loss : 0.2517, test accuracy : 94.40\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.0388, accuracy : 99.08\n",
            "iteration : 100, loss : 0.0403, accuracy : 99.11\n",
            "iteration : 150, loss : 0.0385, accuracy : 99.19\n",
            "iteration : 200, loss : 0.0381, accuracy : 99.19\n",
            "iteration : 250, loss : 0.0381, accuracy : 99.17\n",
            "iteration : 300, loss : 0.0398, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0397, accuracy : 99.10\n",
            "Epoch : 241, training loss : 0.0395, training accuracy : 99.11, test loss : 0.2478, test accuracy : 94.55\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.0401, accuracy : 99.06\n",
            "iteration : 100, loss : 0.0406, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0396, accuracy : 99.08\n",
            "iteration : 200, loss : 0.0398, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0392, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0391, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0386, accuracy : 99.12\n",
            "Epoch : 242, training loss : 0.0384, training accuracy : 99.13, test loss : 0.2512, test accuracy : 94.41\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0366, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0428, accuracy : 99.05\n",
            "iteration : 150, loss : 0.0438, accuracy : 99.01\n",
            "iteration : 200, loss : 0.0420, accuracy : 99.05\n",
            "iteration : 250, loss : 0.0405, accuracy : 99.08\n",
            "iteration : 300, loss : 0.0399, accuracy : 99.11\n",
            "iteration : 350, loss : 0.0392, accuracy : 99.12\n",
            "Epoch : 243, training loss : 0.0394, training accuracy : 99.11, test loss : 0.2503, test accuracy : 94.63\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.0369, accuracy : 99.12\n",
            "iteration : 100, loss : 0.0382, accuracy : 99.14\n",
            "iteration : 150, loss : 0.0373, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0384, accuracy : 99.15\n",
            "iteration : 250, loss : 0.0382, accuracy : 99.16\n",
            "iteration : 300, loss : 0.0389, accuracy : 99.14\n",
            "iteration : 350, loss : 0.0388, accuracy : 99.13\n",
            "Epoch : 244, training loss : 0.0385, training accuracy : 99.14, test loss : 0.2530, test accuracy : 94.39\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.0371, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0394, accuracy : 99.16\n",
            "iteration : 150, loss : 0.0397, accuracy : 99.11\n",
            "iteration : 200, loss : 0.0397, accuracy : 99.12\n",
            "iteration : 250, loss : 0.0398, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0396, accuracy : 99.10\n",
            "iteration : 350, loss : 0.0391, accuracy : 99.12\n",
            "Epoch : 245, training loss : 0.0391, training accuracy : 99.12, test loss : 0.2519, test accuracy : 94.55\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.0370, accuracy : 99.14\n",
            "iteration : 100, loss : 0.0412, accuracy : 99.04\n",
            "iteration : 150, loss : 0.0406, accuracy : 99.04\n",
            "iteration : 200, loss : 0.0409, accuracy : 99.02\n",
            "iteration : 250, loss : 0.0398, accuracy : 99.06\n",
            "iteration : 300, loss : 0.0404, accuracy : 99.07\n",
            "iteration : 350, loss : 0.0401, accuracy : 99.07\n",
            "Epoch : 246, training loss : 0.0398, training accuracy : 99.09, test loss : 0.2522, test accuracy : 94.43\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.0415, accuracy : 99.05\n",
            "iteration : 100, loss : 0.0382, accuracy : 99.17\n",
            "iteration : 150, loss : 0.0381, accuracy : 99.21\n",
            "iteration : 200, loss : 0.0373, accuracy : 99.21\n",
            "iteration : 250, loss : 0.0375, accuracy : 99.19\n",
            "iteration : 300, loss : 0.0374, accuracy : 99.20\n",
            "iteration : 350, loss : 0.0369, accuracy : 99.23\n",
            "Epoch : 247, training loss : 0.0369, training accuracy : 99.24, test loss : 0.2485, test accuracy : 94.55\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0319, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0326, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0340, accuracy : 99.30\n",
            "iteration : 200, loss : 0.0345, accuracy : 99.28\n",
            "iteration : 250, loss : 0.0355, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0360, accuracy : 99.21\n",
            "iteration : 350, loss : 0.0366, accuracy : 99.21\n",
            "Epoch : 248, training loss : 0.0367, training accuracy : 99.21, test loss : 0.2493, test accuracy : 94.60\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0313, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0329, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0342, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0348, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0353, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0355, accuracy : 99.27\n",
            "iteration : 350, loss : 0.0351, accuracy : 99.28\n",
            "Epoch : 249, training loss : 0.0349, training accuracy : 99.29, test loss : 0.2567, test accuracy : 94.39\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0337, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0354, accuracy : 99.22\n",
            "iteration : 150, loss : 0.0358, accuracy : 99.20\n",
            "iteration : 200, loss : 0.0359, accuracy : 99.18\n",
            "iteration : 250, loss : 0.0371, accuracy : 99.12\n",
            "iteration : 300, loss : 0.0369, accuracy : 99.13\n",
            "iteration : 350, loss : 0.0373, accuracy : 99.12\n",
            "Epoch : 250, training loss : 0.0371, training accuracy : 99.12, test loss : 0.2493, test accuracy : 94.61\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0353, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0357, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0339, accuracy : 99.26\n",
            "iteration : 200, loss : 0.0346, accuracy : 99.26\n",
            "iteration : 250, loss : 0.0348, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0350, accuracy : 99.24\n",
            "iteration : 350, loss : 0.0352, accuracy : 99.25\n",
            "Epoch : 251, training loss : 0.0357, training accuracy : 99.23, test loss : 0.2543, test accuracy : 94.56\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0366, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0326, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0331, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0342, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0359, accuracy : 99.26\n",
            "iteration : 300, loss : 0.0355, accuracy : 99.26\n",
            "iteration : 350, loss : 0.0355, accuracy : 99.27\n",
            "Epoch : 252, training loss : 0.0363, training accuracy : 99.25, test loss : 0.2516, test accuracy : 94.57\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0359, accuracy : 99.20\n",
            "iteration : 100, loss : 0.0358, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0351, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0350, accuracy : 99.25\n",
            "iteration : 250, loss : 0.0346, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0349, accuracy : 99.25\n",
            "iteration : 350, loss : 0.0358, accuracy : 99.23\n",
            "Epoch : 253, training loss : 0.0354, training accuracy : 99.23, test loss : 0.2479, test accuracy : 94.63\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0350, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0340, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0336, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0330, accuracy : 99.29\n",
            "iteration : 250, loss : 0.0336, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0337, accuracy : 99.26\n",
            "iteration : 350, loss : 0.0341, accuracy : 99.24\n",
            "Epoch : 254, training loss : 0.0344, training accuracy : 99.22, test loss : 0.2507, test accuracy : 94.60\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0369, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0363, accuracy : 99.19\n",
            "iteration : 150, loss : 0.0381, accuracy : 99.14\n",
            "iteration : 200, loss : 0.0397, accuracy : 99.13\n",
            "iteration : 250, loss : 0.0384, accuracy : 99.15\n",
            "iteration : 300, loss : 0.0369, accuracy : 99.20\n",
            "iteration : 350, loss : 0.0362, accuracy : 99.23\n",
            "Epoch : 255, training loss : 0.0360, training accuracy : 99.23, test loss : 0.2466, test accuracy : 94.59\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0335, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0350, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0360, accuracy : 99.22\n",
            "iteration : 200, loss : 0.0344, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0346, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0344, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0345, accuracy : 99.27\n",
            "Epoch : 256, training loss : 0.0343, training accuracy : 99.28, test loss : 0.2442, test accuracy : 94.63\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0347, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0338, accuracy : 99.26\n",
            "iteration : 150, loss : 0.0342, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0329, accuracy : 99.28\n",
            "iteration : 250, loss : 0.0340, accuracy : 99.25\n",
            "iteration : 300, loss : 0.0338, accuracy : 99.26\n",
            "iteration : 350, loss : 0.0337, accuracy : 99.24\n",
            "Epoch : 257, training loss : 0.0338, training accuracy : 99.24, test loss : 0.2524, test accuracy : 94.52\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0358, accuracy : 99.27\n",
            "iteration : 100, loss : 0.0342, accuracy : 99.32\n",
            "iteration : 150, loss : 0.0325, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0326, accuracy : 99.35\n",
            "iteration : 250, loss : 0.0324, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0324, accuracy : 99.35\n",
            "iteration : 350, loss : 0.0330, accuracy : 99.35\n",
            "Epoch : 258, training loss : 0.0331, training accuracy : 99.34, test loss : 0.2478, test accuracy : 94.67\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0308, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0314, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0315, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0320, accuracy : 99.35\n",
            "iteration : 250, loss : 0.0331, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0322, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0331, accuracy : 99.33\n",
            "Epoch : 259, training loss : 0.0336, training accuracy : 99.32, test loss : 0.2517, test accuracy : 94.58\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0307, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0311, accuracy : 99.40\n",
            "iteration : 150, loss : 0.0314, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0302, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0302, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0308, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0311, accuracy : 99.39\n",
            "Epoch : 260, training loss : 0.0316, training accuracy : 99.36, test loss : 0.2507, test accuracy : 94.60\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0345, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0310, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0305, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0312, accuracy : 99.35\n",
            "iteration : 250, loss : 0.0314, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0312, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0306, accuracy : 99.40\n",
            "Epoch : 261, training loss : 0.0310, training accuracy : 99.39, test loss : 0.2540, test accuracy : 94.55\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0268, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0295, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0292, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0294, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0298, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0298, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0300, accuracy : 99.41\n",
            "Epoch : 262, training loss : 0.0302, training accuracy : 99.40, test loss : 0.2519, test accuracy : 94.46\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0309, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0301, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0294, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0285, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0293, accuracy : 99.44\n",
            "iteration : 300, loss : 0.0293, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0293, accuracy : 99.45\n",
            "Epoch : 263, training loss : 0.0297, training accuracy : 99.44, test loss : 0.2509, test accuracy : 94.69\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.0332, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0356, accuracy : 99.29\n",
            "iteration : 150, loss : 0.0339, accuracy : 99.32\n",
            "iteration : 200, loss : 0.0330, accuracy : 99.35\n",
            "iteration : 250, loss : 0.0326, accuracy : 99.34\n",
            "iteration : 300, loss : 0.0328, accuracy : 99.35\n",
            "iteration : 350, loss : 0.0327, accuracy : 99.35\n",
            "Epoch : 264, training loss : 0.0328, training accuracy : 99.36, test loss : 0.2486, test accuracy : 94.80\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0291, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0283, accuracy : 99.50\n",
            "iteration : 150, loss : 0.0292, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0293, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0294, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0295, accuracy : 99.44\n",
            "iteration : 350, loss : 0.0294, accuracy : 99.43\n",
            "Epoch : 265, training loss : 0.0297, training accuracy : 99.41, test loss : 0.2524, test accuracy : 94.60\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0319, accuracy : 99.28\n",
            "iteration : 100, loss : 0.0335, accuracy : 99.28\n",
            "iteration : 150, loss : 0.0327, accuracy : 99.31\n",
            "iteration : 200, loss : 0.0319, accuracy : 99.35\n",
            "iteration : 250, loss : 0.0326, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0318, accuracy : 99.37\n",
            "iteration : 350, loss : 0.0312, accuracy : 99.38\n",
            "Epoch : 266, training loss : 0.0311, training accuracy : 99.38, test loss : 0.2514, test accuracy : 94.61\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0307, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0291, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0292, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0304, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0304, accuracy : 99.40\n",
            "iteration : 300, loss : 0.0304, accuracy : 99.40\n",
            "iteration : 350, loss : 0.0300, accuracy : 99.41\n",
            "Epoch : 267, training loss : 0.0303, training accuracy : 99.41, test loss : 0.2506, test accuracy : 94.66\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0288, accuracy : 99.45\n",
            "iteration : 100, loss : 0.0302, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0307, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0303, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0311, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0308, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0306, accuracy : 99.40\n",
            "Epoch : 268, training loss : 0.0304, training accuracy : 99.40, test loss : 0.2494, test accuracy : 94.64\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0302, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0305, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0311, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0305, accuracy : 99.43\n",
            "iteration : 250, loss : 0.0296, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0289, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0298, accuracy : 99.42\n",
            "Epoch : 269, training loss : 0.0297, training accuracy : 99.43, test loss : 0.2464, test accuracy : 94.77\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0300, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0313, accuracy : 99.32\n",
            "iteration : 150, loss : 0.0316, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0318, accuracy : 99.34\n",
            "iteration : 250, loss : 0.0324, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0326, accuracy : 99.32\n",
            "iteration : 350, loss : 0.0317, accuracy : 99.35\n",
            "Epoch : 270, training loss : 0.0317, training accuracy : 99.36, test loss : 0.2497, test accuracy : 94.63\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0250, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0249, accuracy : 99.61\n",
            "iteration : 150, loss : 0.0268, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0283, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0287, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0289, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0289, accuracy : 99.48\n",
            "Epoch : 271, training loss : 0.0287, training accuracy : 99.48, test loss : 0.2450, test accuracy : 94.66\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0326, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0303, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0293, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0294, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0295, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0297, accuracy : 99.44\n",
            "iteration : 350, loss : 0.0291, accuracy : 99.46\n",
            "Epoch : 272, training loss : 0.0293, training accuracy : 99.46, test loss : 0.2504, test accuracy : 94.64\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0256, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0247, accuracy : 99.60\n",
            "iteration : 150, loss : 0.0262, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0264, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0271, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0277, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0276, accuracy : 99.49\n",
            "Epoch : 273, training loss : 0.0278, training accuracy : 99.48, test loss : 0.2496, test accuracy : 94.64\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0257, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0268, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0271, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0270, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0271, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0272, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0274, accuracy : 99.49\n",
            "Epoch : 274, training loss : 0.0279, training accuracy : 99.48, test loss : 0.2504, test accuracy : 94.65\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0296, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0289, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0287, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0283, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0283, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0277, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0279, accuracy : 99.48\n",
            "Epoch : 275, training loss : 0.0280, training accuracy : 99.48, test loss : 0.2440, test accuracy : 94.75\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0304, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0281, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0275, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0283, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0282, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0283, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0284, accuracy : 99.48\n",
            "Epoch : 276, training loss : 0.0286, training accuracy : 99.47, test loss : 0.2520, test accuracy : 94.60\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0266, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0293, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0291, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0281, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0276, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0278, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0278, accuracy : 99.49\n",
            "Epoch : 277, training loss : 0.0278, training accuracy : 99.49, test loss : 0.2511, test accuracy : 94.59\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0287, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0273, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0267, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0262, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0273, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0265, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0259, accuracy : 99.57\n",
            "Epoch : 278, training loss : 0.0260, training accuracy : 99.57, test loss : 0.2546, test accuracy : 94.58\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0265, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0259, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0266, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0262, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0270, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0267, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0266, accuracy : 99.53\n",
            "Epoch : 279, training loss : 0.0266, training accuracy : 99.53, test loss : 0.2507, test accuracy : 94.71\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0307, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0268, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0277, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0281, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0276, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0273, accuracy : 99.50\n",
            "iteration : 350, loss : 0.0273, accuracy : 99.50\n",
            "Epoch : 280, training loss : 0.0274, training accuracy : 99.50, test loss : 0.2467, test accuracy : 94.78\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0278, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0266, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0272, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0270, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0266, accuracy : 99.52\n",
            "iteration : 300, loss : 0.0267, accuracy : 99.52\n",
            "iteration : 350, loss : 0.0265, accuracy : 99.52\n",
            "Epoch : 281, training loss : 0.0267, training accuracy : 99.52, test loss : 0.2552, test accuracy : 94.55\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0283, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0275, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0278, accuracy : 99.45\n",
            "iteration : 200, loss : 0.0272, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0272, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0273, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0268, accuracy : 99.50\n",
            "Epoch : 282, training loss : 0.0269, training accuracy : 99.50, test loss : 0.2511, test accuracy : 94.70\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0219, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0230, accuracy : 99.64\n",
            "iteration : 150, loss : 0.0255, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0264, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0264, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0270, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0270, accuracy : 99.53\n",
            "Epoch : 283, training loss : 0.0269, training accuracy : 99.54, test loss : 0.2487, test accuracy : 94.78\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0290, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0285, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0279, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0270, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0274, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0274, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0276, accuracy : 99.50\n",
            "Epoch : 284, training loss : 0.0280, training accuracy : 99.49, test loss : 0.2456, test accuracy : 94.83\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0261, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0260, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0267, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0273, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0272, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0274, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0267, accuracy : 99.49\n",
            "Epoch : 285, training loss : 0.0271, training accuracy : 99.49, test loss : 0.2516, test accuracy : 94.61\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0247, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0277, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0265, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0278, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0275, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0270, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0271, accuracy : 99.49\n",
            "Epoch : 286, training loss : 0.0268, training accuracy : 99.50, test loss : 0.2550, test accuracy : 94.64\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0222, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0268, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0265, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0258, accuracy : 99.51\n",
            "iteration : 250, loss : 0.0266, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0265, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0261, accuracy : 99.51\n",
            "Epoch : 287, training loss : 0.0259, training accuracy : 99.52, test loss : 0.2562, test accuracy : 94.54\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0283, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0259, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0254, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0243, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0256, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0254, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0257, accuracy : 99.54\n",
            "Epoch : 288, training loss : 0.0258, training accuracy : 99.54, test loss : 0.2509, test accuracy : 94.64\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0271, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0262, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0273, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0280, accuracy : 99.49\n",
            "iteration : 250, loss : 0.0274, accuracy : 99.51\n",
            "iteration : 300, loss : 0.0268, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0268, accuracy : 99.53\n",
            "Epoch : 289, training loss : 0.0267, training accuracy : 99.53, test loss : 0.2491, test accuracy : 94.74\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0257, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0278, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0278, accuracy : 99.53\n",
            "iteration : 200, loss : 0.0266, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0266, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0262, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0262, accuracy : 99.55\n",
            "Epoch : 290, training loss : 0.0263, training accuracy : 99.55, test loss : 0.2446, test accuracy : 94.77\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0272, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0268, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0259, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0256, accuracy : 99.59\n",
            "iteration : 250, loss : 0.0256, accuracy : 99.58\n",
            "iteration : 300, loss : 0.0261, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0259, accuracy : 99.57\n",
            "Epoch : 291, training loss : 0.0258, training accuracy : 99.57, test loss : 0.2555, test accuracy : 94.64\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0295, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0294, accuracy : 99.39\n",
            "iteration : 150, loss : 0.0279, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0276, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0276, accuracy : 99.45\n",
            "iteration : 300, loss : 0.0276, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0271, accuracy : 99.47\n",
            "Epoch : 292, training loss : 0.0274, training accuracy : 99.46, test loss : 0.2516, test accuracy : 94.63\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0289, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0299, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0293, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0289, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0285, accuracy : 99.45\n",
            "iteration : 300, loss : 0.0284, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0280, accuracy : 99.47\n",
            "Epoch : 293, training loss : 0.0277, training accuracy : 99.49, test loss : 0.2456, test accuracy : 94.79\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0249, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0252, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0260, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0267, accuracy : 99.57\n",
            "iteration : 250, loss : 0.0259, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0262, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0265, accuracy : 99.54\n",
            "Epoch : 294, training loss : 0.0262, training accuracy : 99.56, test loss : 0.2450, test accuracy : 94.88\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0233, accuracy : 99.70\n",
            "iteration : 100, loss : 0.0247, accuracy : 99.62\n",
            "iteration : 150, loss : 0.0250, accuracy : 99.59\n",
            "iteration : 200, loss : 0.0258, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0253, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0253, accuracy : 99.56\n",
            "iteration : 350, loss : 0.0255, accuracy : 99.55\n",
            "Epoch : 295, training loss : 0.0259, training accuracy : 99.54, test loss : 0.2513, test accuracy : 94.67\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0247, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0235, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0241, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0254, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0253, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0253, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0254, accuracy : 99.54\n",
            "Epoch : 296, training loss : 0.0251, training accuracy : 99.55, test loss : 0.2495, test accuracy : 94.79\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0264, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0257, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0266, accuracy : 99.58\n",
            "iteration : 200, loss : 0.0275, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0269, accuracy : 99.57\n",
            "iteration : 300, loss : 0.0265, accuracy : 99.57\n",
            "iteration : 350, loss : 0.0267, accuracy : 99.55\n",
            "Epoch : 297, training loss : 0.0268, training accuracy : 99.55, test loss : 0.2478, test accuracy : 94.68\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0244, accuracy : 99.61\n",
            "iteration : 100, loss : 0.0248, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0258, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0261, accuracy : 99.56\n",
            "iteration : 250, loss : 0.0256, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0261, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0267, accuracy : 99.52\n",
            "Epoch : 298, training loss : 0.0266, training accuracy : 99.52, test loss : 0.2470, test accuracy : 94.66\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0256, accuracy : 99.56\n",
            "iteration : 100, loss : 0.0257, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0249, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0254, accuracy : 99.58\n",
            "iteration : 250, loss : 0.0262, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0256, accuracy : 99.54\n",
            "iteration : 350, loss : 0.0262, accuracy : 99.53\n",
            "Epoch : 299, training loss : 0.0261, training accuracy : 99.54, test loss : 0.2461, test accuracy : 94.68\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0233, accuracy : 99.64\n",
            "iteration : 100, loss : 0.0261, accuracy : 99.58\n",
            "iteration : 150, loss : 0.0260, accuracy : 99.56\n",
            "iteration : 200, loss : 0.0256, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0256, accuracy : 99.55\n",
            "iteration : 300, loss : 0.0256, accuracy : 99.55\n",
            "iteration : 350, loss : 0.0258, accuracy : 99.54\n",
            "Epoch : 300, training loss : 0.0259, training accuracy : 99.53, test loss : 0.2503, test accuracy : 94.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "id": "Ai2oFzKKniFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17087e79-c3ad-4c6c-9992-7c0b4a5645be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.16823063889408812, 96.49277811923785)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : Label smoothing factor = 0.001\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IRtUqPJRniNI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "4e9abc0e-61aa-4327-922f-196bafde96ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : Label smoothing factor = 0.001\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hWxo6saIniUC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "08fde84e-c1a9-493f-93e8-41564c632c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_ls0001 = {train_loss_list}\") \n",
        "print(f\"train_acc_list_ls0001 = {train_acc_list}\")\n",
        "print(f\"test_loss_list_ls0001 = {test_loss_list}\")\n",
        "print(f\"test_acc_list_ls0001 = {test_acc_list}\")"
      ],
      "metadata": {
        "id": "qlDSuLyhnqbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5810ceb-60ab-4673-eeee-82731c2942b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_ls0001 = [1.2821029141181852, 0.4606649320784623, 0.40836672977541844, 0.37926377065298034, 0.3635530332481958, 0.345831044563433, 0.3270037200675424, 0.3167617663054608, 0.30828977119793416, 0.29592982378755484, 0.28403947266136725, 0.275649338476057, 0.2685100892331542, 0.2555903730394071, 0.25604943717075235, 0.2512754402792228, 0.24380625170582354, 0.23820029088071368, 0.2337998496242332, 0.2293923183225681, 0.22452238008258787, 0.22621242665663, 0.21976554880378046, 0.2171372272538622, 0.21352828061839108, 0.21289718714429112, 0.2059814103533259, 0.20443682470497723, 0.20443902247481877, 0.2062645934744256, 0.20045099193406946, 0.2019952979635417, 0.19847539907225425, 0.1955009141184773, 0.19031431603682072, 0.19435248684471215, 0.18663890816291495, 0.18795300055002456, 0.18769593308692378, 0.18489367706665824, 0.1857685616749735, 0.18103757403730378, 0.18059604179851085, 0.18188588272749892, 0.18230158158232204, 0.17923328647930126, 0.17821672942418715, 0.1782106015336546, 0.17466056642695493, 0.1761790116123229, 0.17386309174583534, 0.17503206392492704, 0.17227052319744415, 0.1731901451299184, 0.16723601289880954, 0.16773097328639774, 0.16576332111305336, 0.16984056236176956, 0.1688815733988272, 0.17095942626679492, 0.16649497691087606, 0.16232554665022103, 0.16310092879504692, 0.164779980735081, 0.16586144016121784, 0.1623216041825651, 0.16317995787070697, 0.16370599901651947, 0.16138635638364285, 0.1589418155491998, 0.15976342665874538, 0.15478705410873342, 0.15820391033965397, 0.15529920563586358, 0.15794524758408063, 0.15397934836706495, 0.1543355199197928, 0.15745798997116606, 0.1521688463585325, 0.15188901548382389, 0.14929091305469433, 0.14760772322574606, 0.14837162131014556, 0.1494877573536825, 0.15080355424787295, 0.14502787356817626, 0.145977151688683, 0.145898418374824, 0.15140795437139548, 0.1448897874351278, 0.14474816779860958, 0.14205117689238655, 0.14293640921634387, 0.13969239146325, 0.1415051514217201, 0.14372633470267784, 0.14307593488636702, 0.14013307091343372, 0.13680426844049923, 0.13791783092427384, 0.13662838542122183, 0.13953894756350738, 0.13574504507024113, 0.1354489474983881, 0.13697636990648945, 0.13409695788450682, 0.1341445848527478, 0.13019113699958576, 0.13362151345990214, 0.13265881958975378, 0.13019365613376546, 0.12858783817145883, 0.12620674917691446, 0.1299155467873828, 0.12543811610720668, 0.1295502600398894, 0.12845180089759634, 0.1269437730574834, 0.1271431707552455, 0.12254504246642273, 0.12090262618208643, 0.12200955525141582, 0.12331769869821828, 0.1253069130587707, 0.12126807825069441, 0.1193198974176151, 0.11992631606293241, 0.12004051528123982, 0.1204929109864684, 0.11313034043846738, 0.11773807760378532, 0.11620774038317727, 0.11534621272246204, 0.11056431200163475, 0.11508381656474537, 0.11690368201977354, 0.10924771433156034, 0.11015543621790604, 0.11156427728007157, 0.10886944571464527, 0.10739432360603396, 0.10606734497033483, 0.1102901396005942, 0.10687257804299596, 0.1074057935527589, 0.10361439587301031, 0.1026271724704966, 0.10355194469757358, 0.10306216881365633, 0.10470880725014663, 0.10193988674277374, 0.10265541381548414, 0.09965628099332495, 0.09571173101118426, 0.10345231556916624, 0.09756934063302146, 0.09746604933224234, 0.0976584020870811, 0.0962353422278715, 0.09346834380876243, 0.09450408472581123, 0.09759261904329788, 0.09210262642946185, 0.09114886878366231, 0.09056318886981908, 0.09203791171615201, 0.08971117644855969, 0.08709307289543514, 0.0888723720692926, 0.08429488152023254, 0.08330216348615442, 0.08703962576684106, 0.0849922022259817, 0.0838554838374459, 0.082297914050681, 0.0829765994990179, 0.08506461244005821, 0.08137384761720493, 0.08143888224612891, 0.08025524031804991, 0.08216103062356714, 0.07699430027224508, 0.07736524795674211, 0.07969436044918328, 0.07561844564610865, 0.07905571249018355, 0.07398648452528608, 0.07354809018987708, 0.07363684127627187, 0.06980580199499764, 0.07455518443470563, 0.06948973685704919, 0.06969636477287708, 0.06682590113039101, 0.06929411632601969, 0.06918049230162045, 0.06753029557182214, 0.06387730556613384, 0.06610618795957265, 0.06673317671145204, 0.06479948119015551, 0.06294229492383598, 0.0637362053451257, 0.06255791264366571, 0.06069549541607459, 0.06127910473230005, 0.060466610980510385, 0.05954703034928982, 0.06105490651001856, 0.05933400873740918, 0.05870942316626307, 0.05819866883247929, 0.056481048987224336, 0.05534794351696806, 0.05635350948416686, 0.05665013588571209, 0.054782006142504495, 0.05484316927862926, 0.052700793595232456, 0.05267349375174621, 0.05192781796774809, 0.04906429009648358, 0.05029141231058897, 0.05159176989814373, 0.04961986729981211, 0.046992943066911805, 0.047570749223838006, 0.04759910133059028, 0.045914400951951984, 0.04603220443152961, 0.04521586166992582, 0.04539316300514752, 0.04373067965211099, 0.04341385071859935, 0.04209496792617852, 0.044134347601150106, 0.041565995817362936, 0.04248671896685026, 0.04046368150439285, 0.03937313831625834, 0.039491642844487175, 0.0384247151776381, 0.03937673995771059, 0.03845314124755581, 0.039115767411100184, 0.03980038157565807, 0.03685175531561459, 0.03667296176942868, 0.03490798150600618, 0.03712914120103043, 0.03574267752609925, 0.03634545136354157, 0.03544450965949756, 0.0344041074174368, 0.0360235846841198, 0.03425272996852876, 0.03377667297848922, 0.03309069696288849, 0.03355455500777901, 0.03162642000217829, 0.03097138585406671, 0.03023723832667278, 0.029719059534754535, 0.032833904850733314, 0.02974499381200327, 0.031108877593828087, 0.030257471625094977, 0.03040940692108175, 0.029650318231887934, 0.03172184641505806, 0.028709624936869797, 0.029256403855070834, 0.02782894898367526, 0.027922781560378186, 0.028019856285495647, 0.02857950115500669, 0.027777009625946925, 0.02598382472779935, 0.026571328769410207, 0.027407457575020267, 0.02673846037003406, 0.02689466125717977, 0.02691402661344062, 0.028018657070127767, 0.027103412230082644, 0.02681950747027872, 0.025905901790775742, 0.02577358180137104, 0.026708768165814196, 0.026341752724616023, 0.02584395786812958, 0.027387296980332713, 0.0277085056891892, 0.026235776559669313, 0.02587787169195853, 0.025072741690710148, 0.026834654639447286, 0.02658912477646336, 0.026130885097796354, 0.025945517287506322]\n",
            "train_acc_list_ls0001 = [55.97247220751721, 85.87612493382743, 87.66754896770779, 88.6945473795659, 89.22392800423505, 89.85494970884065, 90.38009528851244, 90.83112758073055, 91.11275807305452, 91.33086289041822, 91.85812599258867, 92.14187400741133, 92.43832715722604, 92.68184224457384, 92.66490206458444, 92.85336156696665, 93.12440444679724, 93.30651138168344, 93.40391741662255, 93.52249867654844, 93.7511911064055, 93.67496029645315, 93.73213340391742, 93.84859714134463, 93.98200105876126, 93.9142403388036, 94.24033880359978, 94.2212811011117, 94.26151402858656, 94.15563790365273, 94.30809952355744, 94.36739015352038, 94.45844362096348, 94.45844362096348, 94.76760190577025, 94.57490735839069, 94.78665960825833, 94.7358390682901, 94.77607199576495, 94.79512969825305, 94.78242456326099, 94.9518263631551, 94.95817893065114, 95.00476442562203, 94.85018528321864, 95.04711487559555, 95.02593965060879, 95.01111699311805, 95.1233456855479, 95.0344097406035, 95.14452091053468, 95.18051879301217, 95.15299100052938, 95.13181577554262, 95.44309158284807, 95.42403388035999, 95.44309158284807, 95.27368978295394, 95.29274748544204, 95.23769190047645, 95.3943885653785, 95.59555320275278, 95.3668607728957, 95.47061937533087, 95.40497617787189, 95.45579671784013, 95.38803599788248, 95.43673901535203, 95.49179460031763, 95.64425622022235, 95.51085230280572, 95.66119640021175, 95.61672842773955, 95.70778189518263, 95.59131815775542, 95.7649550026469, 95.72472207517205, 95.58708311275808, 95.7564849126522, 95.81577554261514, 95.79036527263102, 95.85600847008999, 95.88141874007411, 95.82212811011117, 95.76707252514558, 95.928004235045, 96.00423504499736, 95.98729486500794, 95.79671784012704, 96.06564319745897, 96.01905770248808, 96.07411328745368, 96.07834833245103, 96.11011116993119, 96.11011116993119, 96.05505558496559, 95.98941238750662, 96.06987824245633, 96.3070407623081, 96.1715193223928, 96.28798305982001, 96.13975648491265, 96.30068819481207, 96.19481206987824, 96.19481206987824, 96.29433562731604, 96.34727368978295, 96.45526733721546, 96.29857067231339, 96.32186341979883, 96.42138697723664, 96.4785600847009, 96.55690841715193, 96.40232927474854, 96.58231868713605, 96.4510322922181, 96.42350449973532, 96.4785600847009, 96.46797247220752, 96.62890418210694, 96.67125463208046, 96.71995764955003, 96.54843832715723, 96.54843832715723, 96.66278454208576, 96.68184224457384, 96.74748544203283, 96.62890418210694, 96.74536791953415, 96.94865007940709, 96.78771836950767, 96.75383800952885, 96.8131286394918, 97.03123345685547, 96.83006881948121, 96.83430386447856, 97.07993647432504, 97.01217575436739, 96.89994706193754, 97.02488088935945, 97.09052408681842, 97.09687665431446, 96.9571201694018, 97.10534674430916, 97.03970354685019, 97.1561672842774, 97.21757543673901, 97.19640021175225, 97.21334039174167, 97.10746426680784, 97.26839597670725, 97.22181048173637, 97.23028057173107, 97.42721016410799, 97.20910534674431, 97.35097935415564, 97.3806246691371, 97.31498147167814, 97.38274219163578, 97.49708840656432, 97.45685547908947, 97.43779777660137, 97.54579142403388, 97.571201694018, 97.55849655902594, 97.52461619904712, 97.62837480148227, 97.643197458973, 97.63049232398095, 97.79777660137638, 97.7067231339333, 97.66649020645845, 97.7257808364214, 97.75966119640022, 97.7787188988883, 97.76389624139756, 97.66649020645845, 97.8062466913711, 97.82530439385918, 97.93329804129169, 97.8231868713605, 97.90365272631021, 97.9417681312864, 97.92694547379566, 98.00105876124934, 97.91212281630493, 98.06458443620963, 98.0307040762308, 98.04764425622022, 98.14293276866067, 98.13446267866595, 98.19587083112758, 98.15352038115405, 98.29539438856538, 98.13022763366861, 98.1768131286395, 98.32503970354685, 98.32503970354685, 98.32292218104817, 98.20857596611964, 98.33986236103759, 98.37374272101641, 98.36103758602435, 98.45420857596612, 98.48597141344626, 98.46267866596082, 98.40974060349392, 98.50502911593436, 98.47961884595024, 98.4923239809423, 98.53467443091583, 98.54314452091053, 98.55796717840127, 98.65749073583906, 98.55161461090525, 98.64478560084702, 98.6553732133404, 98.64266807834834, 98.68501852832186, 98.6998411858126, 98.74007411328745, 98.81418740074113, 98.74219163578613, 98.72313393329804, 98.79089465325569, 98.92853361566966, 98.8438327157226, 98.86500794070938, 98.88618316569614, 98.96241397564849, 98.86077289571202, 98.9073583906829, 98.97511911064055, 98.98994176813129, 99.0428798305982, 98.96876654314453, 99.02805717310747, 99.04076230809952, 99.10217046056114, 99.11699311805188, 99.11064055055584, 99.13181577554262, 99.1148755955532, 99.14028586553732, 99.1233456855479, 99.08523028057174, 99.23557437797777, 99.20804658549497, 99.286394917946, 99.1233456855479, 99.23133933298041, 99.25463208046585, 99.23345685547909, 99.22075172048703, 99.22710428798305, 99.28427739544733, 99.23980942297511, 99.3414505029116, 99.31604023292748, 99.35627316040232, 99.38803599788248, 99.39862361037586, 99.44097406034939, 99.35627316040232, 99.41344626786659, 99.38168343038645, 99.40709370037057, 99.40285865537321, 99.42615140285865, 99.358390682901, 99.48120698782425, 99.46003176283747, 99.48332451032292, 99.47697194282689, 99.47697194282689, 99.46850185283219, 99.49179460031763, 99.57014293276866, 99.52779248279514, 99.49602964531498, 99.51932239280042, 99.49602964531498, 99.53838009528852, 99.48967707781895, 99.48967707781895, 99.50238221281101, 99.51932239280042, 99.53838009528852, 99.53202752779248, 99.54685018528322, 99.57226045526734, 99.46426680783483, 99.48755955532027, 99.55532027527792, 99.53838009528852, 99.5489677077819, 99.55320275277924, 99.52355743779778, 99.53626257278984, 99.53414505029116]\n",
            "test_loss_list_ls0001 = [0.6926000357842913, 0.6267305188903621, 0.41281447523072656, 0.44266026174905254, 0.44140984716953013, 0.425681257072617, 0.40519654159160223, 0.39385805783026356, 0.34663725301038983, 0.3815849568037426, 0.3193201936927496, 0.29582576501164953, 0.32276419111910987, 0.2958792201882484, 0.28719688849706276, 0.3020125618021862, 0.2695948461472404, 0.30715090113089366, 0.31171496581359237, 0.2872339423526736, 0.2939882139657058, 0.2765550096610598, 0.29639453506645036, 0.2680652508256482, 0.2787311045343385, 0.2665373562451671, 0.25051620113206846, 0.2650165607531865, 0.2762997670135662, 0.25961911477440713, 0.2567094151368913, 0.2691565604478705, 0.2511041649416381, 0.24585220108137412, 0.24511176512083588, 0.26719818410335805, 0.2600785271063739, 0.2581570347515391, 0.2512194017934449, 0.2573234689440213, 0.24716408492303363, 0.24275594168141776, 0.26490542573817805, 0.27001490367247777, 0.2538728545255521, 0.2734342858779664, 0.2495648948585286, 0.25670715655181925, 0.24812467374345837, 0.2467709734451537, 0.24221190087058964, 0.2708203858129826, 0.2513877789252529, 0.24794376006021218, 0.2438396359154699, 0.24449818829695383, 0.26198740424040484, 0.25560708186936143, 0.24092182097043477, 0.24500929388929815, 0.2567118000531313, 0.23622259347900457, 0.24290751680439593, 0.25033611596068917, 0.23652028073282802, 0.2608188732216756, 0.23736300366912402, 0.23901385671513922, 0.2412118879968629, 0.2430700650986503, 0.2333055655745899, 0.24252896887414596, 0.2403551868948282, 0.24640730417826595, 0.24889417923986912, 0.24529527810712656, 0.2511894301164384, 0.2410746736488506, 0.24416167909900346, 0.24434382209152566, 0.2573013788490903, 0.24091841539769782, 0.23948837758279315, 0.26577260593573254, 0.2450881216894178, 0.25906996553142864, 0.2444982366526828, 0.24404337242537855, 0.2417194598591795, 0.24417856974782898, 0.248081975561731, 0.23907211247612448, 0.2443172898800934, 0.2524608191029698, 0.2491202596027185, 0.24580352375393405, 0.2578393792112668, 0.25864624100572925, 0.25868237515290576, 0.23708039326775893, 0.2503240705997336, 0.25007886008596886, 0.2483417918430824, 0.23719977289803473, 0.24438279315683187, 0.23493162922415078, 0.23835368830637604, 0.2347681490777462, 0.24783222793656237, 0.24756574386036864, 0.24777229820542476, 0.23609183686694094, 0.24882606278155364, 0.2429893311186164, 0.24192699545300475, 0.24355457438265576, 0.23989995729689503, 0.2514470876285843, 0.24089334890538572, 0.2460024814833613, 0.2382542212292844, 0.24533216779430708, 0.2565075444313241, 0.22974171244776717, 0.25108636031840365, 0.24714389297307707, 0.24278283550166616, 0.2489816382089082, 0.23457604332589635, 0.25390376797055497, 0.2665921906922378, 0.24632494341509015, 0.2382579966649121, 0.2447011707460179, 0.23611394575267447, 0.2502153496824059, 0.2553070048418115, 0.24802380595721452, 0.24639706901621586, 0.24759467132389545, 0.2505231913106114, 0.26417682918847774, 0.24281416424349242, 0.24721430519632265, 0.2425150258977916, 0.24242619600366144, 0.24109682294667936, 0.24051191471517086, 0.24580609984695911, 0.23747985912304298, 0.24749138396160275, 0.24506273681261376, 0.2388310698682771, 0.2464255292845123, 0.24220792258925297, 0.24659503706018715, 0.2505872025538017, 0.24187414094294404, 0.24525769336112574, 0.2582315535960244, 0.23210425472215696, 0.24824976754904376, 0.23896974935105034, 0.25043365529135747, 0.24389350969417423, 0.24711458196900055, 0.2466810904443264, 0.24806718429660096, 0.24452097556900745, 0.24753778884369954, 0.248085135484443, 0.2384258982926315, 0.2465875870091658, 0.23896533915517376, 0.25130386363861024, 0.253806537802459, 0.24889859348973808, 0.24596627523173012, 0.2427327691339979, 0.24210923893705888, 0.25086921752960073, 0.2506660321069991, 0.25179610315564216, 0.24547175193826357, 0.24649291242673702, 0.24291574944029837, 0.25328567151126324, 0.24912678654871734, 0.2507133948087108, 0.2462298433825958, 0.24897505890797167, 0.24907624163646616, 0.24627242734034857, 0.2462397112565882, 0.2517813877293877, 0.24789459935809469, 0.2504023476777708, 0.24591281573635107, 0.251361799846385, 0.2531733639611333, 0.2512316147194189, 0.2434102969755437, 0.24435554674881346, 0.24786799726094685, 0.24887829255677907, 0.250514570486677, 0.2566175095225666, 0.2451159154046692, 0.24571499701443256, 0.25336275686162946, 0.25082747921274573, 0.24370002737451418, 0.24827072672618955, 0.24687355468232258, 0.2576789428461708, 0.24772274808264247, 0.25089384379851465, 0.25092660893193064, 0.24947045763552772, 0.2427329163560096, 0.24861770917606704, 0.2488406642212295, 0.25532795740839315, 0.2473309392876485, 0.25257241320522394, 0.25485144093559653, 0.25624520693193464, 0.2524839039051942, 0.2506518292277321, 0.24567519805814123, 0.24687634773698508, 0.248686141809266, 0.25325184866932093, 0.2523150967854057, 0.24802655799716128, 0.24644241918462748, 0.2438966180471813, 0.2541824008392937, 0.2506825534042482, 0.2516993116404788, 0.2477639117552077, 0.251178068857567, 0.2503202641748038, 0.25300761146069156, 0.25187020967988405, 0.2522068395114997, 0.24846704369958708, 0.2492743721049206, 0.2567389773789282, 0.24926202170843004, 0.25426253380582614, 0.25163665623860615, 0.24786491734980076, 0.2506573340289441, 0.24658899992594824, 0.24423893667100108, 0.2523916067062494, 0.24777162120695792, 0.25174126217105225, 0.2507459439483343, 0.2539809211248569, 0.25192630228896934, 0.250868956047092, 0.248642020676212, 0.25237680531526896, 0.25137182533302727, 0.25059773274423447, 0.24939356337064036, 0.24640958385505513, 0.2497486551657465, 0.24495161021603087, 0.25041523366691726, 0.24957243822441966, 0.25035044172888293, 0.24399699036981545, 0.25200897257994204, 0.2510850190038921, 0.2545919693440345, 0.2506695724658522, 0.24666941246273472, 0.255156127462054, 0.25112008497885924, 0.24866370422144732, 0.24556972110169192, 0.25156838096239986, 0.25500264873399453, 0.25620457012772413, 0.25086894871520937, 0.24907094621848241, 0.24464681197651753, 0.2555420820729113, 0.2516124769132219, 0.24559529337520694, 0.24500019421034, 0.251282146145754, 0.24947186692745663, 0.2478056032502768, 0.24698442302863388, 0.24606479568333894, 0.2502952742335551]\n",
            "test_acc_list_ls0001 = [77.97326367547633, 81.02335586969883, 87.72664413030117, 86.47818070067609, 86.74323909035034, 87.4039643515673, 87.89950829748003, 88.24523663183774, 89.78564843269822, 88.79071911493547, 90.86124769514444, 91.46819299323909, 90.51551936078673, 91.60648432698217, 91.88690842040566, 91.38368162261831, 92.51306084818685, 91.29917025199754, 91.10709895513214, 91.82544560540873, 91.63721573448063, 92.1519668100799, 91.6180086047941, 92.42854947756608, 92.11739397664412, 92.4439151813153, 92.99708051628765, 92.66287645974185, 92.340196681008, 92.9240934234788, 92.80500921942225, 92.57068223724647, 92.90872771972957, 93.2160417947142, 93.13921327596803, 92.41702519975415, 92.80116779348494, 92.91256914566686, 93.11232329440688, 93.00092194222495, 93.26213890596189, 93.54640442532268, 92.69744929317763, 92.42470805162877, 92.85110633066995, 92.35940381069453, 93.1200061462815, 92.97019053472648, 93.1661032575292, 93.23140749846343, 93.46957590657652, 92.50921942224954, 93.05470190534726, 93.34280885064535, 93.4081130915796, 93.25445605408727, 92.78580208973571, 92.9778733866011, 93.45421020282728, 93.40043023970497, 92.95866625691457, 93.6539643515673, 93.4580516287646, 93.31976029502151, 93.6539643515673, 92.58988936693301, 93.41579594345421, 93.5118315918869, 93.45421020282728, 93.41963736939152, 93.72311001843885, 93.37738168408113, 93.42732022126613, 93.36969883220651, 93.23524892440074, 93.24677320221267, 93.28134603564843, 93.40427166564228, 93.61555009219423, 93.4119545175169, 93.05470190534726, 93.50799016594961, 93.58097725875845, 92.78580208973571, 93.5041487400123, 93.14305470190534, 93.33512599877075, 93.59634296250768, 93.62323294406883, 93.46573448063921, 93.51567301782421, 93.69622003687769, 93.49262446220037, 93.16226183159189, 93.41579594345421, 93.48878303626306, 93.05470190534726, 93.11232329440688, 93.03165334972341, 93.68853718500307, 93.43116164720344, 93.20067609096496, 93.47725875845114, 93.67701290719116, 93.49262446220037, 93.700061462815, 93.7077443146896, 93.86140135218193, 93.43500307314075, 93.53872157344806, 93.48110018438844, 93.82682851874615, 93.54256299938537, 93.70390288875231, 93.64628149969269, 93.74231714812538, 93.77688998156115, 93.36969883220651, 93.73079287031346, 93.82682851874615, 93.74615857406269, 93.69237861094038, 93.36969883220651, 94.06883835279656, 93.38890596189306, 93.6578057775046, 93.68853718500307, 93.26982175783651, 93.80762138905962, 93.47725875845114, 93.07775046097112, 93.72695144437616, 93.71158574062692, 93.59634296250768, 93.92286416717886, 93.49646588813768, 93.46957590657652, 93.68853718500307, 93.7077443146896, 93.70390288875231, 93.5579287031346, 93.28902888752305, 93.81146281499693, 93.58866011063307, 93.85755992624462, 93.85755992624462, 93.85755992624462, 93.799938537185, 93.65012292563, 93.92670559311617, 93.75384142593731, 93.92286416717886, 93.92286416717886, 93.799938537185, 93.9958512599877, 93.77304855562384, 93.87292562999386, 93.9459127228027, 93.95359557467732, 93.54256299938537, 94.24554394591273, 93.78073140749846, 94.02274124154886, 93.62707437000614, 94.05731407498463, 93.86908420405655, 93.8421942224954, 93.94975414874001, 93.97664413030117, 93.92286416717886, 93.81914566687155, 94.21865396435157, 93.8460356484327, 94.18023970497849, 93.90365703749232, 93.78457283343577, 93.83835279655808, 94.04578979717272, 94.06115550092194, 94.08420405654579, 93.9420712968654, 93.90749846342962, 94.00353411186232, 94.08420405654579, 94.04963122311001, 93.95743700061463, 93.83451137062077, 94.04578979717272, 94.07652120467118, 94.08420405654579, 93.97664413030117, 94.03042409342348, 94.21865396435157, 94.23017824216349, 93.93054701905348, 94.1840811309158, 94.07652120467118, 94.17639827904118, 94.06499692685925, 94.06883835279656, 94.05347264904732, 94.16487400122925, 94.36078672403197, 94.1917639827904, 94.34542102028273, 94.22249539028887, 94.06883835279656, 94.21865396435157, 94.25706822372464, 94.01121696373694, 94.11493546404425, 94.29164105716042, 94.29548248309773, 94.34542102028273, 94.18792255685311, 94.45298094652735, 94.21097111247695, 94.36078672403197, 94.25706822372464, 94.40304240934235, 94.3300553165335, 94.3761524277812, 94.15334972341734, 94.40688383527966, 94.31468961278426, 94.34157959434542, 94.21865396435157, 94.27243392747388, 94.3338967424708, 94.36846957590657, 94.49907805777505, 94.42609096496619, 94.52980946527352, 94.45682237246466, 94.29932390903504, 94.48371235402581, 94.51828518746159, 94.30700676090964, 94.47602950215119, 94.40304240934235, 94.54517516902274, 94.41456668715428, 94.6258451137062, 94.38767670559312, 94.54901659496005, 94.42609096496619, 94.55285802089736, 94.59511370620774, 94.38767670559312, 94.60663798401967, 94.56054087277197, 94.56822372464659, 94.63352796558083, 94.59511370620774, 94.58743085433314, 94.6258451137062, 94.5221266133989, 94.6681007990166, 94.57974800245852, 94.59511370620774, 94.55285802089736, 94.46066379840197, 94.68730792870313, 94.79870928088506, 94.59511370620774, 94.60663798401967, 94.66425937307929, 94.63736939151813, 94.7679778733866, 94.63352796558083, 94.66041794714198, 94.64121081745544, 94.63736939151813, 94.64505224339274, 94.74877074370006, 94.60279655808236, 94.58743085433314, 94.58358942839583, 94.70651505838967, 94.77566072526122, 94.54901659496005, 94.70267363245236, 94.78334357713584, 94.83328211432084, 94.60663798401967, 94.63736939151813, 94.53749231714812, 94.64121081745544, 94.74108789182544, 94.7679778733866, 94.64121081745544, 94.63352796558083, 94.79102642901044, 94.88322065150584, 94.67194222495391, 94.78718500307313, 94.67962507682851, 94.66041794714198, 94.6757836508912, 94.55285802089736]\n"
          ]
        }
      ]
    }
  ]
}