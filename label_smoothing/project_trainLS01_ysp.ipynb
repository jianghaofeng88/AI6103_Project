{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5ff0cab771d40f2b9a085c7ae9ebeec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3024f274ee4044fbb70489298b517254",
              "IPY_MODEL_60b4baf188e445008890d03295ef7593",
              "IPY_MODEL_9b44805fbf214116be9b66b2639b46ef"
            ],
            "layout": "IPY_MODEL_0db6ff1f30614574b74e75b02419a964"
          }
        },
        "3024f274ee4044fbb70489298b517254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_506281eae5a1476bad06f94a48dffaee",
            "placeholder": "​",
            "style": "IPY_MODEL_0b90de1b4c274bb2a6fd238ece7e1edd",
            "value": "100%"
          }
        },
        "60b4baf188e445008890d03295ef7593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e90286187d3434387c143090f627b9b",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3ff8eb4ef01466dbcbee86ab689e18e",
            "value": 182040794
          }
        },
        "9b44805fbf214116be9b66b2639b46ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7131f7b95da24697abc3f7e5ea1a2006",
            "placeholder": "​",
            "style": "IPY_MODEL_8e5769dfb96e42bfac055b49761d47c3",
            "value": " 182040794/182040794 [00:04&lt;00:00, 56245816.85it/s]"
          }
        },
        "0db6ff1f30614574b74e75b02419a964": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "506281eae5a1476bad06f94a48dffaee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b90de1b4c274bb2a6fd238ece7e1edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e90286187d3434387c143090f627b9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3ff8eb4ef01466dbcbee86ab689e18e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7131f7b95da24697abc3f7e5ea1a2006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e5769dfb96e42bfac055b49761d47c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d70c4636ab1a4081a58966f2081390cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7d1a34f74ec462a86032c9c06c5d87e",
              "IPY_MODEL_42910d5549e44b5cabf96083c653d667",
              "IPY_MODEL_e11b6030f09e4b7d97917b75137e81e4"
            ],
            "layout": "IPY_MODEL_74a41b2aaea24c54b4908ee196dc6860"
          }
        },
        "e7d1a34f74ec462a86032c9c06c5d87e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49fa027491334848a69523d212832a8c",
            "placeholder": "​",
            "style": "IPY_MODEL_a0224263d7c144dfb5c969847cd5a0c1",
            "value": "100%"
          }
        },
        "42910d5549e44b5cabf96083c653d667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9696130fa8e4c939bbc8bb74aea4a5e",
            "max": 64275384,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7af51fb857144bc190c533792334b02f",
            "value": 64275384
          }
        },
        "e11b6030f09e4b7d97917b75137e81e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6aaaa76ea4c4dc18b0fcc7993226685",
            "placeholder": "​",
            "style": "IPY_MODEL_65f09a22dd0a4b2b8482fcf7f3bb7766",
            "value": " 64275384/64275384 [00:02&lt;00:00, 54685149.83it/s]"
          }
        },
        "74a41b2aaea24c54b4908ee196dc6860": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49fa027491334848a69523d212832a8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0224263d7c144dfb5c969847cd5a0c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9696130fa8e4c939bbc8bb74aea4a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7af51fb857144bc190c533792334b02f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6aaaa76ea4c4dc18b0fcc7993226685": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65f09a22dd0a4b2b8482fcf7f3bb7766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "b5ff0cab771d40f2b9a085c7ae9ebeec",
            "3024f274ee4044fbb70489298b517254",
            "60b4baf188e445008890d03295ef7593",
            "9b44805fbf214116be9b66b2639b46ef",
            "0db6ff1f30614574b74e75b02419a964",
            "506281eae5a1476bad06f94a48dffaee",
            "0b90de1b4c274bb2a6fd238ece7e1edd",
            "7e90286187d3434387c143090f627b9b",
            "b3ff8eb4ef01466dbcbee86ab689e18e",
            "7131f7b95da24697abc3f7e5ea1a2006",
            "8e5769dfb96e42bfac055b49761d47c3",
            "d70c4636ab1a4081a58966f2081390cf",
            "e7d1a34f74ec462a86032c9c06c5d87e",
            "42910d5549e44b5cabf96083c653d667",
            "e11b6030f09e4b7d97917b75137e81e4",
            "74a41b2aaea24c54b4908ee196dc6860",
            "49fa027491334848a69523d212832a8c",
            "a0224263d7c144dfb5c969847cd5a0c1",
            "b9696130fa8e4c939bbc8bb74aea4a5e",
            "7af51fb857144bc190c533792334b02f",
            "b6aaaa76ea4c4dc18b0fcc7993226685",
            "65f09a22dd0a4b2b8482fcf7f3bb7766"
          ]
        },
        "id": "VgAiImV0uURP",
        "outputId": "cdbd8c5f-456c-4255-bfa7-5612b4c27317"
      },
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.SVHN),\n",
        "    transforms.ToTensor(),\n",
        "    \n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5ff0cab771d40f2b9a085c7ae9ebeec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/64275384 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d70c4636ab1a4081a58966f2081390cf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))\n",
        "print(trainset[4][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "b5d3e5a2-0529-4377-f154-d8bec3e08e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73257 26032\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        #out = F.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        #out = F.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        #out = F.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "b96bc61b-c75b-451d-d019-399e1c01ed23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(class_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "ca7e90b1-64c6-40cd-e5b5-7efa6c94cf7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArgupDVRwB8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45057813-b191-4d6c-e530-37f8af1edb49"
      },
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1).to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3492, accuracy : 16.94\n",
            "iteration : 100, loss : 2.2681, accuracy : 20.05\n",
            "iteration : 150, loss : 2.0877, accuracy : 28.71\n",
            "iteration : 200, loss : 1.8748, accuracy : 38.95\n",
            "iteration : 250, loss : 1.7074, accuracy : 46.88\n",
            "iteration : 300, loss : 1.5800, accuracy : 52.85\n",
            "iteration : 350, loss : 1.4860, accuracy : 57.30\n",
            "Epoch :   1, training loss : 1.4567, training accuracy : 58.67, test loss : 0.9764, test accuracy : 81.77\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.8800, accuracy : 85.73\n",
            "iteration : 100, loss : 0.8736, accuracy : 85.91\n",
            "iteration : 150, loss : 0.8733, accuracy : 85.96\n",
            "iteration : 200, loss : 0.8692, accuracy : 86.17\n",
            "iteration : 250, loss : 0.8663, accuracy : 86.23\n",
            "iteration : 300, loss : 0.8607, accuracy : 86.41\n",
            "iteration : 350, loss : 0.8584, accuracy : 86.50\n",
            "Epoch :   2, training loss : 0.8572, training accuracy : 86.53, test loss : 0.8991, test accuracy : 84.91\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.8173, accuracy : 88.06\n",
            "iteration : 100, loss : 0.8279, accuracy : 87.63\n",
            "iteration : 150, loss : 0.8254, accuracy : 87.82\n",
            "iteration : 200, loss : 0.8237, accuracy : 87.84\n",
            "iteration : 250, loss : 0.8185, accuracy : 87.95\n",
            "iteration : 300, loss : 0.8185, accuracy : 87.97\n",
            "iteration : 350, loss : 0.8162, accuracy : 88.06\n",
            "Epoch :   3, training loss : 0.8159, training accuracy : 88.06, test loss : 0.8541, test accuracy : 86.49\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.7892, accuracy : 88.83\n",
            "iteration : 100, loss : 0.7858, accuracy : 89.12\n",
            "iteration : 150, loss : 0.7873, accuracy : 89.05\n",
            "iteration : 200, loss : 0.7866, accuracy : 88.96\n",
            "iteration : 250, loss : 0.7868, accuracy : 89.02\n",
            "iteration : 300, loss : 0.7868, accuracy : 89.00\n",
            "iteration : 350, loss : 0.7874, accuracy : 89.00\n",
            "Epoch :   4, training loss : 0.7867, training accuracy : 89.03, test loss : 0.8444, test accuracy : 86.35\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.7729, accuracy : 89.98\n",
            "iteration : 100, loss : 0.7750, accuracy : 89.66\n",
            "iteration : 150, loss : 0.7771, accuracy : 89.45\n",
            "iteration : 200, loss : 0.7827, accuracy : 89.14\n",
            "iteration : 250, loss : 0.7812, accuracy : 89.18\n",
            "iteration : 300, loss : 0.7809, accuracy : 89.22\n",
            "iteration : 350, loss : 0.7787, accuracy : 89.23\n",
            "Epoch :   5, training loss : 0.7780, training accuracy : 89.27, test loss : 0.8140, test accuracy : 87.88\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.7524, accuracy : 90.11\n",
            "iteration : 100, loss : 0.7567, accuracy : 90.09\n",
            "iteration : 150, loss : 0.7578, accuracy : 90.03\n",
            "iteration : 200, loss : 0.7588, accuracy : 89.99\n",
            "iteration : 250, loss : 0.7575, accuracy : 89.98\n",
            "iteration : 300, loss : 0.7589, accuracy : 89.94\n",
            "iteration : 350, loss : 0.7586, accuracy : 89.99\n",
            "Epoch :   6, training loss : 0.7582, training accuracy : 90.03, test loss : 0.7621, test accuracy : 89.95\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.7346, accuracy : 90.81\n",
            "iteration : 100, loss : 0.7467, accuracy : 90.35\n",
            "iteration : 150, loss : 0.7429, accuracy : 90.48\n",
            "iteration : 200, loss : 0.7440, accuracy : 90.50\n",
            "iteration : 250, loss : 0.7434, accuracy : 90.55\n",
            "iteration : 300, loss : 0.7435, accuracy : 90.55\n",
            "iteration : 350, loss : 0.7458, accuracy : 90.42\n",
            "Epoch :   7, training loss : 0.7447, training accuracy : 90.47, test loss : 0.7748, test accuracy : 89.40\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.7427, accuracy : 90.42\n",
            "iteration : 100, loss : 0.7396, accuracy : 90.88\n",
            "iteration : 150, loss : 0.7412, accuracy : 90.69\n",
            "iteration : 200, loss : 0.7355, accuracy : 90.89\n",
            "iteration : 250, loss : 0.7371, accuracy : 90.81\n",
            "iteration : 300, loss : 0.7362, accuracy : 90.88\n",
            "iteration : 350, loss : 0.7345, accuracy : 90.92\n",
            "Epoch :   8, training loss : 0.7348, training accuracy : 90.93, test loss : 0.7821, test accuracy : 88.81\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.7411, accuracy : 90.69\n",
            "iteration : 100, loss : 0.7388, accuracy : 90.81\n",
            "iteration : 150, loss : 0.7367, accuracy : 90.91\n",
            "iteration : 200, loss : 0.7364, accuracy : 90.89\n",
            "iteration : 250, loss : 0.7331, accuracy : 91.07\n",
            "iteration : 300, loss : 0.7326, accuracy : 91.08\n",
            "iteration : 350, loss : 0.7318, accuracy : 91.07\n",
            "Epoch :   9, training loss : 0.7315, training accuracy : 91.07, test loss : 0.7488, test accuracy : 90.15\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.6969, accuracy : 92.30\n",
            "iteration : 100, loss : 0.7147, accuracy : 91.73\n",
            "iteration : 150, loss : 0.7143, accuracy : 91.69\n",
            "iteration : 200, loss : 0.7149, accuracy : 91.72\n",
            "iteration : 250, loss : 0.7148, accuracy : 91.72\n",
            "iteration : 300, loss : 0.7161, accuracy : 91.65\n",
            "iteration : 350, loss : 0.7163, accuracy : 91.58\n",
            "Epoch :  10, training loss : 0.7169, training accuracy : 91.56, test loss : 0.7434, test accuracy : 90.38\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.7147, accuracy : 91.81\n",
            "iteration : 100, loss : 0.7134, accuracy : 91.72\n",
            "iteration : 150, loss : 0.7109, accuracy : 91.77\n",
            "iteration : 200, loss : 0.7134, accuracy : 91.76\n",
            "iteration : 250, loss : 0.7121, accuracy : 91.82\n",
            "iteration : 300, loss : 0.7108, accuracy : 91.87\n",
            "iteration : 350, loss : 0.7105, accuracy : 91.85\n",
            "Epoch :  11, training loss : 0.7107, training accuracy : 91.86, test loss : 0.7489, test accuracy : 90.72\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.6984, accuracy : 92.33\n",
            "iteration : 100, loss : 0.7026, accuracy : 92.23\n",
            "iteration : 150, loss : 0.7064, accuracy : 92.05\n",
            "iteration : 200, loss : 0.7057, accuracy : 92.05\n",
            "iteration : 250, loss : 0.7053, accuracy : 92.05\n",
            "iteration : 300, loss : 0.7043, accuracy : 92.12\n",
            "iteration : 350, loss : 0.7040, accuracy : 92.08\n",
            "Epoch :  12, training loss : 0.7042, training accuracy : 92.08, test loss : 0.7370, test accuracy : 90.65\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.7014, accuracy : 92.19\n",
            "iteration : 100, loss : 0.6914, accuracy : 92.63\n",
            "iteration : 150, loss : 0.6872, accuracy : 92.76\n",
            "iteration : 200, loss : 0.6889, accuracy : 92.71\n",
            "iteration : 250, loss : 0.6916, accuracy : 92.63\n",
            "iteration : 300, loss : 0.6930, accuracy : 92.58\n",
            "iteration : 350, loss : 0.6959, accuracy : 92.45\n",
            "Epoch :  13, training loss : 0.6970, training accuracy : 92.40, test loss : 0.7345, test accuracy : 90.62\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.6870, accuracy : 92.56\n",
            "iteration : 100, loss : 0.6859, accuracy : 92.81\n",
            "iteration : 150, loss : 0.6870, accuracy : 92.76\n",
            "iteration : 200, loss : 0.6867, accuracy : 92.73\n",
            "iteration : 250, loss : 0.6899, accuracy : 92.62\n",
            "iteration : 300, loss : 0.6932, accuracy : 92.49\n",
            "iteration : 350, loss : 0.6915, accuracy : 92.56\n",
            "Epoch :  14, training loss : 0.6920, training accuracy : 92.55, test loss : 0.7162, test accuracy : 91.63\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.6804, accuracy : 92.95\n",
            "iteration : 100, loss : 0.6828, accuracy : 92.73\n",
            "iteration : 150, loss : 0.6857, accuracy : 92.74\n",
            "iteration : 200, loss : 0.6846, accuracy : 92.83\n",
            "iteration : 250, loss : 0.6845, accuracy : 92.88\n",
            "iteration : 300, loss : 0.6860, accuracy : 92.83\n",
            "iteration : 350, loss : 0.6865, accuracy : 92.82\n",
            "Epoch :  15, training loss : 0.6867, training accuracy : 92.81, test loss : 0.7069, test accuracy : 91.85\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.6911, accuracy : 92.52\n",
            "iteration : 100, loss : 0.6823, accuracy : 92.96\n",
            "iteration : 150, loss : 0.6806, accuracy : 93.08\n",
            "iteration : 200, loss : 0.6876, accuracy : 92.88\n",
            "iteration : 250, loss : 0.6889, accuracy : 92.78\n",
            "iteration : 300, loss : 0.6882, accuracy : 92.83\n",
            "iteration : 350, loss : 0.6861, accuracy : 92.90\n",
            "Epoch :  16, training loss : 0.6853, training accuracy : 92.91, test loss : 0.7261, test accuracy : 91.20\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.6730, accuracy : 93.38\n",
            "iteration : 100, loss : 0.6744, accuracy : 93.33\n",
            "iteration : 150, loss : 0.6764, accuracy : 93.26\n",
            "iteration : 200, loss : 0.6776, accuracy : 93.17\n",
            "iteration : 250, loss : 0.6782, accuracy : 93.13\n",
            "iteration : 300, loss : 0.6815, accuracy : 93.01\n",
            "iteration : 350, loss : 0.6811, accuracy : 93.05\n",
            "Epoch :  17, training loss : 0.6817, training accuracy : 93.03, test loss : 0.7152, test accuracy : 91.63\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.6729, accuracy : 93.77\n",
            "iteration : 100, loss : 0.6724, accuracy : 93.57\n",
            "iteration : 150, loss : 0.6783, accuracy : 93.21\n",
            "iteration : 200, loss : 0.6787, accuracy : 93.15\n",
            "iteration : 250, loss : 0.6751, accuracy : 93.31\n",
            "iteration : 300, loss : 0.6753, accuracy : 93.32\n",
            "iteration : 350, loss : 0.6756, accuracy : 93.35\n",
            "Epoch :  18, training loss : 0.6761, training accuracy : 93.34, test loss : 0.7025, test accuracy : 92.10\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.6545, accuracy : 94.11\n",
            "iteration : 100, loss : 0.6595, accuracy : 93.98\n",
            "iteration : 150, loss : 0.6684, accuracy : 93.57\n",
            "iteration : 200, loss : 0.6707, accuracy : 93.50\n",
            "iteration : 250, loss : 0.6735, accuracy : 93.41\n",
            "iteration : 300, loss : 0.6739, accuracy : 93.39\n",
            "iteration : 350, loss : 0.6747, accuracy : 93.35\n",
            "Epoch :  19, training loss : 0.6748, training accuracy : 93.35, test loss : 0.7037, test accuracy : 92.13\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.6673, accuracy : 93.77\n",
            "iteration : 100, loss : 0.6676, accuracy : 93.70\n",
            "iteration : 150, loss : 0.6656, accuracy : 93.78\n",
            "iteration : 200, loss : 0.6695, accuracy : 93.61\n",
            "iteration : 250, loss : 0.6686, accuracy : 93.61\n",
            "iteration : 300, loss : 0.6682, accuracy : 93.58\n",
            "iteration : 350, loss : 0.6693, accuracy : 93.53\n",
            "Epoch :  20, training loss : 0.6702, training accuracy : 93.48, test loss : 0.7075, test accuracy : 91.99\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.6647, accuracy : 93.64\n",
            "iteration : 100, loss : 0.6627, accuracy : 93.84\n",
            "iteration : 150, loss : 0.6650, accuracy : 93.77\n",
            "iteration : 200, loss : 0.6689, accuracy : 93.62\n",
            "iteration : 250, loss : 0.6708, accuracy : 93.53\n",
            "iteration : 300, loss : 0.6693, accuracy : 93.56\n",
            "iteration : 350, loss : 0.6696, accuracy : 93.49\n",
            "Epoch :  21, training loss : 0.6686, training accuracy : 93.54, test loss : 0.6879, test accuracy : 92.74\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.6592, accuracy : 93.89\n",
            "iteration : 100, loss : 0.6639, accuracy : 93.67\n",
            "iteration : 150, loss : 0.6627, accuracy : 93.75\n",
            "iteration : 200, loss : 0.6634, accuracy : 93.70\n",
            "iteration : 250, loss : 0.6639, accuracy : 93.68\n",
            "iteration : 300, loss : 0.6660, accuracy : 93.61\n",
            "iteration : 350, loss : 0.6678, accuracy : 93.53\n",
            "Epoch :  22, training loss : 0.6685, training accuracy : 93.52, test loss : 0.6946, test accuracy : 92.53\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.6655, accuracy : 93.67\n",
            "iteration : 100, loss : 0.6614, accuracy : 93.84\n",
            "iteration : 150, loss : 0.6597, accuracy : 93.91\n",
            "iteration : 200, loss : 0.6615, accuracy : 93.88\n",
            "iteration : 250, loss : 0.6628, accuracy : 93.79\n",
            "iteration : 300, loss : 0.6638, accuracy : 93.72\n",
            "iteration : 350, loss : 0.6632, accuracy : 93.70\n",
            "Epoch :  23, training loss : 0.6634, training accuracy : 93.69, test loss : 0.7084, test accuracy : 92.01\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.6520, accuracy : 94.05\n",
            "iteration : 100, loss : 0.6587, accuracy : 93.73\n",
            "iteration : 150, loss : 0.6569, accuracy : 93.85\n",
            "iteration : 200, loss : 0.6598, accuracy : 93.77\n",
            "iteration : 250, loss : 0.6582, accuracy : 93.83\n",
            "iteration : 300, loss : 0.6591, accuracy : 93.82\n",
            "iteration : 350, loss : 0.6589, accuracy : 93.88\n",
            "Epoch :  24, training loss : 0.6598, training accuracy : 93.85, test loss : 0.7113, test accuracy : 91.93\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.6540, accuracy : 94.14\n",
            "iteration : 100, loss : 0.6538, accuracy : 93.95\n",
            "iteration : 150, loss : 0.6559, accuracy : 93.86\n",
            "iteration : 200, loss : 0.6590, accuracy : 93.71\n",
            "iteration : 250, loss : 0.6611, accuracy : 93.69\n",
            "iteration : 300, loss : 0.6591, accuracy : 93.80\n",
            "iteration : 350, loss : 0.6602, accuracy : 93.72\n",
            "Epoch :  25, training loss : 0.6608, training accuracy : 93.70, test loss : 0.7049, test accuracy : 92.09\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.6586, accuracy : 94.00\n",
            "iteration : 100, loss : 0.6584, accuracy : 94.02\n",
            "iteration : 150, loss : 0.6547, accuracy : 94.04\n",
            "iteration : 200, loss : 0.6553, accuracy : 94.03\n",
            "iteration : 250, loss : 0.6562, accuracy : 94.01\n",
            "iteration : 300, loss : 0.6558, accuracy : 94.01\n",
            "iteration : 350, loss : 0.6565, accuracy : 93.99\n",
            "Epoch :  26, training loss : 0.6570, training accuracy : 93.99, test loss : 0.6873, test accuracy : 92.72\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.6423, accuracy : 94.41\n",
            "iteration : 100, loss : 0.6515, accuracy : 94.07\n",
            "iteration : 150, loss : 0.6543, accuracy : 94.09\n",
            "iteration : 200, loss : 0.6567, accuracy : 93.98\n",
            "iteration : 250, loss : 0.6575, accuracy : 93.96\n",
            "iteration : 300, loss : 0.6568, accuracy : 93.98\n",
            "iteration : 350, loss : 0.6546, accuracy : 94.08\n",
            "Epoch :  27, training loss : 0.6549, training accuracy : 94.07, test loss : 0.6907, test accuracy : 92.71\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.6376, accuracy : 94.56\n",
            "iteration : 100, loss : 0.6454, accuracy : 94.50\n",
            "iteration : 150, loss : 0.6501, accuracy : 94.27\n",
            "iteration : 200, loss : 0.6551, accuracy : 94.09\n",
            "iteration : 250, loss : 0.6524, accuracy : 94.19\n",
            "iteration : 300, loss : 0.6531, accuracy : 94.15\n",
            "iteration : 350, loss : 0.6542, accuracy : 94.09\n",
            "Epoch :  28, training loss : 0.6550, training accuracy : 94.07, test loss : 0.7117, test accuracy : 91.69\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.6543, accuracy : 94.08\n",
            "iteration : 100, loss : 0.6549, accuracy : 94.05\n",
            "iteration : 150, loss : 0.6536, accuracy : 94.12\n",
            "iteration : 200, loss : 0.6519, accuracy : 94.20\n",
            "iteration : 250, loss : 0.6562, accuracy : 93.99\n",
            "iteration : 300, loss : 0.6544, accuracy : 94.06\n",
            "iteration : 350, loss : 0.6551, accuracy : 94.04\n",
            "Epoch :  29, training loss : 0.6555, training accuracy : 94.00, test loss : 0.6835, test accuracy : 92.91\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.6485, accuracy : 94.45\n",
            "iteration : 100, loss : 0.6500, accuracy : 94.38\n",
            "iteration : 150, loss : 0.6492, accuracy : 94.38\n",
            "iteration : 200, loss : 0.6497, accuracy : 94.36\n",
            "iteration : 250, loss : 0.6507, accuracy : 94.29\n",
            "iteration : 300, loss : 0.6516, accuracy : 94.29\n",
            "iteration : 350, loss : 0.6529, accuracy : 94.24\n",
            "Epoch :  30, training loss : 0.6529, training accuracy : 94.24, test loss : 0.6951, test accuracy : 92.42\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.6477, accuracy : 94.27\n",
            "iteration : 100, loss : 0.6485, accuracy : 94.08\n",
            "iteration : 150, loss : 0.6490, accuracy : 94.13\n",
            "iteration : 200, loss : 0.6493, accuracy : 94.13\n",
            "iteration : 250, loss : 0.6524, accuracy : 94.01\n",
            "iteration : 300, loss : 0.6520, accuracy : 94.01\n",
            "iteration : 350, loss : 0.6516, accuracy : 94.04\n",
            "Epoch :  31, training loss : 0.6512, training accuracy : 94.04, test loss : 0.6874, test accuracy : 92.67\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.6417, accuracy : 94.30\n",
            "iteration : 100, loss : 0.6414, accuracy : 94.48\n",
            "iteration : 150, loss : 0.6474, accuracy : 94.32\n",
            "iteration : 200, loss : 0.6496, accuracy : 94.23\n",
            "iteration : 250, loss : 0.6503, accuracy : 94.20\n",
            "iteration : 300, loss : 0.6526, accuracy : 94.13\n",
            "iteration : 350, loss : 0.6517, accuracy : 94.17\n",
            "Epoch :  32, training loss : 0.6525, training accuracy : 94.16, test loss : 0.7024, test accuracy : 92.25\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.6389, accuracy : 94.80\n",
            "iteration : 100, loss : 0.6409, accuracy : 94.61\n",
            "iteration : 150, loss : 0.6447, accuracy : 94.45\n",
            "iteration : 200, loss : 0.6452, accuracy : 94.40\n",
            "iteration : 250, loss : 0.6467, accuracy : 94.29\n",
            "iteration : 300, loss : 0.6469, accuracy : 94.32\n",
            "iteration : 350, loss : 0.6475, accuracy : 94.29\n",
            "Epoch :  33, training loss : 0.6480, training accuracy : 94.27, test loss : 0.6911, test accuracy : 92.47\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.6473, accuracy : 94.25\n",
            "iteration : 100, loss : 0.6415, accuracy : 94.38\n",
            "iteration : 150, loss : 0.6429, accuracy : 94.42\n",
            "iteration : 200, loss : 0.6448, accuracy : 94.36\n",
            "iteration : 250, loss : 0.6457, accuracy : 94.37\n",
            "iteration : 300, loss : 0.6482, accuracy : 94.31\n",
            "iteration : 350, loss : 0.6488, accuracy : 94.32\n",
            "Epoch :  34, training loss : 0.6488, training accuracy : 94.30, test loss : 0.6749, test accuracy : 93.19\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.6335, accuracy : 94.92\n",
            "iteration : 100, loss : 0.6358, accuracy : 95.05\n",
            "iteration : 150, loss : 0.6398, accuracy : 94.82\n",
            "iteration : 200, loss : 0.6395, accuracy : 94.77\n",
            "iteration : 250, loss : 0.6401, accuracy : 94.73\n",
            "iteration : 300, loss : 0.6429, accuracy : 94.59\n",
            "iteration : 350, loss : 0.6443, accuracy : 94.54\n",
            "Epoch :  35, training loss : 0.6441, training accuracy : 94.54, test loss : 0.6881, test accuracy : 92.71\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.6444, accuracy : 94.62\n",
            "iteration : 100, loss : 0.6413, accuracy : 94.61\n",
            "iteration : 150, loss : 0.6422, accuracy : 94.55\n",
            "iteration : 200, loss : 0.6445, accuracy : 94.47\n",
            "iteration : 250, loss : 0.6446, accuracy : 94.46\n",
            "iteration : 300, loss : 0.6458, accuracy : 94.44\n",
            "iteration : 350, loss : 0.6465, accuracy : 94.43\n",
            "Epoch :  36, training loss : 0.6465, training accuracy : 94.44, test loss : 0.6676, test accuracy : 93.50\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.6347, accuracy : 94.50\n",
            "iteration : 100, loss : 0.6375, accuracy : 94.52\n",
            "iteration : 150, loss : 0.6393, accuracy : 94.49\n",
            "iteration : 200, loss : 0.6403, accuracy : 94.51\n",
            "iteration : 250, loss : 0.6408, accuracy : 94.52\n",
            "iteration : 300, loss : 0.6423, accuracy : 94.45\n",
            "iteration : 350, loss : 0.6427, accuracy : 94.42\n",
            "Epoch :  37, training loss : 0.6428, training accuracy : 94.43, test loss : 0.6947, test accuracy : 92.60\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.6400, accuracy : 94.66\n",
            "iteration : 100, loss : 0.6396, accuracy : 94.62\n",
            "iteration : 150, loss : 0.6417, accuracy : 94.48\n",
            "iteration : 200, loss : 0.6413, accuracy : 94.54\n",
            "iteration : 250, loss : 0.6412, accuracy : 94.55\n",
            "iteration : 300, loss : 0.6444, accuracy : 94.45\n",
            "iteration : 350, loss : 0.6436, accuracy : 94.49\n",
            "Epoch :  38, training loss : 0.6433, training accuracy : 94.52, test loss : 0.6868, test accuracy : 92.77\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.6380, accuracy : 94.73\n",
            "iteration : 100, loss : 0.6350, accuracy : 94.81\n",
            "iteration : 150, loss : 0.6353, accuracy : 94.82\n",
            "iteration : 200, loss : 0.6379, accuracy : 94.69\n",
            "iteration : 250, loss : 0.6379, accuracy : 94.71\n",
            "iteration : 300, loss : 0.6397, accuracy : 94.65\n",
            "iteration : 350, loss : 0.6421, accuracy : 94.55\n",
            "Epoch :  39, training loss : 0.6425, training accuracy : 94.53, test loss : 0.6795, test accuracy : 93.01\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.6349, accuracy : 94.89\n",
            "iteration : 100, loss : 0.6350, accuracy : 94.88\n",
            "iteration : 150, loss : 0.6365, accuracy : 94.88\n",
            "iteration : 200, loss : 0.6403, accuracy : 94.73\n",
            "iteration : 250, loss : 0.6389, accuracy : 94.77\n",
            "iteration : 300, loss : 0.6412, accuracy : 94.67\n",
            "iteration : 350, loss : 0.6422, accuracy : 94.61\n",
            "Epoch :  40, training loss : 0.6426, training accuracy : 94.59, test loss : 0.6808, test accuracy : 93.09\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.6272, accuracy : 95.16\n",
            "iteration : 100, loss : 0.6338, accuracy : 94.93\n",
            "iteration : 150, loss : 0.6372, accuracy : 94.70\n",
            "iteration : 200, loss : 0.6392, accuracy : 94.65\n",
            "iteration : 250, loss : 0.6400, accuracy : 94.62\n",
            "iteration : 300, loss : 0.6409, accuracy : 94.60\n",
            "iteration : 350, loss : 0.6400, accuracy : 94.65\n",
            "Epoch :  41, training loss : 0.6394, training accuracy : 94.65, test loss : 0.6786, test accuracy : 93.26\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.6385, accuracy : 94.83\n",
            "iteration : 100, loss : 0.6328, accuracy : 95.04\n",
            "iteration : 150, loss : 0.6339, accuracy : 94.99\n",
            "iteration : 200, loss : 0.6367, accuracy : 94.88\n",
            "iteration : 250, loss : 0.6377, accuracy : 94.81\n",
            "iteration : 300, loss : 0.6379, accuracy : 94.77\n",
            "iteration : 350, loss : 0.6395, accuracy : 94.69\n",
            "Epoch :  42, training loss : 0.6396, training accuracy : 94.66, test loss : 0.6756, test accuracy : 93.26\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.6320, accuracy : 95.00\n",
            "iteration : 100, loss : 0.6343, accuracy : 94.95\n",
            "iteration : 150, loss : 0.6351, accuracy : 94.93\n",
            "iteration : 200, loss : 0.6380, accuracy : 94.77\n",
            "iteration : 250, loss : 0.6399, accuracy : 94.64\n",
            "iteration : 300, loss : 0.6415, accuracy : 94.54\n",
            "iteration : 350, loss : 0.6404, accuracy : 94.60\n",
            "Epoch :  43, training loss : 0.6406, training accuracy : 94.60, test loss : 0.6929, test accuracy : 92.61\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.6181, accuracy : 95.50\n",
            "iteration : 100, loss : 0.6234, accuracy : 95.30\n",
            "iteration : 150, loss : 0.6292, accuracy : 95.08\n",
            "iteration : 200, loss : 0.6310, accuracy : 95.04\n",
            "iteration : 250, loss : 0.6328, accuracy : 94.98\n",
            "iteration : 300, loss : 0.6355, accuracy : 94.87\n",
            "iteration : 350, loss : 0.6369, accuracy : 94.80\n",
            "Epoch :  44, training loss : 0.6377, training accuracy : 94.76, test loss : 0.6897, test accuracy : 92.78\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.6449, accuracy : 94.38\n",
            "iteration : 100, loss : 0.6458, accuracy : 94.41\n",
            "iteration : 150, loss : 0.6443, accuracy : 94.48\n",
            "iteration : 200, loss : 0.6435, accuracy : 94.50\n",
            "iteration : 250, loss : 0.6432, accuracy : 94.48\n",
            "iteration : 300, loss : 0.6413, accuracy : 94.56\n",
            "iteration : 350, loss : 0.6414, accuracy : 94.53\n",
            "Epoch :  45, training loss : 0.6406, training accuracy : 94.55, test loss : 0.6718, test accuracy : 93.42\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.6382, accuracy : 94.55\n",
            "iteration : 100, loss : 0.6358, accuracy : 94.72\n",
            "iteration : 150, loss : 0.6343, accuracy : 94.82\n",
            "iteration : 200, loss : 0.6341, accuracy : 94.84\n",
            "iteration : 250, loss : 0.6350, accuracy : 94.83\n",
            "iteration : 300, loss : 0.6355, accuracy : 94.83\n",
            "iteration : 350, loss : 0.6362, accuracy : 94.80\n",
            "Epoch :  46, training loss : 0.6365, training accuracy : 94.79, test loss : 0.6927, test accuracy : 92.39\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.6392, accuracy : 94.81\n",
            "iteration : 100, loss : 0.6303, accuracy : 95.12\n",
            "iteration : 150, loss : 0.6349, accuracy : 94.84\n",
            "iteration : 200, loss : 0.6340, accuracy : 94.83\n",
            "iteration : 250, loss : 0.6331, accuracy : 94.86\n",
            "iteration : 300, loss : 0.6326, accuracy : 94.92\n",
            "iteration : 350, loss : 0.6336, accuracy : 94.88\n",
            "Epoch :  47, training loss : 0.6347, training accuracy : 94.82, test loss : 0.6825, test accuracy : 92.98\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.6364, accuracy : 95.03\n",
            "iteration : 100, loss : 0.6352, accuracy : 94.86\n",
            "iteration : 150, loss : 0.6336, accuracy : 94.91\n",
            "iteration : 200, loss : 0.6334, accuracy : 94.87\n",
            "iteration : 250, loss : 0.6336, accuracy : 94.88\n",
            "iteration : 300, loss : 0.6347, accuracy : 94.82\n",
            "iteration : 350, loss : 0.6365, accuracy : 94.77\n",
            "Epoch :  48, training loss : 0.6364, training accuracy : 94.78, test loss : 0.6752, test accuracy : 93.29\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.6213, accuracy : 95.44\n",
            "iteration : 100, loss : 0.6311, accuracy : 95.03\n",
            "iteration : 150, loss : 0.6335, accuracy : 94.85\n",
            "iteration : 200, loss : 0.6340, accuracy : 94.85\n",
            "iteration : 250, loss : 0.6346, accuracy : 94.83\n",
            "iteration : 300, loss : 0.6346, accuracy : 94.84\n",
            "iteration : 350, loss : 0.6362, accuracy : 94.78\n",
            "Epoch :  49, training loss : 0.6365, training accuracy : 94.76, test loss : 0.7037, test accuracy : 92.13\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.6267, accuracy : 95.11\n",
            "iteration : 100, loss : 0.6258, accuracy : 95.08\n",
            "iteration : 150, loss : 0.6304, accuracy : 95.02\n",
            "iteration : 200, loss : 0.6325, accuracy : 95.00\n",
            "iteration : 250, loss : 0.6336, accuracy : 94.97\n",
            "iteration : 300, loss : 0.6342, accuracy : 94.90\n",
            "iteration : 350, loss : 0.6355, accuracy : 94.83\n",
            "Epoch :  50, training loss : 0.6354, training accuracy : 94.84, test loss : 0.6794, test accuracy : 93.13\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.6228, accuracy : 95.28\n",
            "iteration : 100, loss : 0.6209, accuracy : 95.35\n",
            "iteration : 150, loss : 0.6249, accuracy : 95.21\n",
            "iteration : 200, loss : 0.6264, accuracy : 95.15\n",
            "iteration : 250, loss : 0.6271, accuracy : 95.15\n",
            "iteration : 300, loss : 0.6330, accuracy : 94.92\n",
            "iteration : 350, loss : 0.6324, accuracy : 94.93\n",
            "Epoch :  51, training loss : 0.6326, training accuracy : 94.92, test loss : 0.6836, test accuracy : 92.96\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.6316, accuracy : 94.83\n",
            "iteration : 100, loss : 0.6294, accuracy : 95.07\n",
            "iteration : 150, loss : 0.6303, accuracy : 94.98\n",
            "iteration : 200, loss : 0.6302, accuracy : 95.05\n",
            "iteration : 250, loss : 0.6327, accuracy : 94.96\n",
            "iteration : 300, loss : 0.6326, accuracy : 94.93\n",
            "iteration : 350, loss : 0.6351, accuracy : 94.86\n",
            "Epoch :  52, training loss : 0.6353, training accuracy : 94.83, test loss : 0.6812, test accuracy : 93.08\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.6214, accuracy : 95.41\n",
            "iteration : 100, loss : 0.6244, accuracy : 95.36\n",
            "iteration : 150, loss : 0.6224, accuracy : 95.43\n",
            "iteration : 200, loss : 0.6241, accuracy : 95.34\n",
            "iteration : 250, loss : 0.6285, accuracy : 95.13\n",
            "iteration : 300, loss : 0.6313, accuracy : 95.02\n",
            "iteration : 350, loss : 0.6319, accuracy : 94.98\n",
            "Epoch :  53, training loss : 0.6318, training accuracy : 94.97, test loss : 0.6780, test accuracy : 93.07\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.6329, accuracy : 94.89\n",
            "iteration : 100, loss : 0.6309, accuracy : 95.01\n",
            "iteration : 150, loss : 0.6288, accuracy : 95.09\n",
            "iteration : 200, loss : 0.6292, accuracy : 95.05\n",
            "iteration : 250, loss : 0.6306, accuracy : 94.97\n",
            "iteration : 300, loss : 0.6305, accuracy : 94.98\n",
            "iteration : 350, loss : 0.6316, accuracy : 94.94\n",
            "Epoch :  54, training loss : 0.6321, training accuracy : 94.93, test loss : 0.6749, test accuracy : 93.12\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.6211, accuracy : 95.38\n",
            "iteration : 100, loss : 0.6246, accuracy : 95.22\n",
            "iteration : 150, loss : 0.6286, accuracy : 95.06\n",
            "iteration : 200, loss : 0.6286, accuracy : 95.08\n",
            "iteration : 250, loss : 0.6285, accuracy : 95.14\n",
            "iteration : 300, loss : 0.6298, accuracy : 95.09\n",
            "iteration : 350, loss : 0.6298, accuracy : 95.10\n",
            "Epoch :  55, training loss : 0.6298, training accuracy : 95.08, test loss : 0.6658, test accuracy : 93.64\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.6191, accuracy : 95.44\n",
            "iteration : 100, loss : 0.6147, accuracy : 95.57\n",
            "iteration : 150, loss : 0.6212, accuracy : 95.31\n",
            "iteration : 200, loss : 0.6248, accuracy : 95.19\n",
            "iteration : 250, loss : 0.6259, accuracy : 95.15\n",
            "iteration : 300, loss : 0.6280, accuracy : 95.06\n",
            "iteration : 350, loss : 0.6285, accuracy : 95.03\n",
            "Epoch :  56, training loss : 0.6286, training accuracy : 95.01, test loss : 0.6737, test accuracy : 93.25\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.6268, accuracy : 94.81\n",
            "iteration : 100, loss : 0.6256, accuracy : 95.07\n",
            "iteration : 150, loss : 0.6284, accuracy : 94.94\n",
            "iteration : 200, loss : 0.6279, accuracy : 95.02\n",
            "iteration : 250, loss : 0.6297, accuracy : 94.99\n",
            "iteration : 300, loss : 0.6299, accuracy : 94.96\n",
            "iteration : 350, loss : 0.6291, accuracy : 95.01\n",
            "Epoch :  57, training loss : 0.6287, training accuracy : 95.03, test loss : 0.6889, test accuracy : 92.71\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.6321, accuracy : 94.91\n",
            "iteration : 100, loss : 0.6251, accuracy : 95.34\n",
            "iteration : 150, loss : 0.6280, accuracy : 95.17\n",
            "iteration : 200, loss : 0.6319, accuracy : 95.01\n",
            "iteration : 250, loss : 0.6321, accuracy : 95.05\n",
            "iteration : 300, loss : 0.6318, accuracy : 95.05\n",
            "iteration : 350, loss : 0.6328, accuracy : 94.97\n",
            "Epoch :  58, training loss : 0.6328, training accuracy : 94.98, test loss : 0.6907, test accuracy : 92.75\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.6202, accuracy : 95.47\n",
            "iteration : 100, loss : 0.6195, accuracy : 95.40\n",
            "iteration : 150, loss : 0.6217, accuracy : 95.42\n",
            "iteration : 200, loss : 0.6225, accuracy : 95.36\n",
            "iteration : 250, loss : 0.6255, accuracy : 95.31\n",
            "iteration : 300, loss : 0.6274, accuracy : 95.22\n",
            "iteration : 350, loss : 0.6289, accuracy : 95.12\n",
            "Epoch :  59, training loss : 0.6288, training accuracy : 95.13, test loss : 0.6679, test accuracy : 93.52\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.6207, accuracy : 95.48\n",
            "iteration : 100, loss : 0.6262, accuracy : 95.12\n",
            "iteration : 150, loss : 0.6253, accuracy : 95.12\n",
            "iteration : 200, loss : 0.6265, accuracy : 95.09\n",
            "iteration : 250, loss : 0.6266, accuracy : 95.05\n",
            "iteration : 300, loss : 0.6273, accuracy : 95.09\n",
            "iteration : 350, loss : 0.6286, accuracy : 95.02\n",
            "Epoch :  60, training loss : 0.6293, training accuracy : 95.00, test loss : 0.6743, test accuracy : 93.43\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.6205, accuracy : 95.48\n",
            "iteration : 100, loss : 0.6251, accuracy : 95.32\n",
            "iteration : 150, loss : 0.6264, accuracy : 95.24\n",
            "iteration : 200, loss : 0.6263, accuracy : 95.23\n",
            "iteration : 250, loss : 0.6275, accuracy : 95.16\n",
            "iteration : 300, loss : 0.6288, accuracy : 95.11\n",
            "iteration : 350, loss : 0.6288, accuracy : 95.12\n",
            "Epoch :  61, training loss : 0.6290, training accuracy : 95.10, test loss : 0.6736, test accuracy : 93.42\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.6166, accuracy : 95.75\n",
            "iteration : 100, loss : 0.6169, accuracy : 95.53\n",
            "iteration : 150, loss : 0.6173, accuracy : 95.58\n",
            "iteration : 200, loss : 0.6208, accuracy : 95.44\n",
            "iteration : 250, loss : 0.6238, accuracy : 95.35\n",
            "iteration : 300, loss : 0.6239, accuracy : 95.24\n",
            "iteration : 350, loss : 0.6265, accuracy : 95.16\n",
            "Epoch :  62, training loss : 0.6261, training accuracy : 95.16, test loss : 0.6783, test accuracy : 93.14\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.6258, accuracy : 95.38\n",
            "iteration : 100, loss : 0.6283, accuracy : 95.25\n",
            "iteration : 150, loss : 0.6250, accuracy : 95.38\n",
            "iteration : 200, loss : 0.6288, accuracy : 95.17\n",
            "iteration : 250, loss : 0.6285, accuracy : 95.12\n",
            "iteration : 300, loss : 0.6280, accuracy : 95.13\n",
            "iteration : 350, loss : 0.6294, accuracy : 95.05\n",
            "Epoch :  63, training loss : 0.6288, training accuracy : 95.08, test loss : 0.6675, test accuracy : 93.47\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.6181, accuracy : 95.48\n",
            "iteration : 100, loss : 0.6176, accuracy : 95.45\n",
            "iteration : 150, loss : 0.6210, accuracy : 95.41\n",
            "iteration : 200, loss : 0.6231, accuracy : 95.29\n",
            "iteration : 250, loss : 0.6264, accuracy : 95.21\n",
            "iteration : 300, loss : 0.6269, accuracy : 95.18\n",
            "iteration : 350, loss : 0.6256, accuracy : 95.23\n",
            "Epoch :  64, training loss : 0.6256, training accuracy : 95.24, test loss : 0.6831, test accuracy : 93.05\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.6245, accuracy : 95.11\n",
            "iteration : 100, loss : 0.6252, accuracy : 95.22\n",
            "iteration : 150, loss : 0.6216, accuracy : 95.27\n",
            "iteration : 200, loss : 0.6226, accuracy : 95.25\n",
            "iteration : 250, loss : 0.6261, accuracy : 95.15\n",
            "iteration : 300, loss : 0.6273, accuracy : 95.10\n",
            "iteration : 350, loss : 0.6282, accuracy : 95.05\n",
            "Epoch :  65, training loss : 0.6295, training accuracy : 95.02, test loss : 0.6740, test accuracy : 93.22\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.6215, accuracy : 95.42\n",
            "iteration : 100, loss : 0.6238, accuracy : 95.46\n",
            "iteration : 150, loss : 0.6252, accuracy : 95.32\n",
            "iteration : 200, loss : 0.6251, accuracy : 95.34\n",
            "iteration : 250, loss : 0.6281, accuracy : 95.18\n",
            "iteration : 300, loss : 0.6283, accuracy : 95.14\n",
            "iteration : 350, loss : 0.6281, accuracy : 95.15\n",
            "Epoch :  66, training loss : 0.6286, training accuracy : 95.13, test loss : 0.6773, test accuracy : 93.07\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.6236, accuracy : 95.36\n",
            "iteration : 100, loss : 0.6240, accuracy : 95.32\n",
            "iteration : 150, loss : 0.6233, accuracy : 95.34\n",
            "iteration : 200, loss : 0.6228, accuracy : 95.34\n",
            "iteration : 250, loss : 0.6267, accuracy : 95.14\n",
            "iteration : 300, loss : 0.6280, accuracy : 95.09\n",
            "iteration : 350, loss : 0.6294, accuracy : 95.03\n",
            "Epoch :  67, training loss : 0.6298, training accuracy : 94.99, test loss : 0.6720, test accuracy : 93.26\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.6224, accuracy : 95.30\n",
            "iteration : 100, loss : 0.6231, accuracy : 95.30\n",
            "iteration : 150, loss : 0.6222, accuracy : 95.40\n",
            "iteration : 200, loss : 0.6228, accuracy : 95.37\n",
            "iteration : 250, loss : 0.6238, accuracy : 95.34\n",
            "iteration : 300, loss : 0.6262, accuracy : 95.23\n",
            "iteration : 350, loss : 0.6266, accuracy : 95.21\n",
            "Epoch :  68, training loss : 0.6262, training accuracy : 95.22, test loss : 0.6744, test accuracy : 93.20\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.6294, accuracy : 95.14\n",
            "iteration : 100, loss : 0.6249, accuracy : 95.27\n",
            "iteration : 150, loss : 0.6262, accuracy : 95.26\n",
            "iteration : 200, loss : 0.6260, accuracy : 95.21\n",
            "iteration : 250, loss : 0.6274, accuracy : 95.17\n",
            "iteration : 300, loss : 0.6262, accuracy : 95.24\n",
            "iteration : 350, loss : 0.6259, accuracy : 95.23\n",
            "Epoch :  69, training loss : 0.6262, training accuracy : 95.23, test loss : 0.6687, test accuracy : 93.51\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.6208, accuracy : 95.34\n",
            "iteration : 100, loss : 0.6224, accuracy : 95.39\n",
            "iteration : 150, loss : 0.6208, accuracy : 95.44\n",
            "iteration : 200, loss : 0.6226, accuracy : 95.37\n",
            "iteration : 250, loss : 0.6233, accuracy : 95.32\n",
            "iteration : 300, loss : 0.6229, accuracy : 95.35\n",
            "iteration : 350, loss : 0.6227, accuracy : 95.37\n",
            "Epoch :  70, training loss : 0.6239, training accuracy : 95.34, test loss : 0.6803, test accuracy : 93.02\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.6149, accuracy : 95.44\n",
            "iteration : 100, loss : 0.6195, accuracy : 95.23\n",
            "iteration : 150, loss : 0.6200, accuracy : 95.23\n",
            "iteration : 200, loss : 0.6212, accuracy : 95.18\n",
            "iteration : 250, loss : 0.6241, accuracy : 95.14\n",
            "iteration : 300, loss : 0.6227, accuracy : 95.22\n",
            "iteration : 350, loss : 0.6230, accuracy : 95.22\n",
            "Epoch :  71, training loss : 0.6233, training accuracy : 95.19, test loss : 0.6652, test accuracy : 93.64\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.6173, accuracy : 95.66\n",
            "iteration : 100, loss : 0.6195, accuracy : 95.41\n",
            "iteration : 150, loss : 0.6207, accuracy : 95.44\n",
            "iteration : 200, loss : 0.6218, accuracy : 95.40\n",
            "iteration : 250, loss : 0.6206, accuracy : 95.41\n",
            "iteration : 300, loss : 0.6222, accuracy : 95.35\n",
            "iteration : 350, loss : 0.6235, accuracy : 95.29\n",
            "Epoch :  72, training loss : 0.6241, training accuracy : 95.27, test loss : 0.6668, test accuracy : 93.56\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.6008, accuracy : 96.36\n",
            "iteration : 100, loss : 0.6142, accuracy : 95.74\n",
            "iteration : 150, loss : 0.6187, accuracy : 95.51\n",
            "iteration : 200, loss : 0.6215, accuracy : 95.41\n",
            "iteration : 250, loss : 0.6234, accuracy : 95.39\n",
            "iteration : 300, loss : 0.6233, accuracy : 95.36\n",
            "iteration : 350, loss : 0.6225, accuracy : 95.38\n",
            "Epoch :  73, training loss : 0.6229, training accuracy : 95.35, test loss : 0.6865, test accuracy : 92.87\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.6173, accuracy : 95.58\n",
            "iteration : 100, loss : 0.6266, accuracy : 95.11\n",
            "iteration : 150, loss : 0.6264, accuracy : 95.08\n",
            "iteration : 200, loss : 0.6249, accuracy : 95.17\n",
            "iteration : 250, loss : 0.6229, accuracy : 95.26\n",
            "iteration : 300, loss : 0.6244, accuracy : 95.26\n",
            "iteration : 350, loss : 0.6240, accuracy : 95.28\n",
            "Epoch :  74, training loss : 0.6239, training accuracy : 95.29, test loss : 0.6909, test accuracy : 92.74\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.6141, accuracy : 95.80\n",
            "iteration : 100, loss : 0.6202, accuracy : 95.53\n",
            "iteration : 150, loss : 0.6212, accuracy : 95.47\n",
            "iteration : 200, loss : 0.6218, accuracy : 95.45\n",
            "iteration : 250, loss : 0.6225, accuracy : 95.40\n",
            "iteration : 300, loss : 0.6234, accuracy : 95.34\n",
            "iteration : 350, loss : 0.6234, accuracy : 95.32\n",
            "Epoch :  75, training loss : 0.6230, training accuracy : 95.33, test loss : 0.6734, test accuracy : 93.37\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.6128, accuracy : 95.53\n",
            "iteration : 100, loss : 0.6172, accuracy : 95.50\n",
            "iteration : 150, loss : 0.6203, accuracy : 95.40\n",
            "iteration : 200, loss : 0.6208, accuracy : 95.38\n",
            "iteration : 250, loss : 0.6197, accuracy : 95.43\n",
            "iteration : 300, loss : 0.6216, accuracy : 95.37\n",
            "iteration : 350, loss : 0.6247, accuracy : 95.22\n",
            "Epoch :  76, training loss : 0.6250, training accuracy : 95.23, test loss : 0.6735, test accuracy : 93.27\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.6063, accuracy : 96.05\n",
            "iteration : 100, loss : 0.6108, accuracy : 96.03\n",
            "iteration : 150, loss : 0.6163, accuracy : 95.74\n",
            "iteration : 200, loss : 0.6193, accuracy : 95.61\n",
            "iteration : 250, loss : 0.6207, accuracy : 95.47\n",
            "iteration : 300, loss : 0.6209, accuracy : 95.42\n",
            "iteration : 350, loss : 0.6215, accuracy : 95.40\n",
            "Epoch :  77, training loss : 0.6226, training accuracy : 95.37, test loss : 0.6678, test accuracy : 93.58\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.6185, accuracy : 95.47\n",
            "iteration : 100, loss : 0.6212, accuracy : 95.35\n",
            "iteration : 150, loss : 0.6190, accuracy : 95.48\n",
            "iteration : 200, loss : 0.6222, accuracy : 95.38\n",
            "iteration : 250, loss : 0.6207, accuracy : 95.42\n",
            "iteration : 300, loss : 0.6215, accuracy : 95.36\n",
            "iteration : 350, loss : 0.6228, accuracy : 95.33\n",
            "Epoch :  78, training loss : 0.6225, training accuracy : 95.35, test loss : 0.6684, test accuracy : 93.45\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.6161, accuracy : 95.72\n",
            "iteration : 100, loss : 0.6123, accuracy : 95.97\n",
            "iteration : 150, loss : 0.6129, accuracy : 95.94\n",
            "iteration : 200, loss : 0.6146, accuracy : 95.79\n",
            "iteration : 250, loss : 0.6153, accuracy : 95.73\n",
            "iteration : 300, loss : 0.6171, accuracy : 95.65\n",
            "iteration : 350, loss : 0.6172, accuracy : 95.63\n",
            "Epoch :  79, training loss : 0.6176, training accuracy : 95.60, test loss : 0.6752, test accuracy : 93.32\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.6070, accuracy : 96.08\n",
            "iteration : 100, loss : 0.6084, accuracy : 95.95\n",
            "iteration : 150, loss : 0.6121, accuracy : 95.77\n",
            "iteration : 200, loss : 0.6143, accuracy : 95.65\n",
            "iteration : 250, loss : 0.6171, accuracy : 95.50\n",
            "iteration : 300, loss : 0.6187, accuracy : 95.41\n",
            "iteration : 350, loss : 0.6196, accuracy : 95.40\n",
            "Epoch :  80, training loss : 0.6214, training accuracy : 95.34, test loss : 0.6652, test accuracy : 93.79\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.6185, accuracy : 95.34\n",
            "iteration : 100, loss : 0.6134, accuracy : 95.64\n",
            "iteration : 150, loss : 0.6160, accuracy : 95.58\n",
            "iteration : 200, loss : 0.6195, accuracy : 95.43\n",
            "iteration : 250, loss : 0.6192, accuracy : 95.42\n",
            "iteration : 300, loss : 0.6205, accuracy : 95.38\n",
            "iteration : 350, loss : 0.6202, accuracy : 95.39\n",
            "Epoch :  81, training loss : 0.6201, training accuracy : 95.38, test loss : 0.6936, test accuracy : 92.68\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.6137, accuracy : 95.70\n",
            "iteration : 100, loss : 0.6127, accuracy : 95.76\n",
            "iteration : 150, loss : 0.6130, accuracy : 95.76\n",
            "iteration : 200, loss : 0.6150, accuracy : 95.69\n",
            "iteration : 250, loss : 0.6166, accuracy : 95.64\n",
            "iteration : 300, loss : 0.6179, accuracy : 95.59\n",
            "iteration : 350, loss : 0.6190, accuracy : 95.53\n",
            "Epoch :  82, training loss : 0.6188, training accuracy : 95.53, test loss : 0.6723, test accuracy : 93.40\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.6135, accuracy : 95.69\n",
            "iteration : 100, loss : 0.6128, accuracy : 95.73\n",
            "iteration : 150, loss : 0.6139, accuracy : 95.58\n",
            "iteration : 200, loss : 0.6181, accuracy : 95.45\n",
            "iteration : 250, loss : 0.6189, accuracy : 95.45\n",
            "iteration : 300, loss : 0.6181, accuracy : 95.51\n",
            "iteration : 350, loss : 0.6189, accuracy : 95.49\n",
            "Epoch :  83, training loss : 0.6191, training accuracy : 95.46, test loss : 0.6798, test accuracy : 92.96\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.6112, accuracy : 95.89\n",
            "iteration : 100, loss : 0.6099, accuracy : 95.96\n",
            "iteration : 150, loss : 0.6145, accuracy : 95.74\n",
            "iteration : 200, loss : 0.6149, accuracy : 95.71\n",
            "iteration : 250, loss : 0.6153, accuracy : 95.65\n",
            "iteration : 300, loss : 0.6171, accuracy : 95.55\n",
            "iteration : 350, loss : 0.6181, accuracy : 95.52\n",
            "Epoch :  84, training loss : 0.6194, training accuracy : 95.46, test loss : 0.6832, test accuracy : 92.99\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.6270, accuracy : 95.20\n",
            "iteration : 100, loss : 0.6181, accuracy : 95.59\n",
            "iteration : 150, loss : 0.6147, accuracy : 95.69\n",
            "iteration : 200, loss : 0.6189, accuracy : 95.53\n",
            "iteration : 250, loss : 0.6194, accuracy : 95.49\n",
            "iteration : 300, loss : 0.6183, accuracy : 95.56\n",
            "iteration : 350, loss : 0.6183, accuracy : 95.54\n",
            "Epoch :  85, training loss : 0.6176, training accuracy : 95.56, test loss : 0.6655, test accuracy : 93.74\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.6202, accuracy : 95.62\n",
            "iteration : 100, loss : 0.6177, accuracy : 95.59\n",
            "iteration : 150, loss : 0.6157, accuracy : 95.68\n",
            "iteration : 200, loss : 0.6156, accuracy : 95.64\n",
            "iteration : 250, loss : 0.6180, accuracy : 95.52\n",
            "iteration : 300, loss : 0.6169, accuracy : 95.59\n",
            "iteration : 350, loss : 0.6172, accuracy : 95.56\n",
            "Epoch :  86, training loss : 0.6170, training accuracy : 95.56, test loss : 0.6775, test accuracy : 93.19\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.6105, accuracy : 95.70\n",
            "iteration : 100, loss : 0.6169, accuracy : 95.52\n",
            "iteration : 150, loss : 0.6180, accuracy : 95.52\n",
            "iteration : 200, loss : 0.6196, accuracy : 95.45\n",
            "iteration : 250, loss : 0.6191, accuracy : 95.49\n",
            "iteration : 300, loss : 0.6179, accuracy : 95.55\n",
            "iteration : 350, loss : 0.6192, accuracy : 95.49\n",
            "Epoch :  87, training loss : 0.6199, training accuracy : 95.49, test loss : 0.6731, test accuracy : 93.29\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.6079, accuracy : 95.84\n",
            "iteration : 100, loss : 0.6124, accuracy : 95.62\n",
            "iteration : 150, loss : 0.6126, accuracy : 95.61\n",
            "iteration : 200, loss : 0.6156, accuracy : 95.57\n",
            "iteration : 250, loss : 0.6164, accuracy : 95.52\n",
            "iteration : 300, loss : 0.6164, accuracy : 95.49\n",
            "iteration : 350, loss : 0.6169, accuracy : 95.48\n",
            "Epoch :  88, training loss : 0.6174, training accuracy : 95.47, test loss : 0.6815, test accuracy : 92.99\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.6114, accuracy : 95.67\n",
            "iteration : 100, loss : 0.6066, accuracy : 95.98\n",
            "iteration : 150, loss : 0.6086, accuracy : 95.92\n",
            "iteration : 200, loss : 0.6077, accuracy : 95.92\n",
            "iteration : 250, loss : 0.6120, accuracy : 95.77\n",
            "iteration : 300, loss : 0.6129, accuracy : 95.72\n",
            "iteration : 350, loss : 0.6159, accuracy : 95.62\n",
            "Epoch :  89, training loss : 0.6162, training accuracy : 95.61, test loss : 0.6656, test accuracy : 93.69\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.6033, accuracy : 95.98\n",
            "iteration : 100, loss : 0.6100, accuracy : 95.86\n",
            "iteration : 150, loss : 0.6122, accuracy : 95.78\n",
            "iteration : 200, loss : 0.6140, accuracy : 95.69\n",
            "iteration : 250, loss : 0.6149, accuracy : 95.66\n",
            "iteration : 300, loss : 0.6170, accuracy : 95.56\n",
            "iteration : 350, loss : 0.6169, accuracy : 95.55\n",
            "Epoch :  90, training loss : 0.6159, training accuracy : 95.59, test loss : 0.6694, test accuracy : 93.50\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.6058, accuracy : 95.86\n",
            "iteration : 100, loss : 0.6081, accuracy : 95.87\n",
            "iteration : 150, loss : 0.6099, accuracy : 95.79\n",
            "iteration : 200, loss : 0.6110, accuracy : 95.76\n",
            "iteration : 250, loss : 0.6128, accuracy : 95.75\n",
            "iteration : 300, loss : 0.6151, accuracy : 95.68\n",
            "iteration : 350, loss : 0.6182, accuracy : 95.56\n",
            "Epoch :  91, training loss : 0.6182, training accuracy : 95.56, test loss : 0.6677, test accuracy : 93.54\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.6070, accuracy : 96.00\n",
            "iteration : 100, loss : 0.6091, accuracy : 95.96\n",
            "iteration : 150, loss : 0.6098, accuracy : 95.91\n",
            "iteration : 200, loss : 0.6106, accuracy : 95.86\n",
            "iteration : 250, loss : 0.6101, accuracy : 95.93\n",
            "iteration : 300, loss : 0.6115, accuracy : 95.87\n",
            "iteration : 350, loss : 0.6129, accuracy : 95.81\n",
            "Epoch :  92, training loss : 0.6137, training accuracy : 95.79, test loss : 0.6732, test accuracy : 93.20\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.6015, accuracy : 96.12\n",
            "iteration : 100, loss : 0.6069, accuracy : 95.85\n",
            "iteration : 150, loss : 0.6079, accuracy : 95.89\n",
            "iteration : 200, loss : 0.6144, accuracy : 95.58\n",
            "iteration : 250, loss : 0.6148, accuracy : 95.65\n",
            "iteration : 300, loss : 0.6145, accuracy : 95.68\n",
            "iteration : 350, loss : 0.6150, accuracy : 95.65\n",
            "Epoch :  93, training loss : 0.6159, training accuracy : 95.60, test loss : 0.6670, test accuracy : 93.52\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.6145, accuracy : 95.72\n",
            "iteration : 100, loss : 0.6118, accuracy : 95.86\n",
            "iteration : 150, loss : 0.6121, accuracy : 95.79\n",
            "iteration : 200, loss : 0.6113, accuracy : 95.80\n",
            "iteration : 250, loss : 0.6119, accuracy : 95.75\n",
            "iteration : 300, loss : 0.6133, accuracy : 95.68\n",
            "iteration : 350, loss : 0.6141, accuracy : 95.66\n",
            "Epoch :  94, training loss : 0.6144, training accuracy : 95.66, test loss : 0.6700, test accuracy : 93.47\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.6112, accuracy : 95.86\n",
            "iteration : 100, loss : 0.6057, accuracy : 96.05\n",
            "iteration : 150, loss : 0.6058, accuracy : 96.05\n",
            "iteration : 200, loss : 0.6062, accuracy : 96.02\n",
            "iteration : 250, loss : 0.6078, accuracy : 95.91\n",
            "iteration : 300, loss : 0.6106, accuracy : 95.84\n",
            "iteration : 350, loss : 0.6134, accuracy : 95.75\n",
            "Epoch :  95, training loss : 0.6130, training accuracy : 95.76, test loss : 0.6634, test accuracy : 93.73\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.6001, accuracy : 96.14\n",
            "iteration : 100, loss : 0.6022, accuracy : 96.11\n",
            "iteration : 150, loss : 0.6078, accuracy : 95.84\n",
            "iteration : 200, loss : 0.6116, accuracy : 95.70\n",
            "iteration : 250, loss : 0.6117, accuracy : 95.71\n",
            "iteration : 300, loss : 0.6115, accuracy : 95.72\n",
            "iteration : 350, loss : 0.6114, accuracy : 95.73\n",
            "Epoch :  96, training loss : 0.6133, training accuracy : 95.66, test loss : 0.6662, test accuracy : 93.57\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.6071, accuracy : 96.08\n",
            "iteration : 100, loss : 0.6025, accuracy : 96.24\n",
            "iteration : 150, loss : 0.6056, accuracy : 95.99\n",
            "iteration : 200, loss : 0.6076, accuracy : 95.91\n",
            "iteration : 250, loss : 0.6112, accuracy : 95.76\n",
            "iteration : 300, loss : 0.6125, accuracy : 95.67\n",
            "iteration : 350, loss : 0.6116, accuracy : 95.71\n",
            "Epoch :  97, training loss : 0.6128, training accuracy : 95.67, test loss : 0.6655, test accuracy : 93.79\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.6118, accuracy : 95.81\n",
            "iteration : 100, loss : 0.6110, accuracy : 95.82\n",
            "iteration : 150, loss : 0.6110, accuracy : 95.82\n",
            "iteration : 200, loss : 0.6104, accuracy : 95.92\n",
            "iteration : 250, loss : 0.6103, accuracy : 95.90\n",
            "iteration : 300, loss : 0.6108, accuracy : 95.90\n",
            "iteration : 350, loss : 0.6124, accuracy : 95.81\n",
            "Epoch :  98, training loss : 0.6123, training accuracy : 95.80, test loss : 0.6832, test accuracy : 92.99\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.6099, accuracy : 96.14\n",
            "iteration : 100, loss : 0.6099, accuracy : 95.90\n",
            "iteration : 150, loss : 0.6125, accuracy : 95.78\n",
            "iteration : 200, loss : 0.6102, accuracy : 95.87\n",
            "iteration : 250, loss : 0.6108, accuracy : 95.80\n",
            "iteration : 300, loss : 0.6115, accuracy : 95.77\n",
            "iteration : 350, loss : 0.6125, accuracy : 95.70\n",
            "Epoch :  99, training loss : 0.6124, training accuracy : 95.71, test loss : 0.6702, test accuracy : 93.42\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.6097, accuracy : 95.97\n",
            "iteration : 100, loss : 0.6119, accuracy : 95.78\n",
            "iteration : 150, loss : 0.6093, accuracy : 95.92\n",
            "iteration : 200, loss : 0.6106, accuracy : 95.84\n",
            "iteration : 250, loss : 0.6103, accuracy : 95.78\n",
            "iteration : 300, loss : 0.6106, accuracy : 95.79\n",
            "iteration : 350, loss : 0.6113, accuracy : 95.74\n",
            "Epoch : 100, training loss : 0.6112, training accuracy : 95.74, test loss : 0.6736, test accuracy : 93.33\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.6041, accuracy : 95.98\n",
            "iteration : 100, loss : 0.6053, accuracy : 95.97\n",
            "iteration : 150, loss : 0.6085, accuracy : 95.91\n",
            "iteration : 200, loss : 0.6081, accuracy : 95.86\n",
            "iteration : 250, loss : 0.6087, accuracy : 95.82\n",
            "iteration : 300, loss : 0.6101, accuracy : 95.77\n",
            "iteration : 350, loss : 0.6108, accuracy : 95.75\n",
            "Epoch : 101, training loss : 0.6112, training accuracy : 95.73, test loss : 0.6666, test accuracy : 93.59\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.6018, accuracy : 96.28\n",
            "iteration : 100, loss : 0.6030, accuracy : 96.23\n",
            "iteration : 150, loss : 0.6063, accuracy : 96.04\n",
            "iteration : 200, loss : 0.6061, accuracy : 96.06\n",
            "iteration : 250, loss : 0.6093, accuracy : 95.96\n",
            "iteration : 300, loss : 0.6108, accuracy : 95.89\n",
            "iteration : 350, loss : 0.6104, accuracy : 95.88\n",
            "Epoch : 102, training loss : 0.6115, training accuracy : 95.82, test loss : 0.6779, test accuracy : 93.18\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.6095, accuracy : 95.86\n",
            "iteration : 100, loss : 0.6056, accuracy : 95.95\n",
            "iteration : 150, loss : 0.6038, accuracy : 96.09\n",
            "iteration : 200, loss : 0.6067, accuracy : 95.97\n",
            "iteration : 250, loss : 0.6068, accuracy : 95.91\n",
            "iteration : 300, loss : 0.6078, accuracy : 95.86\n",
            "iteration : 350, loss : 0.6076, accuracy : 95.85\n",
            "Epoch : 103, training loss : 0.6077, training accuracy : 95.84, test loss : 0.6667, test accuracy : 93.73\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.6176, accuracy : 95.50\n",
            "iteration : 100, loss : 0.6119, accuracy : 95.90\n",
            "iteration : 150, loss : 0.6076, accuracy : 95.98\n",
            "iteration : 200, loss : 0.6053, accuracy : 96.07\n",
            "iteration : 250, loss : 0.6062, accuracy : 96.02\n",
            "iteration : 300, loss : 0.6064, accuracy : 96.01\n",
            "iteration : 350, loss : 0.6074, accuracy : 95.95\n",
            "Epoch : 104, training loss : 0.6074, training accuracy : 95.93, test loss : 0.6642, test accuracy : 93.57\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.6087, accuracy : 96.06\n",
            "iteration : 100, loss : 0.6057, accuracy : 96.07\n",
            "iteration : 150, loss : 0.6071, accuracy : 95.96\n",
            "iteration : 200, loss : 0.6075, accuracy : 95.95\n",
            "iteration : 250, loss : 0.6065, accuracy : 95.98\n",
            "iteration : 300, loss : 0.6093, accuracy : 95.85\n",
            "iteration : 350, loss : 0.6106, accuracy : 95.80\n",
            "Epoch : 105, training loss : 0.6104, training accuracy : 95.81, test loss : 0.6616, test accuracy : 93.82\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.6013, accuracy : 96.27\n",
            "iteration : 100, loss : 0.5966, accuracy : 96.41\n",
            "iteration : 150, loss : 0.6000, accuracy : 96.27\n",
            "iteration : 200, loss : 0.6001, accuracy : 96.24\n",
            "iteration : 250, loss : 0.6021, accuracy : 96.19\n",
            "iteration : 300, loss : 0.6048, accuracy : 96.11\n",
            "iteration : 350, loss : 0.6059, accuracy : 96.07\n",
            "Epoch : 106, training loss : 0.6062, training accuracy : 96.04, test loss : 0.6762, test accuracy : 93.26\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.5999, accuracy : 96.27\n",
            "iteration : 100, loss : 0.6017, accuracy : 96.20\n",
            "iteration : 150, loss : 0.6030, accuracy : 96.12\n",
            "iteration : 200, loss : 0.6051, accuracy : 96.02\n",
            "iteration : 250, loss : 0.6060, accuracy : 95.99\n",
            "iteration : 300, loss : 0.6090, accuracy : 95.87\n",
            "iteration : 350, loss : 0.6109, accuracy : 95.78\n",
            "Epoch : 107, training loss : 0.6115, training accuracy : 95.77, test loss : 0.6660, test accuracy : 93.66\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.6044, accuracy : 96.25\n",
            "iteration : 100, loss : 0.6027, accuracy : 96.08\n",
            "iteration : 150, loss : 0.6056, accuracy : 96.02\n",
            "iteration : 200, loss : 0.6070, accuracy : 95.97\n",
            "iteration : 250, loss : 0.6084, accuracy : 95.90\n",
            "iteration : 300, loss : 0.6084, accuracy : 95.90\n",
            "iteration : 350, loss : 0.6086, accuracy : 95.90\n",
            "Epoch : 108, training loss : 0.6089, training accuracy : 95.88, test loss : 0.6657, test accuracy : 93.60\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.5899, accuracy : 96.69\n",
            "iteration : 100, loss : 0.5998, accuracy : 96.16\n",
            "iteration : 150, loss : 0.5988, accuracy : 96.28\n",
            "iteration : 200, loss : 0.6005, accuracy : 96.21\n",
            "iteration : 250, loss : 0.6032, accuracy : 96.12\n",
            "iteration : 300, loss : 0.6055, accuracy : 96.05\n",
            "iteration : 350, loss : 0.6064, accuracy : 96.04\n",
            "Epoch : 109, training loss : 0.6068, training accuracy : 96.04, test loss : 0.6723, test accuracy : 93.35\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.5944, accuracy : 96.30\n",
            "iteration : 100, loss : 0.6011, accuracy : 96.28\n",
            "iteration : 150, loss : 0.6032, accuracy : 96.14\n",
            "iteration : 200, loss : 0.6073, accuracy : 95.92\n",
            "iteration : 250, loss : 0.6080, accuracy : 95.87\n",
            "iteration : 300, loss : 0.6086, accuracy : 95.86\n",
            "iteration : 350, loss : 0.6087, accuracy : 95.86\n",
            "Epoch : 110, training loss : 0.6083, training accuracy : 95.87, test loss : 0.6682, test accuracy : 93.61\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.6035, accuracy : 96.31\n",
            "iteration : 100, loss : 0.5994, accuracy : 96.32\n",
            "iteration : 150, loss : 0.6021, accuracy : 96.23\n",
            "iteration : 200, loss : 0.6014, accuracy : 96.22\n",
            "iteration : 250, loss : 0.6042, accuracy : 96.09\n",
            "iteration : 300, loss : 0.6050, accuracy : 96.05\n",
            "iteration : 350, loss : 0.6070, accuracy : 95.97\n",
            "Epoch : 111, training loss : 0.6074, training accuracy : 95.95, test loss : 0.6656, test accuracy : 93.66\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.6041, accuracy : 96.14\n",
            "iteration : 100, loss : 0.6036, accuracy : 96.18\n",
            "iteration : 150, loss : 0.6021, accuracy : 96.26\n",
            "iteration : 200, loss : 0.6021, accuracy : 96.16\n",
            "iteration : 250, loss : 0.6049, accuracy : 96.03\n",
            "iteration : 300, loss : 0.6034, accuracy : 96.07\n",
            "iteration : 350, loss : 0.6053, accuracy : 96.01\n",
            "Epoch : 112, training loss : 0.6055, training accuracy : 96.01, test loss : 0.6598, test accuracy : 93.88\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.5985, accuracy : 96.14\n",
            "iteration : 100, loss : 0.6008, accuracy : 96.13\n",
            "iteration : 150, loss : 0.6011, accuracy : 96.11\n",
            "iteration : 200, loss : 0.6033, accuracy : 96.04\n",
            "iteration : 250, loss : 0.6034, accuracy : 96.03\n",
            "iteration : 300, loss : 0.6048, accuracy : 95.99\n",
            "iteration : 350, loss : 0.6049, accuracy : 96.04\n",
            "Epoch : 113, training loss : 0.6050, training accuracy : 96.04, test loss : 0.6680, test accuracy : 93.70\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.6035, accuracy : 96.20\n",
            "iteration : 100, loss : 0.6030, accuracy : 96.21\n",
            "iteration : 150, loss : 0.6038, accuracy : 96.12\n",
            "iteration : 200, loss : 0.6052, accuracy : 96.07\n",
            "iteration : 250, loss : 0.6052, accuracy : 96.04\n",
            "iteration : 300, loss : 0.6047, accuracy : 96.05\n",
            "iteration : 350, loss : 0.6050, accuracy : 96.06\n",
            "Epoch : 114, training loss : 0.6051, training accuracy : 96.07, test loss : 0.6724, test accuracy : 93.40\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.5912, accuracy : 96.69\n",
            "iteration : 100, loss : 0.5938, accuracy : 96.54\n",
            "iteration : 150, loss : 0.5944, accuracy : 96.55\n",
            "iteration : 200, loss : 0.5946, accuracy : 96.54\n",
            "iteration : 250, loss : 0.5974, accuracy : 96.41\n",
            "iteration : 300, loss : 0.5990, accuracy : 96.32\n",
            "iteration : 350, loss : 0.6017, accuracy : 96.23\n",
            "Epoch : 115, training loss : 0.6025, training accuracy : 96.21, test loss : 0.6754, test accuracy : 93.29\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.5918, accuracy : 96.58\n",
            "iteration : 100, loss : 0.5963, accuracy : 96.31\n",
            "iteration : 150, loss : 0.5974, accuracy : 96.28\n",
            "iteration : 200, loss : 0.6023, accuracy : 96.09\n",
            "iteration : 250, loss : 0.6031, accuracy : 96.05\n",
            "iteration : 300, loss : 0.6028, accuracy : 96.02\n",
            "iteration : 350, loss : 0.6044, accuracy : 95.95\n",
            "Epoch : 116, training loss : 0.6049, training accuracy : 95.93, test loss : 0.6813, test accuracy : 92.92\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.6080, accuracy : 95.86\n",
            "iteration : 100, loss : 0.6051, accuracy : 95.93\n",
            "iteration : 150, loss : 0.6048, accuracy : 95.91\n",
            "iteration : 200, loss : 0.6071, accuracy : 95.80\n",
            "iteration : 250, loss : 0.6085, accuracy : 95.82\n",
            "iteration : 300, loss : 0.6063, accuracy : 95.91\n",
            "iteration : 350, loss : 0.6057, accuracy : 95.93\n",
            "Epoch : 117, training loss : 0.6056, training accuracy : 95.92, test loss : 0.6676, test accuracy : 93.46\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.6008, accuracy : 96.22\n",
            "iteration : 100, loss : 0.5967, accuracy : 96.37\n",
            "iteration : 150, loss : 0.5996, accuracy : 96.30\n",
            "iteration : 200, loss : 0.6012, accuracy : 96.23\n",
            "iteration : 250, loss : 0.6012, accuracy : 96.20\n",
            "iteration : 300, loss : 0.6013, accuracy : 96.19\n",
            "iteration : 350, loss : 0.6029, accuracy : 96.11\n",
            "Epoch : 118, training loss : 0.6038, training accuracy : 96.08, test loss : 0.6617, test accuracy : 93.81\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.5987, accuracy : 96.42\n",
            "iteration : 100, loss : 0.5952, accuracy : 96.49\n",
            "iteration : 150, loss : 0.5973, accuracy : 96.43\n",
            "iteration : 200, loss : 0.5980, accuracy : 96.41\n",
            "iteration : 250, loss : 0.5990, accuracy : 96.37\n",
            "iteration : 300, loss : 0.6006, accuracy : 96.32\n",
            "iteration : 350, loss : 0.6011, accuracy : 96.32\n",
            "Epoch : 119, training loss : 0.6012, training accuracy : 96.31, test loss : 0.6587, test accuracy : 93.90\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.5966, accuracy : 96.16\n",
            "iteration : 100, loss : 0.6003, accuracy : 96.13\n",
            "iteration : 150, loss : 0.5995, accuracy : 96.20\n",
            "iteration : 200, loss : 0.5989, accuracy : 96.21\n",
            "iteration : 250, loss : 0.5977, accuracy : 96.27\n",
            "iteration : 300, loss : 0.5978, accuracy : 96.25\n",
            "iteration : 350, loss : 0.5990, accuracy : 96.24\n",
            "Epoch : 120, training loss : 0.5989, training accuracy : 96.26, test loss : 0.6610, test accuracy : 94.05\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.6075, accuracy : 95.88\n",
            "iteration : 100, loss : 0.6030, accuracy : 96.02\n",
            "iteration : 150, loss : 0.6004, accuracy : 96.16\n",
            "iteration : 200, loss : 0.6000, accuracy : 96.17\n",
            "iteration : 250, loss : 0.5996, accuracy : 96.17\n",
            "iteration : 300, loss : 0.5996, accuracy : 96.17\n",
            "iteration : 350, loss : 0.6025, accuracy : 96.09\n",
            "Epoch : 121, training loss : 0.6019, training accuracy : 96.12, test loss : 0.6667, test accuracy : 93.63\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.5935, accuracy : 96.59\n",
            "iteration : 100, loss : 0.5991, accuracy : 96.31\n",
            "iteration : 150, loss : 0.5992, accuracy : 96.23\n",
            "iteration : 200, loss : 0.5984, accuracy : 96.27\n",
            "iteration : 250, loss : 0.5971, accuracy : 96.32\n",
            "iteration : 300, loss : 0.5977, accuracy : 96.28\n",
            "iteration : 350, loss : 0.5989, accuracy : 96.23\n",
            "Epoch : 122, training loss : 0.5995, training accuracy : 96.22, test loss : 0.6634, test accuracy : 93.97\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.6060, accuracy : 95.80\n",
            "iteration : 100, loss : 0.5997, accuracy : 96.16\n",
            "iteration : 150, loss : 0.5992, accuracy : 96.23\n",
            "iteration : 200, loss : 0.6010, accuracy : 96.18\n",
            "iteration : 250, loss : 0.6019, accuracy : 96.12\n",
            "iteration : 300, loss : 0.6010, accuracy : 96.12\n",
            "iteration : 350, loss : 0.6018, accuracy : 96.12\n",
            "Epoch : 123, training loss : 0.6020, training accuracy : 96.11, test loss : 0.6669, test accuracy : 93.61\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.5937, accuracy : 96.50\n",
            "iteration : 100, loss : 0.5984, accuracy : 96.24\n",
            "iteration : 150, loss : 0.5987, accuracy : 96.20\n",
            "iteration : 200, loss : 0.5977, accuracy : 96.24\n",
            "iteration : 250, loss : 0.6002, accuracy : 96.17\n",
            "iteration : 300, loss : 0.6015, accuracy : 96.11\n",
            "iteration : 350, loss : 0.6018, accuracy : 96.09\n",
            "Epoch : 124, training loss : 0.6022, training accuracy : 96.08, test loss : 0.6757, test accuracy : 93.39\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.5988, accuracy : 96.31\n",
            "iteration : 100, loss : 0.5999, accuracy : 96.21\n",
            "iteration : 150, loss : 0.6005, accuracy : 96.15\n",
            "iteration : 200, loss : 0.6018, accuracy : 96.06\n",
            "iteration : 250, loss : 0.6019, accuracy : 96.06\n",
            "iteration : 300, loss : 0.6006, accuracy : 96.11\n",
            "iteration : 350, loss : 0.6007, accuracy : 96.11\n",
            "Epoch : 125, training loss : 0.6008, training accuracy : 96.10, test loss : 0.6733, test accuracy : 93.29\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.5964, accuracy : 96.36\n",
            "iteration : 100, loss : 0.5958, accuracy : 96.40\n",
            "iteration : 150, loss : 0.5957, accuracy : 96.42\n",
            "iteration : 200, loss : 0.5989, accuracy : 96.29\n",
            "iteration : 250, loss : 0.5985, accuracy : 96.32\n",
            "iteration : 300, loss : 0.5998, accuracy : 96.24\n",
            "iteration : 350, loss : 0.5998, accuracy : 96.23\n",
            "Epoch : 126, training loss : 0.5997, training accuracy : 96.22, test loss : 0.6698, test accuracy : 93.54\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.5767, accuracy : 97.23\n",
            "iteration : 100, loss : 0.5894, accuracy : 96.72\n",
            "iteration : 150, loss : 0.5943, accuracy : 96.48\n",
            "iteration : 200, loss : 0.5951, accuracy : 96.46\n",
            "iteration : 250, loss : 0.5950, accuracy : 96.45\n",
            "iteration : 300, loss : 0.5971, accuracy : 96.35\n",
            "iteration : 350, loss : 0.5987, accuracy : 96.29\n",
            "Epoch : 127, training loss : 0.5982, training accuracy : 96.32, test loss : 0.6628, test accuracy : 93.73\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.5863, accuracy : 96.83\n",
            "iteration : 100, loss : 0.5891, accuracy : 96.71\n",
            "iteration : 150, loss : 0.5957, accuracy : 96.48\n",
            "iteration : 200, loss : 0.5960, accuracy : 96.50\n",
            "iteration : 250, loss : 0.5975, accuracy : 96.41\n",
            "iteration : 300, loss : 0.5980, accuracy : 96.38\n",
            "iteration : 350, loss : 0.5984, accuracy : 96.33\n",
            "Epoch : 128, training loss : 0.5993, training accuracy : 96.30, test loss : 0.6647, test accuracy : 93.78\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.5884, accuracy : 96.77\n",
            "iteration : 100, loss : 0.5926, accuracy : 96.67\n",
            "iteration : 150, loss : 0.5916, accuracy : 96.64\n",
            "iteration : 200, loss : 0.5910, accuracy : 96.67\n",
            "iteration : 250, loss : 0.5929, accuracy : 96.57\n",
            "iteration : 300, loss : 0.5939, accuracy : 96.53\n",
            "iteration : 350, loss : 0.5946, accuracy : 96.48\n",
            "Epoch : 129, training loss : 0.5954, training accuracy : 96.44, test loss : 0.6643, test accuracy : 93.72\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.5948, accuracy : 96.53\n",
            "iteration : 100, loss : 0.5933, accuracy : 96.56\n",
            "iteration : 150, loss : 0.5922, accuracy : 96.56\n",
            "iteration : 200, loss : 0.5929, accuracy : 96.48\n",
            "iteration : 250, loss : 0.5944, accuracy : 96.43\n",
            "iteration : 300, loss : 0.5954, accuracy : 96.39\n",
            "iteration : 350, loss : 0.5960, accuracy : 96.37\n",
            "Epoch : 130, training loss : 0.5959, training accuracy : 96.37, test loss : 0.6649, test accuracy : 93.83\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.5876, accuracy : 96.81\n",
            "iteration : 100, loss : 0.5891, accuracy : 96.70\n",
            "iteration : 150, loss : 0.5912, accuracy : 96.64\n",
            "iteration : 200, loss : 0.5943, accuracy : 96.43\n",
            "iteration : 250, loss : 0.5967, accuracy : 96.35\n",
            "iteration : 300, loss : 0.5966, accuracy : 96.31\n",
            "iteration : 350, loss : 0.5968, accuracy : 96.32\n",
            "Epoch : 131, training loss : 0.5976, training accuracy : 96.29, test loss : 0.6649, test accuracy : 93.60\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.5958, accuracy : 96.45\n",
            "iteration : 100, loss : 0.5927, accuracy : 96.59\n",
            "iteration : 150, loss : 0.5923, accuracy : 96.57\n",
            "iteration : 200, loss : 0.5901, accuracy : 96.65\n",
            "iteration : 250, loss : 0.5924, accuracy : 96.57\n",
            "iteration : 300, loss : 0.5931, accuracy : 96.50\n",
            "iteration : 350, loss : 0.5950, accuracy : 96.44\n",
            "Epoch : 132, training loss : 0.5949, training accuracy : 96.44, test loss : 0.6629, test accuracy : 93.83\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.5782, accuracy : 97.11\n",
            "iteration : 100, loss : 0.5862, accuracy : 96.81\n",
            "iteration : 150, loss : 0.5871, accuracy : 96.78\n",
            "iteration : 200, loss : 0.5919, accuracy : 96.54\n",
            "iteration : 250, loss : 0.5930, accuracy : 96.53\n",
            "iteration : 300, loss : 0.5944, accuracy : 96.48\n",
            "iteration : 350, loss : 0.5949, accuracy : 96.44\n",
            "Epoch : 133, training loss : 0.5951, training accuracy : 96.43, test loss : 0.6631, test accuracy : 93.67\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.5902, accuracy : 96.61\n",
            "iteration : 100, loss : 0.5995, accuracy : 96.21\n",
            "iteration : 150, loss : 0.5955, accuracy : 96.33\n",
            "iteration : 200, loss : 0.5948, accuracy : 96.43\n",
            "iteration : 250, loss : 0.5939, accuracy : 96.45\n",
            "iteration : 300, loss : 0.5950, accuracy : 96.45\n",
            "iteration : 350, loss : 0.5949, accuracy : 96.44\n",
            "Epoch : 134, training loss : 0.5947, training accuracy : 96.45, test loss : 0.6628, test accuracy : 93.82\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.5924, accuracy : 96.52\n",
            "iteration : 100, loss : 0.5949, accuracy : 96.53\n",
            "iteration : 150, loss : 0.5941, accuracy : 96.48\n",
            "iteration : 200, loss : 0.5948, accuracy : 96.43\n",
            "iteration : 250, loss : 0.5949, accuracy : 96.45\n",
            "iteration : 300, loss : 0.5955, accuracy : 96.43\n",
            "iteration : 350, loss : 0.5941, accuracy : 96.45\n",
            "Epoch : 135, training loss : 0.5942, training accuracy : 96.43, test loss : 0.6645, test accuracy : 93.77\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.5814, accuracy : 97.11\n",
            "iteration : 100, loss : 0.5837, accuracy : 96.98\n",
            "iteration : 150, loss : 0.5859, accuracy : 96.84\n",
            "iteration : 200, loss : 0.5916, accuracy : 96.64\n",
            "iteration : 250, loss : 0.5929, accuracy : 96.59\n",
            "iteration : 300, loss : 0.5945, accuracy : 96.48\n",
            "iteration : 350, loss : 0.5959, accuracy : 96.43\n",
            "Epoch : 136, training loss : 0.5951, training accuracy : 96.45, test loss : 0.6679, test accuracy : 93.65\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.5858, accuracy : 96.84\n",
            "iteration : 100, loss : 0.5865, accuracy : 96.77\n",
            "iteration : 150, loss : 0.5911, accuracy : 96.62\n",
            "iteration : 200, loss : 0.5938, accuracy : 96.51\n",
            "iteration : 250, loss : 0.5939, accuracy : 96.51\n",
            "iteration : 300, loss : 0.5925, accuracy : 96.57\n",
            "iteration : 350, loss : 0.5935, accuracy : 96.53\n",
            "Epoch : 137, training loss : 0.5939, training accuracy : 96.51, test loss : 0.6626, test accuracy : 93.77\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.5937, accuracy : 96.64\n",
            "iteration : 100, loss : 0.5913, accuracy : 96.66\n",
            "iteration : 150, loss : 0.5885, accuracy : 96.69\n",
            "iteration : 200, loss : 0.5886, accuracy : 96.67\n",
            "iteration : 250, loss : 0.5891, accuracy : 96.62\n",
            "iteration : 300, loss : 0.5916, accuracy : 96.55\n",
            "iteration : 350, loss : 0.5932, accuracy : 96.50\n",
            "Epoch : 138, training loss : 0.5932, training accuracy : 96.49, test loss : 0.6674, test accuracy : 93.58\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.5845, accuracy : 96.95\n",
            "iteration : 100, loss : 0.5896, accuracy : 96.75\n",
            "iteration : 150, loss : 0.5908, accuracy : 96.68\n",
            "iteration : 200, loss : 0.5932, accuracy : 96.53\n",
            "iteration : 250, loss : 0.5943, accuracy : 96.47\n",
            "iteration : 300, loss : 0.5934, accuracy : 96.47\n",
            "iteration : 350, loss : 0.5929, accuracy : 96.51\n",
            "Epoch : 139, training loss : 0.5930, training accuracy : 96.51, test loss : 0.6621, test accuracy : 93.76\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.5875, accuracy : 96.69\n",
            "iteration : 100, loss : 0.5869, accuracy : 96.79\n",
            "iteration : 150, loss : 0.5865, accuracy : 96.70\n",
            "iteration : 200, loss : 0.5873, accuracy : 96.66\n",
            "iteration : 250, loss : 0.5883, accuracy : 96.63\n",
            "iteration : 300, loss : 0.5905, accuracy : 96.54\n",
            "iteration : 350, loss : 0.5899, accuracy : 96.59\n",
            "Epoch : 140, training loss : 0.5905, training accuracy : 96.57, test loss : 0.6640, test accuracy : 93.84\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.5766, accuracy : 97.19\n",
            "iteration : 100, loss : 0.5830, accuracy : 96.88\n",
            "iteration : 150, loss : 0.5829, accuracy : 96.89\n",
            "iteration : 200, loss : 0.5849, accuracy : 96.83\n",
            "iteration : 250, loss : 0.5871, accuracy : 96.77\n",
            "iteration : 300, loss : 0.5883, accuracy : 96.75\n",
            "iteration : 350, loss : 0.5902, accuracy : 96.67\n",
            "Epoch : 141, training loss : 0.5900, training accuracy : 96.67, test loss : 0.6682, test accuracy : 93.54\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.5905, accuracy : 96.48\n",
            "iteration : 100, loss : 0.5886, accuracy : 96.59\n",
            "iteration : 150, loss : 0.5886, accuracy : 96.59\n",
            "iteration : 200, loss : 0.5883, accuracy : 96.60\n",
            "iteration : 250, loss : 0.5875, accuracy : 96.59\n",
            "iteration : 300, loss : 0.5897, accuracy : 96.56\n",
            "iteration : 350, loss : 0.5908, accuracy : 96.52\n",
            "Epoch : 142, training loss : 0.5914, training accuracy : 96.50, test loss : 0.6654, test accuracy : 93.63\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.5863, accuracy : 96.81\n",
            "iteration : 100, loss : 0.5867, accuracy : 96.80\n",
            "iteration : 150, loss : 0.5854, accuracy : 96.86\n",
            "iteration : 200, loss : 0.5885, accuracy : 96.79\n",
            "iteration : 250, loss : 0.5909, accuracy : 96.69\n",
            "iteration : 300, loss : 0.5897, accuracy : 96.71\n",
            "iteration : 350, loss : 0.5906, accuracy : 96.65\n",
            "Epoch : 143, training loss : 0.5904, training accuracy : 96.65, test loss : 0.6590, test accuracy : 94.00\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.5849, accuracy : 96.69\n",
            "iteration : 100, loss : 0.5821, accuracy : 96.76\n",
            "iteration : 150, loss : 0.5854, accuracy : 96.69\n",
            "iteration : 200, loss : 0.5877, accuracy : 96.65\n",
            "iteration : 250, loss : 0.5893, accuracy : 96.59\n",
            "iteration : 300, loss : 0.5904, accuracy : 96.54\n",
            "iteration : 350, loss : 0.5905, accuracy : 96.54\n",
            "Epoch : 144, training loss : 0.5914, training accuracy : 96.53, test loss : 0.6721, test accuracy : 93.42\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.5870, accuracy : 96.70\n",
            "iteration : 100, loss : 0.5843, accuracy : 96.89\n",
            "iteration : 150, loss : 0.5843, accuracy : 96.81\n",
            "iteration : 200, loss : 0.5858, accuracy : 96.75\n",
            "iteration : 250, loss : 0.5874, accuracy : 96.63\n",
            "iteration : 300, loss : 0.5880, accuracy : 96.62\n",
            "iteration : 350, loss : 0.5880, accuracy : 96.63\n",
            "Epoch : 145, training loss : 0.5877, training accuracy : 96.64, test loss : 0.6688, test accuracy : 93.60\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.5870, accuracy : 96.78\n",
            "iteration : 100, loss : 0.5846, accuracy : 96.90\n",
            "iteration : 150, loss : 0.5843, accuracy : 96.89\n",
            "iteration : 200, loss : 0.5860, accuracy : 96.82\n",
            "iteration : 250, loss : 0.5856, accuracy : 96.85\n",
            "iteration : 300, loss : 0.5876, accuracy : 96.76\n",
            "iteration : 350, loss : 0.5888, accuracy : 96.72\n",
            "Epoch : 146, training loss : 0.5885, training accuracy : 96.73, test loss : 0.6617, test accuracy : 93.94\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.5747, accuracy : 97.22\n",
            "iteration : 100, loss : 0.5769, accuracy : 97.11\n",
            "iteration : 150, loss : 0.5787, accuracy : 97.06\n",
            "iteration : 200, loss : 0.5791, accuracy : 97.05\n",
            "iteration : 250, loss : 0.5808, accuracy : 96.98\n",
            "iteration : 300, loss : 0.5832, accuracy : 96.90\n",
            "iteration : 350, loss : 0.5834, accuracy : 96.88\n",
            "Epoch : 147, training loss : 0.5832, training accuracy : 96.89, test loss : 0.6549, test accuracy : 94.07\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.5857, accuracy : 96.97\n",
            "iteration : 100, loss : 0.5805, accuracy : 97.06\n",
            "iteration : 150, loss : 0.5791, accuracy : 97.12\n",
            "iteration : 200, loss : 0.5803, accuracy : 97.04\n",
            "iteration : 250, loss : 0.5809, accuracy : 97.03\n",
            "iteration : 300, loss : 0.5820, accuracy : 96.97\n",
            "iteration : 350, loss : 0.5830, accuracy : 96.90\n",
            "Epoch : 148, training loss : 0.5832, training accuracy : 96.90, test loss : 0.6697, test accuracy : 93.59\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.5795, accuracy : 96.98\n",
            "iteration : 100, loss : 0.5806, accuracy : 97.01\n",
            "iteration : 150, loss : 0.5808, accuracy : 96.95\n",
            "iteration : 200, loss : 0.5822, accuracy : 96.93\n",
            "iteration : 250, loss : 0.5853, accuracy : 96.78\n",
            "iteration : 300, loss : 0.5868, accuracy : 96.72\n",
            "iteration : 350, loss : 0.5874, accuracy : 96.69\n",
            "Epoch : 149, training loss : 0.5873, training accuracy : 96.70, test loss : 0.6606, test accuracy : 93.94\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.5825, accuracy : 97.08\n",
            "iteration : 100, loss : 0.5843, accuracy : 96.90\n",
            "iteration : 150, loss : 0.5842, accuracy : 96.92\n",
            "iteration : 200, loss : 0.5844, accuracy : 96.88\n",
            "iteration : 250, loss : 0.5849, accuracy : 96.84\n",
            "iteration : 300, loss : 0.5852, accuracy : 96.82\n",
            "iteration : 350, loss : 0.5867, accuracy : 96.75\n",
            "Epoch : 150, training loss : 0.5873, training accuracy : 96.72, test loss : 0.6626, test accuracy : 93.79\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.5857, accuracy : 96.66\n",
            "iteration : 100, loss : 0.5826, accuracy : 96.83\n",
            "iteration : 150, loss : 0.5832, accuracy : 96.87\n",
            "iteration : 200, loss : 0.5843, accuracy : 96.81\n",
            "iteration : 250, loss : 0.5847, accuracy : 96.78\n",
            "iteration : 300, loss : 0.5850, accuracy : 96.77\n",
            "iteration : 350, loss : 0.5863, accuracy : 96.73\n",
            "Epoch : 151, training loss : 0.5864, training accuracy : 96.72, test loss : 0.6622, test accuracy : 93.93\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.5852, accuracy : 96.95\n",
            "iteration : 100, loss : 0.5815, accuracy : 97.02\n",
            "iteration : 150, loss : 0.5833, accuracy : 96.90\n",
            "iteration : 200, loss : 0.5840, accuracy : 96.82\n",
            "iteration : 250, loss : 0.5846, accuracy : 96.79\n",
            "iteration : 300, loss : 0.5855, accuracy : 96.75\n",
            "iteration : 350, loss : 0.5865, accuracy : 96.69\n",
            "Epoch : 152, training loss : 0.5864, training accuracy : 96.70, test loss : 0.6665, test accuracy : 93.71\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.5793, accuracy : 96.92\n",
            "iteration : 100, loss : 0.5809, accuracy : 96.88\n",
            "iteration : 150, loss : 0.5816, accuracy : 96.94\n",
            "iteration : 200, loss : 0.5846, accuracy : 96.84\n",
            "iteration : 250, loss : 0.5855, accuracy : 96.81\n",
            "iteration : 300, loss : 0.5871, accuracy : 96.74\n",
            "iteration : 350, loss : 0.5862, accuracy : 96.77\n",
            "Epoch : 153, training loss : 0.5864, training accuracy : 96.73, test loss : 0.6572, test accuracy : 94.06\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.5726, accuracy : 97.20\n",
            "iteration : 100, loss : 0.5708, accuracy : 97.34\n",
            "iteration : 150, loss : 0.5767, accuracy : 97.10\n",
            "iteration : 200, loss : 0.5777, accuracy : 97.07\n",
            "iteration : 250, loss : 0.5771, accuracy : 97.11\n",
            "iteration : 300, loss : 0.5796, accuracy : 97.01\n",
            "iteration : 350, loss : 0.5794, accuracy : 97.05\n",
            "Epoch : 154, training loss : 0.5802, training accuracy : 97.02, test loss : 0.6641, test accuracy : 93.84\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.5815, accuracy : 96.91\n",
            "iteration : 100, loss : 0.5803, accuracy : 97.01\n",
            "iteration : 150, loss : 0.5813, accuracy : 96.97\n",
            "iteration : 200, loss : 0.5815, accuracy : 96.98\n",
            "iteration : 250, loss : 0.5833, accuracy : 96.91\n",
            "iteration : 300, loss : 0.5826, accuracy : 96.93\n",
            "iteration : 350, loss : 0.5841, accuracy : 96.87\n",
            "Epoch : 155, training loss : 0.5841, training accuracy : 96.88, test loss : 0.6566, test accuracy : 94.11\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.5811, accuracy : 97.16\n",
            "iteration : 100, loss : 0.5801, accuracy : 97.18\n",
            "iteration : 150, loss : 0.5794, accuracy : 97.12\n",
            "iteration : 200, loss : 0.5787, accuracy : 97.13\n",
            "iteration : 250, loss : 0.5795, accuracy : 97.10\n",
            "iteration : 300, loss : 0.5801, accuracy : 97.08\n",
            "iteration : 350, loss : 0.5802, accuracy : 97.04\n",
            "Epoch : 156, training loss : 0.5808, training accuracy : 97.00, test loss : 0.6742, test accuracy : 93.62\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.5887, accuracy : 96.94\n",
            "iteration : 100, loss : 0.5833, accuracy : 96.98\n",
            "iteration : 150, loss : 0.5833, accuracy : 96.90\n",
            "iteration : 200, loss : 0.5836, accuracy : 96.91\n",
            "iteration : 250, loss : 0.5849, accuracy : 96.84\n",
            "iteration : 300, loss : 0.5840, accuracy : 96.85\n",
            "iteration : 350, loss : 0.5837, accuracy : 96.84\n",
            "Epoch : 157, training loss : 0.5834, training accuracy : 96.86, test loss : 0.6623, test accuracy : 93.94\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.5832, accuracy : 96.91\n",
            "iteration : 100, loss : 0.5769, accuracy : 97.12\n",
            "iteration : 150, loss : 0.5816, accuracy : 96.97\n",
            "iteration : 200, loss : 0.5817, accuracy : 96.95\n",
            "iteration : 250, loss : 0.5820, accuracy : 96.94\n",
            "iteration : 300, loss : 0.5811, accuracy : 96.99\n",
            "iteration : 350, loss : 0.5810, accuracy : 96.99\n",
            "Epoch : 158, training loss : 0.5809, training accuracy : 96.98, test loss : 0.6631, test accuracy : 93.82\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.5826, accuracy : 97.05\n",
            "iteration : 100, loss : 0.5818, accuracy : 97.03\n",
            "iteration : 150, loss : 0.5794, accuracy : 97.15\n",
            "iteration : 200, loss : 0.5821, accuracy : 96.98\n",
            "iteration : 250, loss : 0.5814, accuracy : 96.98\n",
            "iteration : 300, loss : 0.5823, accuracy : 96.94\n",
            "iteration : 350, loss : 0.5828, accuracy : 96.92\n",
            "Epoch : 159, training loss : 0.5821, training accuracy : 96.95, test loss : 0.6639, test accuracy : 93.77\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.5769, accuracy : 97.17\n",
            "iteration : 100, loss : 0.5760, accuracy : 97.13\n",
            "iteration : 150, loss : 0.5779, accuracy : 97.09\n",
            "iteration : 200, loss : 0.5784, accuracy : 97.04\n",
            "iteration : 250, loss : 0.5781, accuracy : 97.05\n",
            "iteration : 300, loss : 0.5801, accuracy : 96.99\n",
            "iteration : 350, loss : 0.5794, accuracy : 97.01\n",
            "Epoch : 160, training loss : 0.5787, training accuracy : 97.03, test loss : 0.6577, test accuracy : 94.15\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.5747, accuracy : 97.30\n",
            "iteration : 100, loss : 0.5744, accuracy : 97.31\n",
            "iteration : 150, loss : 0.5756, accuracy : 97.17\n",
            "iteration : 200, loss : 0.5772, accuracy : 97.10\n",
            "iteration : 250, loss : 0.5783, accuracy : 97.09\n",
            "iteration : 300, loss : 0.5795, accuracy : 97.07\n",
            "iteration : 350, loss : 0.5780, accuracy : 97.11\n",
            "Epoch : 161, training loss : 0.5783, training accuracy : 97.09, test loss : 0.6701, test accuracy : 93.64\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.5755, accuracy : 97.17\n",
            "iteration : 100, loss : 0.5774, accuracy : 97.12\n",
            "iteration : 150, loss : 0.5798, accuracy : 97.06\n",
            "iteration : 200, loss : 0.5815, accuracy : 97.03\n",
            "iteration : 250, loss : 0.5818, accuracy : 97.00\n",
            "iteration : 300, loss : 0.5829, accuracy : 96.96\n",
            "iteration : 350, loss : 0.5820, accuracy : 96.99\n",
            "Epoch : 162, training loss : 0.5818, training accuracy : 97.00, test loss : 0.6576, test accuracy : 94.20\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.5754, accuracy : 97.23\n",
            "iteration : 100, loss : 0.5704, accuracy : 97.39\n",
            "iteration : 150, loss : 0.5704, accuracy : 97.38\n",
            "iteration : 200, loss : 0.5711, accuracy : 97.36\n",
            "iteration : 250, loss : 0.5742, accuracy : 97.22\n",
            "iteration : 300, loss : 0.5762, accuracy : 97.14\n",
            "iteration : 350, loss : 0.5763, accuracy : 97.15\n",
            "Epoch : 163, training loss : 0.5772, training accuracy : 97.11, test loss : 0.6678, test accuracy : 93.68\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.5713, accuracy : 97.31\n",
            "iteration : 100, loss : 0.5726, accuracy : 97.28\n",
            "iteration : 150, loss : 0.5750, accuracy : 97.18\n",
            "iteration : 200, loss : 0.5746, accuracy : 97.26\n",
            "iteration : 250, loss : 0.5750, accuracy : 97.24\n",
            "iteration : 300, loss : 0.5754, accuracy : 97.22\n",
            "iteration : 350, loss : 0.5771, accuracy : 97.18\n",
            "Epoch : 164, training loss : 0.5770, training accuracy : 97.17, test loss : 0.6571, test accuracy : 94.17\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.5707, accuracy : 97.27\n",
            "iteration : 100, loss : 0.5707, accuracy : 97.32\n",
            "iteration : 150, loss : 0.5726, accuracy : 97.27\n",
            "iteration : 200, loss : 0.5735, accuracy : 97.25\n",
            "iteration : 250, loss : 0.5757, accuracy : 97.15\n",
            "iteration : 300, loss : 0.5762, accuracy : 97.15\n",
            "iteration : 350, loss : 0.5771, accuracy : 97.13\n",
            "Epoch : 165, training loss : 0.5774, training accuracy : 97.13, test loss : 0.6624, test accuracy : 93.89\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.5658, accuracy : 97.44\n",
            "iteration : 100, loss : 0.5759, accuracy : 97.18\n",
            "iteration : 150, loss : 0.5801, accuracy : 97.02\n",
            "iteration : 200, loss : 0.5785, accuracy : 97.08\n",
            "iteration : 250, loss : 0.5783, accuracy : 97.09\n",
            "iteration : 300, loss : 0.5782, accuracy : 97.07\n",
            "iteration : 350, loss : 0.5782, accuracy : 97.04\n",
            "Epoch : 166, training loss : 0.5784, training accuracy : 97.04, test loss : 0.6628, test accuracy : 94.04\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.5791, accuracy : 97.20\n",
            "iteration : 100, loss : 0.5799, accuracy : 97.09\n",
            "iteration : 150, loss : 0.5771, accuracy : 97.14\n",
            "iteration : 200, loss : 0.5775, accuracy : 97.14\n",
            "iteration : 250, loss : 0.5768, accuracy : 97.16\n",
            "iteration : 300, loss : 0.5774, accuracy : 97.12\n",
            "iteration : 350, loss : 0.5764, accuracy : 97.16\n",
            "Epoch : 167, training loss : 0.5762, training accuracy : 97.15, test loss : 0.6552, test accuracy : 94.19\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.5739, accuracy : 97.55\n",
            "iteration : 100, loss : 0.5716, accuracy : 97.48\n",
            "iteration : 150, loss : 0.5743, accuracy : 97.38\n",
            "iteration : 200, loss : 0.5741, accuracy : 97.35\n",
            "iteration : 250, loss : 0.5747, accuracy : 97.27\n",
            "iteration : 300, loss : 0.5757, accuracy : 97.23\n",
            "iteration : 350, loss : 0.5756, accuracy : 97.25\n",
            "Epoch : 168, training loss : 0.5761, training accuracy : 97.22, test loss : 0.6607, test accuracy : 93.91\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.5699, accuracy : 97.44\n",
            "iteration : 100, loss : 0.5751, accuracy : 97.23\n",
            "iteration : 150, loss : 0.5751, accuracy : 97.23\n",
            "iteration : 200, loss : 0.5770, accuracy : 97.20\n",
            "iteration : 250, loss : 0.5768, accuracy : 97.20\n",
            "iteration : 300, loss : 0.5761, accuracy : 97.22\n",
            "iteration : 350, loss : 0.5764, accuracy : 97.21\n",
            "Epoch : 169, training loss : 0.5764, training accuracy : 97.21, test loss : 0.6590, test accuracy : 94.03\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.5637, accuracy : 97.64\n",
            "iteration : 100, loss : 0.5718, accuracy : 97.40\n",
            "iteration : 150, loss : 0.5725, accuracy : 97.33\n",
            "iteration : 200, loss : 0.5725, accuracy : 97.34\n",
            "iteration : 250, loss : 0.5721, accuracy : 97.37\n",
            "iteration : 300, loss : 0.5728, accuracy : 97.33\n",
            "iteration : 350, loss : 0.5722, accuracy : 97.35\n",
            "Epoch : 170, training loss : 0.5727, training accuracy : 97.31, test loss : 0.6560, test accuracy : 94.28\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.5683, accuracy : 97.45\n",
            "iteration : 100, loss : 0.5696, accuracy : 97.38\n",
            "iteration : 150, loss : 0.5693, accuracy : 97.41\n",
            "iteration : 200, loss : 0.5692, accuracy : 97.46\n",
            "iteration : 250, loss : 0.5705, accuracy : 97.41\n",
            "iteration : 300, loss : 0.5707, accuracy : 97.42\n",
            "iteration : 350, loss : 0.5713, accuracy : 97.38\n",
            "Epoch : 171, training loss : 0.5717, training accuracy : 97.37, test loss : 0.6606, test accuracy : 94.15\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.5627, accuracy : 97.72\n",
            "iteration : 100, loss : 0.5691, accuracy : 97.45\n",
            "iteration : 150, loss : 0.5708, accuracy : 97.35\n",
            "iteration : 200, loss : 0.5709, accuracy : 97.34\n",
            "iteration : 250, loss : 0.5713, accuracy : 97.34\n",
            "iteration : 300, loss : 0.5728, accuracy : 97.31\n",
            "iteration : 350, loss : 0.5726, accuracy : 97.31\n",
            "Epoch : 172, training loss : 0.5726, training accuracy : 97.31, test loss : 0.6562, test accuracy : 94.21\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.5731, accuracy : 97.33\n",
            "iteration : 100, loss : 0.5700, accuracy : 97.47\n",
            "iteration : 150, loss : 0.5728, accuracy : 97.35\n",
            "iteration : 200, loss : 0.5728, accuracy : 97.33\n",
            "iteration : 250, loss : 0.5750, accuracy : 97.22\n",
            "iteration : 300, loss : 0.5751, accuracy : 97.22\n",
            "iteration : 350, loss : 0.5750, accuracy : 97.24\n",
            "Epoch : 173, training loss : 0.5743, training accuracy : 97.27, test loss : 0.6606, test accuracy : 94.06\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.5695, accuracy : 97.50\n",
            "iteration : 100, loss : 0.5689, accuracy : 97.48\n",
            "iteration : 150, loss : 0.5667, accuracy : 97.51\n",
            "iteration : 200, loss : 0.5696, accuracy : 97.41\n",
            "iteration : 250, loss : 0.5705, accuracy : 97.34\n",
            "iteration : 300, loss : 0.5719, accuracy : 97.32\n",
            "iteration : 350, loss : 0.5727, accuracy : 97.29\n",
            "Epoch : 174, training loss : 0.5725, training accuracy : 97.30, test loss : 0.6520, test accuracy : 94.31\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.5614, accuracy : 97.72\n",
            "iteration : 100, loss : 0.5697, accuracy : 97.29\n",
            "iteration : 150, loss : 0.5694, accuracy : 97.35\n",
            "iteration : 200, loss : 0.5695, accuracy : 97.35\n",
            "iteration : 250, loss : 0.5695, accuracy : 97.33\n",
            "iteration : 300, loss : 0.5705, accuracy : 97.30\n",
            "iteration : 350, loss : 0.5717, accuracy : 97.26\n",
            "Epoch : 175, training loss : 0.5713, training accuracy : 97.26, test loss : 0.6570, test accuracy : 94.05\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.5690, accuracy : 97.38\n",
            "iteration : 100, loss : 0.5701, accuracy : 97.41\n",
            "iteration : 150, loss : 0.5699, accuracy : 97.41\n",
            "iteration : 200, loss : 0.5714, accuracy : 97.38\n",
            "iteration : 250, loss : 0.5704, accuracy : 97.41\n",
            "iteration : 300, loss : 0.5700, accuracy : 97.41\n",
            "iteration : 350, loss : 0.5704, accuracy : 97.38\n",
            "Epoch : 176, training loss : 0.5706, training accuracy : 97.36, test loss : 0.6595, test accuracy : 94.05\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.5768, accuracy : 97.17\n",
            "iteration : 100, loss : 0.5700, accuracy : 97.36\n",
            "iteration : 150, loss : 0.5678, accuracy : 97.46\n",
            "iteration : 200, loss : 0.5704, accuracy : 97.39\n",
            "iteration : 250, loss : 0.5714, accuracy : 97.37\n",
            "iteration : 300, loss : 0.5715, accuracy : 97.35\n",
            "iteration : 350, loss : 0.5731, accuracy : 97.27\n",
            "Epoch : 177, training loss : 0.5725, training accuracy : 97.29, test loss : 0.6613, test accuracy : 93.95\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.5691, accuracy : 97.44\n",
            "iteration : 100, loss : 0.5721, accuracy : 97.33\n",
            "iteration : 150, loss : 0.5733, accuracy : 97.24\n",
            "iteration : 200, loss : 0.5732, accuracy : 97.23\n",
            "iteration : 250, loss : 0.5722, accuracy : 97.28\n",
            "iteration : 300, loss : 0.5728, accuracy : 97.23\n",
            "iteration : 350, loss : 0.5725, accuracy : 97.25\n",
            "Epoch : 178, training loss : 0.5725, training accuracy : 97.24, test loss : 0.6548, test accuracy : 94.25\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.5660, accuracy : 97.67\n",
            "iteration : 100, loss : 0.5652, accuracy : 97.66\n",
            "iteration : 150, loss : 0.5682, accuracy : 97.56\n",
            "iteration : 200, loss : 0.5685, accuracy : 97.50\n",
            "iteration : 250, loss : 0.5694, accuracy : 97.47\n",
            "iteration : 300, loss : 0.5691, accuracy : 97.46\n",
            "iteration : 350, loss : 0.5695, accuracy : 97.44\n",
            "Epoch : 179, training loss : 0.5702, training accuracy : 97.42, test loss : 0.6517, test accuracy : 94.40\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.5636, accuracy : 97.56\n",
            "iteration : 100, loss : 0.5639, accuracy : 97.55\n",
            "iteration : 150, loss : 0.5660, accuracy : 97.49\n",
            "iteration : 200, loss : 0.5668, accuracy : 97.48\n",
            "iteration : 250, loss : 0.5675, accuracy : 97.46\n",
            "iteration : 300, loss : 0.5686, accuracy : 97.43\n",
            "iteration : 350, loss : 0.5686, accuracy : 97.42\n",
            "Epoch : 180, training loss : 0.5691, training accuracy : 97.40, test loss : 0.6657, test accuracy : 93.89\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.5708, accuracy : 97.42\n",
            "iteration : 100, loss : 0.5652, accuracy : 97.55\n",
            "iteration : 150, loss : 0.5660, accuracy : 97.55\n",
            "iteration : 200, loss : 0.5687, accuracy : 97.49\n",
            "iteration : 250, loss : 0.5698, accuracy : 97.45\n",
            "iteration : 300, loss : 0.5687, accuracy : 97.48\n",
            "iteration : 350, loss : 0.5692, accuracy : 97.46\n",
            "Epoch : 181, training loss : 0.5696, training accuracy : 97.43, test loss : 0.6587, test accuracy : 94.13\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.5622, accuracy : 97.73\n",
            "iteration : 100, loss : 0.5636, accuracy : 97.58\n",
            "iteration : 150, loss : 0.5637, accuracy : 97.58\n",
            "iteration : 200, loss : 0.5650, accuracy : 97.55\n",
            "iteration : 250, loss : 0.5659, accuracy : 97.51\n",
            "iteration : 300, loss : 0.5676, accuracy : 97.47\n",
            "iteration : 350, loss : 0.5675, accuracy : 97.47\n",
            "Epoch : 182, training loss : 0.5677, training accuracy : 97.46, test loss : 0.6597, test accuracy : 94.01\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.5646, accuracy : 97.55\n",
            "iteration : 100, loss : 0.5662, accuracy : 97.53\n",
            "iteration : 150, loss : 0.5632, accuracy : 97.68\n",
            "iteration : 200, loss : 0.5640, accuracy : 97.67\n",
            "iteration : 250, loss : 0.5653, accuracy : 97.60\n",
            "iteration : 300, loss : 0.5654, accuracy : 97.58\n",
            "iteration : 350, loss : 0.5676, accuracy : 97.54\n",
            "Epoch : 183, training loss : 0.5678, training accuracy : 97.53, test loss : 0.6546, test accuracy : 94.30\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.5614, accuracy : 97.81\n",
            "iteration : 100, loss : 0.5631, accuracy : 97.77\n",
            "iteration : 150, loss : 0.5643, accuracy : 97.71\n",
            "iteration : 200, loss : 0.5643, accuracy : 97.70\n",
            "iteration : 250, loss : 0.5650, accuracy : 97.64\n",
            "iteration : 300, loss : 0.5650, accuracy : 97.62\n",
            "iteration : 350, loss : 0.5665, accuracy : 97.56\n",
            "Epoch : 184, training loss : 0.5670, training accuracy : 97.54, test loss : 0.6557, test accuracy : 94.30\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.5643, accuracy : 97.83\n",
            "iteration : 100, loss : 0.5649, accuracy : 97.73\n",
            "iteration : 150, loss : 0.5652, accuracy : 97.67\n",
            "iteration : 200, loss : 0.5652, accuracy : 97.64\n",
            "iteration : 250, loss : 0.5646, accuracy : 97.67\n",
            "iteration : 300, loss : 0.5649, accuracy : 97.63\n",
            "iteration : 350, loss : 0.5654, accuracy : 97.59\n",
            "Epoch : 185, training loss : 0.5657, training accuracy : 97.58, test loss : 0.6604, test accuracy : 93.98\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.5638, accuracy : 97.73\n",
            "iteration : 100, loss : 0.5677, accuracy : 97.48\n",
            "iteration : 150, loss : 0.5684, accuracy : 97.42\n",
            "iteration : 200, loss : 0.5680, accuracy : 97.48\n",
            "iteration : 250, loss : 0.5673, accuracy : 97.52\n",
            "iteration : 300, loss : 0.5677, accuracy : 97.50\n",
            "iteration : 350, loss : 0.5679, accuracy : 97.49\n",
            "Epoch : 186, training loss : 0.5682, training accuracy : 97.49, test loss : 0.6510, test accuracy : 94.41\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.5541, accuracy : 97.95\n",
            "iteration : 100, loss : 0.5628, accuracy : 97.56\n",
            "iteration : 150, loss : 0.5626, accuracy : 97.62\n",
            "iteration : 200, loss : 0.5615, accuracy : 97.68\n",
            "iteration : 250, loss : 0.5613, accuracy : 97.72\n",
            "iteration : 300, loss : 0.5628, accuracy : 97.66\n",
            "iteration : 350, loss : 0.5631, accuracy : 97.64\n",
            "Epoch : 187, training loss : 0.5630, training accuracy : 97.64, test loss : 0.6636, test accuracy : 93.98\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.5580, accuracy : 97.84\n",
            "iteration : 100, loss : 0.5592, accuracy : 97.81\n",
            "iteration : 150, loss : 0.5614, accuracy : 97.71\n",
            "iteration : 200, loss : 0.5615, accuracy : 97.72\n",
            "iteration : 250, loss : 0.5630, accuracy : 97.64\n",
            "iteration : 300, loss : 0.5643, accuracy : 97.61\n",
            "iteration : 350, loss : 0.5642, accuracy : 97.60\n",
            "Epoch : 188, training loss : 0.5647, training accuracy : 97.57, test loss : 0.6550, test accuracy : 94.33\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.5662, accuracy : 97.61\n",
            "iteration : 100, loss : 0.5639, accuracy : 97.66\n",
            "iteration : 150, loss : 0.5625, accuracy : 97.67\n",
            "iteration : 200, loss : 0.5634, accuracy : 97.66\n",
            "iteration : 250, loss : 0.5634, accuracy : 97.68\n",
            "iteration : 300, loss : 0.5642, accuracy : 97.63\n",
            "iteration : 350, loss : 0.5635, accuracy : 97.67\n",
            "Epoch : 189, training loss : 0.5634, training accuracy : 97.67, test loss : 0.6591, test accuracy : 94.23\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.5525, accuracy : 97.97\n",
            "iteration : 100, loss : 0.5613, accuracy : 97.70\n",
            "iteration : 150, loss : 0.5599, accuracy : 97.75\n",
            "iteration : 200, loss : 0.5610, accuracy : 97.69\n",
            "iteration : 250, loss : 0.5609, accuracy : 97.71\n",
            "iteration : 300, loss : 0.5635, accuracy : 97.62\n",
            "iteration : 350, loss : 0.5628, accuracy : 97.68\n",
            "Epoch : 190, training loss : 0.5630, training accuracy : 97.66, test loss : 0.6537, test accuracy : 94.23\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.5702, accuracy : 97.39\n",
            "iteration : 100, loss : 0.5650, accuracy : 97.60\n",
            "iteration : 150, loss : 0.5668, accuracy : 97.55\n",
            "iteration : 200, loss : 0.5652, accuracy : 97.62\n",
            "iteration : 250, loss : 0.5649, accuracy : 97.62\n",
            "iteration : 300, loss : 0.5642, accuracy : 97.64\n",
            "iteration : 350, loss : 0.5644, accuracy : 97.65\n",
            "Epoch : 191, training loss : 0.5648, training accuracy : 97.64, test loss : 0.6583, test accuracy : 94.16\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.5658, accuracy : 97.50\n",
            "iteration : 100, loss : 0.5641, accuracy : 97.59\n",
            "iteration : 150, loss : 0.5642, accuracy : 97.62\n",
            "iteration : 200, loss : 0.5626, accuracy : 97.70\n",
            "iteration : 250, loss : 0.5623, accuracy : 97.71\n",
            "iteration : 300, loss : 0.5620, accuracy : 97.71\n",
            "iteration : 350, loss : 0.5621, accuracy : 97.69\n",
            "Epoch : 192, training loss : 0.5630, training accuracy : 97.66, test loss : 0.6596, test accuracy : 94.02\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.5593, accuracy : 97.92\n",
            "iteration : 100, loss : 0.5583, accuracy : 97.92\n",
            "iteration : 150, loss : 0.5606, accuracy : 97.82\n",
            "iteration : 200, loss : 0.5615, accuracy : 97.75\n",
            "iteration : 250, loss : 0.5614, accuracy : 97.78\n",
            "iteration : 300, loss : 0.5621, accuracy : 97.77\n",
            "iteration : 350, loss : 0.5621, accuracy : 97.75\n",
            "Epoch : 193, training loss : 0.5619, training accuracy : 97.76, test loss : 0.6547, test accuracy : 94.38\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.5605, accuracy : 97.81\n",
            "iteration : 100, loss : 0.5586, accuracy : 97.91\n",
            "iteration : 150, loss : 0.5573, accuracy : 97.96\n",
            "iteration : 200, loss : 0.5611, accuracy : 97.81\n",
            "iteration : 250, loss : 0.5610, accuracy : 97.77\n",
            "iteration : 300, loss : 0.5603, accuracy : 97.78\n",
            "iteration : 350, loss : 0.5612, accuracy : 97.73\n",
            "Epoch : 194, training loss : 0.5618, training accuracy : 97.70, test loss : 0.6581, test accuracy : 94.23\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.5552, accuracy : 98.06\n",
            "iteration : 100, loss : 0.5595, accuracy : 97.91\n",
            "iteration : 150, loss : 0.5577, accuracy : 97.92\n",
            "iteration : 200, loss : 0.5590, accuracy : 97.84\n",
            "iteration : 250, loss : 0.5593, accuracy : 97.83\n",
            "iteration : 300, loss : 0.5600, accuracy : 97.82\n",
            "iteration : 350, loss : 0.5611, accuracy : 97.78\n",
            "Epoch : 195, training loss : 0.5610, training accuracy : 97.77, test loss : 0.6543, test accuracy : 94.32\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.5593, accuracy : 97.78\n",
            "iteration : 100, loss : 0.5592, accuracy : 97.77\n",
            "iteration : 150, loss : 0.5585, accuracy : 97.85\n",
            "iteration : 200, loss : 0.5594, accuracy : 97.78\n",
            "iteration : 250, loss : 0.5610, accuracy : 97.74\n",
            "iteration : 300, loss : 0.5619, accuracy : 97.72\n",
            "iteration : 350, loss : 0.5607, accuracy : 97.75\n",
            "Epoch : 196, training loss : 0.5611, training accuracy : 97.76, test loss : 0.6529, test accuracy : 94.44\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.5605, accuracy : 97.77\n",
            "iteration : 100, loss : 0.5590, accuracy : 97.91\n",
            "iteration : 150, loss : 0.5586, accuracy : 97.91\n",
            "iteration : 200, loss : 0.5596, accuracy : 97.84\n",
            "iteration : 250, loss : 0.5606, accuracy : 97.86\n",
            "iteration : 300, loss : 0.5608, accuracy : 97.84\n",
            "iteration : 350, loss : 0.5605, accuracy : 97.82\n",
            "Epoch : 197, training loss : 0.5606, training accuracy : 97.80, test loss : 0.6575, test accuracy : 94.17\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.5573, accuracy : 97.88\n",
            "iteration : 100, loss : 0.5564, accuracy : 97.93\n",
            "iteration : 150, loss : 0.5589, accuracy : 97.82\n",
            "iteration : 200, loss : 0.5565, accuracy : 97.92\n",
            "iteration : 250, loss : 0.5572, accuracy : 97.90\n",
            "iteration : 300, loss : 0.5569, accuracy : 97.90\n",
            "iteration : 350, loss : 0.5580, accuracy : 97.84\n",
            "Epoch : 198, training loss : 0.5581, training accuracy : 97.83, test loss : 0.6565, test accuracy : 94.36\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.5582, accuracy : 97.89\n",
            "iteration : 100, loss : 0.5543, accuracy : 98.07\n",
            "iteration : 150, loss : 0.5559, accuracy : 97.97\n",
            "iteration : 200, loss : 0.5560, accuracy : 97.95\n",
            "iteration : 250, loss : 0.5570, accuracy : 97.92\n",
            "iteration : 300, loss : 0.5585, accuracy : 97.86\n",
            "iteration : 350, loss : 0.5579, accuracy : 97.86\n",
            "Epoch : 199, training loss : 0.5588, training accuracy : 97.84, test loss : 0.6575, test accuracy : 94.31\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.5574, accuracy : 98.03\n",
            "iteration : 100, loss : 0.5554, accuracy : 98.01\n",
            "iteration : 150, loss : 0.5551, accuracy : 98.01\n",
            "iteration : 200, loss : 0.5568, accuracy : 97.93\n",
            "iteration : 250, loss : 0.5560, accuracy : 97.99\n",
            "iteration : 300, loss : 0.5559, accuracy : 97.96\n",
            "iteration : 350, loss : 0.5561, accuracy : 97.95\n",
            "Epoch : 200, training loss : 0.5569, training accuracy : 97.91, test loss : 0.6608, test accuracy : 94.12\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.5558, accuracy : 97.98\n",
            "iteration : 100, loss : 0.5564, accuracy : 97.91\n",
            "iteration : 150, loss : 0.5567, accuracy : 97.93\n",
            "iteration : 200, loss : 0.5555, accuracy : 97.98\n",
            "iteration : 250, loss : 0.5555, accuracy : 97.99\n",
            "iteration : 300, loss : 0.5562, accuracy : 97.98\n",
            "iteration : 350, loss : 0.5563, accuracy : 97.98\n",
            "Epoch : 201, training loss : 0.5561, training accuracy : 97.99, test loss : 0.6592, test accuracy : 94.06\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.5585, accuracy : 97.95\n",
            "iteration : 100, loss : 0.5593, accuracy : 97.91\n",
            "iteration : 150, loss : 0.5581, accuracy : 97.96\n",
            "iteration : 200, loss : 0.5562, accuracy : 98.02\n",
            "iteration : 250, loss : 0.5549, accuracy : 98.07\n",
            "iteration : 300, loss : 0.5557, accuracy : 98.00\n",
            "iteration : 350, loss : 0.5569, accuracy : 97.95\n",
            "Epoch : 202, training loss : 0.5571, training accuracy : 97.94, test loss : 0.6530, test accuracy : 94.25\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.5449, accuracy : 98.39\n",
            "iteration : 100, loss : 0.5469, accuracy : 98.34\n",
            "iteration : 150, loss : 0.5521, accuracy : 98.15\n",
            "iteration : 200, loss : 0.5542, accuracy : 98.05\n",
            "iteration : 250, loss : 0.5547, accuracy : 98.06\n",
            "iteration : 300, loss : 0.5558, accuracy : 98.03\n",
            "iteration : 350, loss : 0.5555, accuracy : 98.03\n",
            "Epoch : 203, training loss : 0.5552, training accuracy : 98.03, test loss : 0.6519, test accuracy : 94.46\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.5523, accuracy : 98.03\n",
            "iteration : 100, loss : 0.5552, accuracy : 98.00\n",
            "iteration : 150, loss : 0.5550, accuracy : 97.99\n",
            "iteration : 200, loss : 0.5541, accuracy : 98.02\n",
            "iteration : 250, loss : 0.5546, accuracy : 97.98\n",
            "iteration : 300, loss : 0.5549, accuracy : 97.96\n",
            "iteration : 350, loss : 0.5551, accuracy : 97.98\n",
            "Epoch : 204, training loss : 0.5554, training accuracy : 97.97, test loss : 0.6554, test accuracy : 94.20\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.5478, accuracy : 98.39\n",
            "iteration : 100, loss : 0.5512, accuracy : 98.24\n",
            "iteration : 150, loss : 0.5556, accuracy : 98.06\n",
            "iteration : 200, loss : 0.5546, accuracy : 98.10\n",
            "iteration : 250, loss : 0.5540, accuracy : 98.12\n",
            "iteration : 300, loss : 0.5539, accuracy : 98.09\n",
            "iteration : 350, loss : 0.5547, accuracy : 98.06\n",
            "Epoch : 205, training loss : 0.5550, training accuracy : 98.04, test loss : 0.6567, test accuracy : 94.34\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.5509, accuracy : 98.23\n",
            "iteration : 100, loss : 0.5521, accuracy : 98.14\n",
            "iteration : 150, loss : 0.5504, accuracy : 98.16\n",
            "iteration : 200, loss : 0.5521, accuracy : 98.09\n",
            "iteration : 250, loss : 0.5521, accuracy : 98.12\n",
            "iteration : 300, loss : 0.5528, accuracy : 98.08\n",
            "iteration : 350, loss : 0.5545, accuracy : 98.00\n",
            "Epoch : 206, training loss : 0.5552, training accuracy : 97.98, test loss : 0.6526, test accuracy : 94.38\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.5523, accuracy : 98.16\n",
            "iteration : 100, loss : 0.5516, accuracy : 98.18\n",
            "iteration : 150, loss : 0.5519, accuracy : 98.15\n",
            "iteration : 200, loss : 0.5521, accuracy : 98.12\n",
            "iteration : 250, loss : 0.5532, accuracy : 98.10\n",
            "iteration : 300, loss : 0.5530, accuracy : 98.10\n",
            "iteration : 350, loss : 0.5534, accuracy : 98.10\n",
            "Epoch : 207, training loss : 0.5533, training accuracy : 98.10, test loss : 0.6520, test accuracy : 94.42\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.5524, accuracy : 98.25\n",
            "iteration : 100, loss : 0.5583, accuracy : 97.86\n",
            "iteration : 150, loss : 0.5563, accuracy : 97.95\n",
            "iteration : 200, loss : 0.5564, accuracy : 97.90\n",
            "iteration : 250, loss : 0.5559, accuracy : 97.90\n",
            "iteration : 300, loss : 0.5555, accuracy : 97.93\n",
            "iteration : 350, loss : 0.5552, accuracy : 97.94\n",
            "Epoch : 208, training loss : 0.5553, training accuracy : 97.95, test loss : 0.6568, test accuracy : 94.21\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.5453, accuracy : 98.34\n",
            "iteration : 100, loss : 0.5474, accuracy : 98.23\n",
            "iteration : 150, loss : 0.5487, accuracy : 98.22\n",
            "iteration : 200, loss : 0.5502, accuracy : 98.17\n",
            "iteration : 250, loss : 0.5501, accuracy : 98.18\n",
            "iteration : 300, loss : 0.5515, accuracy : 98.14\n",
            "iteration : 350, loss : 0.5519, accuracy : 98.12\n",
            "Epoch : 209, training loss : 0.5519, training accuracy : 98.12, test loss : 0.6545, test accuracy : 94.40\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.5509, accuracy : 98.17\n",
            "iteration : 100, loss : 0.5515, accuracy : 98.21\n",
            "iteration : 150, loss : 0.5510, accuracy : 98.21\n",
            "iteration : 200, loss : 0.5514, accuracy : 98.17\n",
            "iteration : 250, loss : 0.5509, accuracy : 98.19\n",
            "iteration : 300, loss : 0.5513, accuracy : 98.16\n",
            "iteration : 350, loss : 0.5527, accuracy : 98.10\n",
            "Epoch : 210, training loss : 0.5530, training accuracy : 98.08, test loss : 0.6501, test accuracy : 94.53\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.5469, accuracy : 98.33\n",
            "iteration : 100, loss : 0.5528, accuracy : 98.10\n",
            "iteration : 150, loss : 0.5521, accuracy : 98.11\n",
            "iteration : 200, loss : 0.5512, accuracy : 98.13\n",
            "iteration : 250, loss : 0.5515, accuracy : 98.10\n",
            "iteration : 300, loss : 0.5517, accuracy : 98.14\n",
            "iteration : 350, loss : 0.5511, accuracy : 98.16\n",
            "Epoch : 211, training loss : 0.5511, training accuracy : 98.15, test loss : 0.6540, test accuracy : 94.47\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.5435, accuracy : 98.45\n",
            "iteration : 100, loss : 0.5487, accuracy : 98.30\n",
            "iteration : 150, loss : 0.5494, accuracy : 98.23\n",
            "iteration : 200, loss : 0.5503, accuracy : 98.20\n",
            "iteration : 250, loss : 0.5513, accuracy : 98.19\n",
            "iteration : 300, loss : 0.5510, accuracy : 98.20\n",
            "iteration : 350, loss : 0.5517, accuracy : 98.13\n",
            "Epoch : 212, training loss : 0.5514, training accuracy : 98.13, test loss : 0.6523, test accuracy : 94.46\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.5474, accuracy : 98.19\n",
            "iteration : 100, loss : 0.5478, accuracy : 98.32\n",
            "iteration : 150, loss : 0.5465, accuracy : 98.37\n",
            "iteration : 200, loss : 0.5484, accuracy : 98.28\n",
            "iteration : 250, loss : 0.5496, accuracy : 98.23\n",
            "iteration : 300, loss : 0.5504, accuracy : 98.17\n",
            "iteration : 350, loss : 0.5503, accuracy : 98.19\n",
            "Epoch : 213, training loss : 0.5513, training accuracy : 98.16, test loss : 0.6532, test accuracy : 94.38\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.5477, accuracy : 98.31\n",
            "iteration : 100, loss : 0.5471, accuracy : 98.30\n",
            "iteration : 150, loss : 0.5474, accuracy : 98.33\n",
            "iteration : 200, loss : 0.5485, accuracy : 98.25\n",
            "iteration : 250, loss : 0.5493, accuracy : 98.21\n",
            "iteration : 300, loss : 0.5504, accuracy : 98.20\n",
            "iteration : 350, loss : 0.5503, accuracy : 98.20\n",
            "Epoch : 214, training loss : 0.5507, training accuracy : 98.17, test loss : 0.6494, test accuracy : 94.49\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.5477, accuracy : 98.28\n",
            "iteration : 100, loss : 0.5483, accuracy : 98.29\n",
            "iteration : 150, loss : 0.5480, accuracy : 98.30\n",
            "iteration : 200, loss : 0.5475, accuracy : 98.32\n",
            "iteration : 250, loss : 0.5471, accuracy : 98.32\n",
            "iteration : 300, loss : 0.5485, accuracy : 98.28\n",
            "iteration : 350, loss : 0.5492, accuracy : 98.25\n",
            "Epoch : 215, training loss : 0.5496, training accuracy : 98.23, test loss : 0.6548, test accuracy : 94.35\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.5459, accuracy : 98.39\n",
            "iteration : 100, loss : 0.5486, accuracy : 98.30\n",
            "iteration : 150, loss : 0.5488, accuracy : 98.34\n",
            "iteration : 200, loss : 0.5492, accuracy : 98.32\n",
            "iteration : 250, loss : 0.5486, accuracy : 98.34\n",
            "iteration : 300, loss : 0.5486, accuracy : 98.35\n",
            "iteration : 350, loss : 0.5490, accuracy : 98.33\n",
            "Epoch : 216, training loss : 0.5489, training accuracy : 98.33, test loss : 0.6533, test accuracy : 94.48\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.5492, accuracy : 98.28\n",
            "iteration : 100, loss : 0.5462, accuracy : 98.41\n",
            "iteration : 150, loss : 0.5450, accuracy : 98.45\n",
            "iteration : 200, loss : 0.5447, accuracy : 98.45\n",
            "iteration : 250, loss : 0.5450, accuracy : 98.41\n",
            "iteration : 300, loss : 0.5462, accuracy : 98.38\n",
            "iteration : 350, loss : 0.5477, accuracy : 98.33\n",
            "Epoch : 217, training loss : 0.5486, training accuracy : 98.30, test loss : 0.6530, test accuracy : 94.46\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.5427, accuracy : 98.53\n",
            "iteration : 100, loss : 0.5428, accuracy : 98.55\n",
            "iteration : 150, loss : 0.5455, accuracy : 98.46\n",
            "iteration : 200, loss : 0.5480, accuracy : 98.36\n",
            "iteration : 250, loss : 0.5468, accuracy : 98.41\n",
            "iteration : 300, loss : 0.5469, accuracy : 98.41\n",
            "iteration : 350, loss : 0.5471, accuracy : 98.37\n",
            "Epoch : 218, training loss : 0.5470, training accuracy : 98.37, test loss : 0.6538, test accuracy : 94.38\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.5470, accuracy : 98.45\n",
            "iteration : 100, loss : 0.5476, accuracy : 98.45\n",
            "iteration : 150, loss : 0.5473, accuracy : 98.35\n",
            "iteration : 200, loss : 0.5471, accuracy : 98.33\n",
            "iteration : 250, loss : 0.5466, accuracy : 98.34\n",
            "iteration : 300, loss : 0.5470, accuracy : 98.33\n",
            "iteration : 350, loss : 0.5470, accuracy : 98.33\n",
            "Epoch : 219, training loss : 0.5471, training accuracy : 98.33, test loss : 0.6493, test accuracy : 94.66\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.5503, accuracy : 98.23\n",
            "iteration : 100, loss : 0.5484, accuracy : 98.27\n",
            "iteration : 150, loss : 0.5479, accuracy : 98.32\n",
            "iteration : 200, loss : 0.5465, accuracy : 98.38\n",
            "iteration : 250, loss : 0.5468, accuracy : 98.38\n",
            "iteration : 300, loss : 0.5472, accuracy : 98.35\n",
            "iteration : 350, loss : 0.5467, accuracy : 98.37\n",
            "Epoch : 220, training loss : 0.5468, training accuracy : 98.36, test loss : 0.6592, test accuracy : 94.17\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.5436, accuracy : 98.50\n",
            "iteration : 100, loss : 0.5426, accuracy : 98.49\n",
            "iteration : 150, loss : 0.5422, accuracy : 98.47\n",
            "iteration : 200, loss : 0.5420, accuracy : 98.48\n",
            "iteration : 250, loss : 0.5428, accuracy : 98.46\n",
            "iteration : 300, loss : 0.5441, accuracy : 98.43\n",
            "iteration : 350, loss : 0.5444, accuracy : 98.44\n",
            "Epoch : 221, training loss : 0.5450, training accuracy : 98.42, test loss : 0.6523, test accuracy : 94.50\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.5459, accuracy : 98.41\n",
            "iteration : 100, loss : 0.5451, accuracy : 98.41\n",
            "iteration : 150, loss : 0.5458, accuracy : 98.36\n",
            "iteration : 200, loss : 0.5462, accuracy : 98.35\n",
            "iteration : 250, loss : 0.5453, accuracy : 98.41\n",
            "iteration : 300, loss : 0.5457, accuracy : 98.36\n",
            "iteration : 350, loss : 0.5456, accuracy : 98.38\n",
            "Epoch : 222, training loss : 0.5455, training accuracy : 98.37, test loss : 0.6526, test accuracy : 94.43\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.5450, accuracy : 98.38\n",
            "iteration : 100, loss : 0.5421, accuracy : 98.49\n",
            "iteration : 150, loss : 0.5437, accuracy : 98.43\n",
            "iteration : 200, loss : 0.5452, accuracy : 98.38\n",
            "iteration : 250, loss : 0.5445, accuracy : 98.41\n",
            "iteration : 300, loss : 0.5450, accuracy : 98.39\n",
            "iteration : 350, loss : 0.5447, accuracy : 98.39\n",
            "Epoch : 223, training loss : 0.5443, training accuracy : 98.39, test loss : 0.6559, test accuracy : 94.46\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.5498, accuracy : 98.25\n",
            "iteration : 100, loss : 0.5485, accuracy : 98.26\n",
            "iteration : 150, loss : 0.5481, accuracy : 98.29\n",
            "iteration : 200, loss : 0.5465, accuracy : 98.39\n",
            "iteration : 250, loss : 0.5455, accuracy : 98.42\n",
            "iteration : 300, loss : 0.5464, accuracy : 98.38\n",
            "iteration : 350, loss : 0.5460, accuracy : 98.39\n",
            "Epoch : 224, training loss : 0.5461, training accuracy : 98.38, test loss : 0.6575, test accuracy : 94.31\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.5472, accuracy : 98.36\n",
            "iteration : 100, loss : 0.5443, accuracy : 98.45\n",
            "iteration : 150, loss : 0.5446, accuracy : 98.47\n",
            "iteration : 200, loss : 0.5436, accuracy : 98.50\n",
            "iteration : 250, loss : 0.5437, accuracy : 98.48\n",
            "iteration : 300, loss : 0.5432, accuracy : 98.47\n",
            "iteration : 350, loss : 0.5434, accuracy : 98.45\n",
            "Epoch : 225, training loss : 0.5439, training accuracy : 98.43, test loss : 0.6546, test accuracy : 94.50\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.5429, accuracy : 98.66\n",
            "iteration : 100, loss : 0.5408, accuracy : 98.61\n",
            "iteration : 150, loss : 0.5410, accuracy : 98.58\n",
            "iteration : 200, loss : 0.5406, accuracy : 98.59\n",
            "iteration : 250, loss : 0.5415, accuracy : 98.58\n",
            "iteration : 300, loss : 0.5427, accuracy : 98.53\n",
            "iteration : 350, loss : 0.5424, accuracy : 98.54\n",
            "Epoch : 226, training loss : 0.5420, training accuracy : 98.54, test loss : 0.6506, test accuracy : 94.60\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.5471, accuracy : 98.33\n",
            "iteration : 100, loss : 0.5433, accuracy : 98.50\n",
            "iteration : 150, loss : 0.5419, accuracy : 98.55\n",
            "iteration : 200, loss : 0.5422, accuracy : 98.55\n",
            "iteration : 250, loss : 0.5421, accuracy : 98.54\n",
            "iteration : 300, loss : 0.5420, accuracy : 98.54\n",
            "iteration : 350, loss : 0.5427, accuracy : 98.52\n",
            "Epoch : 227, training loss : 0.5427, training accuracy : 98.53, test loss : 0.6554, test accuracy : 94.43\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.5373, accuracy : 98.75\n",
            "iteration : 100, loss : 0.5397, accuracy : 98.63\n",
            "iteration : 150, loss : 0.5395, accuracy : 98.63\n",
            "iteration : 200, loss : 0.5409, accuracy : 98.61\n",
            "iteration : 250, loss : 0.5419, accuracy : 98.54\n",
            "iteration : 300, loss : 0.5425, accuracy : 98.51\n",
            "iteration : 350, loss : 0.5420, accuracy : 98.52\n",
            "Epoch : 228, training loss : 0.5426, training accuracy : 98.49, test loss : 0.6532, test accuracy : 94.58\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.5371, accuracy : 98.81\n",
            "iteration : 100, loss : 0.5371, accuracy : 98.75\n",
            "iteration : 150, loss : 0.5409, accuracy : 98.64\n",
            "iteration : 200, loss : 0.5412, accuracy : 98.59\n",
            "iteration : 250, loss : 0.5408, accuracy : 98.59\n",
            "iteration : 300, loss : 0.5418, accuracy : 98.56\n",
            "iteration : 350, loss : 0.5421, accuracy : 98.54\n",
            "Epoch : 229, training loss : 0.5417, training accuracy : 98.54, test loss : 0.6538, test accuracy : 94.50\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.5436, accuracy : 98.48\n",
            "iteration : 100, loss : 0.5434, accuracy : 98.48\n",
            "iteration : 150, loss : 0.5409, accuracy : 98.56\n",
            "iteration : 200, loss : 0.5413, accuracy : 98.57\n",
            "iteration : 250, loss : 0.5414, accuracy : 98.57\n",
            "iteration : 300, loss : 0.5416, accuracy : 98.57\n",
            "iteration : 350, loss : 0.5409, accuracy : 98.59\n",
            "Epoch : 230, training loss : 0.5410, training accuracy : 98.58, test loss : 0.6514, test accuracy : 94.56\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.5399, accuracy : 98.52\n",
            "iteration : 100, loss : 0.5421, accuracy : 98.45\n",
            "iteration : 150, loss : 0.5411, accuracy : 98.51\n",
            "iteration : 200, loss : 0.5406, accuracy : 98.54\n",
            "iteration : 250, loss : 0.5406, accuracy : 98.57\n",
            "iteration : 300, loss : 0.5415, accuracy : 98.55\n",
            "iteration : 350, loss : 0.5412, accuracy : 98.56\n",
            "Epoch : 231, training loss : 0.5411, training accuracy : 98.56, test loss : 0.6496, test accuracy : 94.55\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.5386, accuracy : 98.80\n",
            "iteration : 100, loss : 0.5380, accuracy : 98.70\n",
            "iteration : 150, loss : 0.5368, accuracy : 98.74\n",
            "iteration : 200, loss : 0.5386, accuracy : 98.64\n",
            "iteration : 250, loss : 0.5389, accuracy : 98.67\n",
            "iteration : 300, loss : 0.5381, accuracy : 98.70\n",
            "iteration : 350, loss : 0.5394, accuracy : 98.65\n",
            "Epoch : 232, training loss : 0.5394, training accuracy : 98.65, test loss : 0.6541, test accuracy : 94.52\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.5489, accuracy : 98.23\n",
            "iteration : 100, loss : 0.5443, accuracy : 98.41\n",
            "iteration : 150, loss : 0.5421, accuracy : 98.48\n",
            "iteration : 200, loss : 0.5420, accuracy : 98.51\n",
            "iteration : 250, loss : 0.5416, accuracy : 98.53\n",
            "iteration : 300, loss : 0.5415, accuracy : 98.55\n",
            "iteration : 350, loss : 0.5403, accuracy : 98.59\n",
            "Epoch : 233, training loss : 0.5406, training accuracy : 98.59, test loss : 0.6495, test accuracy : 94.62\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.5383, accuracy : 98.67\n",
            "iteration : 100, loss : 0.5388, accuracy : 98.71\n",
            "iteration : 150, loss : 0.5391, accuracy : 98.68\n",
            "iteration : 200, loss : 0.5390, accuracy : 98.70\n",
            "iteration : 250, loss : 0.5392, accuracy : 98.68\n",
            "iteration : 300, loss : 0.5384, accuracy : 98.70\n",
            "iteration : 350, loss : 0.5387, accuracy : 98.68\n",
            "Epoch : 234, training loss : 0.5387, training accuracy : 98.67, test loss : 0.6539, test accuracy : 94.51\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.5314, accuracy : 99.02\n",
            "iteration : 100, loss : 0.5337, accuracy : 98.91\n",
            "iteration : 150, loss : 0.5387, accuracy : 98.76\n",
            "iteration : 200, loss : 0.5363, accuracy : 98.81\n",
            "iteration : 250, loss : 0.5373, accuracy : 98.73\n",
            "iteration : 300, loss : 0.5372, accuracy : 98.74\n",
            "iteration : 350, loss : 0.5378, accuracy : 98.72\n",
            "Epoch : 235, training loss : 0.5376, training accuracy : 98.72, test loss : 0.6511, test accuracy : 94.58\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.5377, accuracy : 98.69\n",
            "iteration : 100, loss : 0.5375, accuracy : 98.67\n",
            "iteration : 150, loss : 0.5382, accuracy : 98.61\n",
            "iteration : 200, loss : 0.5373, accuracy : 98.67\n",
            "iteration : 250, loss : 0.5377, accuracy : 98.67\n",
            "iteration : 300, loss : 0.5384, accuracy : 98.65\n",
            "iteration : 350, loss : 0.5385, accuracy : 98.65\n",
            "Epoch : 236, training loss : 0.5392, training accuracy : 98.63, test loss : 0.6518, test accuracy : 94.62\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.5409, accuracy : 98.59\n",
            "iteration : 100, loss : 0.5406, accuracy : 98.56\n",
            "iteration : 150, loss : 0.5387, accuracy : 98.65\n",
            "iteration : 200, loss : 0.5389, accuracy : 98.60\n",
            "iteration : 250, loss : 0.5391, accuracy : 98.62\n",
            "iteration : 300, loss : 0.5390, accuracy : 98.62\n",
            "iteration : 350, loss : 0.5386, accuracy : 98.63\n",
            "Epoch : 237, training loss : 0.5385, training accuracy : 98.63, test loss : 0.6484, test accuracy : 94.68\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.5361, accuracy : 98.66\n",
            "iteration : 100, loss : 0.5361, accuracy : 98.75\n",
            "iteration : 150, loss : 0.5358, accuracy : 98.77\n",
            "iteration : 200, loss : 0.5341, accuracy : 98.81\n",
            "iteration : 250, loss : 0.5353, accuracy : 98.73\n",
            "iteration : 300, loss : 0.5364, accuracy : 98.71\n",
            "iteration : 350, loss : 0.5373, accuracy : 98.68\n",
            "Epoch : 238, training loss : 0.5373, training accuracy : 98.69, test loss : 0.6510, test accuracy : 94.65\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.5374, accuracy : 98.66\n",
            "iteration : 100, loss : 0.5398, accuracy : 98.63\n",
            "iteration : 150, loss : 0.5386, accuracy : 98.68\n",
            "iteration : 200, loss : 0.5389, accuracy : 98.67\n",
            "iteration : 250, loss : 0.5383, accuracy : 98.69\n",
            "iteration : 300, loss : 0.5387, accuracy : 98.69\n",
            "iteration : 350, loss : 0.5384, accuracy : 98.70\n",
            "Epoch : 239, training loss : 0.5384, training accuracy : 98.69, test loss : 0.6510, test accuracy : 94.65\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.5380, accuracy : 98.66\n",
            "iteration : 100, loss : 0.5381, accuracy : 98.64\n",
            "iteration : 150, loss : 0.5382, accuracy : 98.66\n",
            "iteration : 200, loss : 0.5382, accuracy : 98.66\n",
            "iteration : 250, loss : 0.5378, accuracy : 98.68\n",
            "iteration : 300, loss : 0.5382, accuracy : 98.67\n",
            "iteration : 350, loss : 0.5373, accuracy : 98.69\n",
            "Epoch : 240, training loss : 0.5372, training accuracy : 98.69, test loss : 0.6537, test accuracy : 94.53\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.5318, accuracy : 98.94\n",
            "iteration : 100, loss : 0.5345, accuracy : 98.81\n",
            "iteration : 150, loss : 0.5348, accuracy : 98.82\n",
            "iteration : 200, loss : 0.5355, accuracy : 98.80\n",
            "iteration : 250, loss : 0.5360, accuracy : 98.77\n",
            "iteration : 300, loss : 0.5370, accuracy : 98.74\n",
            "iteration : 350, loss : 0.5363, accuracy : 98.76\n",
            "Epoch : 241, training loss : 0.5363, training accuracy : 98.75, test loss : 0.6505, test accuracy : 94.64\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.5340, accuracy : 98.75\n",
            "iteration : 100, loss : 0.5358, accuracy : 98.76\n",
            "iteration : 150, loss : 0.5346, accuracy : 98.80\n",
            "iteration : 200, loss : 0.5363, accuracy : 98.73\n",
            "iteration : 250, loss : 0.5370, accuracy : 98.70\n",
            "iteration : 300, loss : 0.5370, accuracy : 98.70\n",
            "iteration : 350, loss : 0.5363, accuracy : 98.72\n",
            "Epoch : 242, training loss : 0.5362, training accuracy : 98.72, test loss : 0.6505, test accuracy : 94.58\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.5302, accuracy : 98.94\n",
            "iteration : 100, loss : 0.5338, accuracy : 98.84\n",
            "iteration : 150, loss : 0.5366, accuracy : 98.73\n",
            "iteration : 200, loss : 0.5363, accuracy : 98.73\n",
            "iteration : 250, loss : 0.5357, accuracy : 98.75\n",
            "iteration : 300, loss : 0.5355, accuracy : 98.78\n",
            "iteration : 350, loss : 0.5357, accuracy : 98.77\n",
            "Epoch : 243, training loss : 0.5359, training accuracy : 98.77, test loss : 0.6531, test accuracy : 94.53\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.5316, accuracy : 98.95\n",
            "iteration : 100, loss : 0.5322, accuracy : 98.92\n",
            "iteration : 150, loss : 0.5337, accuracy : 98.84\n",
            "iteration : 200, loss : 0.5352, accuracy : 98.81\n",
            "iteration : 250, loss : 0.5356, accuracy : 98.81\n",
            "iteration : 300, loss : 0.5357, accuracy : 98.78\n",
            "iteration : 350, loss : 0.5357, accuracy : 98.78\n",
            "Epoch : 244, training loss : 0.5353, training accuracy : 98.81, test loss : 0.6549, test accuracy : 94.53\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.5357, accuracy : 98.70\n",
            "iteration : 100, loss : 0.5376, accuracy : 98.64\n",
            "iteration : 150, loss : 0.5363, accuracy : 98.71\n",
            "iteration : 200, loss : 0.5361, accuracy : 98.71\n",
            "iteration : 250, loss : 0.5361, accuracy : 98.73\n",
            "iteration : 300, loss : 0.5357, accuracy : 98.74\n",
            "iteration : 350, loss : 0.5351, accuracy : 98.78\n",
            "Epoch : 245, training loss : 0.5351, training accuracy : 98.78, test loss : 0.6474, test accuracy : 94.84\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.5375, accuracy : 98.70\n",
            "iteration : 100, loss : 0.5384, accuracy : 98.66\n",
            "iteration : 150, loss : 0.5375, accuracy : 98.69\n",
            "iteration : 200, loss : 0.5374, accuracy : 98.71\n",
            "iteration : 250, loss : 0.5358, accuracy : 98.75\n",
            "iteration : 300, loss : 0.5365, accuracy : 98.73\n",
            "iteration : 350, loss : 0.5359, accuracy : 98.75\n",
            "Epoch : 246, training loss : 0.5357, training accuracy : 98.77, test loss : 0.6506, test accuracy : 94.64\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.5398, accuracy : 98.58\n",
            "iteration : 100, loss : 0.5365, accuracy : 98.73\n",
            "iteration : 150, loss : 0.5347, accuracy : 98.82\n",
            "iteration : 200, loss : 0.5341, accuracy : 98.83\n",
            "iteration : 250, loss : 0.5337, accuracy : 98.86\n",
            "iteration : 300, loss : 0.5340, accuracy : 98.85\n",
            "iteration : 350, loss : 0.5336, accuracy : 98.87\n",
            "Epoch : 247, training loss : 0.5336, training accuracy : 98.87, test loss : 0.6504, test accuracy : 94.71\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.5311, accuracy : 98.98\n",
            "iteration : 100, loss : 0.5281, accuracy : 99.02\n",
            "iteration : 150, loss : 0.5302, accuracy : 98.91\n",
            "iteration : 200, loss : 0.5314, accuracy : 98.88\n",
            "iteration : 250, loss : 0.5323, accuracy : 98.86\n",
            "iteration : 300, loss : 0.5334, accuracy : 98.84\n",
            "iteration : 350, loss : 0.5338, accuracy : 98.83\n",
            "Epoch : 248, training loss : 0.5340, training accuracy : 98.83, test loss : 0.6487, test accuracy : 94.78\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.5264, accuracy : 99.16\n",
            "iteration : 100, loss : 0.5310, accuracy : 98.91\n",
            "iteration : 150, loss : 0.5315, accuracy : 98.92\n",
            "iteration : 200, loss : 0.5318, accuracy : 98.91\n",
            "iteration : 250, loss : 0.5321, accuracy : 98.92\n",
            "iteration : 300, loss : 0.5327, accuracy : 98.91\n",
            "iteration : 350, loss : 0.5318, accuracy : 98.95\n",
            "Epoch : 249, training loss : 0.5322, training accuracy : 98.93, test loss : 0.6541, test accuracy : 94.57\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.5289, accuracy : 99.03\n",
            "iteration : 100, loss : 0.5284, accuracy : 99.06\n",
            "iteration : 150, loss : 0.5303, accuracy : 98.97\n",
            "iteration : 200, loss : 0.5311, accuracy : 98.93\n",
            "iteration : 250, loss : 0.5307, accuracy : 98.94\n",
            "iteration : 300, loss : 0.5316, accuracy : 98.92\n",
            "iteration : 350, loss : 0.5326, accuracy : 98.88\n",
            "Epoch : 250, training loss : 0.5328, training accuracy : 98.88, test loss : 0.6519, test accuracy : 94.58\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.5325, accuracy : 98.92\n",
            "iteration : 100, loss : 0.5338, accuracy : 98.90\n",
            "iteration : 150, loss : 0.5320, accuracy : 98.94\n",
            "iteration : 200, loss : 0.5318, accuracy : 98.95\n",
            "iteration : 250, loss : 0.5314, accuracy : 98.95\n",
            "iteration : 300, loss : 0.5314, accuracy : 98.95\n",
            "iteration : 350, loss : 0.5321, accuracy : 98.93\n",
            "Epoch : 251, training loss : 0.5328, training accuracy : 98.89, test loss : 0.6552, test accuracy : 94.45\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.5360, accuracy : 98.77\n",
            "iteration : 100, loss : 0.5334, accuracy : 98.85\n",
            "iteration : 150, loss : 0.5339, accuracy : 98.83\n",
            "iteration : 200, loss : 0.5339, accuracy : 98.83\n",
            "iteration : 250, loss : 0.5345, accuracy : 98.81\n",
            "iteration : 300, loss : 0.5337, accuracy : 98.84\n",
            "iteration : 350, loss : 0.5330, accuracy : 98.88\n",
            "Epoch : 252, training loss : 0.5334, training accuracy : 98.88, test loss : 0.6485, test accuracy : 94.73\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.5347, accuracy : 98.75\n",
            "iteration : 100, loss : 0.5347, accuracy : 98.78\n",
            "iteration : 150, loss : 0.5331, accuracy : 98.89\n",
            "iteration : 200, loss : 0.5333, accuracy : 98.88\n",
            "iteration : 250, loss : 0.5320, accuracy : 98.91\n",
            "iteration : 300, loss : 0.5327, accuracy : 98.88\n",
            "iteration : 350, loss : 0.5324, accuracy : 98.90\n",
            "Epoch : 253, training loss : 0.5318, training accuracy : 98.92, test loss : 0.6497, test accuracy : 94.73\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.5278, accuracy : 99.08\n",
            "iteration : 100, loss : 0.5271, accuracy : 99.06\n",
            "iteration : 150, loss : 0.5272, accuracy : 99.06\n",
            "iteration : 200, loss : 0.5290, accuracy : 98.98\n",
            "iteration : 250, loss : 0.5305, accuracy : 98.93\n",
            "iteration : 300, loss : 0.5304, accuracy : 98.94\n",
            "iteration : 350, loss : 0.5310, accuracy : 98.94\n",
            "Epoch : 254, training loss : 0.5316, training accuracy : 98.93, test loss : 0.6486, test accuracy : 94.75\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.5315, accuracy : 98.95\n",
            "iteration : 100, loss : 0.5300, accuracy : 98.97\n",
            "iteration : 150, loss : 0.5321, accuracy : 98.90\n",
            "iteration : 200, loss : 0.5326, accuracy : 98.86\n",
            "iteration : 250, loss : 0.5320, accuracy : 98.89\n",
            "iteration : 300, loss : 0.5314, accuracy : 98.94\n",
            "iteration : 350, loss : 0.5313, accuracy : 98.94\n",
            "Epoch : 255, training loss : 0.5314, training accuracy : 98.93, test loss : 0.6508, test accuracy : 94.66\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.5304, accuracy : 98.92\n",
            "iteration : 100, loss : 0.5293, accuracy : 98.98\n",
            "iteration : 150, loss : 0.5313, accuracy : 98.89\n",
            "iteration : 200, loss : 0.5313, accuracy : 98.91\n",
            "iteration : 250, loss : 0.5323, accuracy : 98.90\n",
            "iteration : 300, loss : 0.5323, accuracy : 98.91\n",
            "iteration : 350, loss : 0.5325, accuracy : 98.90\n",
            "Epoch : 256, training loss : 0.5317, training accuracy : 98.92, test loss : 0.6517, test accuracy : 94.69\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.5316, accuracy : 98.95\n",
            "iteration : 100, loss : 0.5334, accuracy : 98.91\n",
            "iteration : 150, loss : 0.5317, accuracy : 98.97\n",
            "iteration : 200, loss : 0.5306, accuracy : 98.99\n",
            "iteration : 250, loss : 0.5312, accuracy : 98.96\n",
            "iteration : 300, loss : 0.5307, accuracy : 98.99\n",
            "iteration : 350, loss : 0.5306, accuracy : 98.98\n",
            "Epoch : 257, training loss : 0.5305, training accuracy : 98.98, test loss : 0.6555, test accuracy : 94.57\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.5296, accuracy : 99.05\n",
            "iteration : 100, loss : 0.5294, accuracy : 99.05\n",
            "iteration : 150, loss : 0.5280, accuracy : 99.09\n",
            "iteration : 200, loss : 0.5286, accuracy : 99.09\n",
            "iteration : 250, loss : 0.5290, accuracy : 99.06\n",
            "iteration : 300, loss : 0.5291, accuracy : 99.06\n",
            "iteration : 350, loss : 0.5292, accuracy : 99.04\n",
            "Epoch : 258, training loss : 0.5297, training accuracy : 99.03, test loss : 0.6486, test accuracy : 94.78\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.5283, accuracy : 99.03\n",
            "iteration : 100, loss : 0.5275, accuracy : 99.09\n",
            "iteration : 150, loss : 0.5284, accuracy : 99.05\n",
            "iteration : 200, loss : 0.5288, accuracy : 99.03\n",
            "iteration : 250, loss : 0.5298, accuracy : 99.02\n",
            "iteration : 300, loss : 0.5287, accuracy : 99.05\n",
            "iteration : 350, loss : 0.5286, accuracy : 99.06\n",
            "Epoch : 259, training loss : 0.5288, training accuracy : 99.05, test loss : 0.6521, test accuracy : 94.65\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.5288, accuracy : 98.97\n",
            "iteration : 100, loss : 0.5280, accuracy : 99.03\n",
            "iteration : 150, loss : 0.5283, accuracy : 99.03\n",
            "iteration : 200, loss : 0.5276, accuracy : 99.05\n",
            "iteration : 250, loss : 0.5278, accuracy : 99.06\n",
            "iteration : 300, loss : 0.5280, accuracy : 99.05\n",
            "iteration : 350, loss : 0.5280, accuracy : 99.06\n",
            "Epoch : 260, training loss : 0.5286, training accuracy : 99.04, test loss : 0.6506, test accuracy : 94.64\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.5312, accuracy : 98.91\n",
            "iteration : 100, loss : 0.5312, accuracy : 98.95\n",
            "iteration : 150, loss : 0.5303, accuracy : 98.99\n",
            "iteration : 200, loss : 0.5314, accuracy : 98.98\n",
            "iteration : 250, loss : 0.5311, accuracy : 98.98\n",
            "iteration : 300, loss : 0.5304, accuracy : 99.01\n",
            "iteration : 350, loss : 0.5297, accuracy : 99.03\n",
            "Epoch : 261, training loss : 0.5297, training accuracy : 99.03, test loss : 0.6505, test accuracy : 94.76\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.5243, accuracy : 99.11\n",
            "iteration : 100, loss : 0.5272, accuracy : 99.05\n",
            "iteration : 150, loss : 0.5285, accuracy : 99.02\n",
            "iteration : 200, loss : 0.5289, accuracy : 99.01\n",
            "iteration : 250, loss : 0.5293, accuracy : 99.00\n",
            "iteration : 300, loss : 0.5286, accuracy : 99.03\n",
            "iteration : 350, loss : 0.5287, accuracy : 99.02\n",
            "Epoch : 262, training loss : 0.5284, training accuracy : 99.03, test loss : 0.6524, test accuracy : 94.57\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.5312, accuracy : 98.92\n",
            "iteration : 100, loss : 0.5294, accuracy : 99.02\n",
            "iteration : 150, loss : 0.5293, accuracy : 99.02\n",
            "iteration : 200, loss : 0.5283, accuracy : 99.04\n",
            "iteration : 250, loss : 0.5285, accuracy : 99.04\n",
            "iteration : 300, loss : 0.5281, accuracy : 99.07\n",
            "iteration : 350, loss : 0.5275, accuracy : 99.08\n",
            "Epoch : 263, training loss : 0.5274, training accuracy : 99.08, test loss : 0.6489, test accuracy : 94.75\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.5329, accuracy : 99.02\n",
            "iteration : 100, loss : 0.5327, accuracy : 98.97\n",
            "iteration : 150, loss : 0.5314, accuracy : 98.99\n",
            "iteration : 200, loss : 0.5298, accuracy : 99.04\n",
            "iteration : 250, loss : 0.5301, accuracy : 99.01\n",
            "iteration : 300, loss : 0.5307, accuracy : 98.98\n",
            "iteration : 350, loss : 0.5300, accuracy : 99.00\n",
            "Epoch : 264, training loss : 0.5296, training accuracy : 99.02, test loss : 0.6498, test accuracy : 94.74\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.5317, accuracy : 98.92\n",
            "iteration : 100, loss : 0.5276, accuracy : 99.02\n",
            "iteration : 150, loss : 0.5273, accuracy : 99.06\n",
            "iteration : 200, loss : 0.5274, accuracy : 99.04\n",
            "iteration : 250, loss : 0.5275, accuracy : 99.05\n",
            "iteration : 300, loss : 0.5273, accuracy : 99.05\n",
            "iteration : 350, loss : 0.5272, accuracy : 99.08\n",
            "Epoch : 265, training loss : 0.5277, training accuracy : 99.07, test loss : 0.6529, test accuracy : 94.61\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.5279, accuracy : 99.00\n",
            "iteration : 100, loss : 0.5281, accuracy : 99.01\n",
            "iteration : 150, loss : 0.5300, accuracy : 98.98\n",
            "iteration : 200, loss : 0.5302, accuracy : 98.97\n",
            "iteration : 250, loss : 0.5307, accuracy : 98.97\n",
            "iteration : 300, loss : 0.5293, accuracy : 99.02\n",
            "iteration : 350, loss : 0.5291, accuracy : 99.03\n",
            "Epoch : 266, training loss : 0.5291, training accuracy : 99.03, test loss : 0.6483, test accuracy : 94.76\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.5270, accuracy : 99.11\n",
            "iteration : 100, loss : 0.5275, accuracy : 99.16\n",
            "iteration : 150, loss : 0.5276, accuracy : 99.14\n",
            "iteration : 200, loss : 0.5276, accuracy : 99.14\n",
            "iteration : 250, loss : 0.5281, accuracy : 99.12\n",
            "iteration : 300, loss : 0.5281, accuracy : 99.11\n",
            "iteration : 350, loss : 0.5277, accuracy : 99.13\n",
            "Epoch : 267, training loss : 0.5278, training accuracy : 99.12, test loss : 0.6480, test accuracy : 94.82\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.5221, accuracy : 99.30\n",
            "iteration : 100, loss : 0.5258, accuracy : 99.19\n",
            "iteration : 150, loss : 0.5286, accuracy : 99.09\n",
            "iteration : 200, loss : 0.5275, accuracy : 99.12\n",
            "iteration : 250, loss : 0.5286, accuracy : 99.06\n",
            "iteration : 300, loss : 0.5279, accuracy : 99.08\n",
            "iteration : 350, loss : 0.5272, accuracy : 99.11\n",
            "Epoch : 268, training loss : 0.5270, training accuracy : 99.12, test loss : 0.6524, test accuracy : 94.67\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.5265, accuracy : 99.08\n",
            "iteration : 100, loss : 0.5302, accuracy : 98.97\n",
            "iteration : 150, loss : 0.5296, accuracy : 99.02\n",
            "iteration : 200, loss : 0.5292, accuracy : 99.02\n",
            "iteration : 250, loss : 0.5290, accuracy : 99.02\n",
            "iteration : 300, loss : 0.5285, accuracy : 99.05\n",
            "iteration : 350, loss : 0.5289, accuracy : 99.04\n",
            "Epoch : 269, training loss : 0.5285, training accuracy : 99.06, test loss : 0.6514, test accuracy : 94.74\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.5315, accuracy : 99.02\n",
            "iteration : 100, loss : 0.5303, accuracy : 99.05\n",
            "iteration : 150, loss : 0.5302, accuracy : 99.01\n",
            "iteration : 200, loss : 0.5306, accuracy : 99.00\n",
            "iteration : 250, loss : 0.5300, accuracy : 99.02\n",
            "iteration : 300, loss : 0.5299, accuracy : 99.01\n",
            "iteration : 350, loss : 0.5291, accuracy : 99.05\n",
            "Epoch : 270, training loss : 0.5292, training accuracy : 99.04, test loss : 0.6492, test accuracy : 94.69\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.5266, accuracy : 99.16\n",
            "iteration : 100, loss : 0.5238, accuracy : 99.23\n",
            "iteration : 150, loss : 0.5246, accuracy : 99.21\n",
            "iteration : 200, loss : 0.5258, accuracy : 99.20\n",
            "iteration : 250, loss : 0.5258, accuracy : 99.18\n",
            "iteration : 300, loss : 0.5258, accuracy : 99.16\n",
            "iteration : 350, loss : 0.5257, accuracy : 99.17\n",
            "Epoch : 271, training loss : 0.5257, training accuracy : 99.17, test loss : 0.6456, test accuracy : 94.99\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.5308, accuracy : 99.03\n",
            "iteration : 100, loss : 0.5270, accuracy : 99.10\n",
            "iteration : 150, loss : 0.5267, accuracy : 99.10\n",
            "iteration : 200, loss : 0.5254, accuracy : 99.17\n",
            "iteration : 250, loss : 0.5263, accuracy : 99.14\n",
            "iteration : 300, loss : 0.5271, accuracy : 99.12\n",
            "iteration : 350, loss : 0.5269, accuracy : 99.11\n",
            "Epoch : 272, training loss : 0.5269, training accuracy : 99.12, test loss : 0.6483, test accuracy : 94.78\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.5228, accuracy : 99.23\n",
            "iteration : 100, loss : 0.5250, accuracy : 99.12\n",
            "iteration : 150, loss : 0.5257, accuracy : 99.09\n",
            "iteration : 200, loss : 0.5260, accuracy : 99.09\n",
            "iteration : 250, loss : 0.5265, accuracy : 99.09\n",
            "iteration : 300, loss : 0.5274, accuracy : 99.06\n",
            "iteration : 350, loss : 0.5270, accuracy : 99.08\n",
            "Epoch : 273, training loss : 0.5271, training accuracy : 99.08, test loss : 0.6448, test accuracy : 94.92\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.5221, accuracy : 99.25\n",
            "iteration : 100, loss : 0.5256, accuracy : 99.16\n",
            "iteration : 150, loss : 0.5256, accuracy : 99.18\n",
            "iteration : 200, loss : 0.5257, accuracy : 99.17\n",
            "iteration : 250, loss : 0.5259, accuracy : 99.14\n",
            "iteration : 300, loss : 0.5263, accuracy : 99.14\n",
            "iteration : 350, loss : 0.5262, accuracy : 99.12\n",
            "Epoch : 274, training loss : 0.5263, training accuracy : 99.13, test loss : 0.6477, test accuracy : 94.83\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.5267, accuracy : 99.19\n",
            "iteration : 100, loss : 0.5264, accuracy : 99.17\n",
            "iteration : 150, loss : 0.5270, accuracy : 99.13\n",
            "iteration : 200, loss : 0.5260, accuracy : 99.13\n",
            "iteration : 250, loss : 0.5258, accuracy : 99.16\n",
            "iteration : 300, loss : 0.5257, accuracy : 99.17\n",
            "iteration : 350, loss : 0.5254, accuracy : 99.19\n",
            "Epoch : 275, training loss : 0.5254, training accuracy : 99.20, test loss : 0.6499, test accuracy : 94.68\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.5237, accuracy : 99.14\n",
            "iteration : 100, loss : 0.5240, accuracy : 99.14\n",
            "iteration : 150, loss : 0.5242, accuracy : 99.20\n",
            "iteration : 200, loss : 0.5250, accuracy : 99.18\n",
            "iteration : 250, loss : 0.5257, accuracy : 99.14\n",
            "iteration : 300, loss : 0.5263, accuracy : 99.12\n",
            "iteration : 350, loss : 0.5276, accuracy : 99.09\n",
            "Epoch : 276, training loss : 0.5275, training accuracy : 99.09, test loss : 0.6444, test accuracy : 94.94\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.5227, accuracy : 99.27\n",
            "iteration : 100, loss : 0.5259, accuracy : 99.13\n",
            "iteration : 150, loss : 0.5261, accuracy : 99.14\n",
            "iteration : 200, loss : 0.5263, accuracy : 99.14\n",
            "iteration : 250, loss : 0.5259, accuracy : 99.13\n",
            "iteration : 300, loss : 0.5258, accuracy : 99.14\n",
            "iteration : 350, loss : 0.5267, accuracy : 99.11\n",
            "Epoch : 277, training loss : 0.5264, training accuracy : 99.12, test loss : 0.6496, test accuracy : 94.82\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.5241, accuracy : 99.23\n",
            "iteration : 100, loss : 0.5248, accuracy : 99.18\n",
            "iteration : 150, loss : 0.5249, accuracy : 99.15\n",
            "iteration : 200, loss : 0.5249, accuracy : 99.16\n",
            "iteration : 250, loss : 0.5249, accuracy : 99.15\n",
            "iteration : 300, loss : 0.5249, accuracy : 99.17\n",
            "iteration : 350, loss : 0.5252, accuracy : 99.16\n",
            "Epoch : 278, training loss : 0.5253, training accuracy : 99.15, test loss : 0.6467, test accuracy : 94.85\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.5244, accuracy : 99.27\n",
            "iteration : 100, loss : 0.5254, accuracy : 99.20\n",
            "iteration : 150, loss : 0.5268, accuracy : 99.15\n",
            "iteration : 200, loss : 0.5264, accuracy : 99.14\n",
            "iteration : 250, loss : 0.5263, accuracy : 99.14\n",
            "iteration : 300, loss : 0.5255, accuracy : 99.16\n",
            "iteration : 350, loss : 0.5256, accuracy : 99.16\n",
            "Epoch : 279, training loss : 0.5255, training accuracy : 99.16, test loss : 0.6494, test accuracy : 94.74\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.5243, accuracy : 99.22\n",
            "iteration : 100, loss : 0.5239, accuracy : 99.16\n",
            "iteration : 150, loss : 0.5258, accuracy : 99.12\n",
            "iteration : 200, loss : 0.5256, accuracy : 99.13\n",
            "iteration : 250, loss : 0.5255, accuracy : 99.14\n",
            "iteration : 300, loss : 0.5247, accuracy : 99.17\n",
            "iteration : 350, loss : 0.5249, accuracy : 99.17\n",
            "Epoch : 280, training loss : 0.5250, training accuracy : 99.16, test loss : 0.6494, test accuracy : 94.80\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.5246, accuracy : 99.22\n",
            "iteration : 100, loss : 0.5241, accuracy : 99.27\n",
            "iteration : 150, loss : 0.5252, accuracy : 99.20\n",
            "iteration : 200, loss : 0.5246, accuracy : 99.22\n",
            "iteration : 250, loss : 0.5246, accuracy : 99.21\n",
            "iteration : 300, loss : 0.5245, accuracy : 99.22\n",
            "iteration : 350, loss : 0.5242, accuracy : 99.23\n",
            "Epoch : 281, training loss : 0.5245, training accuracy : 99.21, test loss : 0.6509, test accuracy : 94.74\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.5257, accuracy : 99.19\n",
            "iteration : 100, loss : 0.5257, accuracy : 99.18\n",
            "iteration : 150, loss : 0.5265, accuracy : 99.17\n",
            "iteration : 200, loss : 0.5264, accuracy : 99.17\n",
            "iteration : 250, loss : 0.5257, accuracy : 99.19\n",
            "iteration : 300, loss : 0.5262, accuracy : 99.18\n",
            "iteration : 350, loss : 0.5260, accuracy : 99.18\n",
            "Epoch : 282, training loss : 0.5259, training accuracy : 99.18, test loss : 0.6487, test accuracy : 94.85\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.5193, accuracy : 99.39\n",
            "iteration : 100, loss : 0.5214, accuracy : 99.33\n",
            "iteration : 150, loss : 0.5222, accuracy : 99.28\n",
            "iteration : 200, loss : 0.5228, accuracy : 99.27\n",
            "iteration : 250, loss : 0.5230, accuracy : 99.26\n",
            "iteration : 300, loss : 0.5239, accuracy : 99.23\n",
            "iteration : 350, loss : 0.5245, accuracy : 99.21\n",
            "Epoch : 283, training loss : 0.5247, training accuracy : 99.20, test loss : 0.6520, test accuracy : 94.65\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.5263, accuracy : 99.30\n",
            "iteration : 100, loss : 0.5229, accuracy : 99.35\n",
            "iteration : 150, loss : 0.5215, accuracy : 99.38\n",
            "iteration : 200, loss : 0.5216, accuracy : 99.35\n",
            "iteration : 250, loss : 0.5229, accuracy : 99.30\n",
            "iteration : 300, loss : 0.5229, accuracy : 99.30\n",
            "iteration : 350, loss : 0.5237, accuracy : 99.26\n",
            "Epoch : 284, training loss : 0.5241, training accuracy : 99.25, test loss : 0.6515, test accuracy : 94.79\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.5270, accuracy : 99.19\n",
            "iteration : 100, loss : 0.5271, accuracy : 99.15\n",
            "iteration : 150, loss : 0.5260, accuracy : 99.21\n",
            "iteration : 200, loss : 0.5258, accuracy : 99.20\n",
            "iteration : 250, loss : 0.5243, accuracy : 99.23\n",
            "iteration : 300, loss : 0.5240, accuracy : 99.24\n",
            "iteration : 350, loss : 0.5235, accuracy : 99.26\n",
            "Epoch : 285, training loss : 0.5240, training accuracy : 99.24, test loss : 0.6480, test accuracy : 94.90\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.5210, accuracy : 99.39\n",
            "iteration : 100, loss : 0.5245, accuracy : 99.17\n",
            "iteration : 150, loss : 0.5236, accuracy : 99.22\n",
            "iteration : 200, loss : 0.5254, accuracy : 99.16\n",
            "iteration : 250, loss : 0.5250, accuracy : 99.20\n",
            "iteration : 300, loss : 0.5248, accuracy : 99.20\n",
            "iteration : 350, loss : 0.5247, accuracy : 99.20\n",
            "Epoch : 286, training loss : 0.5246, training accuracy : 99.21, test loss : 0.6515, test accuracy : 94.69\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.5199, accuracy : 99.34\n",
            "iteration : 100, loss : 0.5234, accuracy : 99.27\n",
            "iteration : 150, loss : 0.5250, accuracy : 99.23\n",
            "iteration : 200, loss : 0.5245, accuracy : 99.23\n",
            "iteration : 250, loss : 0.5253, accuracy : 99.19\n",
            "iteration : 300, loss : 0.5255, accuracy : 99.20\n",
            "iteration : 350, loss : 0.5254, accuracy : 99.20\n",
            "Epoch : 287, training loss : 0.5253, training accuracy : 99.20, test loss : 0.6497, test accuracy : 94.88\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.5240, accuracy : 99.25\n",
            "iteration : 100, loss : 0.5236, accuracy : 99.26\n",
            "iteration : 150, loss : 0.5244, accuracy : 99.23\n",
            "iteration : 200, loss : 0.5235, accuracy : 99.27\n",
            "iteration : 250, loss : 0.5238, accuracy : 99.25\n",
            "iteration : 300, loss : 0.5233, accuracy : 99.27\n",
            "iteration : 350, loss : 0.5235, accuracy : 99.27\n",
            "Epoch : 288, training loss : 0.5238, training accuracy : 99.26, test loss : 0.6524, test accuracy : 94.68\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.5280, accuracy : 99.06\n",
            "iteration : 100, loss : 0.5277, accuracy : 99.11\n",
            "iteration : 150, loss : 0.5276, accuracy : 99.11\n",
            "iteration : 200, loss : 0.5276, accuracy : 99.11\n",
            "iteration : 250, loss : 0.5274, accuracy : 99.12\n",
            "iteration : 300, loss : 0.5264, accuracy : 99.16\n",
            "iteration : 350, loss : 0.5259, accuracy : 99.17\n",
            "Epoch : 289, training loss : 0.5259, training accuracy : 99.17, test loss : 0.6510, test accuracy : 94.72\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.5273, accuracy : 99.03\n",
            "iteration : 100, loss : 0.5257, accuracy : 99.14\n",
            "iteration : 150, loss : 0.5259, accuracy : 99.12\n",
            "iteration : 200, loss : 0.5259, accuracy : 99.14\n",
            "iteration : 250, loss : 0.5257, accuracy : 99.14\n",
            "iteration : 300, loss : 0.5253, accuracy : 99.15\n",
            "iteration : 350, loss : 0.5245, accuracy : 99.19\n",
            "Epoch : 290, training loss : 0.5242, training accuracy : 99.20, test loss : 0.6477, test accuracy : 94.83\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.5276, accuracy : 99.12\n",
            "iteration : 100, loss : 0.5262, accuracy : 99.17\n",
            "iteration : 150, loss : 0.5241, accuracy : 99.28\n",
            "iteration : 200, loss : 0.5240, accuracy : 99.27\n",
            "iteration : 250, loss : 0.5237, accuracy : 99.28\n",
            "iteration : 300, loss : 0.5238, accuracy : 99.28\n",
            "iteration : 350, loss : 0.5238, accuracy : 99.27\n",
            "Epoch : 291, training loss : 0.5238, training accuracy : 99.26, test loss : 0.6466, test accuracy : 94.93\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.5256, accuracy : 99.19\n",
            "iteration : 100, loss : 0.5253, accuracy : 99.18\n",
            "iteration : 150, loss : 0.5239, accuracy : 99.21\n",
            "iteration : 200, loss : 0.5235, accuracy : 99.25\n",
            "iteration : 250, loss : 0.5235, accuracy : 99.24\n",
            "iteration : 300, loss : 0.5238, accuracy : 99.23\n",
            "iteration : 350, loss : 0.5235, accuracy : 99.24\n",
            "Epoch : 292, training loss : 0.5241, training accuracy : 99.22, test loss : 0.6473, test accuracy : 94.83\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.5224, accuracy : 99.42\n",
            "iteration : 100, loss : 0.5263, accuracy : 99.20\n",
            "iteration : 150, loss : 0.5256, accuracy : 99.23\n",
            "iteration : 200, loss : 0.5254, accuracy : 99.20\n",
            "iteration : 250, loss : 0.5249, accuracy : 99.23\n",
            "iteration : 300, loss : 0.5250, accuracy : 99.22\n",
            "iteration : 350, loss : 0.5248, accuracy : 99.23\n",
            "Epoch : 293, training loss : 0.5246, training accuracy : 99.24, test loss : 0.6494, test accuracy : 94.75\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.5237, accuracy : 99.30\n",
            "iteration : 100, loss : 0.5244, accuracy : 99.24\n",
            "iteration : 150, loss : 0.5250, accuracy : 99.20\n",
            "iteration : 200, loss : 0.5253, accuracy : 99.18\n",
            "iteration : 250, loss : 0.5247, accuracy : 99.21\n",
            "iteration : 300, loss : 0.5249, accuracy : 99.21\n",
            "iteration : 350, loss : 0.5248, accuracy : 99.21\n",
            "Epoch : 294, training loss : 0.5247, training accuracy : 99.21, test loss : 0.6495, test accuracy : 94.79\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.5275, accuracy : 99.22\n",
            "iteration : 100, loss : 0.5241, accuracy : 99.27\n",
            "iteration : 150, loss : 0.5240, accuracy : 99.24\n",
            "iteration : 200, loss : 0.5242, accuracy : 99.24\n",
            "iteration : 250, loss : 0.5240, accuracy : 99.25\n",
            "iteration : 300, loss : 0.5237, accuracy : 99.25\n",
            "iteration : 350, loss : 0.5237, accuracy : 99.25\n",
            "Epoch : 295, training loss : 0.5237, training accuracy : 99.25, test loss : 0.6455, test accuracy : 94.95\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.5256, accuracy : 99.25\n",
            "iteration : 100, loss : 0.5251, accuracy : 99.21\n",
            "iteration : 150, loss : 0.5244, accuracy : 99.26\n",
            "iteration : 200, loss : 0.5246, accuracy : 99.25\n",
            "iteration : 250, loss : 0.5239, accuracy : 99.27\n",
            "iteration : 300, loss : 0.5242, accuracy : 99.25\n",
            "iteration : 350, loss : 0.5239, accuracy : 99.25\n",
            "Epoch : 296, training loss : 0.5236, training accuracy : 99.26, test loss : 0.6495, test accuracy : 94.76\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.5246, accuracy : 99.28\n",
            "iteration : 100, loss : 0.5234, accuracy : 99.28\n",
            "iteration : 150, loss : 0.5236, accuracy : 99.28\n",
            "iteration : 200, loss : 0.5246, accuracy : 99.21\n",
            "iteration : 250, loss : 0.5237, accuracy : 99.24\n",
            "iteration : 300, loss : 0.5232, accuracy : 99.26\n",
            "iteration : 350, loss : 0.5238, accuracy : 99.24\n",
            "Epoch : 297, training loss : 0.5242, training accuracy : 99.22, test loss : 0.6483, test accuracy : 94.81\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.5210, accuracy : 99.31\n",
            "iteration : 100, loss : 0.5212, accuracy : 99.30\n",
            "iteration : 150, loss : 0.5234, accuracy : 99.25\n",
            "iteration : 200, loss : 0.5241, accuracy : 99.25\n",
            "iteration : 250, loss : 0.5240, accuracy : 99.24\n",
            "iteration : 300, loss : 0.5236, accuracy : 99.24\n",
            "iteration : 350, loss : 0.5238, accuracy : 99.25\n",
            "Epoch : 298, training loss : 0.5244, training accuracy : 99.23, test loss : 0.6467, test accuracy : 94.76\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.5235, accuracy : 99.23\n",
            "iteration : 100, loss : 0.5231, accuracy : 99.27\n",
            "iteration : 150, loss : 0.5223, accuracy : 99.29\n",
            "iteration : 200, loss : 0.5232, accuracy : 99.27\n",
            "iteration : 250, loss : 0.5240, accuracy : 99.22\n",
            "iteration : 300, loss : 0.5237, accuracy : 99.24\n",
            "iteration : 350, loss : 0.5233, accuracy : 99.24\n",
            "Epoch : 299, training loss : 0.5234, training accuracy : 99.24, test loss : 0.6513, test accuracy : 94.65\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.5285, accuracy : 99.14\n",
            "iteration : 100, loss : 0.5273, accuracy : 99.12\n",
            "iteration : 150, loss : 0.5265, accuracy : 99.17\n",
            "iteration : 200, loss : 0.5253, accuracy : 99.22\n",
            "iteration : 250, loss : 0.5255, accuracy : 99.17\n",
            "iteration : 300, loss : 0.5248, accuracy : 99.19\n",
            "iteration : 350, loss : 0.5248, accuracy : 99.20\n",
            "Epoch : 300, training loss : 0.5246, training accuracy : 99.21, test loss : 0.6539, test accuracy : 94.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ],
      "metadata": {
        "id": "iUQVIKR-X3v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ba36630-71c5-4b77-ae95-3169b152daff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5943519350360421, 96.73478795328826)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : Label smoothing factor = 0.1\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7CNz1iabSB21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "113e24a8-4e2d-4276-e6c3-7ba12ce2ad25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : Label smoothing factor = 0.1\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "3b949e9d-3013-4409-d4a4-01b7bed70da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"train_loss_list_ls01 = {train_loss_list}\") \n",
        "print(f\"train_acc_list_ls01 = {train_acc_list}\")\n",
        "print(f\"test_loss_list_ls01 = {test_loss_list}\")\n",
        "print(f\"test_acc_list_ls01 = {test_acc_list}\")"
      ],
      "metadata": {
        "id": "3eiY3bTlWipW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07cc196d-8bdc-4c64-f4ba-559241ebcfd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss_list_ls01 = [1.4567190505624787, 0.857216950839128, 0.8159418779660047, 0.7867067433631194, 0.7780243423249986, 0.7582373672384557, 0.7446735199227889, 0.734787348809281, 0.7315205707136532, 0.7168522829285805, 0.7107103497678349, 0.7041651008898003, 0.696974173147827, 0.6920410887335697, 0.6867286152632902, 0.6852550498515286, 0.6816955778979996, 0.676072128743014, 0.6748113008695567, 0.6702401148594492, 0.668579231432783, 0.6685110923397509, 0.6633778867036669, 0.6597549340266199, 0.6608295022310603, 0.657012108864823, 0.654911065489296, 0.6550180714950975, 0.655547545692785, 0.652886451098331, 0.6511861483256022, 0.652469731766357, 0.6479614110496955, 0.6488034595965047, 0.6440895540604423, 0.646520199006812, 0.6427903684174142, 0.6432868524941648, 0.642532073223817, 0.6425715182531817, 0.6394399611284416, 0.6395640258543537, 0.6406160741640623, 0.6376881481509221, 0.6405688008964869, 0.6365481564992166, 0.6347391563379344, 0.6363826185707154, 0.6364762678055905, 0.6354125258399219, 0.6325634835858333, 0.6352905997738929, 0.6317770112820758, 0.6320762421057476, 0.629789118682789, 0.6285953271356701, 0.6286980517834506, 0.6327761859105532, 0.6287573314294582, 0.6293435927005965, 0.6289574757501039, 0.6261206550326773, 0.6288345620884159, 0.6256406924265833, 0.6294706150445188, 0.6285672937305316, 0.6297664547031164, 0.6262098239043218, 0.6262082412313963, 0.6238588863595068, 0.6233082638200382, 0.6240675959806778, 0.6228897127355664, 0.6239003222486191, 0.6229737011720817, 0.6250082750307512, 0.6225569501801881, 0.6224843688450532, 0.617588132054502, 0.621372369892875, 0.6200971958760001, 0.6187685992982652, 0.6190514570975368, 0.6194138008404554, 0.6176424712992619, 0.6170130725798568, 0.6198753977855693, 0.6174065310780595, 0.6161553631953107, 0.6159340165817964, 0.6181808315964572, 0.6137220397551209, 0.6159217722693756, 0.6144214206271701, 0.6130136274063813, 0.6132693381167362, 0.6127545507296637, 0.6122994663592599, 0.6124134257556947, 0.6112082838042965, 0.6112201851880971, 0.6114611643439709, 0.6077449320131524, 0.6074216575803473, 0.6103596850462399, 0.6062257737325136, 0.6114651263567813, 0.6089390627736968, 0.6067718154369655, 0.6083138250723118, 0.6074341624409849, 0.6054510490358037, 0.6049597218107725, 0.6050836229065892, 0.6024723485879458, 0.604860684574458, 0.6055977048912669, 0.6037631968495646, 0.6011888761184403, 0.5989061556857452, 0.6019193194099881, 0.5994924390865212, 0.6020175297731952, 0.602157037594131, 0.6008423161377429, 0.5996514970694131, 0.5981868597550121, 0.5992651862826773, 0.5954193388866539, 0.5959375012857804, 0.5975631767818275, 0.5948758773041288, 0.5950623967137117, 0.5946910496003582, 0.5942175683936453, 0.5951447125049788, 0.5938783848511817, 0.5932377778740756, 0.5929812561205732, 0.5904511331219661, 0.5899892014862722, 0.5913851211064554, 0.5904359728663271, 0.5914018060456769, 0.5877345839167029, 0.5885400534645329, 0.58316178341222, 0.5832276394373679, 0.5872734731774989, 0.5873040993039201, 0.5864078093996539, 0.5864022023632598, 0.586426231596205, 0.580193504246916, 0.5840775551511667, 0.580843867971322, 0.5834378241523495, 0.5809250016845662, 0.5820723592750425, 0.5787471099920712, 0.5782937245963389, 0.5818159924290045, 0.5772111415863037, 0.5770083103722673, 0.5773706187400715, 0.578376999392419, 0.5762032721100784, 0.5760784675758382, 0.5764127669295644, 0.5727277852009306, 0.5716818238338481, 0.5725775251220558, 0.5743100988832592, 0.5724867430482776, 0.5713106821869124, 0.5706201174717932, 0.5725442792988082, 0.5724931112149867, 0.5702306179173271, 0.5690570026232298, 0.569575213319887, 0.5676967934541263, 0.567804945032125, 0.5670377683187242, 0.565734471893569, 0.5682441585755283, 0.5629535386878947, 0.5647344503622391, 0.5634475925427466, 0.562988650346513, 0.564786209323542, 0.5629670054608891, 0.5619222978912395, 0.561768563297706, 0.5610167909120802, 0.5610858426507572, 0.5606121365940021, 0.5581447640408669, 0.5587856430348342, 0.5568625272774115, 0.5560688008137835, 0.5570694350291719, 0.5551810046521629, 0.5553768567922639, 0.5550379885567559, 0.5551909154992762, 0.5533360053853291, 0.5552948519466369, 0.5519362227703498, 0.553003606434437, 0.5511464631654382, 0.551391219705101, 0.5513086118672276, 0.5507176116230043, 0.5496382372489144, 0.548940122935184, 0.5485982625142023, 0.5470030113933532, 0.5470707623293083, 0.546812889537191, 0.5449620110555716, 0.5455263140724926, 0.54432526578102, 0.5461113074930702, 0.5439418168248846, 0.5420032261509883, 0.5426717993366686, 0.5425731363658336, 0.5417386475940383, 0.5409902015998758, 0.5411032777491623, 0.5393945679755069, 0.5405505706624287, 0.5387246558659768, 0.5375631735899907, 0.5391802857238749, 0.5385331089903669, 0.5372761206898263, 0.5383883927895771, 0.5371792406893681, 0.536333799685243, 0.5361910126396634, 0.5358838942961964, 0.5352775599898362, 0.5350677362948575, 0.5356635788914957, 0.5335732249386589, 0.5340233737537208, 0.532200848345511, 0.5327803201791717, 0.5327731015559457, 0.5334281144426444, 0.5317896081180107, 0.5316453647161241, 0.5314306670088109, 0.5316899972879466, 0.5305213824843327, 0.5296615215174874, 0.5288369956055308, 0.5285796406792431, 0.5297204105834651, 0.5284296314567731, 0.5274015707052174, 0.5296021084475323, 0.5277117821905348, 0.529082273080097, 0.527788383527823, 0.5270261393006901, 0.52845838884028, 0.5292315034039299, 0.52572198595781, 0.5268986157285489, 0.5271285037361186, 0.5262999817284788, 0.5254172852368859, 0.5274654864295711, 0.526397116785127, 0.5252914073344491, 0.525484329962795, 0.5249664773785971, 0.5244791181429937, 0.5259394241865412, 0.5246530160671328, 0.5240655921015959, 0.5240480100236288, 0.5245563182727431, 0.5253050619670692, 0.5237608156230068, 0.5258506265113024, 0.5242474817325106, 0.5238483273563023, 0.5241466206586781, 0.524573271998222, 0.5246589922000399, 0.5237023841720932, 0.5235576888086996, 0.5241756854341605, 0.5243666113876715, 0.5234435347683708, 0.5245743167755726]\n",
            "train_acc_list_ls01 = [58.67019587083113, 86.5283218634198, 88.05505558496559, 89.0291159343568, 89.27051349920593, 90.03282159872948, 90.46691371095818, 90.93065113816834, 91.06617257808364, 91.55532027527792, 91.85812599258867, 92.08046585494971, 92.40444679724722, 92.55479089465325, 92.80677607199577, 92.91476971942826, 93.02699841185813, 93.34039174166226, 93.35309687665432, 93.48226574907359, 93.5352038115405, 93.51826363155108, 93.68766543144521, 93.84647961884595, 93.69613552143991, 93.99258867125464, 94.06670195870831, 94.06670195870831, 94.00317628374802, 94.24457384859714, 94.04129168872419, 94.1577554261514, 94.27210164107994, 94.29962943356273, 94.53890947591319, 94.43515087347804, 94.43303335097936, 94.51561672842774, 94.53467443091583, 94.58761249338274, 94.65325569084172, 94.65960825833774, 94.59820010587613, 94.76124933827423, 94.54526204340921, 94.78877713075701, 94.82477501323451, 94.78454208575967, 94.75701429327687, 94.83748014822658, 94.9158284806776, 94.83324510322922, 94.96876654314453, 94.93276866066702, 95.08311275807306, 95.01111699311805, 95.02593965060879, 94.98358920063525, 95.13393329804128, 94.99841185812599, 95.10428798305982, 95.1593435680254, 95.08311275807306, 95.23557437797777, 95.01746956061407, 95.12758073054526, 94.99417681312865, 95.22286924298571, 95.22922181048173, 95.33509793541556, 95.19110640550556, 95.2694547379566, 95.35203811540498, 95.28851244044468, 95.32662784542086, 95.22922181048173, 95.36897829539438, 95.35203811540498, 95.60402329274748, 95.34356802541026, 95.38168343038645, 95.53202752779248, 95.46214928533615, 95.46214928533615, 95.56379036527264, 95.5574377977766, 95.48967707781895, 95.46638433033351, 95.60825833774484, 95.59131815775542, 95.56167284277396, 95.79036527263102, 95.6019057702488, 95.66119640021175, 95.7564849126522, 95.65907887771307, 95.67390153520381, 95.79671784012704, 95.70989941768131, 95.73954473266278, 95.7289571201694, 95.82001058761249, 95.84330333509793, 95.92588671254632, 95.8094229751191, 96.04235044997353, 95.76707252514558, 95.87718369507677, 96.0359978824775, 95.87083112758073, 95.94706193753309, 96.01058761249338, 96.0359978824775, 96.06564319745897, 96.21386977236634, 95.93012175754367, 95.92376919004765, 96.08046585494971, 96.30915828480677, 96.25622022233986, 96.12069878242457, 96.22233986236104, 96.11011116993119, 96.08046585494971, 96.10164107993647, 96.22445738485972, 96.31551085230281, 96.29645314981472, 96.43620963472736, 96.3705664372684, 96.28798305982001, 96.44467972472208, 96.42562202223398, 96.4510322922181, 96.4340921122287, 96.4510322922181, 96.51244044467973, 96.48914769719428, 96.51032292218105, 96.57173107464267, 96.67125463208046, 96.49761778718899, 96.65219692959238, 96.52514557967179, 96.641609317099, 96.7305452620434, 96.88724192694548, 96.89994706193754, 96.70089994706194, 96.71995764955003, 96.71784012705135, 96.69666490206458, 96.72842773954473, 97.01852832186341, 96.8766543144521, 97.00370566437269, 96.857596611964, 96.98464796188459, 96.94865007940709, 97.02699841185813, 97.09052408681842, 97.00158814187401, 97.11381683430386, 97.16675489677078, 97.12652196929592, 97.04182106934886, 97.15404976177872, 97.21545791424033, 97.21122286924299, 97.31286394917946, 97.36791953414505, 97.31074642668078, 97.27051349920593, 97.29804129168872, 97.2641609317099, 97.35733192165166, 97.28533615669666, 97.23875066172577, 97.41662255161461, 97.39756484912652, 97.43356273160403, 97.46109052408681, 97.5267337215458, 97.54155637903652, 97.57755426151402, 97.48650079407093, 97.63896241397565, 97.571201694018, 97.67284277395447, 97.65802011646373, 97.63896241397565, 97.65590259396507, 97.75542615140286, 97.69825304393859, 97.77236633139228, 97.75754367390154, 97.80412916887242, 97.83377448385389, 97.83800952885125, 97.9142403388036, 97.9947061937533, 97.9417681312864, 98.0307040762308, 97.96506087877184, 98.04340921122287, 97.9777660137639, 98.10481736368449, 97.94600317628375, 98.11752249867655, 98.08364213869773, 98.14716781365802, 98.12599258867125, 98.15987294865008, 98.17257808364214, 98.2297511911064, 98.33350979354155, 98.30386447856009, 98.37162519851773, 98.32503970354685, 98.36315510852303, 98.42244573848598, 98.37162519851773, 98.39491794600318, 98.3843303335098, 98.42668078348332, 98.53890947591319, 98.52620434092113, 98.48808893594494, 98.54102699841185, 98.57914240338803, 98.55796717840127, 98.65113816834304, 98.58549497088407, 98.67019587083112, 98.71889888830069, 98.62572789835892, 98.63419798835362, 98.6913710958179, 98.69348861831656, 98.68925357331922, 98.75277924827951, 98.71889888830069, 98.76548438327157, 98.80571731074643, 98.77607199576495, 98.76548438327157, 98.86712546320804, 98.82901005823187, 98.92853361566966, 98.88194812069878, 98.89465325569084, 98.87559555320276, 98.92429857067232, 98.92641609317099, 98.9348861831657, 98.92429857067232, 98.98358920063525, 99.03017469560614, 99.04711487559555, 99.04076230809952, 99.03229221810481, 99.02805717310747, 99.0788777130757, 99.01535203811541, 99.070407623081, 99.03017469560614, 99.12122816304924, 99.11699311805188, 99.0619375330863, 99.04499735309687, 99.17204870301747, 99.11699311805188, 99.08099523557438, 99.12546320804658, 99.19745897300159, 99.08946532556908, 99.1233456855479, 99.14875595553202, 99.16357861302276, 99.16357861302276, 99.214399152991, 99.18475383800953, 99.19957649550027, 99.24827951296983, 99.23980942297511, 99.20592906299629, 99.19957649550027, 99.25886712546321, 99.16781365802012, 99.20381154049761, 99.26310217046056, 99.21863419798835, 99.23980942297511, 99.214399152991, 99.24616199047115, 99.26310217046056, 99.22286924298571, 99.23133933298041, 99.23980942297511, 99.20804658549497]\n",
            "test_loss_list_ls01 = [0.9763527608969632, 0.8990520341139213, 0.8541313792560615, 0.8443569307233773, 0.8140358822602852, 0.7620719197918387, 0.7747672682299334, 0.7820587874043221, 0.7488147593012043, 0.74338318845805, 0.7488887643112856, 0.7370110314850714, 0.7344583927416334, 0.7162326437001135, 0.7068874087988162, 0.7261397765547621, 0.7151892871833315, 0.7025225390406216, 0.7036699316665238, 0.707467323716949, 0.6879033396641413, 0.6946282985748029, 0.7083719507151959, 0.71129930545302, 0.7048738072900211, 0.6873342628572502, 0.690652506608589, 0.7116694491283566, 0.6835243029921663, 0.6950534953206193, 0.6873897986084807, 0.7023722202170128, 0.6911125478206896, 0.6748756810146219, 0.6881312777598699, 0.6675868802795223, 0.6946896922354605, 0.6867804568187863, 0.6794936578063404, 0.6808304415613997, 0.678563632217108, 0.6755960107434029, 0.6928771252141279, 0.6896743277708689, 0.6717938368811327, 0.6927369587561664, 0.6825296011041192, 0.675169273334391, 0.7037197631948134, 0.679401679950602, 0.6836349306737676, 0.6811569558054793, 0.6780343967325547, 0.6748819710577235, 0.6657580940746793, 0.6736946956199759, 0.6888990089589474, 0.6906907418194939, 0.6679097308832056, 0.6743272160198174, 0.6735651688832863, 0.678328550621575, 0.6674647626339221, 0.6830538292141521, 0.6740056922038397, 0.6773041469209334, 0.672011503109745, 0.6743662225849488, 0.6687080903964884, 0.6803400814533234, 0.6652025115840575, 0.6667958159072727, 0.6864659374251085, 0.6909454391867507, 0.6733984152475992, 0.6735112003835977, 0.6678190766011968, 0.6684016930122002, 0.6751790368089489, 0.6652155915896097, 0.6936036108755598, 0.6723233592860839, 0.6797854681225384, 0.683173773043296, 0.6654905799557181, 0.6775162967981077, 0.67307096573652, 0.6815068067288866, 0.6655554163689706, 0.6694277898938048, 0.6677195540830201, 0.6731911988234988, 0.6669743768140382, 0.6699657451872733, 0.6633660156352847, 0.6662144509016299, 0.6655387396321577, 0.6832156754007527, 0.6702279270279641, 0.6736428822372474, 0.6665987708404952, 0.6778715965794582, 0.6667458458858377, 0.6642423874022914, 0.6615659661737143, 0.676241689452938, 0.665988871864244, 0.6656564114724889, 0.6722669636501986, 0.6682107606354881, 0.6656324886808208, 0.659750259682244, 0.6680466366164824, 0.672413244843483, 0.6754223134587792, 0.6812622138098174, 0.6675607959429423, 0.6616823924522773, 0.6587090053979088, 0.6610343719229979, 0.6667233661693686, 0.6633848577153449, 0.6669244666894277, 0.6757207959890366, 0.673329346612388, 0.6697581839912078, 0.6627970837494906, 0.6647052984027302, 0.6642510598781062, 0.6649234741926193, 0.6649466063462052, 0.6628616361641416, 0.6631128963302163, 0.6627651230961669, 0.6645070694240869, 0.6678650554488686, 0.6625736244753295, 0.6674065598670174, 0.6620569810563443, 0.6639638487614837, 0.6681585069380555, 0.665404327359854, 0.6590355725265017, 0.6721230108363956, 0.668774929116754, 0.6617091064359627, 0.6549480367524951, 0.6696905879413381, 0.6606108493664685, 0.6625760264840781, 0.6621741658332301, 0.6664711263249902, 0.6571526667651009, 0.6640663529727974, 0.6565986632132063, 0.6741906308075961, 0.662264836769478, 0.6631299081970664, 0.6639281961263395, 0.6576764677085128, 0.6700584435579824, 0.6576102335079044, 0.6678198444492677, 0.657069601556834, 0.6624002699174133, 0.6627651520219504, 0.6552087763945261, 0.660720128346892, 0.6589535667615778, 0.6560295034272998, 0.66061951658305, 0.6562021538323047, 0.6605539885806102, 0.6519840505777621, 0.6570019534989899, 0.6594532067869224, 0.6613135507293776, 0.6547699427487803, 0.6516720377931408, 0.6657470683841145, 0.6586987609956779, 0.6597077168670356, 0.6546443095978569, 0.6556885654435438, 0.6604248337885913, 0.6510482897945479, 0.6635518705143648, 0.6549987562146842, 0.6590542804961111, 0.6536705333228204, 0.658341131958307, 0.6596238069674548, 0.6546890177563125, 0.6580831440640431, 0.6543162412503186, 0.6528611451971764, 0.6575218068034041, 0.6564911659441742, 0.657498747694726, 0.6608367588590173, 0.6592263795581519, 0.6529991068676406, 0.6519152355544707, 0.6554072566476523, 0.6567071760986366, 0.652612539190872, 0.651959583455441, 0.656812019500078, 0.6545038699519401, 0.6501418664759281, 0.6539727557523578, 0.6523146120940938, 0.6531565537055334, 0.6494225649272695, 0.6547924002595976, 0.6533287109113207, 0.6529751358663335, 0.6537856369041929, 0.6492795088127548, 0.6591676251561034, 0.652287455458267, 0.6525658351533553, 0.6558567154056886, 0.6574661208718431, 0.6545536477191776, 0.6505974139653, 0.6554256271497876, 0.6532190856980342, 0.653755819680644, 0.6513553442908269, 0.6496315206967148, 0.654107889416171, 0.6494756279622808, 0.6538745036312178, 0.6510765222357768, 0.651848007943116, 0.6484085035090353, 0.6509675009577882, 0.650976225733757, 0.6537330919036678, 0.6505422297061658, 0.6505131052405226, 0.6530923574578529, 0.6548510085718304, 0.6474404098356471, 0.6506215915376065, 0.6504216153247684, 0.6487272817130182, 0.6541164873861799, 0.6518888263141408, 0.6552283804790646, 0.6484981924295425, 0.6496508241284127, 0.6486084995900884, 0.650815913198041, 0.6517385507331175, 0.6555252650789186, 0.64858852297652, 0.6521191483034807, 0.6505728051358578, 0.6504743029089535, 0.6524077130883348, 0.6489105826499415, 0.6498247195108264, 0.6528866749768164, 0.6482817156642091, 0.6479750158740025, 0.6523512192800933, 0.651436497183407, 0.6492209992572373, 0.6455635925134023, 0.6482501687372432, 0.6447634311283336, 0.6477168366020801, 0.6498917706456839, 0.644418274362882, 0.6495792672914618, 0.6466710336652457, 0.6493661061805838, 0.6494104774559245, 0.6509454749962863, 0.64865055651057, 0.6519870451268028, 0.6514698357558718, 0.6480210104993746, 0.6515294184871748, 0.6496598594913295, 0.652405335914855, 0.6510159770647684, 0.647732233007749, 0.6466461212611666, 0.6473070564223271, 0.6494322311060101, 0.6495420079605252, 0.6454905850045821, 0.6494774274966296, 0.6482634529763577, 0.6467350762264401, 0.6513191867692798, 0.6539011992075864]\n",
            "test_acc_list_ls01 = [81.76859250153657, 84.90703749231714, 86.49354640442532, 86.35141364474492, 87.88414259373079, 89.95467117393977, 89.40150583896742, 88.8060848186847, 90.15442532267978, 90.38491087891825, 90.72295636140136, 90.65381069452981, 90.61923786109404, 91.62953288260603, 91.84849416103258, 91.19545175169023, 91.63337430854334, 92.09818684695759, 92.12891825445605, 91.99446834665028, 92.73970497848802, 92.53226797787339, 92.0059926244622, 91.93300553165335, 92.08666256914567, 92.72433927473878, 92.70513214505225, 91.69099569760294, 92.90872771972957, 92.42086662569146, 92.67440073755378, 92.2480024585126, 92.47464658881377, 93.19299323909036, 92.70513214505225, 93.49646588813768, 92.60141364474492, 92.76659496004918, 93.00860479409957, 93.08927473878303, 93.26213890596189, 93.25829748002458, 92.60525507068223, 92.7819606637984, 93.41579594345421, 92.390135218193, 92.98171481253841, 93.28902888752305, 92.12507682851874, 93.12768899815612, 92.95866625691457, 93.08159188690843, 93.0700676090965, 93.11616472034419, 93.63859864781807, 93.24677320221267, 92.71281499692687, 92.75122925629994, 93.51567301782421, 93.42732022126613, 93.41963736939152, 93.13921327596803, 93.46573448063921, 93.05086047940996, 93.22372464658882, 93.0700676090965, 93.25829748002458, 93.19683466502765, 93.50799016594961, 93.02397049784881, 93.63859864781807, 93.5579287031346, 92.86647203441917, 92.73970497848802, 93.37354025814382, 93.2659803318992, 93.58097725875845, 93.45036877688999, 93.32360172095882, 93.78841425937308, 92.6820835894284, 93.40427166564228, 92.95866625691457, 92.98939766441303, 93.74231714812538, 93.19299323909036, 93.29287031346036, 92.98939766441303, 93.68853718500307, 93.50030731407499, 93.53872157344806, 93.20067609096496, 93.51951444376152, 93.46957590657652, 93.73463429625077, 93.56945298094652, 93.78841425937308, 92.98939766441303, 93.42347879532882, 93.33128457283344, 93.58866011063307, 93.18146896127843, 93.73463429625077, 93.57329440688383, 93.81914566687155, 93.26213890596189, 93.6578057775046, 93.60018438844499, 93.35049170251997, 93.6078672403196, 93.6578057775046, 93.87676705593117, 93.69622003687769, 93.40427166564228, 93.28518746158574, 92.9240934234788, 93.4580516287646, 93.80762138905962, 93.89981561155501, 94.04578979717272, 93.63091579594345, 93.96896127842655, 93.6078672403196, 93.38506453595575, 93.28902888752305, 93.54256299938537, 93.73079287031346, 93.78457283343577, 93.71542716656423, 93.83066994468346, 93.6040258143823, 93.83451137062077, 93.66933005531654, 93.81530424093424, 93.76920712968654, 93.6539643515673, 93.76536570374923, 93.57713583282114, 93.75768285187462, 93.8421942224954, 93.53872157344806, 93.63475722188076, 93.9958512599877, 93.41579594345421, 93.59634296250768, 93.9420712968654, 94.07267977873387, 93.58866011063307, 93.93822987092808, 93.78841425937308, 93.93054701905348, 93.7077443146896, 94.05731407498463, 93.83835279655808, 94.11109403810694, 93.61939151813154, 93.93822987092808, 93.82298709280884, 93.77304855562384, 94.14950829748003, 93.64244007375538, 94.19560540872772, 93.67701290719116, 94.16871542716656, 93.8921327596804, 94.0419483712354, 94.1917639827904, 93.90749846342962, 94.02658266748617, 94.28011677934849, 94.15334972341734, 94.20712968653964, 94.06115550092194, 94.31084818684695, 94.04578979717272, 94.04963122311001, 93.94975414874001, 94.24554394591273, 94.40304240934235, 93.88829133374308, 94.12645974185618, 94.00737553779963, 94.29548248309773, 94.29548248309773, 93.98048555623848, 94.41072526121697, 93.97664413030117, 94.3300553165335, 94.22633681622618, 94.22633681622618, 94.16487400122925, 94.01889981561156, 94.3761524277812, 94.22633681622618, 94.32237246465888, 94.44145666871543, 94.16871542716656, 94.36462814996926, 94.31084818684695, 94.11877688998156, 94.05731407498463, 94.25322679778733, 94.45682237246466, 94.19560540872772, 94.33773816840811, 94.3799938537185, 94.41840811309157, 94.21097111247695, 94.40304240934235, 94.53365089121081, 94.46834665027659, 94.45682237246466, 94.3761524277812, 94.48755377996312, 94.35310387215735, 94.47602950215119, 94.45682237246466, 94.38383527965581, 94.66425937307929, 94.16871542716656, 94.50291948371235, 94.42609096496619, 94.46450522433928, 94.31468961278426, 94.49523663183774, 94.59511370620774, 94.42609096496619, 94.58358942839583, 94.49523663183774, 94.55669944683467, 94.54901659496005, 94.51828518746159, 94.6220036877689, 94.51444376152428, 94.58358942839583, 94.6220036877689, 94.67962507682851, 94.64889366933005, 94.65273509526736, 94.52980946527352, 94.63736939151813, 94.5759065765212, 94.5259680393362, 94.53365089121081, 94.84096496619546, 94.64121081745544, 94.70651505838967, 94.78334357713584, 94.5720651505839, 94.5759065765212, 94.45298094652735, 94.73340503995082, 94.72572218807622, 94.75261216963737, 94.65657652120467, 94.69499078057775, 94.56822372464659, 94.78334357713584, 94.64889366933005, 94.64121081745544, 94.75645359557468, 94.5720651505839, 94.75261216963737, 94.74108789182544, 94.60663798401967, 94.76029502151198, 94.82175783650891, 94.67194222495391, 94.74492931776275, 94.68730792870313, 94.99078057775046, 94.77566072526122, 94.92163491087892, 94.82944068838353, 94.67962507682851, 94.94084204056546, 94.82175783650891, 94.85248924400737, 94.74492931776275, 94.80255070682237, 94.73724646588813, 94.84864781807006, 94.64889366933005, 94.79486785494775, 94.90242778119237, 94.69114935464044, 94.87937922556853, 94.67962507682851, 94.7180393362016, 94.82944068838353, 94.93315918869084, 94.82559926244622, 94.74877074370006, 94.79102642901044, 94.94852489244008, 94.75645359557468, 94.8140749846343, 94.76029502151198, 94.65273509526736, 94.71419791026429]\n"
          ]
        }
      ]
    }
  ]
}