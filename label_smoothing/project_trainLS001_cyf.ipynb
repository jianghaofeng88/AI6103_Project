{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfGrai_Qt7Ny"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgAiImV0uURP",
        "outputId": "5990e7d8-6a3a-42f5-f6e3-ca3d41ee8ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ./data\\train_32x32.mat\n",
            "Using downloaded and verified file: ./data\\test_32x32.mat\n"
          ]
        }
      ],
      "source": [
        "# these are commonly used data augmentations\n",
        "# random cropping and random horizontal flip\n",
        "# lastly, we normalize each channel into zero mean and unit standard deviation\n",
        "transform_train = transforms.Compose([\n",
        "    #transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.SVHN),\n",
        "    transforms.ToTensor(),\n",
        "    \n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='train', download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(\n",
        "    root='./data', split='test', download=True, transform=transform_test)\n",
        "\n",
        "# we can use a larger batch size during test, because we do not save \n",
        "# intermediate variables for gradient computation, which leaves more memory\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO2fleA52Aex",
        "outputId": "e17defdc-b229-4d91-b1e3-eb9cf9d51b90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "73257 26032\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
        "print(len(trainset), len(testset))\n",
        "print(trainset[4][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hldipDVsv-Jt"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "def train(epoch, net, criterion, trainloader, scheduler):\n",
        "    device = 'cuda'\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if (batch_idx+1) % 50 == 0:\n",
        "          print(\"iteration : %3d, loss : %0.4f, accuracy : %2.2f\" % (batch_idx+1, train_loss/(batch_idx+1), 100.*correct/total))\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss/(batch_idx+1), 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgyCI0U08i2h"
      },
      "source": [
        "Test performance on the test set. Note the use of `torch.inference_mode()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkooK-hQu4a6"
      },
      "outputs": [],
      "source": [
        "def test(epoch, net, criterion, testloader):\n",
        "    device = 'cuda'\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return test_loss/(batch_idx+1), 100.*correct/total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEj8J7xqwAxD"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(net, acc, epoch):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'net': net.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/ckpt.pth')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlCAjBEWwXNo"
      },
      "outputs": [],
      "source": [
        "# defining resnet models\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        #out = F.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        #out = F.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        # four stages with three downsampling\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        #out = F.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
        "\n",
        "\n",
        "def test_resnet18():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1, 3, 32, 32))\n",
        "    print(y.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "odLHjfkTzzaJ",
        "outputId": "6b60ac13-f9cd-4fde-dbb6-0fe8ee0b8a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3234. 8926. 6868. 5501. 4828. 4429. 3753. 3516. 3233. 2937.]\n",
            "[0.06848068 0.18901006 0.14543145 0.11648491 0.10223399 0.09378507\n",
            " 0.07947062 0.07445209 0.0684595  0.06219164]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# partition the trainset\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "new_trainset, validationset= torch.utils.data.random_split(trainset,\n",
        "  [len(trainset)-len(testset), len(testset)], generator=torch.Generator().manual_seed(0))\n",
        "class_freq = np.zeros(10)\n",
        "for i in range(len(new_trainset)):\n",
        "  class_freq[new_trainset[i][1]]+=1\n",
        "class_prop = class_freq/(len(new_trainset))\n",
        "print(class_freq)\n",
        "print(class_prop)\n",
        "\n",
        "# plot the proportion\n",
        "ax = plt.figure(figsize=(10,5)).add_subplot(111)\n",
        "plt.plot(list(classes),class_prop, '.', ms=8)\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Proportion\")\n",
        "for i,j in zip(list(classes),class_prop):\n",
        "    ax.annotate(i+'\\n'+str(round(j*100,3))+'%',xy=(i,j))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGDVy1s3i7J",
        "outputId": "fb7d1c3c-fe4d-4df3-fafa-04c2f3717fdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "47225.0"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(class_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArgupDVRwB8i",
        "outputId": "875fed7e-5549-4ef3-83f5-b0e615b8fab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1\n",
            "iteration :  50, loss : 2.3267, accuracy : 18.98\n",
            "iteration : 100, loss : 2.1630, accuracy : 24.23\n",
            "iteration : 150, loss : 1.8876, accuracy : 34.57\n",
            "iteration : 200, loss : 1.6330, accuracy : 44.11\n",
            "iteration : 250, loss : 1.4436, accuracy : 51.19\n",
            "iteration : 300, loss : 1.3014, accuracy : 56.48\n",
            "iteration : 350, loss : 1.1979, accuracy : 60.37\n",
            "Epoch :   1, training loss : 1.1663, training accuracy : 61.55, test loss : 0.8728, test accuracy : 73.36\n",
            "\n",
            "Epoch: 2\n",
            "iteration :  50, loss : 0.5272, accuracy : 85.39\n",
            "iteration : 100, loss : 0.5151, accuracy : 85.61\n",
            "iteration : 150, loss : 0.5161, accuracy : 85.58\n",
            "iteration : 200, loss : 0.5127, accuracy : 85.65\n",
            "iteration : 250, loss : 0.5107, accuracy : 85.76\n",
            "iteration : 300, loss : 0.5050, accuracy : 85.93\n",
            "iteration : 350, loss : 0.5029, accuracy : 85.96\n",
            "Epoch :   2, training loss : 0.5021, training accuracy : 86.00, test loss : 0.6056, test accuracy : 82.66\n",
            "\n",
            "Epoch: 3\n",
            "iteration :  50, loss : 0.4601, accuracy : 87.45\n",
            "iteration : 100, loss : 0.4679, accuracy : 87.22\n",
            "iteration : 150, loss : 0.4676, accuracy : 87.25\n",
            "iteration : 200, loss : 0.4628, accuracy : 87.44\n",
            "iteration : 250, loss : 0.4564, accuracy : 87.64\n",
            "iteration : 300, loss : 0.4579, accuracy : 87.54\n",
            "iteration : 350, loss : 0.4547, accuracy : 87.64\n",
            "Epoch :   3, training loss : 0.4535, training accuracy : 87.70, test loss : 0.5480, test accuracy : 85.01\n",
            "\n",
            "Epoch: 4\n",
            "iteration :  50, loss : 0.4299, accuracy : 88.59\n",
            "iteration : 100, loss : 0.4270, accuracy : 88.77\n",
            "iteration : 150, loss : 0.4281, accuracy : 88.74\n",
            "iteration : 200, loss : 0.4280, accuracy : 88.70\n",
            "iteration : 250, loss : 0.4297, accuracy : 88.60\n",
            "iteration : 300, loss : 0.4284, accuracy : 88.71\n",
            "iteration : 350, loss : 0.4278, accuracy : 88.75\n",
            "Epoch :   4, training loss : 0.4270, training accuracy : 88.80, test loss : 0.4766, test accuracy : 86.89\n",
            "\n",
            "Epoch: 5\n",
            "iteration :  50, loss : 0.4029, accuracy : 89.77\n",
            "iteration : 100, loss : 0.4072, accuracy : 89.62\n",
            "iteration : 150, loss : 0.4119, accuracy : 89.44\n",
            "iteration : 200, loss : 0.4167, accuracy : 89.22\n",
            "iteration : 250, loss : 0.4162, accuracy : 89.24\n",
            "iteration : 300, loss : 0.4174, accuracy : 89.18\n",
            "iteration : 350, loss : 0.4148, accuracy : 89.28\n",
            "Epoch :   5, training loss : 0.4143, training accuracy : 89.31, test loss : 0.5403, test accuracy : 84.68\n",
            "\n",
            "Epoch: 6\n",
            "iteration :  50, loss : 0.3862, accuracy : 90.23\n",
            "iteration : 100, loss : 0.3930, accuracy : 90.05\n",
            "iteration : 150, loss : 0.3962, accuracy : 89.90\n",
            "iteration : 200, loss : 0.3962, accuracy : 89.82\n",
            "iteration : 250, loss : 0.3967, accuracy : 89.78\n",
            "iteration : 300, loss : 0.3977, accuracy : 89.79\n",
            "iteration : 350, loss : 0.3957, accuracy : 89.84\n",
            "Epoch :   6, training loss : 0.3950, training accuracy : 89.91, test loss : 0.4793, test accuracy : 87.41\n",
            "\n",
            "Epoch: 7\n",
            "iteration :  50, loss : 0.3727, accuracy : 90.58\n",
            "iteration : 100, loss : 0.3852, accuracy : 90.40\n",
            "iteration : 150, loss : 0.3780, accuracy : 90.68\n",
            "iteration : 200, loss : 0.3806, accuracy : 90.57\n",
            "iteration : 250, loss : 0.3799, accuracy : 90.58\n",
            "iteration : 300, loss : 0.3784, accuracy : 90.62\n",
            "iteration : 350, loss : 0.3807, accuracy : 90.52\n",
            "Epoch :   7, training loss : 0.3799, training accuracy : 90.53, test loss : 0.4171, test accuracy : 89.33\n",
            "\n",
            "Epoch: 8\n",
            "iteration :  50, loss : 0.3792, accuracy : 90.33\n",
            "iteration : 100, loss : 0.3751, accuracy : 90.55\n",
            "iteration : 150, loss : 0.3773, accuracy : 90.49\n",
            "iteration : 200, loss : 0.3703, accuracy : 90.66\n",
            "iteration : 250, loss : 0.3722, accuracy : 90.61\n",
            "iteration : 300, loss : 0.3705, accuracy : 90.72\n",
            "iteration : 350, loss : 0.3679, accuracy : 90.79\n",
            "Epoch :   8, training loss : 0.3684, training accuracy : 90.76, test loss : 0.4383, test accuracy : 88.23\n",
            "\n",
            "Epoch: 9\n",
            "iteration :  50, loss : 0.3764, accuracy : 90.52\n",
            "iteration : 100, loss : 0.3692, accuracy : 90.72\n",
            "iteration : 150, loss : 0.3693, accuracy : 90.78\n",
            "iteration : 200, loss : 0.3682, accuracy : 90.84\n",
            "iteration : 250, loss : 0.3653, accuracy : 90.95\n",
            "iteration : 300, loss : 0.3655, accuracy : 90.91\n",
            "iteration : 350, loss : 0.3638, accuracy : 90.93\n",
            "Epoch :   9, training loss : 0.3628, training accuracy : 90.95, test loss : 0.3853, test accuracy : 90.41\n",
            "\n",
            "Epoch: 10\n",
            "iteration :  50, loss : 0.3229, accuracy : 91.91\n",
            "iteration : 100, loss : 0.3426, accuracy : 91.57\n",
            "iteration : 150, loss : 0.3398, accuracy : 91.69\n",
            "iteration : 200, loss : 0.3408, accuracy : 91.64\n",
            "iteration : 250, loss : 0.3413, accuracy : 91.63\n",
            "iteration : 300, loss : 0.3421, accuracy : 91.66\n",
            "iteration : 350, loss : 0.3427, accuracy : 91.64\n",
            "Epoch :  10, training loss : 0.3437, training accuracy : 91.64, test loss : 0.3709, test accuracy : 90.69\n",
            "\n",
            "Epoch: 11\n",
            "iteration :  50, loss : 0.3448, accuracy : 91.77\n",
            "iteration : 100, loss : 0.3349, accuracy : 91.98\n",
            "iteration : 150, loss : 0.3341, accuracy : 91.94\n",
            "iteration : 200, loss : 0.3360, accuracy : 91.89\n",
            "iteration : 250, loss : 0.3368, accuracy : 91.91\n",
            "iteration : 300, loss : 0.3356, accuracy : 91.95\n",
            "iteration : 350, loss : 0.3348, accuracy : 91.95\n",
            "Epoch :  11, training loss : 0.3357, training accuracy : 91.90, test loss : 0.3540, test accuracy : 91.46\n",
            "\n",
            "Epoch: 12\n",
            "iteration :  50, loss : 0.3228, accuracy : 92.23\n",
            "iteration : 100, loss : 0.3273, accuracy : 92.17\n",
            "iteration : 150, loss : 0.3313, accuracy : 91.91\n",
            "iteration : 200, loss : 0.3330, accuracy : 91.88\n",
            "iteration : 250, loss : 0.3316, accuracy : 91.92\n",
            "iteration : 300, loss : 0.3310, accuracy : 91.95\n",
            "iteration : 350, loss : 0.3282, accuracy : 92.01\n",
            "Epoch :  12, training loss : 0.3276, training accuracy : 92.03, test loss : 0.3873, test accuracy : 90.24\n",
            "\n",
            "Epoch: 13\n",
            "iteration :  50, loss : 0.3262, accuracy : 92.05\n",
            "iteration : 100, loss : 0.3115, accuracy : 92.63\n",
            "iteration : 150, loss : 0.3126, accuracy : 92.65\n",
            "iteration : 200, loss : 0.3135, accuracy : 92.68\n",
            "iteration : 250, loss : 0.3156, accuracy : 92.62\n",
            "iteration : 300, loss : 0.3155, accuracy : 92.59\n",
            "iteration : 350, loss : 0.3178, accuracy : 92.50\n",
            "Epoch :  13, training loss : 0.3187, training accuracy : 92.49, test loss : 0.3919, test accuracy : 90.10\n",
            "\n",
            "Epoch: 14\n",
            "iteration :  50, loss : 0.3013, accuracy : 92.89\n",
            "iteration : 100, loss : 0.3050, accuracy : 92.71\n",
            "iteration : 150, loss : 0.3022, accuracy : 92.91\n",
            "iteration : 200, loss : 0.3036, accuracy : 92.91\n",
            "iteration : 250, loss : 0.3069, accuracy : 92.80\n",
            "iteration : 300, loss : 0.3104, accuracy : 92.68\n",
            "iteration : 350, loss : 0.3087, accuracy : 92.71\n",
            "Epoch :  14, training loss : 0.3096, training accuracy : 92.71, test loss : 0.3569, test accuracy : 91.12\n",
            "\n",
            "Epoch: 15\n",
            "iteration :  50, loss : 0.3049, accuracy : 92.81\n",
            "iteration : 100, loss : 0.3016, accuracy : 92.94\n",
            "iteration : 150, loss : 0.3055, accuracy : 92.85\n",
            "iteration : 200, loss : 0.3049, accuracy : 92.90\n",
            "iteration : 250, loss : 0.3047, accuracy : 92.92\n",
            "iteration : 300, loss : 0.3038, accuracy : 92.95\n",
            "iteration : 350, loss : 0.3047, accuracy : 92.92\n",
            "Epoch :  15, training loss : 0.3058, training accuracy : 92.91, test loss : 0.3321, test accuracy : 92.06\n",
            "\n",
            "Epoch: 16\n",
            "iteration :  50, loss : 0.3090, accuracy : 92.83\n",
            "iteration : 100, loss : 0.2970, accuracy : 93.12\n",
            "iteration : 150, loss : 0.2974, accuracy : 93.07\n",
            "iteration : 200, loss : 0.3058, accuracy : 92.79\n",
            "iteration : 250, loss : 0.3071, accuracy : 92.75\n",
            "iteration : 300, loss : 0.3065, accuracy : 92.78\n",
            "iteration : 350, loss : 0.3039, accuracy : 92.87\n",
            "Epoch :  16, training loss : 0.3029, training accuracy : 92.92, test loss : 0.3597, test accuracy : 91.25\n",
            "\n",
            "Epoch: 17\n",
            "iteration :  50, loss : 0.2795, accuracy : 93.58\n",
            "iteration : 100, loss : 0.2785, accuracy : 93.62\n",
            "iteration : 150, loss : 0.2844, accuracy : 93.34\n",
            "iteration : 200, loss : 0.2851, accuracy : 93.36\n",
            "iteration : 250, loss : 0.2872, accuracy : 93.33\n",
            "iteration : 300, loss : 0.2922, accuracy : 93.20\n",
            "iteration : 350, loss : 0.2917, accuracy : 93.19\n",
            "Epoch :  17, training loss : 0.2925, training accuracy : 93.20, test loss : 0.3319, test accuracy : 92.20\n",
            "\n",
            "Epoch: 18\n",
            "iteration :  50, loss : 0.2910, accuracy : 93.19\n",
            "iteration : 100, loss : 0.2881, accuracy : 93.33\n",
            "iteration : 150, loss : 0.2951, accuracy : 93.09\n",
            "iteration : 200, loss : 0.2955, accuracy : 93.15\n",
            "iteration : 250, loss : 0.2902, accuracy : 93.28\n",
            "iteration : 300, loss : 0.2887, accuracy : 93.34\n",
            "iteration : 350, loss : 0.2897, accuracy : 93.33\n",
            "Epoch :  18, training loss : 0.2899, training accuracy : 93.34, test loss : 0.3337, test accuracy : 91.94\n",
            "\n",
            "Epoch: 19\n",
            "iteration :  50, loss : 0.2611, accuracy : 94.16\n",
            "iteration : 100, loss : 0.2711, accuracy : 93.91\n",
            "iteration : 150, loss : 0.2793, accuracy : 93.73\n",
            "iteration : 200, loss : 0.2816, accuracy : 93.64\n",
            "iteration : 250, loss : 0.2864, accuracy : 93.49\n",
            "iteration : 300, loss : 0.2881, accuracy : 93.39\n",
            "iteration : 350, loss : 0.2875, accuracy : 93.39\n",
            "Epoch :  19, training loss : 0.2870, training accuracy : 93.41, test loss : 0.3707, test accuracy : 90.75\n",
            "\n",
            "Epoch: 20\n",
            "iteration :  50, loss : 0.2784, accuracy : 93.64\n",
            "iteration : 100, loss : 0.2734, accuracy : 93.80\n",
            "iteration : 150, loss : 0.2715, accuracy : 93.94\n",
            "iteration : 200, loss : 0.2786, accuracy : 93.76\n",
            "iteration : 250, loss : 0.2771, accuracy : 93.82\n",
            "iteration : 300, loss : 0.2767, accuracy : 93.81\n",
            "iteration : 350, loss : 0.2794, accuracy : 93.71\n",
            "Epoch :  20, training loss : 0.2806, training accuracy : 93.68, test loss : 0.3292, test accuracy : 91.99\n",
            "\n",
            "Epoch: 21\n",
            "iteration :  50, loss : 0.2605, accuracy : 94.25\n",
            "iteration : 100, loss : 0.2675, accuracy : 94.07\n",
            "iteration : 150, loss : 0.2677, accuracy : 94.07\n",
            "iteration : 200, loss : 0.2763, accuracy : 93.86\n",
            "iteration : 250, loss : 0.2785, accuracy : 93.81\n",
            "iteration : 300, loss : 0.2780, accuracy : 93.84\n",
            "iteration : 350, loss : 0.2779, accuracy : 93.82\n",
            "Epoch :  21, training loss : 0.2768, training accuracy : 93.83, test loss : 0.3199, test accuracy : 92.47\n",
            "\n",
            "Epoch: 22\n",
            "iteration :  50, loss : 0.2615, accuracy : 94.09\n",
            "iteration : 100, loss : 0.2725, accuracy : 93.91\n",
            "iteration : 150, loss : 0.2692, accuracy : 93.98\n",
            "iteration : 200, loss : 0.2711, accuracy : 93.88\n",
            "iteration : 250, loss : 0.2715, accuracy : 93.88\n",
            "iteration : 300, loss : 0.2737, accuracy : 93.84\n",
            "iteration : 350, loss : 0.2748, accuracy : 93.81\n",
            "Epoch :  22, training loss : 0.2753, training accuracy : 93.83, test loss : 0.3231, test accuracy : 92.36\n",
            "\n",
            "Epoch: 23\n",
            "iteration :  50, loss : 0.2725, accuracy : 93.97\n",
            "iteration : 100, loss : 0.2657, accuracy : 94.02\n",
            "iteration : 150, loss : 0.2644, accuracy : 94.06\n",
            "iteration : 200, loss : 0.2670, accuracy : 94.03\n",
            "iteration : 250, loss : 0.2704, accuracy : 93.92\n",
            "iteration : 300, loss : 0.2701, accuracy : 93.90\n",
            "iteration : 350, loss : 0.2696, accuracy : 93.93\n",
            "Epoch :  23, training loss : 0.2695, training accuracy : 93.91, test loss : 0.3321, test accuracy : 92.15\n",
            "\n",
            "Epoch: 24\n",
            "iteration :  50, loss : 0.2693, accuracy : 93.81\n",
            "iteration : 100, loss : 0.2680, accuracy : 93.90\n",
            "iteration : 150, loss : 0.2635, accuracy : 94.04\n",
            "iteration : 200, loss : 0.2645, accuracy : 94.07\n",
            "iteration : 250, loss : 0.2650, accuracy : 94.12\n",
            "iteration : 300, loss : 0.2669, accuracy : 94.05\n",
            "iteration : 350, loss : 0.2666, accuracy : 94.09\n",
            "Epoch :  24, training loss : 0.2676, training accuracy : 94.07, test loss : 0.3089, test accuracy : 92.83\n",
            "\n",
            "Epoch: 25\n",
            "iteration :  50, loss : 0.2561, accuracy : 94.17\n",
            "iteration : 100, loss : 0.2542, accuracy : 94.43\n",
            "iteration : 150, loss : 0.2550, accuracy : 94.36\n",
            "iteration : 200, loss : 0.2577, accuracy : 94.25\n",
            "iteration : 250, loss : 0.2605, accuracy : 94.18\n",
            "iteration : 300, loss : 0.2593, accuracy : 94.24\n",
            "iteration : 350, loss : 0.2608, accuracy : 94.20\n",
            "Epoch :  25, training loss : 0.2630, training accuracy : 94.14, test loss : 0.3158, test accuracy : 92.63\n",
            "\n",
            "Epoch: 26\n",
            "iteration :  50, loss : 0.2483, accuracy : 94.77\n",
            "iteration : 100, loss : 0.2593, accuracy : 94.40\n",
            "iteration : 150, loss : 0.2576, accuracy : 94.44\n",
            "iteration : 200, loss : 0.2569, accuracy : 94.49\n",
            "iteration : 250, loss : 0.2603, accuracy : 94.39\n",
            "iteration : 300, loss : 0.2599, accuracy : 94.38\n",
            "iteration : 350, loss : 0.2622, accuracy : 94.30\n",
            "Epoch :  26, training loss : 0.2626, training accuracy : 94.29, test loss : 0.3062, test accuracy : 92.76\n",
            "\n",
            "Epoch: 27\n",
            "iteration :  50, loss : 0.2438, accuracy : 94.48\n",
            "iteration : 100, loss : 0.2525, accuracy : 94.46\n",
            "iteration : 150, loss : 0.2517, accuracy : 94.52\n",
            "iteration : 200, loss : 0.2558, accuracy : 94.40\n",
            "iteration : 250, loss : 0.2564, accuracy : 94.42\n",
            "iteration : 300, loss : 0.2573, accuracy : 94.38\n",
            "iteration : 350, loss : 0.2558, accuracy : 94.42\n",
            "Epoch :  27, training loss : 0.2565, training accuracy : 94.40, test loss : 0.3120, test accuracy : 92.62\n",
            "\n",
            "Epoch: 28\n",
            "iteration :  50, loss : 0.2409, accuracy : 94.94\n",
            "iteration : 100, loss : 0.2506, accuracy : 94.44\n",
            "iteration : 150, loss : 0.2544, accuracy : 94.28\n",
            "iteration : 200, loss : 0.2613, accuracy : 94.03\n",
            "iteration : 250, loss : 0.2600, accuracy : 94.16\n",
            "iteration : 300, loss : 0.2591, accuracy : 94.22\n",
            "iteration : 350, loss : 0.2595, accuracy : 94.20\n",
            "Epoch :  28, training loss : 0.2600, training accuracy : 94.18, test loss : 0.3078, test accuracy : 92.87\n",
            "\n",
            "Epoch: 29\n",
            "iteration :  50, loss : 0.2593, accuracy : 94.36\n",
            "iteration : 100, loss : 0.2523, accuracy : 94.56\n",
            "iteration : 150, loss : 0.2531, accuracy : 94.45\n",
            "iteration : 200, loss : 0.2524, accuracy : 94.50\n",
            "iteration : 250, loss : 0.2557, accuracy : 94.34\n",
            "iteration : 300, loss : 0.2553, accuracy : 94.35\n",
            "iteration : 350, loss : 0.2565, accuracy : 94.33\n",
            "Epoch :  29, training loss : 0.2580, training accuracy : 94.27, test loss : 0.3190, test accuracy : 92.50\n",
            "\n",
            "Epoch: 30\n",
            "iteration :  50, loss : 0.2406, accuracy : 94.92\n",
            "iteration : 100, loss : 0.2484, accuracy : 94.86\n",
            "iteration : 150, loss : 0.2501, accuracy : 94.76\n",
            "iteration : 200, loss : 0.2479, accuracy : 94.77\n",
            "iteration : 250, loss : 0.2495, accuracy : 94.67\n",
            "iteration : 300, loss : 0.2517, accuracy : 94.58\n",
            "iteration : 350, loss : 0.2547, accuracy : 94.46\n",
            "Epoch :  30, training loss : 0.2547, training accuracy : 94.44, test loss : 0.3222, test accuracy : 92.39\n",
            "\n",
            "Epoch: 31\n",
            "iteration :  50, loss : 0.2436, accuracy : 94.73\n",
            "iteration : 100, loss : 0.2468, accuracy : 94.52\n",
            "iteration : 150, loss : 0.2478, accuracy : 94.49\n",
            "iteration : 200, loss : 0.2503, accuracy : 94.54\n",
            "iteration : 250, loss : 0.2540, accuracy : 94.40\n",
            "iteration : 300, loss : 0.2533, accuracy : 94.39\n",
            "iteration : 350, loss : 0.2541, accuracy : 94.34\n",
            "Epoch :  31, training loss : 0.2543, training accuracy : 94.35, test loss : 0.3253, test accuracy : 92.21\n",
            "\n",
            "Epoch: 32\n",
            "iteration :  50, loss : 0.2289, accuracy : 95.19\n",
            "iteration : 100, loss : 0.2343, accuracy : 95.12\n",
            "iteration : 150, loss : 0.2428, accuracy : 94.85\n",
            "iteration : 200, loss : 0.2458, accuracy : 94.78\n",
            "iteration : 250, loss : 0.2458, accuracy : 94.76\n",
            "iteration : 300, loss : 0.2489, accuracy : 94.65\n",
            "iteration : 350, loss : 0.2473, accuracy : 94.71\n",
            "Epoch :  32, training loss : 0.2493, training accuracy : 94.64, test loss : 0.3272, test accuracy : 92.37\n",
            "\n",
            "Epoch: 33\n",
            "iteration :  50, loss : 0.2423, accuracy : 94.75\n",
            "iteration : 100, loss : 0.2413, accuracy : 94.84\n",
            "iteration : 150, loss : 0.2436, accuracy : 94.75\n",
            "iteration : 200, loss : 0.2427, accuracy : 94.79\n",
            "iteration : 250, loss : 0.2450, accuracy : 94.74\n",
            "iteration : 300, loss : 0.2463, accuracy : 94.74\n",
            "iteration : 350, loss : 0.2464, accuracy : 94.75\n",
            "Epoch :  33, training loss : 0.2469, training accuracy : 94.71, test loss : 0.3090, test accuracy : 92.77\n",
            "\n",
            "Epoch: 34\n",
            "iteration :  50, loss : 0.2414, accuracy : 94.78\n",
            "iteration : 100, loss : 0.2349, accuracy : 95.05\n",
            "iteration : 150, loss : 0.2395, accuracy : 94.89\n",
            "iteration : 200, loss : 0.2413, accuracy : 94.87\n",
            "iteration : 250, loss : 0.2426, accuracy : 94.84\n",
            "iteration : 300, loss : 0.2450, accuracy : 94.76\n",
            "iteration : 350, loss : 0.2482, accuracy : 94.63\n",
            "Epoch :  34, training loss : 0.2484, training accuracy : 94.62, test loss : 0.2866, test accuracy : 93.52\n",
            "\n",
            "Epoch: 35\n",
            "iteration :  50, loss : 0.2266, accuracy : 95.36\n",
            "iteration : 100, loss : 0.2331, accuracy : 95.13\n",
            "iteration : 150, loss : 0.2361, accuracy : 95.10\n",
            "iteration : 200, loss : 0.2359, accuracy : 95.03\n",
            "iteration : 250, loss : 0.2384, accuracy : 94.97\n",
            "iteration : 300, loss : 0.2413, accuracy : 94.91\n",
            "iteration : 350, loss : 0.2422, accuracy : 94.89\n",
            "Epoch :  35, training loss : 0.2426, training accuracy : 94.88, test loss : 0.3019, test accuracy : 93.15\n",
            "\n",
            "Epoch: 36\n",
            "iteration :  50, loss : 0.2442, accuracy : 94.83\n",
            "iteration : 100, loss : 0.2378, accuracy : 94.97\n",
            "iteration : 150, loss : 0.2403, accuracy : 94.90\n",
            "iteration : 200, loss : 0.2413, accuracy : 94.89\n",
            "iteration : 250, loss : 0.2425, accuracy : 94.84\n",
            "iteration : 300, loss : 0.2440, accuracy : 94.78\n",
            "iteration : 350, loss : 0.2454, accuracy : 94.73\n",
            "Epoch :  36, training loss : 0.2452, training accuracy : 94.73, test loss : 0.2982, test accuracy : 93.07\n",
            "\n",
            "Epoch: 37\n",
            "iteration :  50, loss : 0.2305, accuracy : 95.06\n",
            "iteration : 100, loss : 0.2301, accuracy : 95.15\n",
            "iteration : 150, loss : 0.2350, accuracy : 94.92\n",
            "iteration : 200, loss : 0.2377, accuracy : 94.86\n",
            "iteration : 250, loss : 0.2400, accuracy : 94.82\n",
            "iteration : 300, loss : 0.2409, accuracy : 94.77\n",
            "iteration : 350, loss : 0.2430, accuracy : 94.69\n",
            "Epoch :  37, training loss : 0.2436, training accuracy : 94.67, test loss : 0.3111, test accuracy : 92.74\n",
            "\n",
            "Epoch: 38\n",
            "iteration :  50, loss : 0.2320, accuracy : 95.20\n",
            "iteration : 100, loss : 0.2339, accuracy : 95.19\n",
            "iteration : 150, loss : 0.2371, accuracy : 94.99\n",
            "iteration : 200, loss : 0.2384, accuracy : 94.93\n",
            "iteration : 250, loss : 0.2400, accuracy : 94.91\n",
            "iteration : 300, loss : 0.2438, accuracy : 94.76\n",
            "iteration : 350, loss : 0.2429, accuracy : 94.79\n",
            "Epoch :  38, training loss : 0.2434, training accuracy : 94.77, test loss : 0.3018, test accuracy : 93.28\n",
            "\n",
            "Epoch: 39\n",
            "iteration :  50, loss : 0.2354, accuracy : 95.22\n",
            "iteration : 100, loss : 0.2367, accuracy : 94.98\n",
            "iteration : 150, loss : 0.2366, accuracy : 95.08\n",
            "iteration : 200, loss : 0.2350, accuracy : 95.06\n",
            "iteration : 250, loss : 0.2363, accuracy : 94.99\n",
            "iteration : 300, loss : 0.2389, accuracy : 94.89\n",
            "iteration : 350, loss : 0.2419, accuracy : 94.78\n",
            "Epoch :  39, training loss : 0.2426, training accuracy : 94.77, test loss : 0.2958, test accuracy : 93.37\n",
            "\n",
            "Epoch: 40\n",
            "iteration :  50, loss : 0.2335, accuracy : 94.88\n",
            "iteration : 100, loss : 0.2348, accuracy : 95.00\n",
            "iteration : 150, loss : 0.2363, accuracy : 94.99\n",
            "iteration : 200, loss : 0.2393, accuracy : 94.91\n",
            "iteration : 250, loss : 0.2384, accuracy : 94.97\n",
            "iteration : 300, loss : 0.2413, accuracy : 94.90\n",
            "iteration : 350, loss : 0.2406, accuracy : 94.90\n",
            "Epoch :  40, training loss : 0.2408, training accuracy : 94.89, test loss : 0.2979, test accuracy : 93.36\n",
            "\n",
            "Epoch: 41\n",
            "iteration :  50, loss : 0.2157, accuracy : 95.77\n",
            "iteration : 100, loss : 0.2293, accuracy : 95.38\n",
            "iteration : 150, loss : 0.2377, accuracy : 95.04\n",
            "iteration : 200, loss : 0.2403, accuracy : 94.93\n",
            "iteration : 250, loss : 0.2397, accuracy : 94.97\n",
            "iteration : 300, loss : 0.2401, accuracy : 94.96\n",
            "iteration : 350, loss : 0.2400, accuracy : 94.98\n",
            "Epoch :  41, training loss : 0.2396, training accuracy : 94.98, test loss : 0.3267, test accuracy : 92.42\n",
            "\n",
            "Epoch: 42\n",
            "iteration :  50, loss : 0.2390, accuracy : 95.19\n",
            "iteration : 100, loss : 0.2337, accuracy : 95.16\n",
            "iteration : 150, loss : 0.2339, accuracy : 95.14\n",
            "iteration : 200, loss : 0.2370, accuracy : 95.07\n",
            "iteration : 250, loss : 0.2377, accuracy : 95.01\n",
            "iteration : 300, loss : 0.2379, accuracy : 94.99\n",
            "iteration : 350, loss : 0.2375, accuracy : 94.99\n",
            "Epoch :  42, training loss : 0.2368, training accuracy : 95.00, test loss : 0.2892, test accuracy : 93.49\n",
            "\n",
            "Epoch: 43\n",
            "iteration :  50, loss : 0.2202, accuracy : 95.84\n",
            "iteration : 100, loss : 0.2261, accuracy : 95.43\n",
            "iteration : 150, loss : 0.2260, accuracy : 95.50\n",
            "iteration : 200, loss : 0.2324, accuracy : 95.29\n",
            "iteration : 250, loss : 0.2339, accuracy : 95.22\n",
            "iteration : 300, loss : 0.2365, accuracy : 95.13\n",
            "iteration : 350, loss : 0.2353, accuracy : 95.13\n",
            "Epoch :  43, training loss : 0.2361, training accuracy : 95.11, test loss : 0.2983, test accuracy : 93.24\n",
            "\n",
            "Epoch: 44\n",
            "iteration :  50, loss : 0.2108, accuracy : 95.91\n",
            "iteration : 100, loss : 0.2139, accuracy : 95.68\n",
            "iteration : 150, loss : 0.2214, accuracy : 95.54\n",
            "iteration : 200, loss : 0.2260, accuracy : 95.37\n",
            "iteration : 250, loss : 0.2283, accuracy : 95.27\n",
            "iteration : 300, loss : 0.2311, accuracy : 95.22\n",
            "iteration : 350, loss : 0.2329, accuracy : 95.14\n",
            "Epoch :  44, training loss : 0.2338, training accuracy : 95.11, test loss : 0.3085, test accuracy : 92.71\n",
            "\n",
            "Epoch: 45\n",
            "iteration :  50, loss : 0.2381, accuracy : 94.97\n",
            "iteration : 100, loss : 0.2442, accuracy : 94.85\n",
            "iteration : 150, loss : 0.2423, accuracy : 94.86\n",
            "iteration : 200, loss : 0.2411, accuracy : 94.88\n",
            "iteration : 250, loss : 0.2403, accuracy : 94.89\n",
            "iteration : 300, loss : 0.2386, accuracy : 94.91\n",
            "iteration : 350, loss : 0.2393, accuracy : 94.88\n",
            "Epoch :  45, training loss : 0.2383, training accuracy : 94.91, test loss : 0.2976, test accuracy : 93.19\n",
            "\n",
            "Epoch: 46\n",
            "iteration :  50, loss : 0.2321, accuracy : 95.08\n",
            "iteration : 100, loss : 0.2324, accuracy : 95.00\n",
            "iteration : 150, loss : 0.2347, accuracy : 94.94\n",
            "iteration : 200, loss : 0.2308, accuracy : 95.09\n",
            "iteration : 250, loss : 0.2305, accuracy : 95.12\n",
            "iteration : 300, loss : 0.2295, accuracy : 95.17\n",
            "iteration : 350, loss : 0.2318, accuracy : 95.06\n",
            "Epoch :  46, training loss : 0.2327, training accuracy : 95.04, test loss : 0.3053, test accuracy : 92.99\n",
            "\n",
            "Epoch: 47\n",
            "iteration :  50, loss : 0.2304, accuracy : 95.14\n",
            "iteration : 100, loss : 0.2218, accuracy : 95.44\n",
            "iteration : 150, loss : 0.2261, accuracy : 95.35\n",
            "iteration : 200, loss : 0.2264, accuracy : 95.27\n",
            "iteration : 250, loss : 0.2288, accuracy : 95.19\n",
            "iteration : 300, loss : 0.2296, accuracy : 95.16\n",
            "iteration : 350, loss : 0.2312, accuracy : 95.12\n",
            "Epoch :  47, training loss : 0.2324, training accuracy : 95.11, test loss : 0.2986, test accuracy : 93.12\n",
            "\n",
            "Epoch: 48\n",
            "iteration :  50, loss : 0.2284, accuracy : 95.31\n",
            "iteration : 100, loss : 0.2223, accuracy : 95.38\n",
            "iteration : 150, loss : 0.2232, accuracy : 95.42\n",
            "iteration : 200, loss : 0.2236, accuracy : 95.39\n",
            "iteration : 250, loss : 0.2263, accuracy : 95.34\n",
            "iteration : 300, loss : 0.2282, accuracy : 95.29\n",
            "iteration : 350, loss : 0.2301, accuracy : 95.26\n",
            "Epoch :  48, training loss : 0.2301, training accuracy : 95.24, test loss : 0.3145, test accuracy : 92.81\n",
            "\n",
            "Epoch: 49\n",
            "iteration :  50, loss : 0.2229, accuracy : 95.61\n",
            "iteration : 100, loss : 0.2271, accuracy : 95.35\n",
            "iteration : 150, loss : 0.2273, accuracy : 95.38\n",
            "iteration : 200, loss : 0.2316, accuracy : 95.21\n",
            "iteration : 250, loss : 0.2344, accuracy : 95.14\n",
            "iteration : 300, loss : 0.2341, accuracy : 95.12\n",
            "iteration : 350, loss : 0.2363, accuracy : 95.04\n",
            "Epoch :  49, training loss : 0.2362, training accuracy : 95.01, test loss : 0.3328, test accuracy : 92.14\n",
            "\n",
            "Epoch: 50\n",
            "iteration :  50, loss : 0.2202, accuracy : 95.41\n",
            "iteration : 100, loss : 0.2195, accuracy : 95.32\n",
            "iteration : 150, loss : 0.2233, accuracy : 95.24\n",
            "iteration : 200, loss : 0.2279, accuracy : 95.16\n",
            "iteration : 250, loss : 0.2290, accuracy : 95.14\n",
            "iteration : 300, loss : 0.2303, accuracy : 95.14\n",
            "iteration : 350, loss : 0.2317, accuracy : 95.08\n",
            "Epoch :  50, training loss : 0.2321, training accuracy : 95.08, test loss : 0.2932, test accuracy : 93.17\n",
            "\n",
            "Epoch: 51\n",
            "iteration :  50, loss : 0.2289, accuracy : 95.09\n",
            "iteration : 100, loss : 0.2224, accuracy : 95.32\n",
            "iteration : 150, loss : 0.2261, accuracy : 95.22\n",
            "iteration : 200, loss : 0.2252, accuracy : 95.26\n",
            "iteration : 250, loss : 0.2250, accuracy : 95.28\n",
            "iteration : 300, loss : 0.2290, accuracy : 95.17\n",
            "iteration : 350, loss : 0.2296, accuracy : 95.14\n",
            "Epoch :  51, training loss : 0.2304, training accuracy : 95.10, test loss : 0.2969, test accuracy : 93.40\n",
            "\n",
            "Epoch: 52\n",
            "iteration :  50, loss : 0.2261, accuracy : 95.41\n",
            "iteration : 100, loss : 0.2237, accuracy : 95.50\n",
            "iteration : 150, loss : 0.2220, accuracy : 95.47\n",
            "iteration : 200, loss : 0.2241, accuracy : 95.45\n",
            "iteration : 250, loss : 0.2270, accuracy : 95.35\n",
            "iteration : 300, loss : 0.2262, accuracy : 95.39\n",
            "iteration : 350, loss : 0.2299, accuracy : 95.24\n",
            "Epoch :  52, training loss : 0.2304, training accuracy : 95.22, test loss : 0.2978, test accuracy : 93.23\n",
            "\n",
            "Epoch: 53\n",
            "iteration :  50, loss : 0.2263, accuracy : 95.31\n",
            "iteration : 100, loss : 0.2235, accuracy : 95.27\n",
            "iteration : 150, loss : 0.2201, accuracy : 95.47\n",
            "iteration : 200, loss : 0.2205, accuracy : 95.49\n",
            "iteration : 250, loss : 0.2270, accuracy : 95.26\n",
            "iteration : 300, loss : 0.2300, accuracy : 95.17\n",
            "iteration : 350, loss : 0.2297, accuracy : 95.15\n",
            "Epoch :  53, training loss : 0.2290, training accuracy : 95.17, test loss : 0.2958, test accuracy : 93.25\n",
            "\n",
            "Epoch: 54\n",
            "iteration :  50, loss : 0.2270, accuracy : 95.28\n",
            "iteration : 100, loss : 0.2255, accuracy : 95.29\n",
            "iteration : 150, loss : 0.2227, accuracy : 95.35\n",
            "iteration : 200, loss : 0.2226, accuracy : 95.38\n",
            "iteration : 250, loss : 0.2221, accuracy : 95.38\n",
            "iteration : 300, loss : 0.2234, accuracy : 95.37\n",
            "iteration : 350, loss : 0.2255, accuracy : 95.29\n",
            "Epoch :  54, training loss : 0.2259, training accuracy : 95.28, test loss : 0.2890, test accuracy : 93.65\n",
            "\n",
            "Epoch: 55\n",
            "iteration :  50, loss : 0.2122, accuracy : 95.59\n",
            "iteration : 100, loss : 0.2196, accuracy : 95.53\n",
            "iteration : 150, loss : 0.2259, accuracy : 95.28\n",
            "iteration : 200, loss : 0.2273, accuracy : 95.29\n",
            "iteration : 250, loss : 0.2264, accuracy : 95.36\n",
            "iteration : 300, loss : 0.2290, accuracy : 95.21\n",
            "iteration : 350, loss : 0.2272, accuracy : 95.27\n",
            "Epoch :  55, training loss : 0.2271, training accuracy : 95.26, test loss : 0.3024, test accuracy : 92.94\n",
            "\n",
            "Epoch: 56\n",
            "iteration :  50, loss : 0.2166, accuracy : 95.62\n",
            "iteration : 100, loss : 0.2114, accuracy : 95.82\n",
            "iteration : 150, loss : 0.2178, accuracy : 95.56\n",
            "iteration : 200, loss : 0.2208, accuracy : 95.39\n",
            "iteration : 250, loss : 0.2226, accuracy : 95.33\n",
            "iteration : 300, loss : 0.2240, accuracy : 95.29\n",
            "iteration : 350, loss : 0.2249, accuracy : 95.28\n",
            "Epoch :  56, training loss : 0.2252, training accuracy : 95.27, test loss : 0.3033, test accuracy : 93.15\n",
            "\n",
            "Epoch: 57\n",
            "iteration :  50, loss : 0.2252, accuracy : 95.31\n",
            "iteration : 100, loss : 0.2221, accuracy : 95.47\n",
            "iteration : 150, loss : 0.2251, accuracy : 95.32\n",
            "iteration : 200, loss : 0.2265, accuracy : 95.25\n",
            "iteration : 250, loss : 0.2270, accuracy : 95.24\n",
            "iteration : 300, loss : 0.2269, accuracy : 95.26\n",
            "iteration : 350, loss : 0.2263, accuracy : 95.31\n",
            "Epoch :  57, training loss : 0.2263, training accuracy : 95.32, test loss : 0.3059, test accuracy : 92.99\n",
            "\n",
            "Epoch: 58\n",
            "iteration :  50, loss : 0.2252, accuracy : 95.31\n",
            "iteration : 100, loss : 0.2199, accuracy : 95.46\n",
            "iteration : 150, loss : 0.2186, accuracy : 95.53\n",
            "iteration : 200, loss : 0.2243, accuracy : 95.38\n",
            "iteration : 250, loss : 0.2245, accuracy : 95.41\n",
            "iteration : 300, loss : 0.2244, accuracy : 95.38\n",
            "iteration : 350, loss : 0.2255, accuracy : 95.34\n",
            "Epoch :  58, training loss : 0.2257, training accuracy : 95.31, test loss : 0.3089, test accuracy : 92.84\n",
            "\n",
            "Epoch: 59\n",
            "iteration :  50, loss : 0.2174, accuracy : 95.39\n",
            "iteration : 100, loss : 0.2088, accuracy : 95.84\n",
            "iteration : 150, loss : 0.2123, accuracy : 95.74\n",
            "iteration : 200, loss : 0.2142, accuracy : 95.71\n",
            "iteration : 250, loss : 0.2179, accuracy : 95.59\n",
            "iteration : 300, loss : 0.2199, accuracy : 95.49\n",
            "iteration : 350, loss : 0.2212, accuracy : 95.46\n",
            "Epoch :  59, training loss : 0.2214, training accuracy : 95.47, test loss : 0.3110, test accuracy : 92.95\n",
            "\n",
            "Epoch: 60\n",
            "iteration :  50, loss : 0.2130, accuracy : 95.70\n",
            "iteration : 100, loss : 0.2135, accuracy : 95.78\n",
            "iteration : 150, loss : 0.2150, accuracy : 95.68\n",
            "iteration : 200, loss : 0.2191, accuracy : 95.52\n",
            "iteration : 250, loss : 0.2192, accuracy : 95.49\n",
            "iteration : 300, loss : 0.2199, accuracy : 95.49\n",
            "iteration : 350, loss : 0.2212, accuracy : 95.44\n",
            "Epoch :  60, training loss : 0.2220, training accuracy : 95.44, test loss : 0.2924, test accuracy : 93.36\n",
            "\n",
            "Epoch: 61\n",
            "iteration :  50, loss : 0.2173, accuracy : 95.59\n",
            "iteration : 100, loss : 0.2213, accuracy : 95.50\n",
            "iteration : 150, loss : 0.2206, accuracy : 95.54\n",
            "iteration : 200, loss : 0.2210, accuracy : 95.48\n",
            "iteration : 250, loss : 0.2240, accuracy : 95.37\n",
            "iteration : 300, loss : 0.2253, accuracy : 95.31\n",
            "iteration : 350, loss : 0.2240, accuracy : 95.34\n",
            "Epoch :  61, training loss : 0.2243, training accuracy : 95.31, test loss : 0.2949, test accuracy : 93.56\n",
            "\n",
            "Epoch: 62\n",
            "iteration :  50, loss : 0.2147, accuracy : 95.84\n",
            "iteration : 100, loss : 0.2116, accuracy : 95.87\n",
            "iteration : 150, loss : 0.2090, accuracy : 95.92\n",
            "iteration : 200, loss : 0.2152, accuracy : 95.69\n",
            "iteration : 250, loss : 0.2195, accuracy : 95.57\n",
            "iteration : 300, loss : 0.2197, accuracy : 95.55\n",
            "iteration : 350, loss : 0.2234, accuracy : 95.45\n",
            "Epoch :  62, training loss : 0.2233, training accuracy : 95.43, test loss : 0.3177, test accuracy : 92.66\n",
            "\n",
            "Epoch: 63\n",
            "iteration :  50, loss : 0.2261, accuracy : 95.33\n",
            "iteration : 100, loss : 0.2206, accuracy : 95.40\n",
            "iteration : 150, loss : 0.2183, accuracy : 95.45\n",
            "iteration : 200, loss : 0.2196, accuracy : 95.36\n",
            "iteration : 250, loss : 0.2213, accuracy : 95.31\n",
            "iteration : 300, loss : 0.2210, accuracy : 95.30\n",
            "iteration : 350, loss : 0.2235, accuracy : 95.27\n",
            "Epoch :  63, training loss : 0.2233, training accuracy : 95.26, test loss : 0.2833, test accuracy : 93.78\n",
            "\n",
            "Epoch: 64\n",
            "iteration :  50, loss : 0.2140, accuracy : 95.78\n",
            "iteration : 100, loss : 0.2132, accuracy : 95.67\n",
            "iteration : 150, loss : 0.2182, accuracy : 95.52\n",
            "iteration : 200, loss : 0.2185, accuracy : 95.45\n",
            "iteration : 250, loss : 0.2208, accuracy : 95.46\n",
            "iteration : 300, loss : 0.2196, accuracy : 95.51\n",
            "iteration : 350, loss : 0.2192, accuracy : 95.52\n",
            "Epoch :  64, training loss : 0.2190, training accuracy : 95.52, test loss : 0.2910, test accuracy : 93.48\n",
            "\n",
            "Epoch: 65\n",
            "iteration :  50, loss : 0.2160, accuracy : 95.61\n",
            "iteration : 100, loss : 0.2235, accuracy : 95.34\n",
            "iteration : 150, loss : 0.2148, accuracy : 95.64\n",
            "iteration : 200, loss : 0.2162, accuracy : 95.58\n",
            "iteration : 250, loss : 0.2197, accuracy : 95.47\n",
            "iteration : 300, loss : 0.2215, accuracy : 95.41\n",
            "iteration : 350, loss : 0.2216, accuracy : 95.43\n",
            "Epoch :  65, training loss : 0.2223, training accuracy : 95.40, test loss : 0.2888, test accuracy : 93.74\n",
            "\n",
            "Epoch: 66\n",
            "iteration :  50, loss : 0.2116, accuracy : 95.94\n",
            "iteration : 100, loss : 0.2127, accuracy : 95.85\n",
            "iteration : 150, loss : 0.2116, accuracy : 95.82\n",
            "iteration : 200, loss : 0.2137, accuracy : 95.73\n",
            "iteration : 250, loss : 0.2169, accuracy : 95.60\n",
            "iteration : 300, loss : 0.2185, accuracy : 95.55\n",
            "iteration : 350, loss : 0.2186, accuracy : 95.56\n",
            "Epoch :  66, training loss : 0.2191, training accuracy : 95.53, test loss : 0.2860, test accuracy : 93.54\n",
            "\n",
            "Epoch: 67\n",
            "iteration :  50, loss : 0.2035, accuracy : 95.77\n",
            "iteration : 100, loss : 0.2071, accuracy : 95.80\n",
            "iteration : 150, loss : 0.2088, accuracy : 95.83\n",
            "iteration : 200, loss : 0.2114, accuracy : 95.82\n",
            "iteration : 250, loss : 0.2154, accuracy : 95.65\n",
            "iteration : 300, loss : 0.2167, accuracy : 95.62\n",
            "iteration : 350, loss : 0.2187, accuracy : 95.56\n",
            "Epoch :  67, training loss : 0.2195, training accuracy : 95.54, test loss : 0.3063, test accuracy : 92.95\n",
            "\n",
            "Epoch: 68\n",
            "iteration :  50, loss : 0.2151, accuracy : 95.70\n",
            "iteration : 100, loss : 0.2152, accuracy : 95.76\n",
            "iteration : 150, loss : 0.2195, accuracy : 95.59\n",
            "iteration : 200, loss : 0.2188, accuracy : 95.64\n",
            "iteration : 250, loss : 0.2209, accuracy : 95.54\n",
            "iteration : 300, loss : 0.2233, accuracy : 95.43\n",
            "iteration : 350, loss : 0.2235, accuracy : 95.43\n",
            "Epoch :  68, training loss : 0.2235, training accuracy : 95.42, test loss : 0.2874, test accuracy : 93.63\n",
            "\n",
            "Epoch: 69\n",
            "iteration :  50, loss : 0.2257, accuracy : 95.17\n",
            "iteration : 100, loss : 0.2142, accuracy : 95.54\n",
            "iteration : 150, loss : 0.2175, accuracy : 95.49\n",
            "iteration : 200, loss : 0.2145, accuracy : 95.65\n",
            "iteration : 250, loss : 0.2198, accuracy : 95.50\n",
            "iteration : 300, loss : 0.2215, accuracy : 95.41\n",
            "iteration : 350, loss : 0.2215, accuracy : 95.41\n",
            "Epoch :  69, training loss : 0.2216, training accuracy : 95.38, test loss : 0.2915, test accuracy : 93.45\n",
            "\n",
            "Epoch: 70\n",
            "iteration :  50, loss : 0.2023, accuracy : 96.12\n",
            "iteration : 100, loss : 0.2097, accuracy : 95.80\n",
            "iteration : 150, loss : 0.2107, accuracy : 95.74\n",
            "iteration : 200, loss : 0.2120, accuracy : 95.70\n",
            "iteration : 250, loss : 0.2136, accuracy : 95.66\n",
            "iteration : 300, loss : 0.2125, accuracy : 95.69\n",
            "iteration : 350, loss : 0.2143, accuracy : 95.66\n",
            "Epoch :  70, training loss : 0.2158, training accuracy : 95.64, test loss : 0.2819, test accuracy : 93.79\n",
            "\n",
            "Epoch: 71\n",
            "iteration :  50, loss : 0.1976, accuracy : 96.20\n",
            "iteration : 100, loss : 0.2025, accuracy : 96.05\n",
            "iteration : 150, loss : 0.2061, accuracy : 95.92\n",
            "iteration : 200, loss : 0.2103, accuracy : 95.74\n",
            "iteration : 250, loss : 0.2155, accuracy : 95.56\n",
            "iteration : 300, loss : 0.2165, accuracy : 95.51\n",
            "iteration : 350, loss : 0.2172, accuracy : 95.52\n",
            "Epoch :  71, training loss : 0.2169, training accuracy : 95.54, test loss : 0.2810, test accuracy : 93.79\n",
            "\n",
            "Epoch: 72\n",
            "iteration :  50, loss : 0.2054, accuracy : 96.03\n",
            "iteration : 100, loss : 0.2069, accuracy : 95.85\n",
            "iteration : 150, loss : 0.2096, accuracy : 95.79\n",
            "iteration : 200, loss : 0.2125, accuracy : 95.73\n",
            "iteration : 250, loss : 0.2107, accuracy : 95.76\n",
            "iteration : 300, loss : 0.2141, accuracy : 95.70\n",
            "iteration : 350, loss : 0.2143, accuracy : 95.67\n",
            "Epoch :  72, training loss : 0.2143, training accuracy : 95.66, test loss : 0.2849, test accuracy : 93.63\n",
            "\n",
            "Epoch: 73\n",
            "iteration :  50, loss : 0.1978, accuracy : 96.20\n",
            "iteration : 100, loss : 0.2113, accuracy : 95.75\n",
            "iteration : 150, loss : 0.2159, accuracy : 95.55\n",
            "iteration : 200, loss : 0.2171, accuracy : 95.47\n",
            "iteration : 250, loss : 0.2198, accuracy : 95.41\n",
            "iteration : 300, loss : 0.2191, accuracy : 95.41\n",
            "iteration : 350, loss : 0.2175, accuracy : 95.44\n",
            "Epoch :  73, training loss : 0.2182, training accuracy : 95.43, test loss : 0.2994, test accuracy : 93.14\n",
            "\n",
            "Epoch: 74\n",
            "iteration :  50, loss : 0.2005, accuracy : 96.31\n",
            "iteration : 100, loss : 0.2118, accuracy : 95.91\n",
            "iteration : 150, loss : 0.2106, accuracy : 95.94\n",
            "iteration : 200, loss : 0.2107, accuracy : 95.80\n",
            "iteration : 250, loss : 0.2112, accuracy : 95.73\n",
            "iteration : 300, loss : 0.2154, accuracy : 95.59\n",
            "iteration : 350, loss : 0.2160, accuracy : 95.56\n",
            "Epoch :  74, training loss : 0.2155, training accuracy : 95.58, test loss : 0.2918, test accuracy : 93.57\n",
            "\n",
            "Epoch: 75\n",
            "iteration :  50, loss : 0.2015, accuracy : 96.22\n",
            "iteration : 100, loss : 0.2140, accuracy : 95.87\n",
            "iteration : 150, loss : 0.2170, accuracy : 95.62\n",
            "iteration : 200, loss : 0.2131, accuracy : 95.75\n",
            "iteration : 250, loss : 0.2133, accuracy : 95.71\n",
            "iteration : 300, loss : 0.2150, accuracy : 95.67\n",
            "iteration : 350, loss : 0.2143, accuracy : 95.69\n",
            "Epoch :  75, training loss : 0.2136, training accuracy : 95.70, test loss : 0.2953, test accuracy : 93.34\n",
            "\n",
            "Epoch: 76\n",
            "iteration :  50, loss : 0.2043, accuracy : 95.88\n",
            "iteration : 100, loss : 0.2056, accuracy : 95.77\n",
            "iteration : 150, loss : 0.2082, accuracy : 95.80\n",
            "iteration : 200, loss : 0.2082, accuracy : 95.80\n",
            "iteration : 250, loss : 0.2088, accuracy : 95.76\n",
            "iteration : 300, loss : 0.2103, accuracy : 95.69\n",
            "iteration : 350, loss : 0.2118, accuracy : 95.67\n",
            "Epoch :  76, training loss : 0.2120, training accuracy : 95.69, test loss : 0.2765, test accuracy : 93.92\n",
            "\n",
            "Epoch: 77\n",
            "iteration :  50, loss : 0.1989, accuracy : 96.11\n",
            "iteration : 100, loss : 0.2043, accuracy : 96.03\n",
            "iteration : 150, loss : 0.2120, accuracy : 95.76\n",
            "iteration : 200, loss : 0.2116, accuracy : 95.80\n",
            "iteration : 250, loss : 0.2111, accuracy : 95.82\n",
            "iteration : 300, loss : 0.2116, accuracy : 95.79\n",
            "iteration : 350, loss : 0.2134, accuracy : 95.71\n",
            "Epoch :  77, training loss : 0.2141, training accuracy : 95.71, test loss : 0.2811, test accuracy : 93.71\n",
            "\n",
            "Epoch: 78\n",
            "iteration :  50, loss : 0.2092, accuracy : 95.94\n",
            "iteration : 100, loss : 0.2154, accuracy : 95.79\n",
            "iteration : 150, loss : 0.2118, accuracy : 95.80\n",
            "iteration : 200, loss : 0.2167, accuracy : 95.66\n",
            "iteration : 250, loss : 0.2149, accuracy : 95.69\n",
            "iteration : 300, loss : 0.2154, accuracy : 95.67\n",
            "iteration : 350, loss : 0.2152, accuracy : 95.67\n",
            "Epoch :  78, training loss : 0.2156, training accuracy : 95.65, test loss : 0.2878, test accuracy : 93.63\n",
            "\n",
            "Epoch: 79\n",
            "iteration :  50, loss : 0.2109, accuracy : 95.73\n",
            "iteration : 100, loss : 0.2109, accuracy : 95.75\n",
            "iteration : 150, loss : 0.2103, accuracy : 95.83\n",
            "iteration : 200, loss : 0.2091, accuracy : 95.84\n",
            "iteration : 250, loss : 0.2088, accuracy : 95.90\n",
            "iteration : 300, loss : 0.2111, accuracy : 95.80\n",
            "iteration : 350, loss : 0.2114, accuracy : 95.79\n",
            "Epoch :  79, training loss : 0.2113, training accuracy : 95.78, test loss : 0.2896, test accuracy : 93.54\n",
            "\n",
            "Epoch: 80\n",
            "iteration :  50, loss : 0.1998, accuracy : 96.03\n",
            "iteration : 100, loss : 0.1977, accuracy : 96.12\n",
            "iteration : 150, loss : 0.1998, accuracy : 96.11\n",
            "iteration : 200, loss : 0.2033, accuracy : 95.98\n",
            "iteration : 250, loss : 0.2079, accuracy : 95.85\n",
            "iteration : 300, loss : 0.2074, accuracy : 95.85\n",
            "iteration : 350, loss : 0.2088, accuracy : 95.79\n",
            "Epoch :  80, training loss : 0.2117, training accuracy : 95.70, test loss : 0.2999, test accuracy : 93.28\n",
            "\n",
            "Epoch: 81\n",
            "iteration :  50, loss : 0.2061, accuracy : 95.69\n",
            "iteration : 100, loss : 0.2056, accuracy : 95.74\n",
            "iteration : 150, loss : 0.2057, accuracy : 95.74\n",
            "iteration : 200, loss : 0.2079, accuracy : 95.71\n",
            "iteration : 250, loss : 0.2080, accuracy : 95.74\n",
            "iteration : 300, loss : 0.2087, accuracy : 95.75\n",
            "iteration : 350, loss : 0.2083, accuracy : 95.76\n",
            "Epoch :  81, training loss : 0.2083, training accuracy : 95.76, test loss : 0.3092, test accuracy : 93.01\n",
            "\n",
            "Epoch: 82\n",
            "iteration :  50, loss : 0.1969, accuracy : 96.14\n",
            "iteration : 100, loss : 0.1951, accuracy : 96.26\n",
            "iteration : 150, loss : 0.1976, accuracy : 96.12\n",
            "iteration : 200, loss : 0.2011, accuracy : 96.09\n",
            "iteration : 250, loss : 0.2042, accuracy : 96.02\n",
            "iteration : 300, loss : 0.2071, accuracy : 95.92\n",
            "iteration : 350, loss : 0.2093, accuracy : 95.84\n",
            "Epoch :  82, training loss : 0.2093, training accuracy : 95.84, test loss : 0.2890, test accuracy : 93.66\n",
            "\n",
            "Epoch: 83\n",
            "iteration :  50, loss : 0.2060, accuracy : 95.97\n",
            "iteration : 100, loss : 0.1983, accuracy : 96.19\n",
            "iteration : 150, loss : 0.1986, accuracy : 96.13\n",
            "iteration : 200, loss : 0.2032, accuracy : 96.00\n",
            "iteration : 250, loss : 0.2040, accuracy : 96.00\n",
            "iteration : 300, loss : 0.2037, accuracy : 96.00\n",
            "iteration : 350, loss : 0.2045, accuracy : 95.99\n",
            "Epoch :  83, training loss : 0.2052, training accuracy : 95.97, test loss : 0.2924, test accuracy : 93.61\n",
            "\n",
            "Epoch: 84\n",
            "iteration :  50, loss : 0.2000, accuracy : 96.23\n",
            "iteration : 100, loss : 0.2037, accuracy : 96.02\n",
            "iteration : 150, loss : 0.2122, accuracy : 95.77\n",
            "iteration : 200, loss : 0.2110, accuracy : 95.80\n",
            "iteration : 250, loss : 0.2118, accuracy : 95.81\n",
            "iteration : 300, loss : 0.2121, accuracy : 95.74\n",
            "iteration : 350, loss : 0.2139, accuracy : 95.68\n",
            "Epoch :  84, training loss : 0.2144, training accuracy : 95.66, test loss : 0.2945, test accuracy : 93.32\n",
            "\n",
            "Epoch: 85\n",
            "iteration :  50, loss : 0.2149, accuracy : 95.73\n",
            "iteration : 100, loss : 0.2087, accuracy : 95.88\n",
            "iteration : 150, loss : 0.2029, accuracy : 95.99\n",
            "iteration : 200, loss : 0.2061, accuracy : 95.85\n",
            "iteration : 250, loss : 0.2079, accuracy : 95.79\n",
            "iteration : 300, loss : 0.2086, accuracy : 95.78\n",
            "iteration : 350, loss : 0.2086, accuracy : 95.78\n",
            "Epoch :  85, training loss : 0.2077, training accuracy : 95.81, test loss : 0.2916, test accuracy : 93.53\n",
            "\n",
            "Epoch: 86\n",
            "iteration :  50, loss : 0.2025, accuracy : 96.03\n",
            "iteration : 100, loss : 0.2045, accuracy : 96.03\n",
            "iteration : 150, loss : 0.2033, accuracy : 96.04\n",
            "iteration : 200, loss : 0.2034, accuracy : 96.00\n",
            "iteration : 250, loss : 0.2038, accuracy : 95.97\n",
            "iteration : 300, loss : 0.2033, accuracy : 95.96\n",
            "iteration : 350, loss : 0.2043, accuracy : 95.95\n",
            "Epoch :  86, training loss : 0.2050, training accuracy : 95.92, test loss : 0.2900, test accuracy : 93.60\n",
            "\n",
            "Epoch: 87\n",
            "iteration :  50, loss : 0.1972, accuracy : 96.19\n",
            "iteration : 100, loss : 0.2023, accuracy : 95.91\n",
            "iteration : 150, loss : 0.2050, accuracy : 95.80\n",
            "iteration : 200, loss : 0.2077, accuracy : 95.77\n",
            "iteration : 250, loss : 0.2080, accuracy : 95.74\n",
            "iteration : 300, loss : 0.2073, accuracy : 95.77\n",
            "iteration : 350, loss : 0.2089, accuracy : 95.72\n",
            "Epoch :  87, training loss : 0.2091, training accuracy : 95.72, test loss : 0.2885, test accuracy : 93.61\n",
            "\n",
            "Epoch: 88\n",
            "iteration :  50, loss : 0.1923, accuracy : 96.36\n",
            "iteration : 100, loss : 0.1948, accuracy : 96.20\n",
            "iteration : 150, loss : 0.1964, accuracy : 96.14\n",
            "iteration : 200, loss : 0.1999, accuracy : 96.05\n",
            "iteration : 250, loss : 0.2035, accuracy : 95.98\n",
            "iteration : 300, loss : 0.2039, accuracy : 95.98\n",
            "iteration : 350, loss : 0.2045, accuracy : 95.99\n",
            "Epoch :  88, training loss : 0.2055, training accuracy : 95.95, test loss : 0.2804, test accuracy : 93.77\n",
            "\n",
            "Epoch: 89\n",
            "iteration :  50, loss : 0.1968, accuracy : 96.20\n",
            "iteration : 100, loss : 0.1967, accuracy : 96.16\n",
            "iteration : 150, loss : 0.1996, accuracy : 96.11\n",
            "iteration : 200, loss : 0.1984, accuracy : 96.12\n",
            "iteration : 250, loss : 0.2007, accuracy : 96.08\n",
            "iteration : 300, loss : 0.2023, accuracy : 95.99\n",
            "iteration : 350, loss : 0.2066, accuracy : 95.90\n",
            "Epoch :  89, training loss : 0.2064, training accuracy : 95.89, test loss : 0.2861, test accuracy : 93.61\n",
            "\n",
            "Epoch: 90\n",
            "iteration :  50, loss : 0.1879, accuracy : 96.58\n",
            "iteration : 100, loss : 0.1891, accuracy : 96.55\n",
            "iteration : 150, loss : 0.1951, accuracy : 96.32\n",
            "iteration : 200, loss : 0.1978, accuracy : 96.20\n",
            "iteration : 250, loss : 0.1997, accuracy : 96.11\n",
            "iteration : 300, loss : 0.2025, accuracy : 96.01\n",
            "iteration : 350, loss : 0.2031, accuracy : 95.98\n",
            "Epoch :  90, training loss : 0.2026, training accuracy : 95.98, test loss : 0.2944, test accuracy : 93.51\n",
            "\n",
            "Epoch: 91\n",
            "iteration :  50, loss : 0.1916, accuracy : 96.08\n",
            "iteration : 100, loss : 0.1952, accuracy : 96.26\n",
            "iteration : 150, loss : 0.1940, accuracy : 96.31\n",
            "iteration : 200, loss : 0.1942, accuracy : 96.25\n",
            "iteration : 250, loss : 0.1973, accuracy : 96.19\n",
            "iteration : 300, loss : 0.2002, accuracy : 96.13\n",
            "iteration : 350, loss : 0.2033, accuracy : 96.01\n",
            "Epoch :  91, training loss : 0.2033, training accuracy : 96.00, test loss : 0.2945, test accuracy : 93.50\n",
            "\n",
            "Epoch: 92\n",
            "iteration :  50, loss : 0.1937, accuracy : 96.50\n",
            "iteration : 100, loss : 0.1980, accuracy : 96.29\n",
            "iteration : 150, loss : 0.1979, accuracy : 96.26\n",
            "iteration : 200, loss : 0.1997, accuracy : 96.16\n",
            "iteration : 250, loss : 0.1997, accuracy : 96.17\n",
            "iteration : 300, loss : 0.2030, accuracy : 96.02\n",
            "iteration : 350, loss : 0.2035, accuracy : 96.03\n",
            "Epoch :  92, training loss : 0.2043, training accuracy : 96.00, test loss : 0.2849, test accuracy : 93.55\n",
            "\n",
            "Epoch: 93\n",
            "iteration :  50, loss : 0.1790, accuracy : 96.78\n",
            "iteration : 100, loss : 0.1911, accuracy : 96.38\n",
            "iteration : 150, loss : 0.1965, accuracy : 96.19\n",
            "iteration : 200, loss : 0.2038, accuracy : 95.94\n",
            "iteration : 250, loss : 0.2044, accuracy : 95.92\n",
            "iteration : 300, loss : 0.2050, accuracy : 95.88\n",
            "iteration : 350, loss : 0.2063, accuracy : 95.82\n",
            "Epoch :  93, training loss : 0.2073, training accuracy : 95.82, test loss : 0.2874, test accuracy : 93.62\n",
            "\n",
            "Epoch: 94\n",
            "iteration :  50, loss : 0.1917, accuracy : 96.25\n",
            "iteration : 100, loss : 0.1943, accuracy : 96.16\n",
            "iteration : 150, loss : 0.1965, accuracy : 96.16\n",
            "iteration : 200, loss : 0.1977, accuracy : 96.12\n",
            "iteration : 250, loss : 0.1986, accuracy : 96.09\n",
            "iteration : 300, loss : 0.2008, accuracy : 96.03\n",
            "iteration : 350, loss : 0.2025, accuracy : 95.96\n",
            "Epoch :  94, training loss : 0.2039, training accuracy : 95.92, test loss : 0.2902, test accuracy : 93.60\n",
            "\n",
            "Epoch: 95\n",
            "iteration :  50, loss : 0.1960, accuracy : 96.12\n",
            "iteration : 100, loss : 0.1897, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1882, accuracy : 96.46\n",
            "iteration : 200, loss : 0.1904, accuracy : 96.41\n",
            "iteration : 250, loss : 0.1927, accuracy : 96.31\n",
            "iteration : 300, loss : 0.1968, accuracy : 96.20\n",
            "iteration : 350, loss : 0.2009, accuracy : 96.12\n",
            "Epoch :  95, training loss : 0.2017, training accuracy : 96.08, test loss : 0.2858, test accuracy : 93.70\n",
            "\n",
            "Epoch: 96\n",
            "iteration :  50, loss : 0.1889, accuracy : 96.34\n",
            "iteration : 100, loss : 0.1952, accuracy : 96.05\n",
            "iteration : 150, loss : 0.2004, accuracy : 95.99\n",
            "iteration : 200, loss : 0.2024, accuracy : 96.03\n",
            "iteration : 250, loss : 0.2046, accuracy : 95.97\n",
            "iteration : 300, loss : 0.2037, accuracy : 96.00\n",
            "iteration : 350, loss : 0.2027, accuracy : 96.02\n",
            "Epoch :  96, training loss : 0.2047, training accuracy : 95.96, test loss : 0.2890, test accuracy : 93.56\n",
            "\n",
            "Epoch: 97\n",
            "iteration :  50, loss : 0.1970, accuracy : 96.14\n",
            "iteration : 100, loss : 0.1957, accuracy : 96.19\n",
            "iteration : 150, loss : 0.1951, accuracy : 96.26\n",
            "iteration : 200, loss : 0.1947, accuracy : 96.29\n",
            "iteration : 250, loss : 0.1980, accuracy : 96.18\n",
            "iteration : 300, loss : 0.1987, accuracy : 96.14\n",
            "iteration : 350, loss : 0.1991, accuracy : 96.11\n",
            "Epoch :  97, training loss : 0.2006, training accuracy : 96.09, test loss : 0.2906, test accuracy : 93.53\n",
            "\n",
            "Epoch: 98\n",
            "iteration :  50, loss : 0.2008, accuracy : 96.20\n",
            "iteration : 100, loss : 0.2019, accuracy : 96.02\n",
            "iteration : 150, loss : 0.1996, accuracy : 96.04\n",
            "iteration : 200, loss : 0.2014, accuracy : 95.99\n",
            "iteration : 250, loss : 0.2006, accuracy : 96.07\n",
            "iteration : 300, loss : 0.2004, accuracy : 96.07\n",
            "iteration : 350, loss : 0.2013, accuracy : 96.05\n",
            "Epoch :  98, training loss : 0.2018, training accuracy : 96.03, test loss : 0.3014, test accuracy : 93.15\n",
            "\n",
            "Epoch: 99\n",
            "iteration :  50, loss : 0.2002, accuracy : 96.42\n",
            "iteration : 100, loss : 0.2006, accuracy : 96.13\n",
            "iteration : 150, loss : 0.2006, accuracy : 96.10\n",
            "iteration : 200, loss : 0.1993, accuracy : 96.12\n",
            "iteration : 250, loss : 0.1987, accuracy : 96.14\n",
            "iteration : 300, loss : 0.1988, accuracy : 96.15\n",
            "iteration : 350, loss : 0.1990, accuracy : 96.16\n",
            "Epoch :  99, training loss : 0.1986, training accuracy : 96.17, test loss : 0.2817, test accuracy : 93.86\n",
            "\n",
            "Epoch: 100\n",
            "iteration :  50, loss : 0.1955, accuracy : 96.25\n",
            "iteration : 100, loss : 0.1949, accuracy : 96.28\n",
            "iteration : 150, loss : 0.1955, accuracy : 96.21\n",
            "iteration : 200, loss : 0.1984, accuracy : 96.09\n",
            "iteration : 250, loss : 0.1965, accuracy : 96.15\n",
            "iteration : 300, loss : 0.1981, accuracy : 96.12\n",
            "iteration : 350, loss : 0.1978, accuracy : 96.15\n",
            "Epoch : 100, training loss : 0.1974, training accuracy : 96.15, test loss : 0.2830, test accuracy : 93.80\n",
            "\n",
            "Epoch: 101\n",
            "iteration :  50, loss : 0.1920, accuracy : 96.23\n",
            "iteration : 100, loss : 0.1886, accuracy : 96.34\n",
            "iteration : 150, loss : 0.1959, accuracy : 96.11\n",
            "iteration : 200, loss : 0.1980, accuracy : 96.09\n",
            "iteration : 250, loss : 0.1976, accuracy : 96.09\n",
            "iteration : 300, loss : 0.1988, accuracy : 96.08\n",
            "iteration : 350, loss : 0.1991, accuracy : 96.04\n",
            "Epoch : 101, training loss : 0.1998, training accuracy : 96.04, test loss : 0.2941, test accuracy : 93.48\n",
            "\n",
            "Epoch: 102\n",
            "iteration :  50, loss : 0.1951, accuracy : 96.41\n",
            "iteration : 100, loss : 0.1923, accuracy : 96.30\n",
            "iteration : 150, loss : 0.1937, accuracy : 96.26\n",
            "iteration : 200, loss : 0.1930, accuracy : 96.23\n",
            "iteration : 250, loss : 0.1970, accuracy : 96.15\n",
            "iteration : 300, loss : 0.1975, accuracy : 96.14\n",
            "iteration : 350, loss : 0.1974, accuracy : 96.12\n",
            "Epoch : 102, training loss : 0.1978, training accuracy : 96.11, test loss : 0.2844, test accuracy : 93.67\n",
            "\n",
            "Epoch: 103\n",
            "iteration :  50, loss : 0.1925, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1928, accuracy : 96.34\n",
            "iteration : 150, loss : 0.1917, accuracy : 96.38\n",
            "iteration : 200, loss : 0.1941, accuracy : 96.29\n",
            "iteration : 250, loss : 0.1954, accuracy : 96.23\n",
            "iteration : 300, loss : 0.1959, accuracy : 96.23\n",
            "iteration : 350, loss : 0.1966, accuracy : 96.22\n",
            "Epoch : 103, training loss : 0.1962, training accuracy : 96.22, test loss : 0.2803, test accuracy : 93.88\n",
            "\n",
            "Epoch: 104\n",
            "iteration :  50, loss : 0.1985, accuracy : 96.36\n",
            "iteration : 100, loss : 0.1991, accuracy : 96.25\n",
            "iteration : 150, loss : 0.1942, accuracy : 96.33\n",
            "iteration : 200, loss : 0.1927, accuracy : 96.34\n",
            "iteration : 250, loss : 0.1945, accuracy : 96.30\n",
            "iteration : 300, loss : 0.1941, accuracy : 96.30\n",
            "iteration : 350, loss : 0.1944, accuracy : 96.29\n",
            "Epoch : 104, training loss : 0.1946, training accuracy : 96.29, test loss : 0.2849, test accuracy : 93.72\n",
            "\n",
            "Epoch: 105\n",
            "iteration :  50, loss : 0.1883, accuracy : 96.34\n",
            "iteration : 100, loss : 0.1864, accuracy : 96.44\n",
            "iteration : 150, loss : 0.1881, accuracy : 96.41\n",
            "iteration : 200, loss : 0.1894, accuracy : 96.39\n",
            "iteration : 250, loss : 0.1885, accuracy : 96.40\n",
            "iteration : 300, loss : 0.1911, accuracy : 96.34\n",
            "iteration : 350, loss : 0.1932, accuracy : 96.27\n",
            "Epoch : 105, training loss : 0.1935, training accuracy : 96.27, test loss : 0.2915, test accuracy : 93.57\n",
            "\n",
            "Epoch: 106\n",
            "iteration :  50, loss : 0.1752, accuracy : 96.94\n",
            "iteration : 100, loss : 0.1791, accuracy : 96.78\n",
            "iteration : 150, loss : 0.1836, accuracy : 96.64\n",
            "iteration : 200, loss : 0.1879, accuracy : 96.52\n",
            "iteration : 250, loss : 0.1894, accuracy : 96.50\n",
            "iteration : 300, loss : 0.1925, accuracy : 96.34\n",
            "iteration : 350, loss : 0.1940, accuracy : 96.31\n",
            "Epoch : 106, training loss : 0.1940, training accuracy : 96.30, test loss : 0.2844, test accuracy : 93.57\n",
            "\n",
            "Epoch: 107\n",
            "iteration :  50, loss : 0.1769, accuracy : 96.67\n",
            "iteration : 100, loss : 0.1830, accuracy : 96.44\n",
            "iteration : 150, loss : 0.1887, accuracy : 96.24\n",
            "iteration : 200, loss : 0.1887, accuracy : 96.31\n",
            "iteration : 250, loss : 0.1886, accuracy : 96.33\n",
            "iteration : 300, loss : 0.1937, accuracy : 96.21\n",
            "iteration : 350, loss : 0.1968, accuracy : 96.16\n",
            "Epoch : 107, training loss : 0.1977, training accuracy : 96.15, test loss : 0.2788, test accuracy : 93.93\n",
            "\n",
            "Epoch: 108\n",
            "iteration :  50, loss : 0.1817, accuracy : 96.69\n",
            "iteration : 100, loss : 0.1814, accuracy : 96.58\n",
            "iteration : 150, loss : 0.1876, accuracy : 96.43\n",
            "iteration : 200, loss : 0.1893, accuracy : 96.36\n",
            "iteration : 250, loss : 0.1913, accuracy : 96.32\n",
            "iteration : 300, loss : 0.1928, accuracy : 96.31\n",
            "iteration : 350, loss : 0.1934, accuracy : 96.28\n",
            "Epoch : 108, training loss : 0.1935, training accuracy : 96.27, test loss : 0.2766, test accuracy : 93.98\n",
            "\n",
            "Epoch: 109\n",
            "iteration :  50, loss : 0.1763, accuracy : 96.61\n",
            "iteration : 100, loss : 0.1921, accuracy : 96.17\n",
            "iteration : 150, loss : 0.1891, accuracy : 96.31\n",
            "iteration : 200, loss : 0.1899, accuracy : 96.30\n",
            "iteration : 250, loss : 0.1908, accuracy : 96.33\n",
            "iteration : 300, loss : 0.1929, accuracy : 96.27\n",
            "iteration : 350, loss : 0.1937, accuracy : 96.26\n",
            "Epoch : 109, training loss : 0.1941, training accuracy : 96.25, test loss : 0.2943, test accuracy : 93.43\n",
            "\n",
            "Epoch: 110\n",
            "iteration :  50, loss : 0.1773, accuracy : 96.91\n",
            "iteration : 100, loss : 0.1866, accuracy : 96.55\n",
            "iteration : 150, loss : 0.1899, accuracy : 96.47\n",
            "iteration : 200, loss : 0.1941, accuracy : 96.34\n",
            "iteration : 250, loss : 0.1947, accuracy : 96.31\n",
            "iteration : 300, loss : 0.1941, accuracy : 96.29\n",
            "iteration : 350, loss : 0.1934, accuracy : 96.29\n",
            "Epoch : 110, training loss : 0.1941, training accuracy : 96.26, test loss : 0.2836, test accuracy : 93.89\n",
            "\n",
            "Epoch: 111\n",
            "iteration :  50, loss : 0.1905, accuracy : 96.28\n",
            "iteration : 100, loss : 0.1852, accuracy : 96.48\n",
            "iteration : 150, loss : 0.1871, accuracy : 96.39\n",
            "iteration : 200, loss : 0.1891, accuracy : 96.36\n",
            "iteration : 250, loss : 0.1901, accuracy : 96.31\n",
            "iteration : 300, loss : 0.1913, accuracy : 96.28\n",
            "iteration : 350, loss : 0.1928, accuracy : 96.24\n",
            "Epoch : 111, training loss : 0.1927, training accuracy : 96.24, test loss : 0.2937, test accuracy : 93.52\n",
            "\n",
            "Epoch: 112\n",
            "iteration :  50, loss : 0.1860, accuracy : 96.47\n",
            "iteration : 100, loss : 0.1852, accuracy : 96.62\n",
            "iteration : 150, loss : 0.1861, accuracy : 96.56\n",
            "iteration : 200, loss : 0.1848, accuracy : 96.52\n",
            "iteration : 250, loss : 0.1900, accuracy : 96.43\n",
            "iteration : 300, loss : 0.1885, accuracy : 96.46\n",
            "iteration : 350, loss : 0.1889, accuracy : 96.44\n",
            "Epoch : 112, training loss : 0.1902, training accuracy : 96.41, test loss : 0.2854, test accuracy : 93.82\n",
            "\n",
            "Epoch: 113\n",
            "iteration :  50, loss : 0.1870, accuracy : 96.58\n",
            "iteration : 100, loss : 0.1851, accuracy : 96.52\n",
            "iteration : 150, loss : 0.1867, accuracy : 96.51\n",
            "iteration : 200, loss : 0.1887, accuracy : 96.43\n",
            "iteration : 250, loss : 0.1879, accuracy : 96.47\n",
            "iteration : 300, loss : 0.1881, accuracy : 96.43\n",
            "iteration : 350, loss : 0.1892, accuracy : 96.43\n",
            "Epoch : 113, training loss : 0.1901, training accuracy : 96.42, test loss : 0.2948, test accuracy : 93.65\n",
            "\n",
            "Epoch: 114\n",
            "iteration :  50, loss : 0.1939, accuracy : 96.38\n",
            "iteration : 100, loss : 0.1894, accuracy : 96.41\n",
            "iteration : 150, loss : 0.1923, accuracy : 96.29\n",
            "iteration : 200, loss : 0.1931, accuracy : 96.26\n",
            "iteration : 250, loss : 0.1928, accuracy : 96.28\n",
            "iteration : 300, loss : 0.1914, accuracy : 96.30\n",
            "iteration : 350, loss : 0.1919, accuracy : 96.30\n",
            "Epoch : 114, training loss : 0.1921, training accuracy : 96.28, test loss : 0.2817, test accuracy : 93.72\n",
            "\n",
            "Epoch: 115\n",
            "iteration :  50, loss : 0.1725, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1729, accuracy : 97.00\n",
            "iteration : 150, loss : 0.1775, accuracy : 96.77\n",
            "iteration : 200, loss : 0.1780, accuracy : 96.75\n",
            "iteration : 250, loss : 0.1842, accuracy : 96.54\n",
            "iteration : 300, loss : 0.1856, accuracy : 96.48\n",
            "iteration : 350, loss : 0.1873, accuracy : 96.45\n",
            "Epoch : 115, training loss : 0.1880, training accuracy : 96.46, test loss : 0.2838, test accuracy : 93.97\n",
            "\n",
            "Epoch: 116\n",
            "iteration :  50, loss : 0.1769, accuracy : 96.75\n",
            "iteration : 100, loss : 0.1793, accuracy : 96.75\n",
            "iteration : 150, loss : 0.1812, accuracy : 96.72\n",
            "iteration : 200, loss : 0.1878, accuracy : 96.47\n",
            "iteration : 250, loss : 0.1870, accuracy : 96.50\n",
            "iteration : 300, loss : 0.1888, accuracy : 96.44\n",
            "iteration : 350, loss : 0.1900, accuracy : 96.39\n",
            "Epoch : 116, training loss : 0.1898, training accuracy : 96.40, test loss : 0.3077, test accuracy : 93.09\n",
            "\n",
            "Epoch: 117\n",
            "iteration :  50, loss : 0.1856, accuracy : 96.70\n",
            "iteration : 100, loss : 0.1872, accuracy : 96.66\n",
            "iteration : 150, loss : 0.1874, accuracy : 96.59\n",
            "iteration : 200, loss : 0.1887, accuracy : 96.55\n",
            "iteration : 250, loss : 0.1891, accuracy : 96.50\n",
            "iteration : 300, loss : 0.1888, accuracy : 96.53\n",
            "iteration : 350, loss : 0.1892, accuracy : 96.53\n",
            "Epoch : 117, training loss : 0.1896, training accuracy : 96.51, test loss : 0.2871, test accuracy : 93.78\n",
            "\n",
            "Epoch: 118\n",
            "iteration :  50, loss : 0.1834, accuracy : 96.69\n",
            "iteration : 100, loss : 0.1799, accuracy : 96.74\n",
            "iteration : 150, loss : 0.1830, accuracy : 96.67\n",
            "iteration : 200, loss : 0.1855, accuracy : 96.55\n",
            "iteration : 250, loss : 0.1869, accuracy : 96.53\n",
            "iteration : 300, loss : 0.1881, accuracy : 96.47\n",
            "iteration : 350, loss : 0.1897, accuracy : 96.43\n",
            "Epoch : 118, training loss : 0.1905, training accuracy : 96.42, test loss : 0.2819, test accuracy : 93.82\n",
            "\n",
            "Epoch: 119\n",
            "iteration :  50, loss : 0.1866, accuracy : 96.34\n",
            "iteration : 100, loss : 0.1824, accuracy : 96.45\n",
            "iteration : 150, loss : 0.1848, accuracy : 96.42\n",
            "iteration : 200, loss : 0.1841, accuracy : 96.46\n",
            "iteration : 250, loss : 0.1846, accuracy : 96.50\n",
            "iteration : 300, loss : 0.1855, accuracy : 96.42\n",
            "iteration : 350, loss : 0.1856, accuracy : 96.46\n",
            "Epoch : 119, training loss : 0.1857, training accuracy : 96.47, test loss : 0.2863, test accuracy : 93.75\n",
            "\n",
            "Epoch: 120\n",
            "iteration :  50, loss : 0.1826, accuracy : 96.55\n",
            "iteration : 100, loss : 0.1848, accuracy : 96.46\n",
            "iteration : 150, loss : 0.1837, accuracy : 96.57\n",
            "iteration : 200, loss : 0.1826, accuracy : 96.61\n",
            "iteration : 250, loss : 0.1838, accuracy : 96.60\n",
            "iteration : 300, loss : 0.1836, accuracy : 96.59\n",
            "iteration : 350, loss : 0.1846, accuracy : 96.59\n",
            "Epoch : 120, training loss : 0.1852, training accuracy : 96.57, test loss : 0.2762, test accuracy : 94.11\n",
            "\n",
            "Epoch: 121\n",
            "iteration :  50, loss : 0.1752, accuracy : 96.83\n",
            "iteration : 100, loss : 0.1799, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1783, accuracy : 96.75\n",
            "iteration : 200, loss : 0.1813, accuracy : 96.64\n",
            "iteration : 250, loss : 0.1814, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1825, accuracy : 96.65\n",
            "iteration : 350, loss : 0.1836, accuracy : 96.65\n",
            "Epoch : 121, training loss : 0.1831, training accuracy : 96.68, test loss : 0.2985, test accuracy : 93.56\n",
            "\n",
            "Epoch: 122\n",
            "iteration :  50, loss : 0.1779, accuracy : 96.62\n",
            "iteration : 100, loss : 0.1854, accuracy : 96.66\n",
            "iteration : 150, loss : 0.1835, accuracy : 96.69\n",
            "iteration : 200, loss : 0.1817, accuracy : 96.73\n",
            "iteration : 250, loss : 0.1823, accuracy : 96.65\n",
            "iteration : 300, loss : 0.1823, accuracy : 96.67\n",
            "iteration : 350, loss : 0.1841, accuracy : 96.61\n",
            "Epoch : 122, training loss : 0.1846, training accuracy : 96.59, test loss : 0.2942, test accuracy : 93.65\n",
            "\n",
            "Epoch: 123\n",
            "iteration :  50, loss : 0.1861, accuracy : 96.72\n",
            "iteration : 100, loss : 0.1851, accuracy : 96.62\n",
            "iteration : 150, loss : 0.1851, accuracy : 96.62\n",
            "iteration : 200, loss : 0.1847, accuracy : 96.60\n",
            "iteration : 250, loss : 0.1870, accuracy : 96.51\n",
            "iteration : 300, loss : 0.1859, accuracy : 96.53\n",
            "iteration : 350, loss : 0.1864, accuracy : 96.52\n",
            "Epoch : 123, training loss : 0.1875, training accuracy : 96.48, test loss : 0.2875, test accuracy : 93.72\n",
            "\n",
            "Epoch: 124\n",
            "iteration :  50, loss : 0.1751, accuracy : 96.77\n",
            "iteration : 100, loss : 0.1754, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1789, accuracy : 96.68\n",
            "iteration : 200, loss : 0.1772, accuracy : 96.73\n",
            "iteration : 250, loss : 0.1817, accuracy : 96.62\n",
            "iteration : 300, loss : 0.1838, accuracy : 96.58\n",
            "iteration : 350, loss : 0.1835, accuracy : 96.59\n",
            "Epoch : 124, training loss : 0.1841, training accuracy : 96.58, test loss : 0.2971, test accuracy : 93.48\n",
            "\n",
            "Epoch: 125\n",
            "iteration :  50, loss : 0.1828, accuracy : 96.75\n",
            "iteration : 100, loss : 0.1861, accuracy : 96.59\n",
            "iteration : 150, loss : 0.1875, accuracy : 96.57\n",
            "iteration : 200, loss : 0.1855, accuracy : 96.59\n",
            "iteration : 250, loss : 0.1873, accuracy : 96.53\n",
            "iteration : 300, loss : 0.1869, accuracy : 96.52\n",
            "iteration : 350, loss : 0.1877, accuracy : 96.48\n",
            "Epoch : 125, training loss : 0.1880, training accuracy : 96.46, test loss : 0.2838, test accuracy : 93.80\n",
            "\n",
            "Epoch: 126\n",
            "iteration :  50, loss : 0.1777, accuracy : 96.80\n",
            "iteration : 100, loss : 0.1736, accuracy : 96.84\n",
            "iteration : 150, loss : 0.1764, accuracy : 96.74\n",
            "iteration : 200, loss : 0.1816, accuracy : 96.60\n",
            "iteration : 250, loss : 0.1801, accuracy : 96.66\n",
            "iteration : 300, loss : 0.1821, accuracy : 96.58\n",
            "iteration : 350, loss : 0.1826, accuracy : 96.58\n",
            "Epoch : 126, training loss : 0.1828, training accuracy : 96.58, test loss : 0.2868, test accuracy : 93.84\n",
            "\n",
            "Epoch: 127\n",
            "iteration :  50, loss : 0.1667, accuracy : 97.03\n",
            "iteration : 100, loss : 0.1718, accuracy : 96.91\n",
            "iteration : 150, loss : 0.1740, accuracy : 96.85\n",
            "iteration : 200, loss : 0.1771, accuracy : 96.75\n",
            "iteration : 250, loss : 0.1768, accuracy : 96.77\n",
            "iteration : 300, loss : 0.1805, accuracy : 96.63\n",
            "iteration : 350, loss : 0.1811, accuracy : 96.64\n",
            "Epoch : 127, training loss : 0.1802, training accuracy : 96.66, test loss : 0.3098, test accuracy : 93.04\n",
            "\n",
            "Epoch: 128\n",
            "iteration :  50, loss : 0.1650, accuracy : 97.05\n",
            "iteration : 100, loss : 0.1671, accuracy : 96.98\n",
            "iteration : 150, loss : 0.1740, accuracy : 96.90\n",
            "iteration : 200, loss : 0.1751, accuracy : 96.80\n",
            "iteration : 250, loss : 0.1770, accuracy : 96.74\n",
            "iteration : 300, loss : 0.1793, accuracy : 96.66\n",
            "iteration : 350, loss : 0.1809, accuracy : 96.62\n",
            "Epoch : 128, training loss : 0.1813, training accuracy : 96.62, test loss : 0.3040, test accuracy : 93.26\n",
            "\n",
            "Epoch: 129\n",
            "iteration :  50, loss : 0.1795, accuracy : 96.89\n",
            "iteration : 100, loss : 0.1810, accuracy : 96.87\n",
            "iteration : 150, loss : 0.1761, accuracy : 96.94\n",
            "iteration : 200, loss : 0.1767, accuracy : 96.92\n",
            "iteration : 250, loss : 0.1775, accuracy : 96.88\n",
            "iteration : 300, loss : 0.1772, accuracy : 96.83\n",
            "iteration : 350, loss : 0.1773, accuracy : 96.81\n",
            "Epoch : 129, training loss : 0.1783, training accuracy : 96.80, test loss : 0.3021, test accuracy : 93.25\n",
            "\n",
            "Epoch: 130\n",
            "iteration :  50, loss : 0.1702, accuracy : 96.91\n",
            "iteration : 100, loss : 0.1715, accuracy : 97.00\n",
            "iteration : 150, loss : 0.1729, accuracy : 96.93\n",
            "iteration : 200, loss : 0.1725, accuracy : 96.95\n",
            "iteration : 250, loss : 0.1755, accuracy : 96.85\n",
            "iteration : 300, loss : 0.1750, accuracy : 96.86\n",
            "iteration : 350, loss : 0.1766, accuracy : 96.81\n",
            "Epoch : 130, training loss : 0.1768, training accuracy : 96.79, test loss : 0.2894, test accuracy : 93.75\n",
            "\n",
            "Epoch: 131\n",
            "iteration :  50, loss : 0.1847, accuracy : 96.55\n",
            "iteration : 100, loss : 0.1776, accuracy : 96.73\n",
            "iteration : 150, loss : 0.1765, accuracy : 96.77\n",
            "iteration : 200, loss : 0.1777, accuracy : 96.77\n",
            "iteration : 250, loss : 0.1809, accuracy : 96.67\n",
            "iteration : 300, loss : 0.1803, accuracy : 96.67\n",
            "iteration : 350, loss : 0.1797, accuracy : 96.69\n",
            "Epoch : 131, training loss : 0.1807, training accuracy : 96.67, test loss : 0.2878, test accuracy : 93.83\n",
            "\n",
            "Epoch: 132\n",
            "iteration :  50, loss : 0.1752, accuracy : 96.91\n",
            "iteration : 100, loss : 0.1722, accuracy : 96.93\n",
            "iteration : 150, loss : 0.1716, accuracy : 96.89\n",
            "iteration : 200, loss : 0.1706, accuracy : 96.94\n",
            "iteration : 250, loss : 0.1747, accuracy : 96.82\n",
            "iteration : 300, loss : 0.1757, accuracy : 96.80\n",
            "iteration : 350, loss : 0.1777, accuracy : 96.77\n",
            "Epoch : 132, training loss : 0.1780, training accuracy : 96.75, test loss : 0.2876, test accuracy : 93.80\n",
            "\n",
            "Epoch: 133\n",
            "iteration :  50, loss : 0.1713, accuracy : 97.05\n",
            "iteration : 100, loss : 0.1700, accuracy : 96.94\n",
            "iteration : 150, loss : 0.1701, accuracy : 96.99\n",
            "iteration : 200, loss : 0.1737, accuracy : 96.90\n",
            "iteration : 250, loss : 0.1771, accuracy : 96.78\n",
            "iteration : 300, loss : 0.1781, accuracy : 96.76\n",
            "iteration : 350, loss : 0.1787, accuracy : 96.75\n",
            "Epoch : 133, training loss : 0.1793, training accuracy : 96.74, test loss : 0.2775, test accuracy : 94.06\n",
            "\n",
            "Epoch: 134\n",
            "iteration :  50, loss : 0.1626, accuracy : 97.22\n",
            "iteration : 100, loss : 0.1787, accuracy : 96.78\n",
            "iteration : 150, loss : 0.1758, accuracy : 96.87\n",
            "iteration : 200, loss : 0.1760, accuracy : 96.85\n",
            "iteration : 250, loss : 0.1758, accuracy : 96.86\n",
            "iteration : 300, loss : 0.1756, accuracy : 96.87\n",
            "iteration : 350, loss : 0.1764, accuracy : 96.81\n",
            "Epoch : 134, training loss : 0.1771, training accuracy : 96.78, test loss : 0.2895, test accuracy : 93.62\n",
            "\n",
            "Epoch: 135\n",
            "iteration :  50, loss : 0.1759, accuracy : 96.70\n",
            "iteration : 100, loss : 0.1784, accuracy : 96.71\n",
            "iteration : 150, loss : 0.1792, accuracy : 96.76\n",
            "iteration : 200, loss : 0.1778, accuracy : 96.77\n",
            "iteration : 250, loss : 0.1765, accuracy : 96.80\n",
            "iteration : 300, loss : 0.1771, accuracy : 96.77\n",
            "iteration : 350, loss : 0.1753, accuracy : 96.81\n",
            "Epoch : 135, training loss : 0.1752, training accuracy : 96.79, test loss : 0.2810, test accuracy : 94.03\n",
            "\n",
            "Epoch: 136\n",
            "iteration :  50, loss : 0.1596, accuracy : 97.41\n",
            "iteration : 100, loss : 0.1701, accuracy : 97.12\n",
            "iteration : 150, loss : 0.1697, accuracy : 97.14\n",
            "iteration : 200, loss : 0.1744, accuracy : 96.95\n",
            "iteration : 250, loss : 0.1738, accuracy : 96.92\n",
            "iteration : 300, loss : 0.1774, accuracy : 96.82\n",
            "iteration : 350, loss : 0.1778, accuracy : 96.80\n",
            "Epoch : 136, training loss : 0.1774, training accuracy : 96.81, test loss : 0.2849, test accuracy : 93.91\n",
            "\n",
            "Epoch: 137\n",
            "iteration :  50, loss : 0.1741, accuracy : 96.72\n",
            "iteration : 100, loss : 0.1750, accuracy : 96.92\n",
            "iteration : 150, loss : 0.1779, accuracy : 96.84\n",
            "iteration : 200, loss : 0.1796, accuracy : 96.76\n",
            "iteration : 250, loss : 0.1803, accuracy : 96.79\n",
            "iteration : 300, loss : 0.1776, accuracy : 96.83\n",
            "iteration : 350, loss : 0.1774, accuracy : 96.82\n",
            "Epoch : 137, training loss : 0.1782, training accuracy : 96.79, test loss : 0.2927, test accuracy : 93.65\n",
            "\n",
            "Epoch: 138\n",
            "iteration :  50, loss : 0.1735, accuracy : 96.94\n",
            "iteration : 100, loss : 0.1693, accuracy : 97.05\n",
            "iteration : 150, loss : 0.1684, accuracy : 97.09\n",
            "iteration : 200, loss : 0.1691, accuracy : 97.07\n",
            "iteration : 250, loss : 0.1710, accuracy : 97.01\n",
            "iteration : 300, loss : 0.1737, accuracy : 96.97\n",
            "iteration : 350, loss : 0.1753, accuracy : 96.92\n",
            "Epoch : 138, training loss : 0.1753, training accuracy : 96.92, test loss : 0.2936, test accuracy : 93.68\n",
            "\n",
            "Epoch: 139\n",
            "iteration :  50, loss : 0.1645, accuracy : 97.34\n",
            "iteration : 100, loss : 0.1668, accuracy : 97.10\n",
            "iteration : 150, loss : 0.1681, accuracy : 97.07\n",
            "iteration : 200, loss : 0.1712, accuracy : 97.00\n",
            "iteration : 250, loss : 0.1717, accuracy : 96.99\n",
            "iteration : 300, loss : 0.1723, accuracy : 96.96\n",
            "iteration : 350, loss : 0.1734, accuracy : 96.90\n",
            "Epoch : 139, training loss : 0.1743, training accuracy : 96.89, test loss : 0.2896, test accuracy : 93.72\n",
            "\n",
            "Epoch: 140\n",
            "iteration :  50, loss : 0.1725, accuracy : 96.98\n",
            "iteration : 100, loss : 0.1670, accuracy : 97.15\n",
            "iteration : 150, loss : 0.1657, accuracy : 97.19\n",
            "iteration : 200, loss : 0.1658, accuracy : 97.18\n",
            "iteration : 250, loss : 0.1696, accuracy : 97.01\n",
            "iteration : 300, loss : 0.1716, accuracy : 96.96\n",
            "iteration : 350, loss : 0.1712, accuracy : 96.96\n",
            "Epoch : 140, training loss : 0.1714, training accuracy : 96.96, test loss : 0.2904, test accuracy : 93.70\n",
            "\n",
            "Epoch: 141\n",
            "iteration :  50, loss : 0.1553, accuracy : 97.42\n",
            "iteration : 100, loss : 0.1646, accuracy : 97.27\n",
            "iteration : 150, loss : 0.1632, accuracy : 97.33\n",
            "iteration : 200, loss : 0.1634, accuracy : 97.30\n",
            "iteration : 250, loss : 0.1686, accuracy : 97.11\n",
            "iteration : 300, loss : 0.1702, accuracy : 97.04\n",
            "iteration : 350, loss : 0.1736, accuracy : 96.93\n",
            "Epoch : 141, training loss : 0.1733, training accuracy : 96.94, test loss : 0.2775, test accuracy : 94.15\n",
            "\n",
            "Epoch: 142\n",
            "iteration :  50, loss : 0.1684, accuracy : 97.02\n",
            "iteration : 100, loss : 0.1653, accuracy : 97.16\n",
            "iteration : 150, loss : 0.1680, accuracy : 97.06\n",
            "iteration : 200, loss : 0.1665, accuracy : 97.11\n",
            "iteration : 250, loss : 0.1681, accuracy : 97.02\n",
            "iteration : 300, loss : 0.1699, accuracy : 96.99\n",
            "iteration : 350, loss : 0.1702, accuracy : 96.98\n",
            "Epoch : 142, training loss : 0.1707, training accuracy : 96.97, test loss : 0.2900, test accuracy : 93.70\n",
            "\n",
            "Epoch: 143\n",
            "iteration :  50, loss : 0.1604, accuracy : 97.31\n",
            "iteration : 100, loss : 0.1671, accuracy : 97.05\n",
            "iteration : 150, loss : 0.1685, accuracy : 96.97\n",
            "iteration : 200, loss : 0.1704, accuracy : 96.96\n",
            "iteration : 250, loss : 0.1725, accuracy : 96.90\n",
            "iteration : 300, loss : 0.1720, accuracy : 96.89\n",
            "iteration : 350, loss : 0.1732, accuracy : 96.87\n",
            "Epoch : 143, training loss : 0.1730, training accuracy : 96.89, test loss : 0.2706, test accuracy : 94.26\n",
            "\n",
            "Epoch: 144\n",
            "iteration :  50, loss : 0.1605, accuracy : 97.45\n",
            "iteration : 100, loss : 0.1573, accuracy : 97.52\n",
            "iteration : 150, loss : 0.1635, accuracy : 97.31\n",
            "iteration : 200, loss : 0.1654, accuracy : 97.25\n",
            "iteration : 250, loss : 0.1670, accuracy : 97.20\n",
            "iteration : 300, loss : 0.1678, accuracy : 97.15\n",
            "iteration : 350, loss : 0.1686, accuracy : 97.10\n",
            "Epoch : 144, training loss : 0.1699, training accuracy : 97.06, test loss : 0.2859, test accuracy : 93.74\n",
            "\n",
            "Epoch: 145\n",
            "iteration :  50, loss : 0.1664, accuracy : 97.22\n",
            "iteration : 100, loss : 0.1692, accuracy : 97.09\n",
            "iteration : 150, loss : 0.1668, accuracy : 97.06\n",
            "iteration : 200, loss : 0.1671, accuracy : 97.09\n",
            "iteration : 250, loss : 0.1669, accuracy : 97.12\n",
            "iteration : 300, loss : 0.1674, accuracy : 97.08\n",
            "iteration : 350, loss : 0.1677, accuracy : 97.09\n",
            "Epoch : 145, training loss : 0.1671, training accuracy : 97.12, test loss : 0.2860, test accuracy : 94.01\n",
            "\n",
            "Epoch: 146\n",
            "iteration :  50, loss : 0.1630, accuracy : 97.44\n",
            "iteration : 100, loss : 0.1617, accuracy : 97.44\n",
            "iteration : 150, loss : 0.1647, accuracy : 97.29\n",
            "iteration : 200, loss : 0.1659, accuracy : 97.18\n",
            "iteration : 250, loss : 0.1662, accuracy : 97.16\n",
            "iteration : 300, loss : 0.1682, accuracy : 97.09\n",
            "iteration : 350, loss : 0.1694, accuracy : 97.04\n",
            "Epoch : 146, training loss : 0.1690, training accuracy : 97.06, test loss : 0.2805, test accuracy : 93.98\n",
            "\n",
            "Epoch: 147\n",
            "iteration :  50, loss : 0.1501, accuracy : 97.73\n",
            "iteration : 100, loss : 0.1565, accuracy : 97.55\n",
            "iteration : 150, loss : 0.1606, accuracy : 97.41\n",
            "iteration : 200, loss : 0.1624, accuracy : 97.33\n",
            "iteration : 250, loss : 0.1639, accuracy : 97.26\n",
            "iteration : 300, loss : 0.1667, accuracy : 97.17\n",
            "iteration : 350, loss : 0.1658, accuracy : 97.19\n",
            "Epoch : 147, training loss : 0.1663, training accuracy : 97.16, test loss : 0.2739, test accuracy : 94.28\n",
            "\n",
            "Epoch: 148\n",
            "iteration :  50, loss : 0.1656, accuracy : 97.50\n",
            "iteration : 100, loss : 0.1614, accuracy : 97.52\n",
            "iteration : 150, loss : 0.1606, accuracy : 97.51\n",
            "iteration : 200, loss : 0.1626, accuracy : 97.43\n",
            "iteration : 250, loss : 0.1642, accuracy : 97.34\n",
            "iteration : 300, loss : 0.1658, accuracy : 97.25\n",
            "iteration : 350, loss : 0.1664, accuracy : 97.22\n",
            "Epoch : 148, training loss : 0.1670, training accuracy : 97.20, test loss : 0.2840, test accuracy : 93.96\n",
            "\n",
            "Epoch: 149\n",
            "iteration :  50, loss : 0.1505, accuracy : 97.52\n",
            "iteration : 100, loss : 0.1561, accuracy : 97.40\n",
            "iteration : 150, loss : 0.1560, accuracy : 97.38\n",
            "iteration : 200, loss : 0.1612, accuracy : 97.24\n",
            "iteration : 250, loss : 0.1654, accuracy : 97.08\n",
            "iteration : 300, loss : 0.1673, accuracy : 97.02\n",
            "iteration : 350, loss : 0.1659, accuracy : 97.08\n",
            "Epoch : 149, training loss : 0.1657, training accuracy : 97.08, test loss : 0.2852, test accuracy : 93.83\n",
            "\n",
            "Epoch: 150\n",
            "iteration :  50, loss : 0.1634, accuracy : 97.19\n",
            "iteration : 100, loss : 0.1644, accuracy : 97.16\n",
            "iteration : 150, loss : 0.1635, accuracy : 97.29\n",
            "iteration : 200, loss : 0.1655, accuracy : 97.18\n",
            "iteration : 250, loss : 0.1641, accuracy : 97.18\n",
            "iteration : 300, loss : 0.1647, accuracy : 97.14\n",
            "iteration : 350, loss : 0.1664, accuracy : 97.11\n",
            "Epoch : 150, training loss : 0.1667, training accuracy : 97.09, test loss : 0.2750, test accuracy : 94.22\n",
            "\n",
            "Epoch: 151\n",
            "iteration :  50, loss : 0.1552, accuracy : 97.56\n",
            "iteration : 100, loss : 0.1588, accuracy : 97.43\n",
            "iteration : 150, loss : 0.1599, accuracy : 97.34\n",
            "iteration : 200, loss : 0.1621, accuracy : 97.30\n",
            "iteration : 250, loss : 0.1614, accuracy : 97.35\n",
            "iteration : 300, loss : 0.1636, accuracy : 97.26\n",
            "iteration : 350, loss : 0.1658, accuracy : 97.19\n",
            "Epoch : 151, training loss : 0.1661, training accuracy : 97.16, test loss : 0.2901, test accuracy : 93.87\n",
            "\n",
            "Epoch: 152\n",
            "iteration :  50, loss : 0.1638, accuracy : 97.16\n",
            "iteration : 100, loss : 0.1612, accuracy : 97.25\n",
            "iteration : 150, loss : 0.1665, accuracy : 97.07\n",
            "iteration : 200, loss : 0.1653, accuracy : 97.12\n",
            "iteration : 250, loss : 0.1657, accuracy : 97.09\n",
            "iteration : 300, loss : 0.1656, accuracy : 97.08\n",
            "iteration : 350, loss : 0.1661, accuracy : 97.06\n",
            "Epoch : 152, training loss : 0.1666, training accuracy : 97.06, test loss : 0.2852, test accuracy : 93.86\n",
            "\n",
            "Epoch: 153\n",
            "iteration :  50, loss : 0.1563, accuracy : 97.38\n",
            "iteration : 100, loss : 0.1614, accuracy : 97.24\n",
            "iteration : 150, loss : 0.1615, accuracy : 97.27\n",
            "iteration : 200, loss : 0.1625, accuracy : 97.27\n",
            "iteration : 250, loss : 0.1628, accuracy : 97.25\n",
            "iteration : 300, loss : 0.1651, accuracy : 97.16\n",
            "iteration : 350, loss : 0.1652, accuracy : 97.16\n",
            "Epoch : 153, training loss : 0.1648, training accuracy : 97.16, test loss : 0.2817, test accuracy : 93.97\n",
            "\n",
            "Epoch: 154\n",
            "iteration :  50, loss : 0.1562, accuracy : 97.23\n",
            "iteration : 100, loss : 0.1529, accuracy : 97.41\n",
            "iteration : 150, loss : 0.1559, accuracy : 97.38\n",
            "iteration : 200, loss : 0.1565, accuracy : 97.35\n",
            "iteration : 250, loss : 0.1559, accuracy : 97.37\n",
            "iteration : 300, loss : 0.1572, accuracy : 97.36\n",
            "iteration : 350, loss : 0.1583, accuracy : 97.34\n",
            "Epoch : 154, training loss : 0.1588, training accuracy : 97.33, test loss : 0.2767, test accuracy : 94.18\n",
            "\n",
            "Epoch: 155\n",
            "iteration :  50, loss : 0.1675, accuracy : 97.03\n",
            "iteration : 100, loss : 0.1625, accuracy : 97.28\n",
            "iteration : 150, loss : 0.1629, accuracy : 97.28\n",
            "iteration : 200, loss : 0.1637, accuracy : 97.28\n",
            "iteration : 250, loss : 0.1648, accuracy : 97.22\n",
            "iteration : 300, loss : 0.1635, accuracy : 97.20\n",
            "iteration : 350, loss : 0.1644, accuracy : 97.16\n",
            "Epoch : 155, training loss : 0.1637, training accuracy : 97.18, test loss : 0.2784, test accuracy : 94.14\n",
            "\n",
            "Epoch: 156\n",
            "iteration :  50, loss : 0.1527, accuracy : 97.39\n",
            "iteration : 100, loss : 0.1590, accuracy : 97.41\n",
            "iteration : 150, loss : 0.1618, accuracy : 97.30\n",
            "iteration : 200, loss : 0.1598, accuracy : 97.34\n",
            "iteration : 250, loss : 0.1621, accuracy : 97.28\n",
            "iteration : 300, loss : 0.1640, accuracy : 97.25\n",
            "iteration : 350, loss : 0.1652, accuracy : 97.19\n",
            "Epoch : 156, training loss : 0.1649, training accuracy : 97.20, test loss : 0.2752, test accuracy : 94.19\n",
            "\n",
            "Epoch: 157\n",
            "iteration :  50, loss : 0.1569, accuracy : 97.38\n",
            "iteration : 100, loss : 0.1537, accuracy : 97.48\n",
            "iteration : 150, loss : 0.1566, accuracy : 97.38\n",
            "iteration : 200, loss : 0.1609, accuracy : 97.26\n",
            "iteration : 250, loss : 0.1623, accuracy : 97.19\n",
            "iteration : 300, loss : 0.1602, accuracy : 97.25\n",
            "iteration : 350, loss : 0.1601, accuracy : 97.24\n",
            "Epoch : 157, training loss : 0.1599, training accuracy : 97.26, test loss : 0.2851, test accuracy : 94.07\n",
            "\n",
            "Epoch: 158\n",
            "iteration :  50, loss : 0.1677, accuracy : 97.25\n",
            "iteration : 100, loss : 0.1596, accuracy : 97.41\n",
            "iteration : 150, loss : 0.1631, accuracy : 97.27\n",
            "iteration : 200, loss : 0.1636, accuracy : 97.23\n",
            "iteration : 250, loss : 0.1641, accuracy : 97.23\n",
            "iteration : 300, loss : 0.1634, accuracy : 97.24\n",
            "iteration : 350, loss : 0.1626, accuracy : 97.28\n",
            "Epoch : 158, training loss : 0.1621, training accuracy : 97.30, test loss : 0.2932, test accuracy : 93.92\n",
            "\n",
            "Epoch: 159\n",
            "iteration :  50, loss : 0.1611, accuracy : 97.22\n",
            "iteration : 100, loss : 0.1670, accuracy : 97.01\n",
            "iteration : 150, loss : 0.1653, accuracy : 97.08\n",
            "iteration : 200, loss : 0.1663, accuracy : 97.09\n",
            "iteration : 250, loss : 0.1646, accuracy : 97.15\n",
            "iteration : 300, loss : 0.1643, accuracy : 97.20\n",
            "iteration : 350, loss : 0.1635, accuracy : 97.21\n",
            "Epoch : 159, training loss : 0.1625, training accuracy : 97.23, test loss : 0.2913, test accuracy : 93.71\n",
            "\n",
            "Epoch: 160\n",
            "iteration :  50, loss : 0.1453, accuracy : 97.58\n",
            "iteration : 100, loss : 0.1510, accuracy : 97.41\n",
            "iteration : 150, loss : 0.1551, accuracy : 97.36\n",
            "iteration : 200, loss : 0.1553, accuracy : 97.36\n",
            "iteration : 250, loss : 0.1552, accuracy : 97.39\n",
            "iteration : 300, loss : 0.1577, accuracy : 97.34\n",
            "iteration : 350, loss : 0.1562, accuracy : 97.40\n",
            "Epoch : 160, training loss : 0.1556, training accuracy : 97.41, test loss : 0.2865, test accuracy : 94.07\n",
            "\n",
            "Epoch: 161\n",
            "iteration :  50, loss : 0.1501, accuracy : 97.70\n",
            "iteration : 100, loss : 0.1496, accuracy : 97.69\n",
            "iteration : 150, loss : 0.1543, accuracy : 97.49\n",
            "iteration : 200, loss : 0.1560, accuracy : 97.43\n",
            "iteration : 250, loss : 0.1569, accuracy : 97.38\n",
            "iteration : 300, loss : 0.1581, accuracy : 97.38\n",
            "iteration : 350, loss : 0.1561, accuracy : 97.42\n",
            "Epoch : 161, training loss : 0.1561, training accuracy : 97.42, test loss : 0.2774, test accuracy : 94.30\n",
            "\n",
            "Epoch: 162\n",
            "iteration :  50, loss : 0.1468, accuracy : 97.73\n",
            "iteration : 100, loss : 0.1562, accuracy : 97.36\n",
            "iteration : 150, loss : 0.1574, accuracy : 97.35\n",
            "iteration : 200, loss : 0.1566, accuracy : 97.34\n",
            "iteration : 250, loss : 0.1565, accuracy : 97.38\n",
            "iteration : 300, loss : 0.1583, accuracy : 97.35\n",
            "iteration : 350, loss : 0.1591, accuracy : 97.34\n",
            "Epoch : 162, training loss : 0.1595, training accuracy : 97.32, test loss : 0.2780, test accuracy : 94.26\n",
            "\n",
            "Epoch: 163\n",
            "iteration :  50, loss : 0.1571, accuracy : 97.42\n",
            "iteration : 100, loss : 0.1522, accuracy : 97.55\n",
            "iteration : 150, loss : 0.1506, accuracy : 97.66\n",
            "iteration : 200, loss : 0.1513, accuracy : 97.62\n",
            "iteration : 250, loss : 0.1542, accuracy : 97.53\n",
            "iteration : 300, loss : 0.1562, accuracy : 97.51\n",
            "iteration : 350, loss : 0.1559, accuracy : 97.50\n",
            "Epoch : 163, training loss : 0.1561, training accuracy : 97.49, test loss : 0.2864, test accuracy : 93.95\n",
            "\n",
            "Epoch: 164\n",
            "iteration :  50, loss : 0.1475, accuracy : 97.83\n",
            "iteration : 100, loss : 0.1474, accuracy : 97.72\n",
            "iteration : 150, loss : 0.1484, accuracy : 97.65\n",
            "iteration : 200, loss : 0.1500, accuracy : 97.63\n",
            "iteration : 250, loss : 0.1510, accuracy : 97.59\n",
            "iteration : 300, loss : 0.1510, accuracy : 97.60\n",
            "iteration : 350, loss : 0.1539, accuracy : 97.54\n",
            "Epoch : 164, training loss : 0.1538, training accuracy : 97.52, test loss : 0.2873, test accuracy : 93.87\n",
            "\n",
            "Epoch: 165\n",
            "iteration :  50, loss : 0.1415, accuracy : 97.83\n",
            "iteration : 100, loss : 0.1465, accuracy : 97.63\n",
            "iteration : 150, loss : 0.1494, accuracy : 97.63\n",
            "iteration : 200, loss : 0.1498, accuracy : 97.64\n",
            "iteration : 250, loss : 0.1531, accuracy : 97.56\n",
            "iteration : 300, loss : 0.1538, accuracy : 97.54\n",
            "iteration : 350, loss : 0.1545, accuracy : 97.51\n",
            "Epoch : 165, training loss : 0.1547, training accuracy : 97.51, test loss : 0.2809, test accuracy : 94.16\n",
            "\n",
            "Epoch: 166\n",
            "iteration :  50, loss : 0.1407, accuracy : 97.84\n",
            "iteration : 100, loss : 0.1499, accuracy : 97.61\n",
            "iteration : 150, loss : 0.1523, accuracy : 97.53\n",
            "iteration : 200, loss : 0.1543, accuracy : 97.45\n",
            "iteration : 250, loss : 0.1526, accuracy : 97.52\n",
            "iteration : 300, loss : 0.1518, accuracy : 97.53\n",
            "iteration : 350, loss : 0.1515, accuracy : 97.53\n",
            "Epoch : 166, training loss : 0.1519, training accuracy : 97.52, test loss : 0.2878, test accuracy : 94.13\n",
            "\n",
            "Epoch: 167\n",
            "iteration :  50, loss : 0.1583, accuracy : 97.25\n",
            "iteration : 100, loss : 0.1596, accuracy : 97.30\n",
            "iteration : 150, loss : 0.1539, accuracy : 97.47\n",
            "iteration : 200, loss : 0.1543, accuracy : 97.49\n",
            "iteration : 250, loss : 0.1517, accuracy : 97.58\n",
            "iteration : 300, loss : 0.1527, accuracy : 97.57\n",
            "iteration : 350, loss : 0.1536, accuracy : 97.54\n",
            "Epoch : 167, training loss : 0.1535, training accuracy : 97.55, test loss : 0.2775, test accuracy : 94.25\n",
            "\n",
            "Epoch: 168\n",
            "iteration :  50, loss : 0.1513, accuracy : 97.77\n",
            "iteration : 100, loss : 0.1499, accuracy : 97.68\n",
            "iteration : 150, loss : 0.1548, accuracy : 97.51\n",
            "iteration : 200, loss : 0.1541, accuracy : 97.51\n",
            "iteration : 250, loss : 0.1553, accuracy : 97.47\n",
            "iteration : 300, loss : 0.1553, accuracy : 97.45\n",
            "iteration : 350, loss : 0.1558, accuracy : 97.45\n",
            "Epoch : 168, training loss : 0.1556, training accuracy : 97.47, test loss : 0.2865, test accuracy : 93.95\n",
            "\n",
            "Epoch: 169\n",
            "iteration :  50, loss : 0.1458, accuracy : 97.81\n",
            "iteration : 100, loss : 0.1497, accuracy : 97.70\n",
            "iteration : 150, loss : 0.1490, accuracy : 97.73\n",
            "iteration : 200, loss : 0.1527, accuracy : 97.63\n",
            "iteration : 250, loss : 0.1523, accuracy : 97.60\n",
            "iteration : 300, loss : 0.1522, accuracy : 97.60\n",
            "iteration : 350, loss : 0.1532, accuracy : 97.58\n",
            "Epoch : 169, training loss : 0.1538, training accuracy : 97.56, test loss : 0.2776, test accuracy : 94.20\n",
            "\n",
            "Epoch: 170\n",
            "iteration :  50, loss : 0.1411, accuracy : 97.86\n",
            "iteration : 100, loss : 0.1508, accuracy : 97.52\n",
            "iteration : 150, loss : 0.1494, accuracy : 97.60\n",
            "iteration : 200, loss : 0.1495, accuracy : 97.62\n",
            "iteration : 250, loss : 0.1511, accuracy : 97.51\n",
            "iteration : 300, loss : 0.1516, accuracy : 97.52\n",
            "iteration : 350, loss : 0.1515, accuracy : 97.51\n",
            "Epoch : 170, training loss : 0.1516, training accuracy : 97.52, test loss : 0.2780, test accuracy : 94.16\n",
            "\n",
            "Epoch: 171\n",
            "iteration :  50, loss : 0.1433, accuracy : 97.91\n",
            "iteration : 100, loss : 0.1433, accuracy : 97.92\n",
            "iteration : 150, loss : 0.1422, accuracy : 97.95\n",
            "iteration : 200, loss : 0.1445, accuracy : 97.88\n",
            "iteration : 250, loss : 0.1449, accuracy : 97.83\n",
            "iteration : 300, loss : 0.1473, accuracy : 97.76\n",
            "iteration : 350, loss : 0.1480, accuracy : 97.73\n",
            "Epoch : 171, training loss : 0.1477, training accuracy : 97.73, test loss : 0.2844, test accuracy : 94.23\n",
            "\n",
            "Epoch: 172\n",
            "iteration :  50, loss : 0.1364, accuracy : 97.89\n",
            "iteration : 100, loss : 0.1455, accuracy : 97.71\n",
            "iteration : 150, loss : 0.1471, accuracy : 97.70\n",
            "iteration : 200, loss : 0.1484, accuracy : 97.68\n",
            "iteration : 250, loss : 0.1487, accuracy : 97.65\n",
            "iteration : 300, loss : 0.1509, accuracy : 97.58\n",
            "iteration : 350, loss : 0.1501, accuracy : 97.60\n",
            "Epoch : 172, training loss : 0.1497, training accuracy : 97.62, test loss : 0.2823, test accuracy : 94.25\n",
            "\n",
            "Epoch: 173\n",
            "iteration :  50, loss : 0.1502, accuracy : 97.77\n",
            "iteration : 100, loss : 0.1465, accuracy : 97.84\n",
            "iteration : 150, loss : 0.1481, accuracy : 97.77\n",
            "iteration : 200, loss : 0.1467, accuracy : 97.79\n",
            "iteration : 250, loss : 0.1471, accuracy : 97.78\n",
            "iteration : 300, loss : 0.1476, accuracy : 97.76\n",
            "iteration : 350, loss : 0.1482, accuracy : 97.72\n",
            "Epoch : 173, training loss : 0.1484, training accuracy : 97.71, test loss : 0.2848, test accuracy : 94.05\n",
            "\n",
            "Epoch: 174\n",
            "iteration :  50, loss : 0.1523, accuracy : 97.75\n",
            "iteration : 100, loss : 0.1466, accuracy : 97.78\n",
            "iteration : 150, loss : 0.1430, accuracy : 97.92\n",
            "iteration : 200, loss : 0.1432, accuracy : 97.91\n",
            "iteration : 250, loss : 0.1456, accuracy : 97.80\n",
            "iteration : 300, loss : 0.1461, accuracy : 97.78\n",
            "iteration : 350, loss : 0.1475, accuracy : 97.72\n",
            "Epoch : 174, training loss : 0.1475, training accuracy : 97.72, test loss : 0.2756, test accuracy : 94.35\n",
            "\n",
            "Epoch: 175\n",
            "iteration :  50, loss : 0.1351, accuracy : 98.03\n",
            "iteration : 100, loss : 0.1423, accuracy : 97.80\n",
            "iteration : 150, loss : 0.1431, accuracy : 97.76\n",
            "iteration : 200, loss : 0.1437, accuracy : 97.73\n",
            "iteration : 250, loss : 0.1436, accuracy : 97.78\n",
            "iteration : 300, loss : 0.1454, accuracy : 97.72\n",
            "iteration : 350, loss : 0.1462, accuracy : 97.69\n",
            "Epoch : 175, training loss : 0.1459, training accuracy : 97.70, test loss : 0.2851, test accuracy : 94.03\n",
            "\n",
            "Epoch: 176\n",
            "iteration :  50, loss : 0.1401, accuracy : 97.97\n",
            "iteration : 100, loss : 0.1438, accuracy : 97.82\n",
            "iteration : 150, loss : 0.1462, accuracy : 97.77\n",
            "iteration : 200, loss : 0.1474, accuracy : 97.76\n",
            "iteration : 250, loss : 0.1470, accuracy : 97.74\n",
            "iteration : 300, loss : 0.1472, accuracy : 97.74\n",
            "iteration : 350, loss : 0.1479, accuracy : 97.70\n",
            "Epoch : 176, training loss : 0.1478, training accuracy : 97.69, test loss : 0.2874, test accuracy : 94.06\n",
            "\n",
            "Epoch: 177\n",
            "iteration :  50, loss : 0.1597, accuracy : 97.31\n",
            "iteration : 100, loss : 0.1498, accuracy : 97.73\n",
            "iteration : 150, loss : 0.1459, accuracy : 97.87\n",
            "iteration : 200, loss : 0.1475, accuracy : 97.80\n",
            "iteration : 250, loss : 0.1463, accuracy : 97.83\n",
            "iteration : 300, loss : 0.1465, accuracy : 97.83\n",
            "iteration : 350, loss : 0.1472, accuracy : 97.80\n",
            "Epoch : 177, training loss : 0.1466, training accuracy : 97.83, test loss : 0.2839, test accuracy : 94.07\n",
            "\n",
            "Epoch: 178\n",
            "iteration :  50, loss : 0.1380, accuracy : 98.00\n",
            "iteration : 100, loss : 0.1390, accuracy : 98.02\n",
            "iteration : 150, loss : 0.1427, accuracy : 97.86\n",
            "iteration : 200, loss : 0.1432, accuracy : 97.81\n",
            "iteration : 250, loss : 0.1435, accuracy : 97.80\n",
            "iteration : 300, loss : 0.1429, accuracy : 97.82\n",
            "iteration : 350, loss : 0.1427, accuracy : 97.83\n",
            "Epoch : 178, training loss : 0.1434, training accuracy : 97.80, test loss : 0.2841, test accuracy : 94.18\n",
            "\n",
            "Epoch: 179\n",
            "iteration :  50, loss : 0.1404, accuracy : 97.92\n",
            "iteration : 100, loss : 0.1376, accuracy : 98.11\n",
            "iteration : 150, loss : 0.1430, accuracy : 97.90\n",
            "iteration : 200, loss : 0.1428, accuracy : 97.88\n",
            "iteration : 250, loss : 0.1430, accuracy : 97.86\n",
            "iteration : 300, loss : 0.1436, accuracy : 97.83\n",
            "iteration : 350, loss : 0.1436, accuracy : 97.81\n",
            "Epoch : 179, training loss : 0.1444, training accuracy : 97.79, test loss : 0.2827, test accuracy : 94.06\n",
            "\n",
            "Epoch: 180\n",
            "iteration :  50, loss : 0.1456, accuracy : 97.95\n",
            "iteration : 100, loss : 0.1409, accuracy : 97.92\n",
            "iteration : 150, loss : 0.1435, accuracy : 97.86\n",
            "iteration : 200, loss : 0.1437, accuracy : 97.85\n",
            "iteration : 250, loss : 0.1437, accuracy : 97.86\n",
            "iteration : 300, loss : 0.1447, accuracy : 97.81\n",
            "iteration : 350, loss : 0.1450, accuracy : 97.76\n",
            "Epoch : 180, training loss : 0.1453, training accuracy : 97.76, test loss : 0.2759, test accuracy : 94.40\n",
            "\n",
            "Epoch: 181\n",
            "iteration :  50, loss : 0.1453, accuracy : 97.92\n",
            "iteration : 100, loss : 0.1396, accuracy : 98.04\n",
            "iteration : 150, loss : 0.1404, accuracy : 97.93\n",
            "iteration : 200, loss : 0.1434, accuracy : 97.87\n",
            "iteration : 250, loss : 0.1450, accuracy : 97.80\n",
            "iteration : 300, loss : 0.1451, accuracy : 97.78\n",
            "iteration : 350, loss : 0.1459, accuracy : 97.77\n",
            "Epoch : 181, training loss : 0.1465, training accuracy : 97.76, test loss : 0.2800, test accuracy : 94.24\n",
            "\n",
            "Epoch: 182\n",
            "iteration :  50, loss : 0.1438, accuracy : 97.75\n",
            "iteration : 100, loss : 0.1399, accuracy : 97.91\n",
            "iteration : 150, loss : 0.1421, accuracy : 97.83\n",
            "iteration : 200, loss : 0.1425, accuracy : 97.82\n",
            "iteration : 250, loss : 0.1440, accuracy : 97.84\n",
            "iteration : 300, loss : 0.1442, accuracy : 97.82\n",
            "iteration : 350, loss : 0.1443, accuracy : 97.78\n",
            "Epoch : 182, training loss : 0.1446, training accuracy : 97.77, test loss : 0.2793, test accuracy : 94.30\n",
            "\n",
            "Epoch: 183\n",
            "iteration :  50, loss : 0.1352, accuracy : 98.14\n",
            "iteration : 100, loss : 0.1397, accuracy : 97.84\n",
            "iteration : 150, loss : 0.1368, accuracy : 97.96\n",
            "iteration : 200, loss : 0.1380, accuracy : 97.94\n",
            "iteration : 250, loss : 0.1394, accuracy : 97.88\n",
            "iteration : 300, loss : 0.1399, accuracy : 97.88\n",
            "iteration : 350, loss : 0.1421, accuracy : 97.83\n",
            "Epoch : 183, training loss : 0.1426, training accuracy : 97.80, test loss : 0.2828, test accuracy : 94.19\n",
            "\n",
            "Epoch: 184\n",
            "iteration :  50, loss : 0.1407, accuracy : 97.89\n",
            "iteration : 100, loss : 0.1408, accuracy : 97.92\n",
            "iteration : 150, loss : 0.1418, accuracy : 97.85\n",
            "iteration : 200, loss : 0.1417, accuracy : 97.90\n",
            "iteration : 250, loss : 0.1414, accuracy : 97.89\n",
            "iteration : 300, loss : 0.1419, accuracy : 97.87\n",
            "iteration : 350, loss : 0.1426, accuracy : 97.85\n",
            "Epoch : 184, training loss : 0.1427, training accuracy : 97.86, test loss : 0.2786, test accuracy : 94.28\n",
            "\n",
            "Epoch: 185\n",
            "iteration :  50, loss : 0.1419, accuracy : 97.98\n",
            "iteration : 100, loss : 0.1425, accuracy : 97.99\n",
            "iteration : 150, loss : 0.1426, accuracy : 97.94\n",
            "iteration : 200, loss : 0.1416, accuracy : 97.96\n",
            "iteration : 250, loss : 0.1409, accuracy : 97.96\n",
            "iteration : 300, loss : 0.1412, accuracy : 97.94\n",
            "iteration : 350, loss : 0.1413, accuracy : 97.95\n",
            "Epoch : 185, training loss : 0.1420, training accuracy : 97.93, test loss : 0.2850, test accuracy : 94.16\n",
            "\n",
            "Epoch: 186\n",
            "iteration :  50, loss : 0.1390, accuracy : 98.19\n",
            "iteration : 100, loss : 0.1435, accuracy : 97.95\n",
            "iteration : 150, loss : 0.1436, accuracy : 97.95\n",
            "iteration : 200, loss : 0.1432, accuracy : 97.93\n",
            "iteration : 250, loss : 0.1416, accuracy : 97.99\n",
            "iteration : 300, loss : 0.1409, accuracy : 98.00\n",
            "iteration : 350, loss : 0.1410, accuracy : 98.00\n",
            "Epoch : 186, training loss : 0.1421, training accuracy : 97.98, test loss : 0.2811, test accuracy : 94.19\n",
            "\n",
            "Epoch: 187\n",
            "iteration :  50, loss : 0.1278, accuracy : 98.25\n",
            "iteration : 100, loss : 0.1349, accuracy : 98.06\n",
            "iteration : 150, loss : 0.1359, accuracy : 98.04\n",
            "iteration : 200, loss : 0.1376, accuracy : 97.99\n",
            "iteration : 250, loss : 0.1388, accuracy : 97.95\n",
            "iteration : 300, loss : 0.1396, accuracy : 97.96\n",
            "iteration : 350, loss : 0.1406, accuracy : 97.95\n",
            "Epoch : 187, training loss : 0.1402, training accuracy : 97.95, test loss : 0.2891, test accuracy : 93.87\n",
            "\n",
            "Epoch: 188\n",
            "iteration :  50, loss : 0.1355, accuracy : 98.08\n",
            "iteration : 100, loss : 0.1334, accuracy : 98.17\n",
            "iteration : 150, loss : 0.1329, accuracy : 98.20\n",
            "iteration : 200, loss : 0.1328, accuracy : 98.20\n",
            "iteration : 250, loss : 0.1342, accuracy : 98.14\n",
            "iteration : 300, loss : 0.1370, accuracy : 98.07\n",
            "iteration : 350, loss : 0.1382, accuracy : 98.00\n",
            "Epoch : 188, training loss : 0.1389, training accuracy : 97.97, test loss : 0.2820, test accuracy : 94.13\n",
            "\n",
            "Epoch: 189\n",
            "iteration :  50, loss : 0.1431, accuracy : 97.83\n",
            "iteration : 100, loss : 0.1384, accuracy : 97.98\n",
            "iteration : 150, loss : 0.1363, accuracy : 98.06\n",
            "iteration : 200, loss : 0.1374, accuracy : 98.02\n",
            "iteration : 250, loss : 0.1376, accuracy : 98.02\n",
            "iteration : 300, loss : 0.1387, accuracy : 97.97\n",
            "iteration : 350, loss : 0.1397, accuracy : 97.95\n",
            "Epoch : 189, training loss : 0.1401, training accuracy : 97.93, test loss : 0.2840, test accuracy : 94.10\n",
            "\n",
            "Epoch: 190\n",
            "iteration :  50, loss : 0.1244, accuracy : 98.52\n",
            "iteration : 100, loss : 0.1314, accuracy : 98.23\n",
            "iteration : 150, loss : 0.1312, accuracy : 98.26\n",
            "iteration : 200, loss : 0.1315, accuracy : 98.29\n",
            "iteration : 250, loss : 0.1318, accuracy : 98.26\n",
            "iteration : 300, loss : 0.1347, accuracy : 98.17\n",
            "iteration : 350, loss : 0.1345, accuracy : 98.17\n",
            "Epoch : 190, training loss : 0.1351, training accuracy : 98.15, test loss : 0.2944, test accuracy : 93.98\n",
            "\n",
            "Epoch: 191\n",
            "iteration :  50, loss : 0.1504, accuracy : 97.72\n",
            "iteration : 100, loss : 0.1422, accuracy : 97.94\n",
            "iteration : 150, loss : 0.1427, accuracy : 97.94\n",
            "iteration : 200, loss : 0.1435, accuracy : 97.91\n",
            "iteration : 250, loss : 0.1436, accuracy : 97.87\n",
            "iteration : 300, loss : 0.1415, accuracy : 97.92\n",
            "iteration : 350, loss : 0.1418, accuracy : 97.91\n",
            "Epoch : 191, training loss : 0.1417, training accuracy : 97.92, test loss : 0.2847, test accuracy : 94.26\n",
            "\n",
            "Epoch: 192\n",
            "iteration :  50, loss : 0.1400, accuracy : 97.83\n",
            "iteration : 100, loss : 0.1394, accuracy : 98.02\n",
            "iteration : 150, loss : 0.1375, accuracy : 98.05\n",
            "iteration : 200, loss : 0.1360, accuracy : 98.10\n",
            "iteration : 250, loss : 0.1371, accuracy : 98.07\n",
            "iteration : 300, loss : 0.1368, accuracy : 98.08\n",
            "iteration : 350, loss : 0.1369, accuracy : 98.07\n",
            "Epoch : 192, training loss : 0.1369, training accuracy : 98.08, test loss : 0.2805, test accuracy : 94.33\n",
            "\n",
            "Epoch: 193\n",
            "iteration :  50, loss : 0.1305, accuracy : 98.27\n",
            "iteration : 100, loss : 0.1329, accuracy : 98.12\n",
            "iteration : 150, loss : 0.1347, accuracy : 98.09\n",
            "iteration : 200, loss : 0.1355, accuracy : 98.06\n",
            "iteration : 250, loss : 0.1351, accuracy : 98.07\n",
            "iteration : 300, loss : 0.1350, accuracy : 98.07\n",
            "iteration : 350, loss : 0.1357, accuracy : 98.05\n",
            "Epoch : 193, training loss : 0.1360, training accuracy : 98.03, test loss : 0.2788, test accuracy : 94.36\n",
            "\n",
            "Epoch: 194\n",
            "iteration :  50, loss : 0.1325, accuracy : 98.14\n",
            "iteration : 100, loss : 0.1282, accuracy : 98.34\n",
            "iteration : 150, loss : 0.1281, accuracy : 98.27\n",
            "iteration : 200, loss : 0.1307, accuracy : 98.22\n",
            "iteration : 250, loss : 0.1308, accuracy : 98.20\n",
            "iteration : 300, loss : 0.1305, accuracy : 98.20\n",
            "iteration : 350, loss : 0.1317, accuracy : 98.17\n",
            "Epoch : 194, training loss : 0.1318, training accuracy : 98.17, test loss : 0.2841, test accuracy : 94.30\n",
            "\n",
            "Epoch: 195\n",
            "iteration :  50, loss : 0.1296, accuracy : 98.00\n",
            "iteration : 100, loss : 0.1314, accuracy : 98.10\n",
            "iteration : 150, loss : 0.1307, accuracy : 98.12\n",
            "iteration : 200, loss : 0.1326, accuracy : 98.12\n",
            "iteration : 250, loss : 0.1320, accuracy : 98.15\n",
            "iteration : 300, loss : 0.1333, accuracy : 98.12\n",
            "iteration : 350, loss : 0.1340, accuracy : 98.08\n",
            "Epoch : 195, training loss : 0.1338, training accuracy : 98.09, test loss : 0.2953, test accuracy : 94.04\n",
            "\n",
            "Epoch: 196\n",
            "iteration :  50, loss : 0.1289, accuracy : 98.19\n",
            "iteration : 100, loss : 0.1349, accuracy : 98.03\n",
            "iteration : 150, loss : 0.1337, accuracy : 98.12\n",
            "iteration : 200, loss : 0.1343, accuracy : 98.14\n",
            "iteration : 250, loss : 0.1335, accuracy : 98.20\n",
            "iteration : 300, loss : 0.1334, accuracy : 98.17\n",
            "iteration : 350, loss : 0.1317, accuracy : 98.22\n",
            "Epoch : 196, training loss : 0.1327, training accuracy : 98.19, test loss : 0.2793, test accuracy : 94.41\n",
            "\n",
            "Epoch: 197\n",
            "iteration :  50, loss : 0.1310, accuracy : 98.06\n",
            "iteration : 100, loss : 0.1298, accuracy : 98.26\n",
            "iteration : 150, loss : 0.1272, accuracy : 98.36\n",
            "iteration : 200, loss : 0.1272, accuracy : 98.35\n",
            "iteration : 250, loss : 0.1289, accuracy : 98.29\n",
            "iteration : 300, loss : 0.1294, accuracy : 98.27\n",
            "iteration : 350, loss : 0.1293, accuracy : 98.28\n",
            "Epoch : 197, training loss : 0.1294, training accuracy : 98.27, test loss : 0.2849, test accuracy : 94.16\n",
            "\n",
            "Epoch: 198\n",
            "iteration :  50, loss : 0.1288, accuracy : 98.31\n",
            "iteration : 100, loss : 0.1270, accuracy : 98.30\n",
            "iteration : 150, loss : 0.1301, accuracy : 98.19\n",
            "iteration : 200, loss : 0.1291, accuracy : 98.26\n",
            "iteration : 250, loss : 0.1294, accuracy : 98.23\n",
            "iteration : 300, loss : 0.1286, accuracy : 98.26\n",
            "iteration : 350, loss : 0.1292, accuracy : 98.23\n",
            "Epoch : 198, training loss : 0.1298, training accuracy : 98.21, test loss : 0.2790, test accuracy : 94.25\n",
            "\n",
            "Epoch: 199\n",
            "iteration :  50, loss : 0.1242, accuracy : 98.53\n",
            "iteration : 100, loss : 0.1248, accuracy : 98.47\n",
            "iteration : 150, loss : 0.1270, accuracy : 98.34\n",
            "iteration : 200, loss : 0.1280, accuracy : 98.29\n",
            "iteration : 250, loss : 0.1298, accuracy : 98.25\n",
            "iteration : 300, loss : 0.1311, accuracy : 98.22\n",
            "iteration : 350, loss : 0.1306, accuracy : 98.20\n",
            "Epoch : 199, training loss : 0.1322, training accuracy : 98.16, test loss : 0.2852, test accuracy : 94.07\n",
            "\n",
            "Epoch: 200\n",
            "iteration :  50, loss : 0.1280, accuracy : 98.27\n",
            "iteration : 100, loss : 0.1261, accuracy : 98.33\n",
            "iteration : 150, loss : 0.1287, accuracy : 98.26\n",
            "iteration : 200, loss : 0.1308, accuracy : 98.22\n",
            "iteration : 250, loss : 0.1297, accuracy : 98.27\n",
            "iteration : 300, loss : 0.1297, accuracy : 98.27\n",
            "iteration : 350, loss : 0.1296, accuracy : 98.28\n",
            "Epoch : 200, training loss : 0.1295, training accuracy : 98.29, test loss : 0.2852, test accuracy : 94.24\n",
            "\n",
            "Epoch: 201\n",
            "iteration :  50, loss : 0.1295, accuracy : 98.23\n",
            "iteration : 100, loss : 0.1253, accuracy : 98.32\n",
            "iteration : 150, loss : 0.1258, accuracy : 98.30\n",
            "iteration : 200, loss : 0.1244, accuracy : 98.36\n",
            "iteration : 250, loss : 0.1253, accuracy : 98.34\n",
            "iteration : 300, loss : 0.1277, accuracy : 98.29\n",
            "iteration : 350, loss : 0.1277, accuracy : 98.30\n",
            "Epoch : 201, training loss : 0.1279, training accuracy : 98.30, test loss : 0.2794, test accuracy : 94.46\n",
            "\n",
            "Epoch: 202\n",
            "iteration :  50, loss : 0.1258, accuracy : 98.20\n",
            "iteration : 100, loss : 0.1301, accuracy : 98.22\n",
            "iteration : 150, loss : 0.1276, accuracy : 98.29\n",
            "iteration : 200, loss : 0.1276, accuracy : 98.27\n",
            "iteration : 250, loss : 0.1263, accuracy : 98.33\n",
            "iteration : 300, loss : 0.1277, accuracy : 98.28\n",
            "iteration : 350, loss : 0.1278, accuracy : 98.28\n",
            "Epoch : 202, training loss : 0.1283, training accuracy : 98.27, test loss : 0.2764, test accuracy : 94.44\n",
            "\n",
            "Epoch: 203\n",
            "iteration :  50, loss : 0.1154, accuracy : 98.67\n",
            "iteration : 100, loss : 0.1213, accuracy : 98.52\n",
            "iteration : 150, loss : 0.1235, accuracy : 98.48\n",
            "iteration : 200, loss : 0.1262, accuracy : 98.44\n",
            "iteration : 250, loss : 0.1266, accuracy : 98.40\n",
            "iteration : 300, loss : 0.1277, accuracy : 98.33\n",
            "iteration : 350, loss : 0.1271, accuracy : 98.33\n",
            "Epoch : 203, training loss : 0.1268, training accuracy : 98.34, test loss : 0.2838, test accuracy : 94.46\n",
            "\n",
            "Epoch: 204\n",
            "iteration :  50, loss : 0.1186, accuracy : 98.42\n",
            "iteration : 100, loss : 0.1227, accuracy : 98.38\n",
            "iteration : 150, loss : 0.1249, accuracy : 98.30\n",
            "iteration : 200, loss : 0.1255, accuracy : 98.27\n",
            "iteration : 250, loss : 0.1262, accuracy : 98.27\n",
            "iteration : 300, loss : 0.1261, accuracy : 98.28\n",
            "iteration : 350, loss : 0.1264, accuracy : 98.27\n",
            "Epoch : 204, training loss : 0.1271, training accuracy : 98.25, test loss : 0.2823, test accuracy : 94.56\n",
            "\n",
            "Epoch: 205\n",
            "iteration :  50, loss : 0.1241, accuracy : 98.52\n",
            "iteration : 100, loss : 0.1243, accuracy : 98.48\n",
            "iteration : 150, loss : 0.1275, accuracy : 98.40\n",
            "iteration : 200, loss : 0.1272, accuracy : 98.40\n",
            "iteration : 250, loss : 0.1259, accuracy : 98.41\n",
            "iteration : 300, loss : 0.1252, accuracy : 98.40\n",
            "iteration : 350, loss : 0.1263, accuracy : 98.37\n",
            "Epoch : 205, training loss : 0.1266, training accuracy : 98.36, test loss : 0.2846, test accuracy : 94.36\n",
            "\n",
            "Epoch: 206\n",
            "iteration :  50, loss : 0.1193, accuracy : 98.48\n",
            "iteration : 100, loss : 0.1215, accuracy : 98.45\n",
            "iteration : 150, loss : 0.1224, accuracy : 98.43\n",
            "iteration : 200, loss : 0.1242, accuracy : 98.38\n",
            "iteration : 250, loss : 0.1231, accuracy : 98.39\n",
            "iteration : 300, loss : 0.1245, accuracy : 98.37\n",
            "iteration : 350, loss : 0.1250, accuracy : 98.35\n",
            "Epoch : 206, training loss : 0.1262, training accuracy : 98.31, test loss : 0.2804, test accuracy : 94.58\n",
            "\n",
            "Epoch: 207\n",
            "iteration :  50, loss : 0.1235, accuracy : 98.56\n",
            "iteration : 100, loss : 0.1207, accuracy : 98.59\n",
            "iteration : 150, loss : 0.1228, accuracy : 98.52\n",
            "iteration : 200, loss : 0.1234, accuracy : 98.45\n",
            "iteration : 250, loss : 0.1249, accuracy : 98.42\n",
            "iteration : 300, loss : 0.1248, accuracy : 98.39\n",
            "iteration : 350, loss : 0.1245, accuracy : 98.44\n",
            "Epoch : 207, training loss : 0.1247, training accuracy : 98.42, test loss : 0.2811, test accuracy : 94.51\n",
            "\n",
            "Epoch: 208\n",
            "iteration :  50, loss : 0.1175, accuracy : 98.53\n",
            "iteration : 100, loss : 0.1248, accuracy : 98.37\n",
            "iteration : 150, loss : 0.1231, accuracy : 98.42\n",
            "iteration : 200, loss : 0.1228, accuracy : 98.47\n",
            "iteration : 250, loss : 0.1230, accuracy : 98.46\n",
            "iteration : 300, loss : 0.1236, accuracy : 98.43\n",
            "iteration : 350, loss : 0.1239, accuracy : 98.42\n",
            "Epoch : 208, training loss : 0.1239, training accuracy : 98.42, test loss : 0.2877, test accuracy : 94.24\n",
            "\n",
            "Epoch: 209\n",
            "iteration :  50, loss : 0.1187, accuracy : 98.61\n",
            "iteration : 100, loss : 0.1175, accuracy : 98.60\n",
            "iteration : 150, loss : 0.1195, accuracy : 98.58\n",
            "iteration : 200, loss : 0.1211, accuracy : 98.56\n",
            "iteration : 250, loss : 0.1220, accuracy : 98.52\n",
            "iteration : 300, loss : 0.1227, accuracy : 98.50\n",
            "iteration : 350, loss : 0.1240, accuracy : 98.45\n",
            "Epoch : 209, training loss : 0.1245, training accuracy : 98.43, test loss : 0.2836, test accuracy : 94.37\n",
            "\n",
            "Epoch: 210\n",
            "iteration :  50, loss : 0.1210, accuracy : 98.55\n",
            "iteration : 100, loss : 0.1222, accuracy : 98.48\n",
            "iteration : 150, loss : 0.1229, accuracy : 98.49\n",
            "iteration : 200, loss : 0.1238, accuracy : 98.47\n",
            "iteration : 250, loss : 0.1246, accuracy : 98.41\n",
            "iteration : 300, loss : 0.1235, accuracy : 98.45\n",
            "iteration : 350, loss : 0.1256, accuracy : 98.38\n",
            "Epoch : 210, training loss : 0.1260, training accuracy : 98.37, test loss : 0.2809, test accuracy : 94.42\n",
            "\n",
            "Epoch: 211\n",
            "iteration :  50, loss : 0.1213, accuracy : 98.50\n",
            "iteration : 100, loss : 0.1212, accuracy : 98.54\n",
            "iteration : 150, loss : 0.1233, accuracy : 98.48\n",
            "iteration : 200, loss : 0.1236, accuracy : 98.48\n",
            "iteration : 250, loss : 0.1223, accuracy : 98.52\n",
            "iteration : 300, loss : 0.1221, accuracy : 98.52\n",
            "iteration : 350, loss : 0.1216, accuracy : 98.54\n",
            "Epoch : 211, training loss : 0.1220, training accuracy : 98.53, test loss : 0.2874, test accuracy : 94.39\n",
            "\n",
            "Epoch: 212\n",
            "iteration :  50, loss : 0.1141, accuracy : 98.67\n",
            "iteration : 100, loss : 0.1155, accuracy : 98.63\n",
            "iteration : 150, loss : 0.1156, accuracy : 98.66\n",
            "iteration : 200, loss : 0.1180, accuracy : 98.61\n",
            "iteration : 250, loss : 0.1202, accuracy : 98.56\n",
            "iteration : 300, loss : 0.1201, accuracy : 98.57\n",
            "iteration : 350, loss : 0.1209, accuracy : 98.53\n",
            "Epoch : 212, training loss : 0.1209, training accuracy : 98.53, test loss : 0.2833, test accuracy : 94.34\n",
            "\n",
            "Epoch: 213\n",
            "iteration :  50, loss : 0.1203, accuracy : 98.59\n",
            "iteration : 100, loss : 0.1186, accuracy : 98.55\n",
            "iteration : 150, loss : 0.1200, accuracy : 98.56\n",
            "iteration : 200, loss : 0.1219, accuracy : 98.50\n",
            "iteration : 250, loss : 0.1219, accuracy : 98.47\n",
            "iteration : 300, loss : 0.1212, accuracy : 98.49\n",
            "iteration : 350, loss : 0.1218, accuracy : 98.49\n",
            "Epoch : 213, training loss : 0.1232, training accuracy : 98.44, test loss : 0.2856, test accuracy : 94.38\n",
            "\n",
            "Epoch: 214\n",
            "iteration :  50, loss : 0.1242, accuracy : 98.47\n",
            "iteration : 100, loss : 0.1211, accuracy : 98.55\n",
            "iteration : 150, loss : 0.1200, accuracy : 98.57\n",
            "iteration : 200, loss : 0.1199, accuracy : 98.57\n",
            "iteration : 250, loss : 0.1210, accuracy : 98.53\n",
            "iteration : 300, loss : 0.1215, accuracy : 98.52\n",
            "iteration : 350, loss : 0.1219, accuracy : 98.52\n",
            "Epoch : 214, training loss : 0.1225, training accuracy : 98.50, test loss : 0.2775, test accuracy : 94.62\n",
            "\n",
            "Epoch: 215\n",
            "iteration :  50, loss : 0.1160, accuracy : 98.72\n",
            "iteration : 100, loss : 0.1184, accuracy : 98.64\n",
            "iteration : 150, loss : 0.1191, accuracy : 98.57\n",
            "iteration : 200, loss : 0.1182, accuracy : 98.59\n",
            "iteration : 250, loss : 0.1185, accuracy : 98.57\n",
            "iteration : 300, loss : 0.1195, accuracy : 98.53\n",
            "iteration : 350, loss : 0.1201, accuracy : 98.53\n",
            "Epoch : 215, training loss : 0.1207, training accuracy : 98.51, test loss : 0.2835, test accuracy : 94.45\n",
            "\n",
            "Epoch: 216\n",
            "iteration :  50, loss : 0.1140, accuracy : 98.77\n",
            "iteration : 100, loss : 0.1148, accuracy : 98.80\n",
            "iteration : 150, loss : 0.1168, accuracy : 98.70\n",
            "iteration : 200, loss : 0.1187, accuracy : 98.65\n",
            "iteration : 250, loss : 0.1192, accuracy : 98.61\n",
            "iteration : 300, loss : 0.1199, accuracy : 98.60\n",
            "iteration : 350, loss : 0.1205, accuracy : 98.58\n",
            "Epoch : 216, training loss : 0.1205, training accuracy : 98.59, test loss : 0.2802, test accuracy : 94.39\n",
            "\n",
            "Epoch: 217\n",
            "iteration :  50, loss : 0.1166, accuracy : 98.56\n",
            "iteration : 100, loss : 0.1161, accuracy : 98.58\n",
            "iteration : 150, loss : 0.1165, accuracy : 98.57\n",
            "iteration : 200, loss : 0.1160, accuracy : 98.58\n",
            "iteration : 250, loss : 0.1162, accuracy : 98.59\n",
            "iteration : 300, loss : 0.1171, accuracy : 98.56\n",
            "iteration : 350, loss : 0.1194, accuracy : 98.52\n",
            "Epoch : 217, training loss : 0.1196, training accuracy : 98.52, test loss : 0.2808, test accuracy : 94.55\n",
            "\n",
            "Epoch: 218\n",
            "iteration :  50, loss : 0.1136, accuracy : 98.67\n",
            "iteration : 100, loss : 0.1115, accuracy : 98.75\n",
            "iteration : 150, loss : 0.1160, accuracy : 98.64\n",
            "iteration : 200, loss : 0.1179, accuracy : 98.59\n",
            "iteration : 250, loss : 0.1181, accuracy : 98.60\n",
            "iteration : 300, loss : 0.1189, accuracy : 98.60\n",
            "iteration : 350, loss : 0.1187, accuracy : 98.61\n",
            "Epoch : 218, training loss : 0.1187, training accuracy : 98.61, test loss : 0.2859, test accuracy : 94.53\n",
            "\n",
            "Epoch: 219\n",
            "iteration :  50, loss : 0.1152, accuracy : 98.78\n",
            "iteration : 100, loss : 0.1162, accuracy : 98.72\n",
            "iteration : 150, loss : 0.1188, accuracy : 98.58\n",
            "iteration : 200, loss : 0.1174, accuracy : 98.62\n",
            "iteration : 250, loss : 0.1170, accuracy : 98.63\n",
            "iteration : 300, loss : 0.1178, accuracy : 98.61\n",
            "iteration : 350, loss : 0.1176, accuracy : 98.62\n",
            "Epoch : 219, training loss : 0.1179, training accuracy : 98.62, test loss : 0.2822, test accuracy : 94.35\n",
            "\n",
            "Epoch: 220\n",
            "iteration :  50, loss : 0.1169, accuracy : 98.72\n",
            "iteration : 100, loss : 0.1149, accuracy : 98.70\n",
            "iteration : 150, loss : 0.1170, accuracy : 98.66\n",
            "iteration : 200, loss : 0.1168, accuracy : 98.68\n",
            "iteration : 250, loss : 0.1178, accuracy : 98.63\n",
            "iteration : 300, loss : 0.1174, accuracy : 98.63\n",
            "iteration : 350, loss : 0.1175, accuracy : 98.61\n",
            "Epoch : 220, training loss : 0.1179, training accuracy : 98.60, test loss : 0.2846, test accuracy : 94.45\n",
            "\n",
            "Epoch: 221\n",
            "iteration :  50, loss : 0.1173, accuracy : 98.66\n",
            "iteration : 100, loss : 0.1155, accuracy : 98.70\n",
            "iteration : 150, loss : 0.1135, accuracy : 98.74\n",
            "iteration : 200, loss : 0.1131, accuracy : 98.76\n",
            "iteration : 250, loss : 0.1143, accuracy : 98.73\n",
            "iteration : 300, loss : 0.1143, accuracy : 98.74\n",
            "iteration : 350, loss : 0.1147, accuracy : 98.72\n",
            "Epoch : 221, training loss : 0.1149, training accuracy : 98.72, test loss : 0.2822, test accuracy : 94.51\n",
            "\n",
            "Epoch: 222\n",
            "iteration :  50, loss : 0.1106, accuracy : 98.84\n",
            "iteration : 100, loss : 0.1135, accuracy : 98.79\n",
            "iteration : 150, loss : 0.1138, accuracy : 98.81\n",
            "iteration : 200, loss : 0.1147, accuracy : 98.77\n",
            "iteration : 250, loss : 0.1149, accuracy : 98.73\n",
            "iteration : 300, loss : 0.1152, accuracy : 98.72\n",
            "iteration : 350, loss : 0.1159, accuracy : 98.70\n",
            "Epoch : 222, training loss : 0.1161, training accuracy : 98.69, test loss : 0.2802, test accuracy : 94.48\n",
            "\n",
            "Epoch: 223\n",
            "iteration :  50, loss : 0.1191, accuracy : 98.44\n",
            "iteration : 100, loss : 0.1130, accuracy : 98.72\n",
            "iteration : 150, loss : 0.1132, accuracy : 98.74\n",
            "iteration : 200, loss : 0.1158, accuracy : 98.69\n",
            "iteration : 250, loss : 0.1147, accuracy : 98.73\n",
            "iteration : 300, loss : 0.1150, accuracy : 98.71\n",
            "iteration : 350, loss : 0.1148, accuracy : 98.73\n",
            "Epoch : 223, training loss : 0.1144, training accuracy : 98.72, test loss : 0.2895, test accuracy : 94.29\n",
            "\n",
            "Epoch: 224\n",
            "iteration :  50, loss : 0.1185, accuracy : 98.56\n",
            "iteration : 100, loss : 0.1211, accuracy : 98.58\n",
            "iteration : 150, loss : 0.1193, accuracy : 98.64\n",
            "iteration : 200, loss : 0.1187, accuracy : 98.63\n",
            "iteration : 250, loss : 0.1173, accuracy : 98.67\n",
            "iteration : 300, loss : 0.1178, accuracy : 98.66\n",
            "iteration : 350, loss : 0.1180, accuracy : 98.63\n",
            "Epoch : 224, training loss : 0.1180, training accuracy : 98.63, test loss : 0.2835, test accuracy : 94.50\n",
            "\n",
            "Epoch: 225\n",
            "iteration :  50, loss : 0.1166, accuracy : 98.73\n",
            "iteration : 100, loss : 0.1147, accuracy : 98.77\n",
            "iteration : 150, loss : 0.1172, accuracy : 98.68\n",
            "iteration : 200, loss : 0.1164, accuracy : 98.68\n",
            "iteration : 250, loss : 0.1155, accuracy : 98.70\n",
            "iteration : 300, loss : 0.1152, accuracy : 98.70\n",
            "iteration : 350, loss : 0.1147, accuracy : 98.73\n",
            "Epoch : 225, training loss : 0.1152, training accuracy : 98.71, test loss : 0.2814, test accuracy : 94.59\n",
            "\n",
            "Epoch: 226\n",
            "iteration :  50, loss : 0.1139, accuracy : 98.89\n",
            "iteration : 100, loss : 0.1112, accuracy : 98.92\n",
            "iteration : 150, loss : 0.1120, accuracy : 98.83\n",
            "iteration : 200, loss : 0.1117, accuracy : 98.88\n",
            "iteration : 250, loss : 0.1117, accuracy : 98.88\n",
            "iteration : 300, loss : 0.1129, accuracy : 98.85\n",
            "iteration : 350, loss : 0.1129, accuracy : 98.83\n",
            "Epoch : 226, training loss : 0.1127, training accuracy : 98.84, test loss : 0.2805, test accuracy : 94.50\n",
            "\n",
            "Epoch: 227\n",
            "iteration :  50, loss : 0.1112, accuracy : 98.83\n",
            "iteration : 100, loss : 0.1107, accuracy : 98.84\n",
            "iteration : 150, loss : 0.1102, accuracy : 98.83\n",
            "iteration : 200, loss : 0.1108, accuracy : 98.79\n",
            "iteration : 250, loss : 0.1113, accuracy : 98.78\n",
            "iteration : 300, loss : 0.1113, accuracy : 98.79\n",
            "iteration : 350, loss : 0.1132, accuracy : 98.71\n",
            "Epoch : 227, training loss : 0.1137, training accuracy : 98.69, test loss : 0.2844, test accuracy : 94.39\n",
            "\n",
            "Epoch: 228\n",
            "iteration :  50, loss : 0.1076, accuracy : 98.95\n",
            "iteration : 100, loss : 0.1101, accuracy : 98.84\n",
            "iteration : 150, loss : 0.1090, accuracy : 98.87\n",
            "iteration : 200, loss : 0.1112, accuracy : 98.81\n",
            "iteration : 250, loss : 0.1107, accuracy : 98.83\n",
            "iteration : 300, loss : 0.1114, accuracy : 98.80\n",
            "iteration : 350, loss : 0.1108, accuracy : 98.84\n",
            "Epoch : 228, training loss : 0.1116, training accuracy : 98.81, test loss : 0.2796, test accuracy : 94.57\n",
            "\n",
            "Epoch: 229\n",
            "iteration :  50, loss : 0.1042, accuracy : 99.00\n",
            "iteration : 100, loss : 0.1040, accuracy : 99.06\n",
            "iteration : 150, loss : 0.1092, accuracy : 98.95\n",
            "iteration : 200, loss : 0.1086, accuracy : 98.96\n",
            "iteration : 250, loss : 0.1098, accuracy : 98.90\n",
            "iteration : 300, loss : 0.1099, accuracy : 98.90\n",
            "iteration : 350, loss : 0.1101, accuracy : 98.89\n",
            "Epoch : 229, training loss : 0.1097, training accuracy : 98.91, test loss : 0.2867, test accuracy : 94.48\n",
            "\n",
            "Epoch: 230\n",
            "iteration :  50, loss : 0.1091, accuracy : 98.97\n",
            "iteration : 100, loss : 0.1084, accuracy : 98.88\n",
            "iteration : 150, loss : 0.1081, accuracy : 98.89\n",
            "iteration : 200, loss : 0.1085, accuracy : 98.87\n",
            "iteration : 250, loss : 0.1102, accuracy : 98.84\n",
            "iteration : 300, loss : 0.1105, accuracy : 98.83\n",
            "iteration : 350, loss : 0.1108, accuracy : 98.83\n",
            "Epoch : 230, training loss : 0.1109, training accuracy : 98.82, test loss : 0.2781, test accuracy : 94.63\n",
            "\n",
            "Epoch: 231\n",
            "iteration :  50, loss : 0.1132, accuracy : 98.72\n",
            "iteration : 100, loss : 0.1114, accuracy : 98.78\n",
            "iteration : 150, loss : 0.1114, accuracy : 98.80\n",
            "iteration : 200, loss : 0.1110, accuracy : 98.80\n",
            "iteration : 250, loss : 0.1106, accuracy : 98.81\n",
            "iteration : 300, loss : 0.1110, accuracy : 98.80\n",
            "iteration : 350, loss : 0.1108, accuracy : 98.81\n",
            "Epoch : 231, training loss : 0.1109, training accuracy : 98.80, test loss : 0.2785, test accuracy : 94.63\n",
            "\n",
            "Epoch: 232\n",
            "iteration :  50, loss : 0.1065, accuracy : 98.98\n",
            "iteration : 100, loss : 0.1080, accuracy : 98.96\n",
            "iteration : 150, loss : 0.1055, accuracy : 99.05\n",
            "iteration : 200, loss : 0.1076, accuracy : 98.98\n",
            "iteration : 250, loss : 0.1076, accuracy : 98.98\n",
            "iteration : 300, loss : 0.1075, accuracy : 98.97\n",
            "iteration : 350, loss : 0.1086, accuracy : 98.93\n",
            "Epoch : 232, training loss : 0.1083, training accuracy : 98.93, test loss : 0.2809, test accuracy : 94.59\n",
            "\n",
            "Epoch: 233\n",
            "iteration :  50, loss : 0.1181, accuracy : 98.55\n",
            "iteration : 100, loss : 0.1127, accuracy : 98.76\n",
            "iteration : 150, loss : 0.1110, accuracy : 98.79\n",
            "iteration : 200, loss : 0.1110, accuracy : 98.82\n",
            "iteration : 250, loss : 0.1109, accuracy : 98.84\n",
            "iteration : 300, loss : 0.1104, accuracy : 98.84\n",
            "iteration : 350, loss : 0.1097, accuracy : 98.86\n",
            "Epoch : 233, training loss : 0.1102, training accuracy : 98.85, test loss : 0.2806, test accuracy : 94.53\n",
            "\n",
            "Epoch: 234\n",
            "iteration :  50, loss : 0.1093, accuracy : 98.94\n",
            "iteration : 100, loss : 0.1095, accuracy : 98.89\n",
            "iteration : 150, loss : 0.1104, accuracy : 98.86\n",
            "iteration : 200, loss : 0.1104, accuracy : 98.84\n",
            "iteration : 250, loss : 0.1104, accuracy : 98.83\n",
            "iteration : 300, loss : 0.1095, accuracy : 98.85\n",
            "iteration : 350, loss : 0.1092, accuracy : 98.87\n",
            "Epoch : 234, training loss : 0.1089, training accuracy : 98.88, test loss : 0.2856, test accuracy : 94.47\n",
            "\n",
            "Epoch: 235\n",
            "iteration :  50, loss : 0.1035, accuracy : 99.08\n",
            "iteration : 100, loss : 0.1036, accuracy : 99.12\n",
            "iteration : 150, loss : 0.1083, accuracy : 98.94\n",
            "iteration : 200, loss : 0.1060, accuracy : 98.99\n",
            "iteration : 250, loss : 0.1057, accuracy : 98.98\n",
            "iteration : 300, loss : 0.1058, accuracy : 98.99\n",
            "iteration : 350, loss : 0.1054, accuracy : 99.00\n",
            "Epoch : 235, training loss : 0.1053, training accuracy : 99.01, test loss : 0.2844, test accuracy : 94.43\n",
            "\n",
            "Epoch: 236\n",
            "iteration :  50, loss : 0.1119, accuracy : 98.83\n",
            "iteration : 100, loss : 0.1094, accuracy : 98.88\n",
            "iteration : 150, loss : 0.1078, accuracy : 98.93\n",
            "iteration : 200, loss : 0.1074, accuracy : 98.92\n",
            "iteration : 250, loss : 0.1074, accuracy : 98.94\n",
            "iteration : 300, loss : 0.1073, accuracy : 98.93\n",
            "iteration : 350, loss : 0.1080, accuracy : 98.90\n",
            "Epoch : 236, training loss : 0.1086, training accuracy : 98.88, test loss : 0.2903, test accuracy : 94.46\n",
            "\n",
            "Epoch: 237\n",
            "iteration :  50, loss : 0.1085, accuracy : 98.88\n",
            "iteration : 100, loss : 0.1076, accuracy : 98.91\n",
            "iteration : 150, loss : 0.1065, accuracy : 98.95\n",
            "iteration : 200, loss : 0.1076, accuracy : 98.91\n",
            "iteration : 250, loss : 0.1064, accuracy : 98.99\n",
            "iteration : 300, loss : 0.1066, accuracy : 98.99\n",
            "iteration : 350, loss : 0.1062, accuracy : 99.00\n",
            "Epoch : 237, training loss : 0.1062, training accuracy : 99.00, test loss : 0.2806, test accuracy : 94.61\n",
            "\n",
            "Epoch: 238\n",
            "iteration :  50, loss : 0.1027, accuracy : 99.08\n",
            "iteration : 100, loss : 0.1041, accuracy : 99.02\n",
            "iteration : 150, loss : 0.1047, accuracy : 98.98\n",
            "iteration : 200, loss : 0.1042, accuracy : 99.00\n",
            "iteration : 250, loss : 0.1050, accuracy : 98.98\n",
            "iteration : 300, loss : 0.1063, accuracy : 98.93\n",
            "iteration : 350, loss : 0.1069, accuracy : 98.92\n",
            "Epoch : 238, training loss : 0.1070, training accuracy : 98.92, test loss : 0.2808, test accuracy : 94.50\n",
            "\n",
            "Epoch: 239\n",
            "iteration :  50, loss : 0.1025, accuracy : 99.17\n",
            "iteration : 100, loss : 0.1062, accuracy : 98.96\n",
            "iteration : 150, loss : 0.1081, accuracy : 98.90\n",
            "iteration : 200, loss : 0.1083, accuracy : 98.88\n",
            "iteration : 250, loss : 0.1075, accuracy : 98.92\n",
            "iteration : 300, loss : 0.1066, accuracy : 98.96\n",
            "iteration : 350, loss : 0.1056, accuracy : 98.99\n",
            "Epoch : 239, training loss : 0.1057, training accuracy : 98.98, test loss : 0.2774, test accuracy : 94.71\n",
            "\n",
            "Epoch: 240\n",
            "iteration :  50, loss : 0.1067, accuracy : 98.98\n",
            "iteration : 100, loss : 0.1059, accuracy : 99.00\n",
            "iteration : 150, loss : 0.1080, accuracy : 98.89\n",
            "iteration : 200, loss : 0.1078, accuracy : 98.91\n",
            "iteration : 250, loss : 0.1074, accuracy : 98.93\n",
            "iteration : 300, loss : 0.1075, accuracy : 98.91\n",
            "iteration : 350, loss : 0.1064, accuracy : 98.95\n",
            "Epoch : 240, training loss : 0.1066, training accuracy : 98.94, test loss : 0.2855, test accuracy : 94.51\n",
            "\n",
            "Epoch: 241\n",
            "iteration :  50, loss : 0.1043, accuracy : 99.14\n",
            "iteration : 100, loss : 0.1057, accuracy : 99.06\n",
            "iteration : 150, loss : 0.1054, accuracy : 99.07\n",
            "iteration : 200, loss : 0.1043, accuracy : 99.09\n",
            "iteration : 250, loss : 0.1041, accuracy : 99.07\n",
            "iteration : 300, loss : 0.1043, accuracy : 99.06\n",
            "iteration : 350, loss : 0.1038, accuracy : 99.07\n",
            "Epoch : 241, training loss : 0.1039, training accuracy : 99.07, test loss : 0.2748, test accuracy : 94.84\n",
            "\n",
            "Epoch: 242\n",
            "iteration :  50, loss : 0.1013, accuracy : 99.20\n",
            "iteration : 100, loss : 0.1030, accuracy : 99.07\n",
            "iteration : 150, loss : 0.1012, accuracy : 99.16\n",
            "iteration : 200, loss : 0.1024, accuracy : 99.12\n",
            "iteration : 250, loss : 0.1029, accuracy : 99.11\n",
            "iteration : 300, loss : 0.1026, accuracy : 99.12\n",
            "iteration : 350, loss : 0.1026, accuracy : 99.12\n",
            "Epoch : 242, training loss : 0.1024, training accuracy : 99.13, test loss : 0.2792, test accuracy : 94.67\n",
            "\n",
            "Epoch: 243\n",
            "iteration :  50, loss : 0.0996, accuracy : 99.23\n",
            "iteration : 100, loss : 0.1043, accuracy : 99.09\n",
            "iteration : 150, loss : 0.1064, accuracy : 99.03\n",
            "iteration : 200, loss : 0.1052, accuracy : 99.06\n",
            "iteration : 250, loss : 0.1047, accuracy : 99.04\n",
            "iteration : 300, loss : 0.1047, accuracy : 99.04\n",
            "iteration : 350, loss : 0.1041, accuracy : 99.05\n",
            "Epoch : 243, training loss : 0.1040, training accuracy : 99.04, test loss : 0.2849, test accuracy : 94.49\n",
            "\n",
            "Epoch: 244\n",
            "iteration :  50, loss : 0.1014, accuracy : 98.98\n",
            "iteration : 100, loss : 0.1043, accuracy : 98.98\n",
            "iteration : 150, loss : 0.1044, accuracy : 98.99\n",
            "iteration : 200, loss : 0.1041, accuracy : 99.03\n",
            "iteration : 250, loss : 0.1045, accuracy : 99.02\n",
            "iteration : 300, loss : 0.1043, accuracy : 99.02\n",
            "iteration : 350, loss : 0.1041, accuracy : 99.03\n",
            "Epoch : 244, training loss : 0.1038, training accuracy : 99.03, test loss : 0.2863, test accuracy : 94.53\n",
            "\n",
            "Epoch: 245\n",
            "iteration :  50, loss : 0.1013, accuracy : 99.22\n",
            "iteration : 100, loss : 0.1051, accuracy : 99.02\n",
            "iteration : 150, loss : 0.1048, accuracy : 99.04\n",
            "iteration : 200, loss : 0.1039, accuracy : 99.05\n",
            "iteration : 250, loss : 0.1041, accuracy : 99.06\n",
            "iteration : 300, loss : 0.1042, accuracy : 99.04\n",
            "iteration : 350, loss : 0.1033, accuracy : 99.08\n",
            "Epoch : 245, training loss : 0.1032, training accuracy : 99.09, test loss : 0.2833, test accuracy : 94.69\n",
            "\n",
            "Epoch: 246\n",
            "iteration :  50, loss : 0.1039, accuracy : 98.97\n",
            "iteration : 100, loss : 0.1056, accuracy : 99.02\n",
            "iteration : 150, loss : 0.1048, accuracy : 99.03\n",
            "iteration : 200, loss : 0.1053, accuracy : 99.02\n",
            "iteration : 250, loss : 0.1048, accuracy : 99.03\n",
            "iteration : 300, loss : 0.1057, accuracy : 99.00\n",
            "iteration : 350, loss : 0.1052, accuracy : 99.02\n",
            "Epoch : 246, training loss : 0.1053, training accuracy : 99.02, test loss : 0.2809, test accuracy : 94.63\n",
            "\n",
            "Epoch: 247\n",
            "iteration :  50, loss : 0.1074, accuracy : 99.14\n",
            "iteration : 100, loss : 0.1053, accuracy : 99.11\n",
            "iteration : 150, loss : 0.1032, accuracy : 99.15\n",
            "iteration : 200, loss : 0.1032, accuracy : 99.09\n",
            "iteration : 250, loss : 0.1019, accuracy : 99.11\n",
            "iteration : 300, loss : 0.1021, accuracy : 99.11\n",
            "iteration : 350, loss : 0.1019, accuracy : 99.12\n",
            "Epoch : 247, training loss : 0.1018, training accuracy : 99.13, test loss : 0.2784, test accuracy : 94.74\n",
            "\n",
            "Epoch: 248\n",
            "iteration :  50, loss : 0.0948, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0940, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0968, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0989, accuracy : 99.22\n",
            "iteration : 250, loss : 0.0996, accuracy : 99.23\n",
            "iteration : 300, loss : 0.1000, accuracy : 99.20\n",
            "iteration : 350, loss : 0.1007, accuracy : 99.17\n",
            "Epoch : 248, training loss : 0.1010, training accuracy : 99.16, test loss : 0.2763, test accuracy : 94.75\n",
            "\n",
            "Epoch: 249\n",
            "iteration :  50, loss : 0.0985, accuracy : 99.20\n",
            "iteration : 100, loss : 0.1003, accuracy : 99.19\n",
            "iteration : 150, loss : 0.1003, accuracy : 99.17\n",
            "iteration : 200, loss : 0.1018, accuracy : 99.14\n",
            "iteration : 250, loss : 0.1013, accuracy : 99.14\n",
            "iteration : 300, loss : 0.1009, accuracy : 99.14\n",
            "iteration : 350, loss : 0.1002, accuracy : 99.16\n",
            "Epoch : 249, training loss : 0.1003, training accuracy : 99.16, test loss : 0.2870, test accuracy : 94.51\n",
            "\n",
            "Epoch: 250\n",
            "iteration :  50, loss : 0.0994, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0982, accuracy : 99.32\n",
            "iteration : 150, loss : 0.1003, accuracy : 99.24\n",
            "iteration : 200, loss : 0.1003, accuracy : 99.21\n",
            "iteration : 250, loss : 0.1014, accuracy : 99.17\n",
            "iteration : 300, loss : 0.1016, accuracy : 99.17\n",
            "iteration : 350, loss : 0.1015, accuracy : 99.17\n",
            "Epoch : 250, training loss : 0.1012, training accuracy : 99.17, test loss : 0.2792, test accuracy : 94.74\n",
            "\n",
            "Epoch: 251\n",
            "iteration :  50, loss : 0.0983, accuracy : 99.25\n",
            "iteration : 100, loss : 0.1033, accuracy : 99.08\n",
            "iteration : 150, loss : 0.1017, accuracy : 99.10\n",
            "iteration : 200, loss : 0.1008, accuracy : 99.13\n",
            "iteration : 250, loss : 0.1014, accuracy : 99.11\n",
            "iteration : 300, loss : 0.1011, accuracy : 99.12\n",
            "iteration : 350, loss : 0.1007, accuracy : 99.13\n",
            "Epoch : 251, training loss : 0.1012, training accuracy : 99.12, test loss : 0.2800, test accuracy : 94.63\n",
            "\n",
            "Epoch: 252\n",
            "iteration :  50, loss : 0.0997, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0980, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0988, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0994, accuracy : 99.20\n",
            "iteration : 250, loss : 0.1004, accuracy : 99.18\n",
            "iteration : 300, loss : 0.1004, accuracy : 99.16\n",
            "iteration : 350, loss : 0.1002, accuracy : 99.17\n",
            "Epoch : 252, training loss : 0.1013, training accuracy : 99.13, test loss : 0.2817, test accuracy : 94.75\n",
            "\n",
            "Epoch: 253\n",
            "iteration :  50, loss : 0.0976, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0979, accuracy : 99.33\n",
            "iteration : 150, loss : 0.0980, accuracy : 99.29\n",
            "iteration : 200, loss : 0.0983, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0982, accuracy : 99.29\n",
            "iteration : 300, loss : 0.0982, accuracy : 99.28\n",
            "iteration : 350, loss : 0.0988, accuracy : 99.26\n",
            "Epoch : 253, training loss : 0.0985, training accuracy : 99.26, test loss : 0.2752, test accuracy : 94.88\n",
            "\n",
            "Epoch: 254\n",
            "iteration :  50, loss : 0.0966, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0959, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0970, accuracy : 99.22\n",
            "iteration : 200, loss : 0.0975, accuracy : 99.21\n",
            "iteration : 250, loss : 0.0987, accuracy : 99.20\n",
            "iteration : 300, loss : 0.0987, accuracy : 99.19\n",
            "iteration : 350, loss : 0.0987, accuracy : 99.21\n",
            "Epoch : 254, training loss : 0.0988, training accuracy : 99.20, test loss : 0.2759, test accuracy : 94.65\n",
            "\n",
            "Epoch: 255\n",
            "iteration :  50, loss : 0.0959, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0967, accuracy : 99.22\n",
            "iteration : 150, loss : 0.0985, accuracy : 99.17\n",
            "iteration : 200, loss : 0.0992, accuracy : 99.16\n",
            "iteration : 250, loss : 0.0986, accuracy : 99.18\n",
            "iteration : 300, loss : 0.0983, accuracy : 99.22\n",
            "iteration : 350, loss : 0.0980, accuracy : 99.23\n",
            "Epoch : 255, training loss : 0.0979, training accuracy : 99.23, test loss : 0.2797, test accuracy : 94.84\n",
            "\n",
            "Epoch: 256\n",
            "iteration :  50, loss : 0.0944, accuracy : 99.30\n",
            "iteration : 100, loss : 0.0962, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0977, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0975, accuracy : 99.21\n",
            "iteration : 250, loss : 0.0983, accuracy : 99.21\n",
            "iteration : 300, loss : 0.0987, accuracy : 99.21\n",
            "iteration : 350, loss : 0.0991, accuracy : 99.21\n",
            "Epoch : 256, training loss : 0.0988, training accuracy : 99.22, test loss : 0.2805, test accuracy : 94.59\n",
            "\n",
            "Epoch: 257\n",
            "iteration :  50, loss : 0.0998, accuracy : 99.25\n",
            "iteration : 100, loss : 0.0992, accuracy : 99.21\n",
            "iteration : 150, loss : 0.0983, accuracy : 99.25\n",
            "iteration : 200, loss : 0.0964, accuracy : 99.32\n",
            "iteration : 250, loss : 0.0979, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0980, accuracy : 99.27\n",
            "iteration : 350, loss : 0.0979, accuracy : 99.26\n",
            "Epoch : 257, training loss : 0.0977, training accuracy : 99.26, test loss : 0.2788, test accuracy : 94.76\n",
            "\n",
            "Epoch: 258\n",
            "iteration :  50, loss : 0.0961, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0966, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0949, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0947, accuracy : 99.40\n",
            "iteration : 250, loss : 0.0955, accuracy : 99.37\n",
            "iteration : 300, loss : 0.0957, accuracy : 99.36\n",
            "iteration : 350, loss : 0.0956, accuracy : 99.36\n",
            "Epoch : 258, training loss : 0.0959, training accuracy : 99.34, test loss : 0.2752, test accuracy : 94.79\n",
            "\n",
            "Epoch: 259\n",
            "iteration :  50, loss : 0.0977, accuracy : 99.33\n",
            "iteration : 100, loss : 0.0963, accuracy : 99.31\n",
            "iteration : 150, loss : 0.0961, accuracy : 99.29\n",
            "iteration : 200, loss : 0.0970, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0981, accuracy : 99.27\n",
            "iteration : 300, loss : 0.0971, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0976, accuracy : 99.26\n",
            "Epoch : 259, training loss : 0.0978, training accuracy : 99.25, test loss : 0.2769, test accuracy : 94.78\n",
            "\n",
            "Epoch: 260\n",
            "iteration :  50, loss : 0.0960, accuracy : 99.22\n",
            "iteration : 100, loss : 0.0968, accuracy : 99.20\n",
            "iteration : 150, loss : 0.0966, accuracy : 99.24\n",
            "iteration : 200, loss : 0.0959, accuracy : 99.24\n",
            "iteration : 250, loss : 0.0960, accuracy : 99.26\n",
            "iteration : 300, loss : 0.0962, accuracy : 99.27\n",
            "iteration : 350, loss : 0.0962, accuracy : 99.28\n",
            "Epoch : 260, training loss : 0.0969, training accuracy : 99.25, test loss : 0.2781, test accuracy : 94.73\n",
            "\n",
            "Epoch: 261\n",
            "iteration :  50, loss : 0.0963, accuracy : 99.19\n",
            "iteration : 100, loss : 0.0963, accuracy : 99.21\n",
            "iteration : 150, loss : 0.0959, accuracy : 99.27\n",
            "iteration : 200, loss : 0.0963, accuracy : 99.27\n",
            "iteration : 250, loss : 0.0968, accuracy : 99.26\n",
            "iteration : 300, loss : 0.0960, accuracy : 99.29\n",
            "iteration : 350, loss : 0.0956, accuracy : 99.30\n",
            "Epoch : 261, training loss : 0.0957, training accuracy : 99.30, test loss : 0.2778, test accuracy : 94.91\n",
            "\n",
            "Epoch: 262\n",
            "iteration :  50, loss : 0.0975, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0986, accuracy : 99.23\n",
            "iteration : 150, loss : 0.0957, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0962, accuracy : 99.30\n",
            "iteration : 250, loss : 0.0968, accuracy : 99.28\n",
            "iteration : 300, loss : 0.0961, accuracy : 99.30\n",
            "iteration : 350, loss : 0.0967, accuracy : 99.28\n",
            "Epoch : 262, training loss : 0.0964, training accuracy : 99.29, test loss : 0.2767, test accuracy : 94.79\n",
            "\n",
            "Epoch: 263\n",
            "iteration :  50, loss : 0.0966, accuracy : 99.19\n",
            "iteration : 100, loss : 0.0969, accuracy : 99.25\n",
            "iteration : 150, loss : 0.0953, accuracy : 99.33\n",
            "iteration : 200, loss : 0.0945, accuracy : 99.35\n",
            "iteration : 250, loss : 0.0952, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0950, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0948, accuracy : 99.33\n",
            "Epoch : 263, training loss : 0.0946, training accuracy : 99.34, test loss : 0.2777, test accuracy : 94.89\n",
            "\n",
            "Epoch: 264\n",
            "iteration :  50, loss : 0.1010, accuracy : 99.17\n",
            "iteration : 100, loss : 0.1014, accuracy : 99.12\n",
            "iteration : 150, loss : 0.1003, accuracy : 99.18\n",
            "iteration : 200, loss : 0.0984, accuracy : 99.23\n",
            "iteration : 250, loss : 0.0981, accuracy : 99.23\n",
            "iteration : 300, loss : 0.0982, accuracy : 99.23\n",
            "iteration : 350, loss : 0.0976, accuracy : 99.26\n",
            "Epoch : 264, training loss : 0.0973, training accuracy : 99.28, test loss : 0.2747, test accuracy : 94.91\n",
            "\n",
            "Epoch: 265\n",
            "iteration :  50, loss : 0.0975, accuracy : 99.23\n",
            "iteration : 100, loss : 0.0938, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0939, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0937, accuracy : 99.39\n",
            "iteration : 250, loss : 0.0933, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0932, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0931, accuracy : 99.39\n",
            "Epoch : 265, training loss : 0.0932, training accuracy : 99.38, test loss : 0.2829, test accuracy : 94.68\n",
            "\n",
            "Epoch: 266\n",
            "iteration :  50, loss : 0.0929, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0944, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0945, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0948, accuracy : 99.35\n",
            "iteration : 250, loss : 0.0959, accuracy : 99.31\n",
            "iteration : 300, loss : 0.0953, accuracy : 99.31\n",
            "iteration : 350, loss : 0.0950, accuracy : 99.33\n",
            "Epoch : 266, training loss : 0.0948, training accuracy : 99.33, test loss : 0.2783, test accuracy : 94.88\n",
            "\n",
            "Epoch: 267\n",
            "iteration :  50, loss : 0.0908, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0924, accuracy : 99.46\n",
            "iteration : 150, loss : 0.0935, accuracy : 99.39\n",
            "iteration : 200, loss : 0.0943, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0935, accuracy : 99.39\n",
            "iteration : 300, loss : 0.0938, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0931, accuracy : 99.41\n",
            "Epoch : 267, training loss : 0.0933, training accuracy : 99.41, test loss : 0.2774, test accuracy : 94.79\n",
            "\n",
            "Epoch: 268\n",
            "iteration :  50, loss : 0.0915, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0922, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0945, accuracy : 99.30\n",
            "iteration : 200, loss : 0.0945, accuracy : 99.33\n",
            "iteration : 250, loss : 0.0953, accuracy : 99.31\n",
            "iteration : 300, loss : 0.0949, accuracy : 99.33\n",
            "iteration : 350, loss : 0.0947, accuracy : 99.34\n",
            "Epoch : 268, training loss : 0.0945, training accuracy : 99.33, test loss : 0.2754, test accuracy : 94.87\n",
            "\n",
            "Epoch: 269\n",
            "iteration :  50, loss : 0.0960, accuracy : 99.34\n",
            "iteration : 100, loss : 0.0945, accuracy : 99.36\n",
            "iteration : 150, loss : 0.0955, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0951, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0948, accuracy : 99.37\n",
            "iteration : 300, loss : 0.0943, accuracy : 99.38\n",
            "iteration : 350, loss : 0.0951, accuracy : 99.36\n",
            "Epoch : 269, training loss : 0.0949, training accuracy : 99.37, test loss : 0.2729, test accuracy : 94.89\n",
            "\n",
            "Epoch: 270\n",
            "iteration :  50, loss : 0.0993, accuracy : 99.31\n",
            "iteration : 100, loss : 0.0965, accuracy : 99.37\n",
            "iteration : 150, loss : 0.0966, accuracy : 99.35\n",
            "iteration : 200, loss : 0.0956, accuracy : 99.36\n",
            "iteration : 250, loss : 0.0962, accuracy : 99.36\n",
            "iteration : 300, loss : 0.0963, accuracy : 99.35\n",
            "iteration : 350, loss : 0.0959, accuracy : 99.35\n",
            "Epoch : 270, training loss : 0.0959, training accuracy : 99.35, test loss : 0.2738, test accuracy : 94.96\n",
            "\n",
            "Epoch: 271\n",
            "iteration :  50, loss : 0.0906, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0895, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0906, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0926, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0925, accuracy : 99.41\n",
            "iteration : 300, loss : 0.0923, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0929, accuracy : 99.40\n",
            "Epoch : 271, training loss : 0.0927, training accuracy : 99.41, test loss : 0.2760, test accuracy : 94.87\n",
            "\n",
            "Epoch: 272\n",
            "iteration :  50, loss : 0.0944, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0938, accuracy : 99.38\n",
            "iteration : 150, loss : 0.0942, accuracy : 99.36\n",
            "iteration : 200, loss : 0.0942, accuracy : 99.37\n",
            "iteration : 250, loss : 0.0946, accuracy : 99.33\n",
            "iteration : 300, loss : 0.0943, accuracy : 99.34\n",
            "iteration : 350, loss : 0.0937, accuracy : 99.36\n",
            "Epoch : 272, training loss : 0.0936, training accuracy : 99.36, test loss : 0.2781, test accuracy : 94.82\n",
            "\n",
            "Epoch: 273\n",
            "iteration :  50, loss : 0.0908, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0912, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0919, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0922, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0934, accuracy : 99.38\n",
            "iteration : 300, loss : 0.0939, accuracy : 99.35\n",
            "iteration : 350, loss : 0.0937, accuracy : 99.35\n",
            "Epoch : 273, training loss : 0.0937, training accuracy : 99.35, test loss : 0.2794, test accuracy : 94.83\n",
            "\n",
            "Epoch: 274\n",
            "iteration :  50, loss : 0.0917, accuracy : 99.47\n",
            "iteration : 100, loss : 0.0913, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0923, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0923, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0921, accuracy : 99.46\n",
            "iteration : 300, loss : 0.0927, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0928, accuracy : 99.44\n",
            "Epoch : 274, training loss : 0.0932, training accuracy : 99.43, test loss : 0.2787, test accuracy : 94.84\n",
            "\n",
            "Epoch: 275\n",
            "iteration :  50, loss : 0.0908, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0929, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0928, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0916, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0925, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0918, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0920, accuracy : 99.43\n",
            "Epoch : 275, training loss : 0.0921, training accuracy : 99.43, test loss : 0.2766, test accuracy : 94.90\n",
            "\n",
            "Epoch: 276\n",
            "iteration :  50, loss : 0.0919, accuracy : 99.50\n",
            "iteration : 100, loss : 0.0910, accuracy : 99.50\n",
            "iteration : 150, loss : 0.0915, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0928, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0930, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0925, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0933, accuracy : 99.39\n",
            "Epoch : 276, training loss : 0.0937, training accuracy : 99.38, test loss : 0.2739, test accuracy : 94.91\n",
            "\n",
            "Epoch: 277\n",
            "iteration :  50, loss : 0.0918, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0933, accuracy : 99.34\n",
            "iteration : 150, loss : 0.0933, accuracy : 99.37\n",
            "iteration : 200, loss : 0.0930, accuracy : 99.38\n",
            "iteration : 250, loss : 0.0919, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0923, accuracy : 99.39\n",
            "iteration : 350, loss : 0.0923, accuracy : 99.40\n",
            "Epoch : 277, training loss : 0.0925, training accuracy : 99.40, test loss : 0.2804, test accuracy : 94.73\n",
            "\n",
            "Epoch: 278\n",
            "iteration :  50, loss : 0.0936, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0910, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0913, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0914, accuracy : 99.42\n",
            "iteration : 250, loss : 0.0920, accuracy : 99.40\n",
            "iteration : 300, loss : 0.0916, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0913, accuracy : 99.44\n",
            "Epoch : 278, training loss : 0.0915, training accuracy : 99.43, test loss : 0.2772, test accuracy : 94.82\n",
            "\n",
            "Epoch: 279\n",
            "iteration :  50, loss : 0.0917, accuracy : 99.44\n",
            "iteration : 100, loss : 0.0920, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0930, accuracy : 99.41\n",
            "iteration : 200, loss : 0.0921, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0926, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0919, accuracy : 99.43\n",
            "iteration : 350, loss : 0.0923, accuracy : 99.44\n",
            "Epoch : 279, training loss : 0.0920, training accuracy : 99.44, test loss : 0.2752, test accuracy : 94.79\n",
            "\n",
            "Epoch: 280\n",
            "iteration :  50, loss : 0.0916, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0898, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0914, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0909, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0905, accuracy : 99.46\n",
            "iteration : 300, loss : 0.0902, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0904, accuracy : 99.47\n",
            "Epoch : 280, training loss : 0.0903, training accuracy : 99.47, test loss : 0.2758, test accuracy : 94.83\n",
            "\n",
            "Epoch: 281\n",
            "iteration :  50, loss : 0.0924, accuracy : 99.39\n",
            "iteration : 100, loss : 0.0916, accuracy : 99.45\n",
            "iteration : 150, loss : 0.0928, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0922, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0919, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0922, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0924, accuracy : 99.46\n",
            "Epoch : 281, training loss : 0.0924, training accuracy : 99.46, test loss : 0.2811, test accuracy : 94.78\n",
            "\n",
            "Epoch: 282\n",
            "iteration :  50, loss : 0.0931, accuracy : 99.36\n",
            "iteration : 100, loss : 0.0916, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0939, accuracy : 99.38\n",
            "iteration : 200, loss : 0.0929, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0924, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0925, accuracy : 99.42\n",
            "iteration : 350, loss : 0.0921, accuracy : 99.42\n",
            "Epoch : 282, training loss : 0.0921, training accuracy : 99.42, test loss : 0.2778, test accuracy : 94.88\n",
            "\n",
            "Epoch: 283\n",
            "iteration :  50, loss : 0.0850, accuracy : 99.69\n",
            "iteration : 100, loss : 0.0884, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0899, accuracy : 99.52\n",
            "iteration : 200, loss : 0.0898, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0909, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0917, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0917, accuracy : 99.46\n",
            "Epoch : 283, training loss : 0.0919, training accuracy : 99.45, test loss : 0.2753, test accuracy : 94.92\n",
            "\n",
            "Epoch: 284\n",
            "iteration :  50, loss : 0.0922, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0920, accuracy : 99.55\n",
            "iteration : 150, loss : 0.0912, accuracy : 99.54\n",
            "iteration : 200, loss : 0.0904, accuracy : 99.55\n",
            "iteration : 250, loss : 0.0905, accuracy : 99.53\n",
            "iteration : 300, loss : 0.0904, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0909, accuracy : 99.51\n",
            "Epoch : 284, training loss : 0.0911, training accuracy : 99.50, test loss : 0.2759, test accuracy : 94.94\n",
            "\n",
            "Epoch: 285\n",
            "iteration :  50, loss : 0.0936, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0924, accuracy : 99.43\n",
            "iteration : 150, loss : 0.0918, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0918, accuracy : 99.47\n",
            "iteration : 250, loss : 0.0914, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0916, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0909, accuracy : 99.48\n",
            "Epoch : 285, training loss : 0.0914, training accuracy : 99.45, test loss : 0.2772, test accuracy : 94.88\n",
            "\n",
            "Epoch: 286\n",
            "iteration :  50, loss : 0.0896, accuracy : 99.53\n",
            "iteration : 100, loss : 0.0906, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0903, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0918, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0914, accuracy : 99.46\n",
            "iteration : 300, loss : 0.0917, accuracy : 99.45\n",
            "iteration : 350, loss : 0.0919, accuracy : 99.44\n",
            "Epoch : 286, training loss : 0.0916, training accuracy : 99.46, test loss : 0.2790, test accuracy : 94.81\n",
            "\n",
            "Epoch: 287\n",
            "iteration :  50, loss : 0.0858, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0906, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0909, accuracy : 99.42\n",
            "iteration : 200, loss : 0.0904, accuracy : 99.45\n",
            "iteration : 250, loss : 0.0909, accuracy : 99.45\n",
            "iteration : 300, loss : 0.0914, accuracy : 99.44\n",
            "iteration : 350, loss : 0.0907, accuracy : 99.46\n",
            "Epoch : 287, training loss : 0.0907, training accuracy : 99.46, test loss : 0.2761, test accuracy : 94.83\n",
            "\n",
            "Epoch: 288\n",
            "iteration :  50, loss : 0.0906, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0882, accuracy : 99.52\n",
            "iteration : 150, loss : 0.0890, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0887, accuracy : 99.52\n",
            "iteration : 250, loss : 0.0904, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0896, accuracy : 99.51\n",
            "iteration : 350, loss : 0.0904, accuracy : 99.48\n",
            "Epoch : 288, training loss : 0.0906, training accuracy : 99.47, test loss : 0.2775, test accuracy : 94.91\n",
            "\n",
            "Epoch: 289\n",
            "iteration :  50, loss : 0.0921, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0915, accuracy : 99.41\n",
            "iteration : 150, loss : 0.0918, accuracy : 99.40\n",
            "iteration : 200, loss : 0.0919, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0914, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0917, accuracy : 99.44\n",
            "iteration : 350, loss : 0.0914, accuracy : 99.45\n",
            "Epoch : 289, training loss : 0.0911, training accuracy : 99.46, test loss : 0.2793, test accuracy : 94.86\n",
            "\n",
            "Epoch: 290\n",
            "iteration :  50, loss : 0.0923, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0929, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0921, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0918, accuracy : 99.44\n",
            "iteration : 250, loss : 0.0925, accuracy : 99.42\n",
            "iteration : 300, loss : 0.0924, accuracy : 99.41\n",
            "iteration : 350, loss : 0.0920, accuracy : 99.41\n",
            "Epoch : 290, training loss : 0.0919, training accuracy : 99.42, test loss : 0.2755, test accuracy : 94.89\n",
            "\n",
            "Epoch: 291\n",
            "iteration :  50, loss : 0.0908, accuracy : 99.42\n",
            "iteration : 100, loss : 0.0904, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0908, accuracy : 99.47\n",
            "iteration : 200, loss : 0.0900, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0898, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0903, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0903, accuracy : 99.47\n",
            "Epoch : 291, training loss : 0.0900, training accuracy : 99.47, test loss : 0.2769, test accuracy : 94.65\n",
            "\n",
            "Epoch: 292\n",
            "iteration :  50, loss : 0.0937, accuracy : 99.38\n",
            "iteration : 100, loss : 0.0934, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0925, accuracy : 99.44\n",
            "iteration : 200, loss : 0.0916, accuracy : 99.47\n",
            "iteration : 250, loss : 0.0923, accuracy : 99.45\n",
            "iteration : 300, loss : 0.0919, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0913, accuracy : 99.48\n",
            "Epoch : 292, training loss : 0.0921, training accuracy : 99.46, test loss : 0.2761, test accuracy : 94.91\n",
            "\n",
            "Epoch: 293\n",
            "iteration :  50, loss : 0.0900, accuracy : 99.48\n",
            "iteration : 100, loss : 0.0926, accuracy : 99.44\n",
            "iteration : 150, loss : 0.0914, accuracy : 99.49\n",
            "iteration : 200, loss : 0.0912, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0914, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0918, accuracy : 99.46\n",
            "iteration : 350, loss : 0.0915, accuracy : 99.46\n",
            "Epoch : 293, training loss : 0.0915, training accuracy : 99.46, test loss : 0.2728, test accuracy : 95.02\n",
            "\n",
            "Epoch: 294\n",
            "iteration :  50, loss : 0.0896, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0910, accuracy : 99.49\n",
            "iteration : 150, loss : 0.0915, accuracy : 99.46\n",
            "iteration : 200, loss : 0.0915, accuracy : 99.46\n",
            "iteration : 250, loss : 0.0907, accuracy : 99.48\n",
            "iteration : 300, loss : 0.0909, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0913, accuracy : 99.46\n",
            "Epoch : 294, training loss : 0.0915, training accuracy : 99.45, test loss : 0.2721, test accuracy : 95.06\n",
            "\n",
            "Epoch: 295\n",
            "iteration :  50, loss : 0.0903, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0903, accuracy : 99.48\n",
            "iteration : 150, loss : 0.0900, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0899, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0903, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0903, accuracy : 99.49\n",
            "iteration : 350, loss : 0.0905, accuracy : 99.48\n",
            "Epoch : 295, training loss : 0.0904, training accuracy : 99.49, test loss : 0.2730, test accuracy : 95.01\n",
            "\n",
            "Epoch: 296\n",
            "iteration :  50, loss : 0.0884, accuracy : 99.58\n",
            "iteration : 100, loss : 0.0887, accuracy : 99.59\n",
            "iteration : 150, loss : 0.0888, accuracy : 99.57\n",
            "iteration : 200, loss : 0.0894, accuracy : 99.54\n",
            "iteration : 250, loss : 0.0896, accuracy : 99.54\n",
            "iteration : 300, loss : 0.0900, accuracy : 99.53\n",
            "iteration : 350, loss : 0.0898, accuracy : 99.52\n",
            "Epoch : 296, training loss : 0.0895, training accuracy : 99.53, test loss : 0.2767, test accuracy : 94.83\n",
            "\n",
            "Epoch: 297\n",
            "iteration :  50, loss : 0.0885, accuracy : 99.59\n",
            "iteration : 100, loss : 0.0892, accuracy : 99.54\n",
            "iteration : 150, loss : 0.0904, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0910, accuracy : 99.48\n",
            "iteration : 250, loss : 0.0906, accuracy : 99.49\n",
            "iteration : 300, loss : 0.0913, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0911, accuracy : 99.48\n",
            "Epoch : 297, training loss : 0.0914, training accuracy : 99.47, test loss : 0.2807, test accuracy : 94.76\n",
            "\n",
            "Epoch: 298\n",
            "iteration :  50, loss : 0.0899, accuracy : 99.52\n",
            "iteration : 100, loss : 0.0895, accuracy : 99.50\n",
            "iteration : 150, loss : 0.0906, accuracy : 99.48\n",
            "iteration : 200, loss : 0.0907, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0904, accuracy : 99.50\n",
            "iteration : 300, loss : 0.0911, accuracy : 99.48\n",
            "iteration : 350, loss : 0.0911, accuracy : 99.46\n",
            "Epoch : 298, training loss : 0.0916, training accuracy : 99.45, test loss : 0.2719, test accuracy : 95.04\n",
            "\n",
            "Epoch: 299\n",
            "iteration :  50, loss : 0.0909, accuracy : 99.55\n",
            "iteration : 100, loss : 0.0911, accuracy : 99.51\n",
            "iteration : 150, loss : 0.0909, accuracy : 99.51\n",
            "iteration : 200, loss : 0.0909, accuracy : 99.50\n",
            "iteration : 250, loss : 0.0916, accuracy : 99.47\n",
            "iteration : 300, loss : 0.0910, accuracy : 99.47\n",
            "iteration : 350, loss : 0.0913, accuracy : 99.47\n",
            "Epoch : 299, training loss : 0.0916, training accuracy : 99.46, test loss : 0.2758, test accuracy : 94.96\n",
            "\n",
            "Epoch: 300\n",
            "iteration :  50, loss : 0.0911, accuracy : 99.41\n",
            "iteration : 100, loss : 0.0919, accuracy : 99.42\n",
            "iteration : 150, loss : 0.0915, accuracy : 99.43\n",
            "iteration : 200, loss : 0.0914, accuracy : 99.41\n",
            "iteration : 250, loss : 0.0911, accuracy : 99.43\n",
            "iteration : 300, loss : 0.0907, accuracy : 99.44\n",
            "iteration : 350, loss : 0.0904, accuracy : 99.45\n",
            "Epoch : 300, training loss : 0.0904, training accuracy : 99.45, test loss : 0.2794, test accuracy : 94.80\n"
          ]
        }
      ],
      "source": [
        "# main body\n",
        "config = {\n",
        "    'lr': 0.001,\n",
        "    'weight_decay': 5e-4\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "        new_trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "        validationset, batch_size=128, shuffle=False, num_workers=2)\n",
        "net = ResNet18().to('cuda')\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.01).to('cuda')\n",
        "optimizer = optim.Adam(net.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
        "#scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=300)\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
        "\n",
        "train_loss_list=[] \n",
        "train_acc_list=[]\n",
        "test_loss_list=[]\n",
        "test_acc_list=[]\n",
        "\n",
        "for epoch in range(1, 301):\n",
        "    train_loss, train_acc = train(epoch, net, criterion, trainloader, scheduler)\n",
        "    test_loss, test_acc = test(epoch, net, criterion, testloader)\n",
        "    \n",
        "    train_loss_list.append(train_loss) \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_loss_list.append(test_loss)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print((\"Epoch : %3d, training loss : %0.4f, training accuracy : %2.2f, test loss \" + \\\n",
        "      \": %0.4f, test accuracy : %2.2f\") % (epoch, train_loss, train_acc, test_loss, test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUQVIKR-X3v6",
        "outputId": "b5faab09-c145-4e92-c40c-65ddcfd5d600"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.20942527764275962, 96.58881376767056)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the hold-out test set\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "test_loss, test_acc = test(0, net, criterion, testloader)\n",
        "test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CNz1iabSB21",
        "outputId": "1a9f32b8-ada4-4d95-f779-e4eb0be9067f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_acc_list)), train_acc_list, 'b')\n",
        "plt.plot(range(len(test_acc_list)), test_acc_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy curve : Label smoothing factor = 0.01\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9qb9ItHSC5U",
        "outputId": "b24d74aa-cf91-45b2-bb5b-a1a2bf896cd8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(range(len(train_loss_list)), train_loss_list, 'b')\n",
        "plt.plot(range(len(test_loss_list)), test_loss_list, 'r')\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss curve : Label smoothing factor = 0.01\")\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eiY3bTlWipW",
        "outputId": "cc7f4991-6651-449d-fa69-e148b5b6caeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_loss_list_ls001 = [1.16627063046949, 0.5020903967583406, 0.45346348977024314, 0.42697503225913214, 0.4142754110621243, 0.3950381797099824, 0.3798869217072076, 0.36838602325134484, 0.36279047145268456, 0.34367701084148594, 0.3356654864097352, 0.3275731207151723, 0.3187137166578272, 0.3096301672339116, 0.3058289131578714, 0.30293833559283073, 0.2924698039202832, 0.2898784063776657, 0.28701151043257417, 0.2805858591223151, 0.27683295248969786, 0.27528870739749456, 0.26946698379548906, 0.2675930881887917, 0.26298093262726696, 0.26263014911636107, 0.25650981779344034, 0.25997990482867894, 0.25800914384000667, 0.25466585643892364, 0.25434042856622197, 0.24933684126632968, 0.24694899588742553, 0.24842105066873194, 0.24256293584660785, 0.24523011358772837, 0.2435985110235731, 0.24335885019644812, 0.24257921045873224, 0.24081906106898454, 0.23958712505292762, 0.2368126141265802, 0.23612861302971516, 0.2337702561805888, 0.23827081399800654, 0.23271785050549804, 0.23235927849281124, 0.2301392226724767, 0.23624825061659827, 0.2320807450589772, 0.23035375402387243, 0.23038358707738116, 0.22901073883623288, 0.2258828540400761, 0.2270729804265144, 0.22517493157205867, 0.22626446468154912, 0.22567828449857266, 0.22135756239496918, 0.22198542015662362, 0.22426564508902672, 0.22325662650713107, 0.2232997263794346, 0.2189932009230461, 0.22225999737254326, 0.219105036560759, 0.21952180366529037, 0.22348625170506112, 0.22164006198082514, 0.2158251712679217, 0.2169379233950522, 0.21428723469740008, 0.2182284818109135, 0.21546703324650684, 0.21358807294107066, 0.21195848685939137, 0.2140748142832663, 0.21561579103392314, 0.2113100138863897, 0.21172476226721354, 0.20829249215804466, 0.20925872569000173, 0.20520572338081633, 0.21442997831154645, 0.20765942274554958, 0.20498278613901397, 0.20910973895371446, 0.20554807772077519, 0.20642354858276968, 0.2026453878740631, 0.20325295998636622, 0.2042896620182164, 0.2073271774906453, 0.20391820958717083, 0.20168759702101632, 0.20470611401770497, 0.20058239219069157, 0.20178888207124823, 0.198590917497631, 0.19736705635621296, 0.19980644598239805, 0.19776772202918846, 0.19624215782415577, 0.19459259302070148, 0.19353062312212094, 0.19400355066387312, 0.1977035508044367, 0.19354821929359825, 0.19408886195198308, 0.19408477039195013, 0.1927156234740565, 0.19022999762438822, 0.19013641863496, 0.1920897145339144, 0.1879578249927782, 0.18978520420185596, 0.18964338038106598, 0.19050296601968084, 0.18567866403962863, 0.18524322618394687, 0.1830670412397643, 0.18455850597481094, 0.18750120285484526, 0.18408602149021336, 0.1880154339318999, 0.18277726930169877, 0.18022847005991433, 0.1813020721522127, 0.17830585629797888, 0.176803238144735, 0.18071951042749695, 0.17802846886035872, 0.179284918861499, 0.1771000639250285, 0.17524421596672476, 0.17740786277504794, 0.17817758152963023, 0.17532769802624618, 0.17430420431906615, 0.1714235828094043, 0.1732598282901575, 0.17070506939154653, 0.17296726309106278, 0.1698801281854389, 0.16705300800280196, 0.16900206270418194, 0.16630661348259546, 0.16699023354990372, 0.16572137074015006, 0.166734593549395, 0.16610931027630157, 0.16661747040222008, 0.16477180752893128, 0.15883433421049015, 0.16371891403262853, 0.16488838666257497, 0.15988692203107566, 0.16206823452943708, 0.16245832417392472, 0.15557487789531388, 0.15609480953475002, 0.15946792700103304, 0.15605092787645697, 0.1538461172564566, 0.15465241563594762, 0.1518809469290542, 0.1534780249161126, 0.1556428121478577, 0.15375908593417506, 0.15161718426115792, 0.14765015654366836, 0.14966168034528976, 0.14840135668835988, 0.1475034455660236, 0.14591929025766326, 0.1477654879776443, 0.14655833376374672, 0.1434103749833779, 0.14438531374139837, 0.14529963461606482, 0.14652409434803132, 0.14462936915921648, 0.1426498397659803, 0.14273556663091913, 0.1419771267227364, 0.14206244590563502, 0.14021430756179945, 0.13892480066717477, 0.14009804335147708, 0.13507952579476323, 0.1417151953025562, 0.1368618728668709, 0.13603742137995517, 0.13179661232604567, 0.1337956370013516, 0.13266403888299214, 0.12942212454591018, 0.12979402574824123, 0.13224390403526584, 0.12952057569007563, 0.12791694167668258, 0.12825573786972014, 0.1267749886848739, 0.12712794035511613, 0.12660600703340882, 0.12623424367691444, 0.12465446244491148, 0.12389251361613675, 0.12453229555754157, 0.12597179083724008, 0.12203806150088789, 0.1209106883261262, 0.1231962255224949, 0.12249410590585977, 0.12066052672339649, 0.12045893690934996, 0.11963213233120719, 0.11869528948210765, 0.11786892016728719, 0.11794095163342107, 0.11492950382432963, 0.11612591201939233, 0.11441361003613407, 0.11795812865421378, 0.1151740181090709, 0.11273800456507742, 0.11369458026679227, 0.11159139540460375, 0.10973402391361996, 0.11086462694535734, 0.11085577418164509, 0.10830378411262016, 0.11017947498618103, 0.10893992142787148, 0.10526014093138016, 0.10864186494896405, 0.10615443014274768, 0.10697886614295525, 0.10572260478890039, 0.1065655185924313, 0.10389429595331512, 0.1024348492100633, 0.10400605296620186, 0.10377930124923788, 0.10323447950164154, 0.10527176595719526, 0.10177731990491148, 0.10096107984622965, 0.10034786372730725, 0.10121205152292562, 0.10115370004965361, 0.1013377813021665, 0.09849209835131963, 0.09875404933684563, 0.0978932160187543, 0.09878589612681692, 0.09770073664947578, 0.09593373765225009, 0.09784644834071317, 0.09692943415183039, 0.09566948831404451, 0.0964215007335513, 0.09464860798382177, 0.09726437471988725, 0.09323242326900565, 0.09475445474793272, 0.09334069314768643, 0.09454045759710838, 0.0948985750234224, 0.09585005376074049, 0.09269710502973418, 0.09359729338467605, 0.09365295814627878, 0.09321621058352271, 0.092118161682514, 0.09365359495891142, 0.09254245429746503, 0.09153616890755449, 0.09200262296490552, 0.09034272741657609, 0.09239337173338505, 0.09205646942139964, 0.0919465451301921, 0.0910774872393466, 0.09137309514248597, 0.09160250294095455, 0.09067494004237943, 0.09060847892509244, 0.09113214903972983, 0.09193195877117193, 0.09000478771724675, 0.09211919531428071, 0.09147084437734712, 0.0914960122972646, 0.09039011531002153, 0.08950625541733533, 0.09135613479426882, 0.09155204672073607, 0.09158747146526973, 0.09039319165273088]\n",
            "train_acc_list_ls001 = [61.55002646903123, 86.00317628374802, 87.69719428268925, 88.79830598200105, 89.31286394917946, 89.90577024880889, 90.53255690841715, 90.75701429327687, 90.94970884065643, 91.64425622022235, 91.89835892006353, 92.03176283748014, 92.49126521969296, 92.70513499205929, 92.90629962943356, 92.9211222869243, 93.19640021175225, 93.34039174166226, 93.40603493912123, 93.67919534145051, 93.83377448385389, 93.82742191635786, 93.91212281630493, 94.06881948120699, 94.13869772366331, 94.28692429857067, 94.4012705134992, 94.1768131286395, 94.2742191635786, 94.44362096347274, 94.34621492853361, 94.64055055584966, 94.71254632080466, 94.62361037586024, 94.88194812069878, 94.72525145579672, 94.67443091582848, 94.77183695076761, 94.76760190577025, 94.88830068819482, 94.98358920063525, 95.00052938062467, 95.1148755955532, 95.1148755955532, 94.91371095817892, 95.04076230809952, 95.10852302805718, 95.2419269454738, 95.01323451561673, 95.07676019057702, 95.10005293806246, 95.21863419798835, 95.16569613552144, 95.27580730545262, 95.26098464796189, 95.27368978295394, 95.31815775542616, 95.31180518793012, 95.46850185283219, 95.43885653785071, 95.30968766543144, 95.43462149285337, 95.26098464796189, 95.51932239280042, 95.40497617787189, 95.52567496029646, 95.54261514028586, 95.41556379036527, 95.38380095288512, 95.63790365272631, 95.53838009528852, 95.66331392271043, 95.42615140285865, 95.57649550026468, 95.70354685018528, 95.68660667019587, 95.70778189518263, 95.65484383271573, 95.77554261514028, 95.70354685018528, 95.76071995764956, 95.84330333509793, 95.96611964002118, 95.6569613552144, 95.81365802011646, 95.91741662255161, 95.71625198517734, 95.95129698253044, 95.88777130757015, 95.98094229751192, 96.0, 96.00211752249868, 95.81789306511382, 95.92165166754897, 96.07623080995235, 95.95976707252514, 96.09105346744309, 96.03388035997882, 96.16728427739545, 96.14822657490735, 96.0359978824775, 96.10587612493383, 96.215987294865, 96.28798305982001, 96.2710428798306, 96.29645314981472, 96.14822657490735, 96.26680783483324, 96.24986765484384, 96.25622022233986, 96.2435150873478, 96.4065643197459, 96.41715193223928, 96.28163049232398, 96.4595023822128, 96.39597670725252, 96.50820539968237, 96.4150344097406, 96.47220751720486, 96.57173107464267, 96.67972472207518, 96.58867125463208, 96.4785600847009, 96.58443620963473, 96.46373742721016, 96.58020116463737, 96.66278454208576, 96.62466913710958, 96.80465854949709, 96.79195341450503, 96.67125463208046, 96.74536791953415, 96.73689782953944, 96.78348332451033, 96.78771836950767, 96.8131286394918, 96.78983589200635, 96.92323980942298, 96.8851244044468, 96.9571201694018, 96.94017998941239, 96.97194282689253, 96.8935944944415, 97.06299629433563, 97.11593435680254, 97.0566437268396, 97.1561672842774, 97.20275277924829, 97.0841715193224, 97.09475913181578, 97.1646373742721, 97.0566437268396, 97.1646373742721, 97.3276866066702, 97.17734250926416, 97.19640021175225, 97.25780836421387, 97.29592376919005, 97.22604552673373, 97.40603493912123, 97.41662255161461, 97.32345156167284, 97.49497088406564, 97.52249867654844, 97.50555849655903, 97.52038115404976, 97.55426151402858, 97.46532556908417, 97.55637903652726, 97.52038115404976, 97.73213340391742, 97.6156696664902, 97.71307570142933, 97.71942826892536, 97.69613552143991, 97.68554790894653, 97.83165696135521, 97.79777660137638, 97.78507146638432, 97.75542615140286, 97.75754367390154, 97.7702488088936, 97.79777660137638, 97.85706723133933, 97.93118051879301, 97.97564849126522, 97.95447326627846, 97.9692959237692, 97.92694547379566, 98.1492853361567, 97.91635786130227, 98.08152461619905, 98.0307040762308, 98.16834303864479, 98.0857596611964, 98.18951826363156, 98.27210164107994, 98.20857596611964, 98.1577554261514, 98.28904182106935, 98.29539438856538, 98.2657490735839, 98.34409740603493, 98.24669137109582, 98.36315510852303, 98.30598200105877, 98.42456326098464, 98.4203282159873, 98.43303335097936, 98.37374272101641, 98.53255690841715, 98.5283218634198, 98.44362096347274, 98.50291159343568, 98.51349920592907, 98.58549497088407, 98.52196929592377, 98.61302276336686, 98.62361037586024, 98.59820010587613, 98.72313393329804, 98.6913710958179, 98.72313393329804, 98.62996294335628, 98.71466384330334, 98.8353626257279, 98.6913710958179, 98.81206987824245, 98.90524086818422, 98.82477501323451, 98.79724722075171, 98.9348861831657, 98.84595023822128, 98.87559555320276, 99.01111699311805, 98.8798305982001, 98.99629433562731, 98.91794600317628, 98.98358920063525, 98.9433562731604, 99.06829010058232, 99.12546320804658, 99.0428798305982, 99.03017469560614, 99.08523028057174, 99.01958708311275, 99.12758073054526, 99.16146109052409, 99.15510852302806, 99.17416622551615, 99.11699311805188, 99.13181577554262, 99.25886712546321, 99.19957649550027, 99.23133933298041, 99.21651667548967, 99.25886712546321, 99.33721545791424, 99.2503970354685, 99.25463208046585, 99.30333509793542, 99.28851244044468, 99.33509793541556, 99.2779248279513, 99.38380095288512, 99.33298041291688, 99.40709370037057, 99.33298041291688, 99.3668607728957, 99.35203811540498, 99.40921122286925, 99.36050820539968, 99.3499205929063, 99.43038644785601, 99.43038644785601, 99.37956590788777, 99.40285865537321, 99.43462149285337, 99.44097406034939, 99.47061937533087, 99.45579671784013, 99.41979883536263, 99.44732662784543, 99.50026469031233, 99.45367919534145, 99.45791424033881, 99.46214928533615, 99.47061937533087, 99.45579671784013, 99.4219163578613, 99.47485442032821, 99.46214928533615, 99.45791424033881, 99.45367919534145, 99.4854420328216, 99.52779248279514, 99.47061937533087, 99.44944415034409, 99.45791424033881, 99.44944415034409]\n",
            "test_loss_list_ls001 = [0.8728022195544898, 0.605559805882912, 0.5479553650115051, 0.4766291115506023, 0.5403156232307939, 0.4792816953039637, 0.41714170954975427, 0.4383231574559913, 0.38527990877628326, 0.3708652890488213, 0.35403889965485125, 0.3873171945120774, 0.39193783342546107, 0.3568759689874509, 0.33207617312961935, 0.3597213793911186, 0.33186382347462223, 0.3336566619428934, 0.3706800641090262, 0.3291887050750209, 0.31993744142499625, 0.32311665931460903, 0.3320697773174912, 0.30893746989907, 0.3157727620473095, 0.306221399894532, 0.3119945107575725, 0.30777187455518573, 0.3190213355655764, 0.32224319860631345, 0.3252751786334842, 0.3271528486819828, 0.30897099269079226, 0.28662487586923674, 0.30192962201202617, 0.29823889926660296, 0.3110658262728476, 0.30183594849179773, 0.29584985438222977, 0.2978941637511347, 0.32665057969736117, 0.28921172453784477, 0.298257894375745, 0.3085296011584647, 0.29762618345957176, 0.30534061665336293, 0.298629114586933, 0.3145160325894169, 0.33284351448802385, 0.2932405670483907, 0.29689645416596355, 0.29775476645605237, 0.29582253098487854, 0.2890165223940915, 0.3024244290940902, 0.3033398959566565, 0.3059260470463949, 0.3088860687818013, 0.3109920625300968, 0.29244909917607026, 0.294930148796708, 0.317684635008667, 0.28334712558517267, 0.29104878771684917, 0.2888284330712814, 0.2860430181756908, 0.3063042842801295, 0.28735941071428506, 0.29154760171385374, 0.2819471231424341, 0.28102413711010243, 0.28491242931169625, 0.29939260516388744, 0.29180026909007745, 0.29530372784710396, 0.2764743945616133, 0.2810732735430493, 0.28781543832783607, 0.28955697823388904, 0.2998943431120293, 0.3092014435313496, 0.28897688311396863, 0.2924269812510294, 0.29447110216407213, 0.2915983132141478, 0.2900472894018772, 0.28848450258374214, 0.2804106849008331, 0.28606927592088194, 0.29438594984365446, 0.2944731173269889, 0.2849199046545169, 0.28742652203814656, 0.29015840341647464, 0.285767529378919, 0.2890423273338991, 0.29059645511648236, 0.30143582674802516, 0.28174253399757776, 0.28302806133733077, 0.29407333560726223, 0.2844306000453584, 0.2803428416669953, 0.28488256712900656, 0.2914516050733772, 0.284395902728041, 0.27881897007133444, 0.2765648318272011, 0.29434602824496287, 0.28360227301862895, 0.2936610555385842, 0.28535505501078623, 0.2947779145164817, 0.28174840308287563, 0.28379862894322355, 0.3077467798310168, 0.2871168269319277, 0.28191231549078344, 0.28630522145506215, 0.27621058579169067, 0.29848504044553814, 0.29424115043936994, 0.28751107904256556, 0.29705177291351204, 0.28377047159215985, 0.28683298965002973, 0.30982600641893404, 0.30395600696404773, 0.30213382737893685, 0.2894251478799418, 0.2877723645857152, 0.28762199678549577, 0.2775438205138141, 0.2894848482135464, 0.2809707507783291, 0.28494154734938754, 0.29267438975911514, 0.29357816885207216, 0.2896098973704319, 0.2903688753790715, 0.2774602745239641, 0.2899633486408229, 0.27057444234835165, 0.2858688844477429, 0.28597040993033673, 0.28045108534541785, 0.27386659493341164, 0.2839796936833391, 0.2852090247998051, 0.275014263230796, 0.2901009837275042, 0.28518588548781826, 0.28165073386010003, 0.2767115892002396, 0.2784488614210311, 0.275182878050734, 0.2851197610737062, 0.2931518492599328, 0.291334559227906, 0.2864535338080981, 0.2774323962044482, 0.27803078242668916, 0.28644459617926793, 0.28730375429286675, 0.2808522373145702, 0.2878492454833844, 0.2774755288572872, 0.28648582509919707, 0.27760401660320805, 0.27795950484042076, 0.28438708814335806, 0.2823127313498773, 0.2847585178838641, 0.27557891943291124, 0.28511087311541333, 0.28739307996104746, 0.28394492832469004, 0.2841456345118144, 0.28272215024951625, 0.2759409015085183, 0.2799553152027668, 0.2793287268529336, 0.2827812412059775, 0.27856064675485387, 0.28496650233864784, 0.28112408033042563, 0.2891243390154605, 0.28204679532962684, 0.28400128634244787, 0.29442718362106995, 0.28466574416733254, 0.2805488593145913, 0.2788126759742405, 0.2841160740411165, 0.2953266228238742, 0.27927372772611825, 0.2848947073752974, 0.27899876704403, 0.2852253251362081, 0.28522898059557467, 0.2793606620866294, 0.2764398340939307, 0.2837854262222262, 0.2823465419385363, 0.2846200121384041, 0.28044014022338626, 0.2811030170642862, 0.2877443665821178, 0.28357601355688244, 0.28088564002046396, 0.2873737956305929, 0.2833103071898222, 0.2855655583826935, 0.2775400072409242, 0.28347411748094886, 0.28017186307731795, 0.2808377281853966, 0.28586497358685614, 0.282243424624789, 0.2846282858255447, 0.2821725425036514, 0.280173642670407, 0.28949712158418167, 0.2834744092588331, 0.28142087336848765, 0.28045547132690746, 0.2844207076465382, 0.27962753980183136, 0.28674796643648665, 0.2780630025355255, 0.2784585364016832, 0.2809313420644578, 0.28055307517449063, 0.285568931413924, 0.2843981418831676, 0.29025906857614425, 0.2806070783339879, 0.2807991938555942, 0.2774373362315636, 0.28551858155896853, 0.2748474565279834, 0.2792128798204894, 0.2849407056529148, 0.28633109107613564, 0.2833274624803487, 0.2809031238304634, 0.27838661740807924, 0.27633398234405937, 0.286959255698557, 0.27920154672042996, 0.28001581822686333, 0.28168069012463093, 0.2751919059265478, 0.2759363162970426, 0.27970864758918096, 0.2805293114381094, 0.27884524834214475, 0.2752229996463832, 0.27694617021901935, 0.27805811079109416, 0.27783182741818474, 0.27667236645870347, 0.2777147938077356, 0.2747281850406937, 0.2828748081536854, 0.27831974815504223, 0.2774167600288695, 0.2753835295272224, 0.27288606525490094, 0.27384300040555937, 0.2760259837788694, 0.27808966597213464, 0.27943123146599413, 0.27869501801244184, 0.2765733868833266, 0.2739381828144485, 0.2804276540729345, 0.277219273697804, 0.2751620167537647, 0.2758364564209592, 0.28105059017737705, 0.277811271120228, 0.2753038290230667, 0.27587469148577426, 0.2771741318717307, 0.27899753466686783, 0.2760753255410522, 0.2774646629889806, 0.2792536679509224, 0.2754641664670963, 0.276860656728055, 0.27613553718901146, 0.2727682370254222, 0.27213113334979494, 0.2730102056235659, 0.27667366070490257, 0.2807323412202737, 0.2718759814605993, 0.27576898129693433, 0.27935993094362466]\n",
            "test_acc_list_ls001 = [73.35586969883221, 82.659803318992, 85.01075599262447, 86.88537185003074, 84.68039336201598, 87.41164720344192, 89.32851874615858, 88.23371235402581, 90.40795943454211, 90.68838352796558, 91.46435156730178, 90.24277811923785, 90.10448678549477, 91.12246465888137, 92.06361401352181, 91.25307314074985, 92.2019053472649, 91.93684695759066, 90.75368776889981, 91.98678549477566, 92.47080516287646, 92.35556238475722, 92.14812538414259, 92.8318992009834, 92.63214505224339, 92.76275353411187, 92.62062077443147, 92.87031346035648, 92.49769514443761, 92.390135218193, 92.20958819913952, 92.37092808850646, 92.77043638598647, 93.51951444376152, 93.14689612784265, 93.0700676090965, 92.73970497848802, 93.28134603564843, 93.36585740626921, 93.3581745543946, 92.42086662569146, 93.48878303626306, 93.23524892440074, 92.70897357098956, 93.18531038721574, 92.99323909035034, 93.1238475722188, 92.81269207129687, 92.14044253226798, 93.1699446834665, 93.39658881376766, 93.23140749846343, 93.24677320221267, 93.64628149969269, 92.93561770129072, 93.15457897971727, 92.98939766441303, 92.84342347879533, 92.95482483097726, 93.3620159803319, 93.5579287031346, 92.65903503380454, 93.78457283343577, 93.48110018438844, 93.74231714812538, 93.54256299938537, 92.94714197910264, 93.63091579594345, 93.45421020282728, 93.7922556853104, 93.7922556853104, 93.63091579594345, 93.14305470190534, 93.56945298094652, 93.33896742470804, 93.91902274124155, 93.71158574062692, 93.63475722188076, 93.54256299938537, 93.28134603564843, 93.00860479409957, 93.6578057775046, 93.6078672403196, 93.31976029502151, 93.52719729563614, 93.6040258143823, 93.61170866625692, 93.77304855562384, 93.6078672403196, 93.5118315918869, 93.50030731407499, 93.55024585125999, 93.62323294406883, 93.59634296250768, 93.70390288875231, 93.5579287031346, 93.53103872157345, 93.15073755377996, 93.85755992624462, 93.799938537185, 93.48494161032575, 93.67317148125385, 93.87676705593117, 93.71542716656423, 93.56945298094652, 93.57329440688383, 93.93438844499079, 93.97664413030117, 93.42732022126613, 93.88829133374308, 93.51567301782421, 93.81914566687155, 93.65012292563, 93.72311001843885, 93.96896127842655, 93.08927473878303, 93.78457283343577, 93.81914566687155, 93.75, 94.11109403810694, 93.5579287031346, 93.6539643515673, 93.71542716656423, 93.48494161032575, 93.799938537185, 93.8421942224954, 93.03933620159803, 93.25829748002458, 93.25061462814998, 93.75384142593731, 93.82682851874615, 93.79609711124769, 94.05731407498463, 93.62323294406883, 94.03426551936079, 93.91133988936693, 93.65012292563, 93.68085433312845, 93.71926859250154, 93.70390288875231, 94.14566687154272, 93.70390288875231, 94.26090964966195, 93.74231714812538, 94.01121696373694, 93.98048555623848, 94.27627535341118, 93.95743700061463, 93.83066994468346, 94.22249539028887, 93.87292562999386, 93.86140135218193, 93.96896127842655, 94.1840811309158, 94.1379840196681, 94.1917639827904, 94.07267977873387, 93.91518131530424, 93.7077443146896, 94.06883835279656, 94.29932390903504, 94.25706822372464, 93.95359557467732, 93.86908420405655, 94.16103257529196, 94.13414259373079, 94.24554394591273, 93.94975414874001, 94.19560540872772, 94.15719114935465, 94.2340196681008, 94.24938537185002, 94.04578979717272, 94.34542102028273, 94.02658266748617, 94.06499692685925, 94.07267977873387, 94.1840811309158, 94.06115550092194, 94.39920098340504, 94.24170251997542, 94.29932390903504, 94.1917639827904, 94.27627535341118, 94.16487400122925, 94.18792255685311, 93.87292562999386, 94.13030116779349, 94.10341118623234, 93.98432698217579, 94.26090964966195, 94.3262138905962, 94.36078672403197, 94.30316533497235, 94.0419483712354, 94.41072526121697, 94.16103257529196, 94.24554394591273, 94.07267977873387, 94.24170251997542, 94.46450522433928, 94.43761524277812, 94.45682237246466, 94.55669944683467, 94.36078672403197, 94.58358942839583, 94.50676090964966, 94.23786109403811, 94.36846957590657, 94.42224953902888, 94.38767670559312, 94.34157959434542, 94.3761524277812, 94.6220036877689, 94.45298094652735, 94.39151813153042, 94.54901659496005, 94.52980946527352, 94.34926244622004, 94.44529809465274, 94.51444376152428, 94.4798709280885, 94.28779963122311, 94.49523663183774, 94.59127228027043, 94.50291948371235, 94.38767670559312, 94.5720651505839, 94.48371235402581, 94.6258451137062, 94.62968653964352, 94.59127228027043, 94.52980946527352, 94.46834665027659, 94.4299323909035, 94.46066379840197, 94.61432083589429, 94.50291948371235, 94.70651505838967, 94.50676090964966, 94.84480639213275, 94.6681007990166, 94.48755377996312, 94.53365089121081, 94.69499078057775, 94.6258451137062, 94.74492931776275, 94.74877074370006, 94.51060233558697, 94.74108789182544, 94.6258451137062, 94.75261216963737, 94.87553779963122, 94.64889366933005, 94.83712354025815, 94.59127228027043, 94.76029502151198, 94.79486785494775, 94.77950215119853, 94.72956361401353, 94.90626920712968, 94.79486785494775, 94.89090350338046, 94.91011063306699, 94.67962507682851, 94.88322065150584, 94.79102642901044, 94.87169637369392, 94.89090350338046, 94.9638905961893, 94.87169637369392, 94.8179164105716, 94.83328211432084, 94.84480639213275, 94.89858635525508, 94.91011063306699, 94.72572218807622, 94.8179164105716, 94.79486785494775, 94.83328211432084, 94.77950215119853, 94.87553779963122, 94.92163491087892, 94.93700061462815, 94.87553779963122, 94.8140749846343, 94.83328211432084, 94.91011063306699, 94.86017209588199, 94.89090350338046, 94.64505224339274, 94.91011063306699, 95.02151198524892, 95.0560848186847, 95.0061462814997, 94.82944068838353, 94.76029502151198, 95.04071911493547, 94.96004917025199, 94.80255070682237]\n"
          ]
        }
      ],
      "source": [
        "print(f\"train_loss_list_ls001 = {train_loss_list}\") \n",
        "print(f\"train_acc_list_ls001 = {train_acc_list}\")\n",
        "print(f\"test_loss_list_ls001 = {test_loss_list}\")\n",
        "print(f\"test_acc_list_ls001 = {test_acc_list}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "2d35a857fdeedecb30594b1c7eb95a8c0480700735195f416faf3d51f501baa5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}